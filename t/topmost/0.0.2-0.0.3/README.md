# Comparing `tmp/topmost-0.0.2-5-py3-none-any.whl.zip` & `tmp/topmost-0.0.3-1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,88 +1,86 @@
-Zip file size: 80242 bytes, number of entries: 86
--rw-rw-r--  2.0 unx      117 b- defN 23-Sep-08 14:34 topmost/__init__.py
--rw-rw-r--  2.0 unx      276 b- defN 24-Mar-07 07:52 topmost/data/__init__.py
--rw-rw-r--  2.0 unx     3752 b- defN 24-Mar-15 03:25 topmost/data/basic_dataset_handler.py
--rw-rw-r--  2.0 unx     5009 b- defN 23-Sep-15 15:15 topmost/data/crosslingual_dataset_handler.py
--rw-rw-r--  2.0 unx      827 b- defN 24-Mar-14 08:32 topmost/data/download.py
--rw-rw-r--  2.0 unx     1685 b- defN 23-Sep-08 14:34 topmost/data/download_20ng.py
--rw-rw-r--  2.0 unx     3608 b- defN 23-Sep-08 14:34 topmost/data/dynamic_dataset_handler.py
--rw-rw-r--  2.0 unx      952 b- defN 24-Mar-20 09:33 topmost/data/file_utils.py
--rw-rw-r--  2.0 unx      520 b- defN 24-Mar-07 04:51 topmost/evaluations/__init__.py
--rw-rw-r--  2.0 unx     1104 b- defN 24-Mar-07 03:04 topmost/evaluations/build_hierarchy.py
--rw-rw-r--  2.0 unx     1986 b- defN 24-Mar-07 04:22 topmost/evaluations/classification.py
--rw-rw-r--  2.0 unx     1297 b- defN 24-Mar-07 02:47 topmost/evaluations/clustering.py
--rw-rw-r--  2.0 unx     9263 b- defN 24-Mar-07 04:48 topmost/evaluations/hierarchy_quality.py
--rw-rw-r--  2.0 unx     1413 b- defN 23-Sep-16 09:35 topmost/evaluations/topic_coherence.py
--rw-rw-r--  2.0 unx      686 b- defN 24-Mar-07 04:25 topmost/evaluations/topic_diversity.py
--rw-rw-r--  2.0 unx     1373 b- defN 24-Mar-06 16:18 topmost/models/Encoder copy.py
--rw-rw-r--  2.0 unx     1326 b- defN 23-Sep-18 13:49 topmost/models/Encoder.py
--rw-rw-r--  2.0 unx      512 b- defN 24-Mar-06 16:37 topmost/models/__init__.py
--rw-rw-r--  2.0 unx     3917 b- defN 23-Sep-27 10:44 topmost/models/basic/CombinedTM.py
--rw-rw-r--  2.0 unx     3758 b- defN 23-Sep-12 12:39 topmost/models/basic/DecTM.py
--rw-rw-r--  2.0 unx     2512 b- defN 23-Sep-15 08:34 topmost/models/basic/ETM.py
--rw-rw-r--  2.0 unx     3646 b- defN 23-Sep-12 12:39 topmost/models/basic/ProdLDA.py
--rw-rw-r--  2.0 unx        0 b- defN 23-Sep-08 14:34 topmost/models/basic/__init__.py
--rw-rw-r--  2.0 unx     4619 b- defN 23-Sep-18 13:51 topmost/models/basic/ECRTM/CombinedECRTM.py
--rw-rw-r--  2.0 unx     1503 b- defN 23-Sep-12 12:39 topmost/models/basic/ECRTM/ECR.py
--rw-rw-r--  2.0 unx     4506 b- defN 23-Sep-19 01:56 topmost/models/basic/ECRTM/ECRTM.py
--rw-rw-r--  2.0 unx        0 b- defN 23-Sep-08 14:34 topmost/models/basic/ECRTM/__init__.py
--rw-rw-r--  2.0 unx     2253 b- defN 23-Sep-15 14:48 topmost/models/basic/NSTM/NSTM.py
--rw-rw-r--  2.0 unx        0 b- defN 23-Sep-08 14:34 topmost/models/basic/NSTM/__init__.py
--rw-rw-r--  2.0 unx      850 b- defN 23-Sep-08 14:34 topmost/models/basic/NSTM/auto_diff_sinkhorn.py
--rw-rw-r--  2.0 unx     2941 b- defN 23-Sep-08 14:34 topmost/models/basic/TSCTM/TSC.py
--rw-rw-r--  2.0 unx     2575 b- defN 23-Sep-14 02:18 topmost/models/basic/TSCTM/TSCTM.py
--rw-rw-r--  2.0 unx     1675 b- defN 23-Sep-14 02:23 topmost/models/basic/TSCTM/TopicDistQuant.py
--rw-rw-r--  2.0 unx        0 b- defN 23-Sep-08 14:34 topmost/models/basic/TSCTM/__init__.py
--rw-rw-r--  2.0 unx     4478 b- defN 23-Sep-12 12:39 topmost/models/crosslingual/NMTM.py
--rw-rw-r--  2.0 unx        0 b- defN 23-Sep-08 14:34 topmost/models/crosslingual/__init__.py
--rw-rw-r--  2.0 unx     3546 b- defN 23-Sep-12 12:40 topmost/models/crosslingual/InfoCTM/InfoCTM.py
--rw-rw-r--  2.0 unx     3628 b- defN 23-Sep-12 12:41 topmost/models/crosslingual/InfoCTM/TAMI.py
--rw-rw-r--  2.0 unx        0 b- defN 23-Sep-08 14:34 topmost/models/crosslingual/InfoCTM/__init__.py
--rw-rw-r--  2.0 unx     9766 b- defN 23-Sep-15 15:26 topmost/models/dynamic/DETM.py
--rw-rw-r--  2.0 unx        0 b- defN 23-Sep-08 14:34 topmost/models/dynamic/__init__.py
--rw-rw-r--  2.0 unx        0 b- defN 24-Mar-08 03:21 topmost/models/hierarchical/ProGBN.py
--rw-rw-r--  2.0 unx        0 b- defN 23-Sep-08 14:34 topmost/models/hierarchical/__init__.py
--rw-rw-r--  2.0 unx     5234 b- defN 23-Sep-15 14:54 topmost/models/hierarchical/HyperMiner/HyperMiner.py
--rw-rw-r--  2.0 unx        0 b- defN 23-Sep-08 14:34 topmost/models/hierarchical/HyperMiner/__init__.py
--rw-rw-r--  2.0 unx      107 b- defN 23-Sep-08 14:34 topmost/models/hierarchical/HyperMiner/manifolds/__init__.py
--rw-rw-r--  2.0 unx     6632 b- defN 23-Sep-08 14:34 topmost/models/hierarchical/HyperMiner/manifolds/base.py
--rw-rw-r--  2.0 unx     1386 b- defN 23-Sep-08 14:34 topmost/models/hierarchical/HyperMiner/manifolds/euclidean.py
--rw-rw-r--  2.0 unx     5117 b- defN 23-Sep-08 14:34 topmost/models/hierarchical/HyperMiner/manifolds/math_util.py
--rw-rw-r--  2.0 unx     4403 b- defN 23-Sep-08 14:34 topmost/models/hierarchical/HyperMiner/manifolds/poincare.py
--rw-rw-r--  2.0 unx    10196 b- defN 24-Mar-06 13:51 topmost/models/hierarchical/ProGBN/ProGBN.py
--rw-rw-r--  2.0 unx        0 b- defN 24-Mar-06 13:44 topmost/models/hierarchical/ProGBN/__init__.py
--rw-rw-r--  2.0 unx     4957 b- defN 24-Mar-06 13:45 topmost/models/hierarchical/ProGBN/utils.py
--rw-rw-r--  2.0 unx     7821 b- defN 23-Sep-15 14:53 topmost/models/hierarchical/SawETM/SawETM.py
--rw-rw-r--  2.0 unx        0 b- defN 23-Sep-08 14:34 topmost/models/hierarchical/SawETM/__init__.py
--rw-rw-r--  2.0 unx     1392 b- defN 23-Sep-08 14:34 topmost/models/hierarchical/SawETM/block.py
--rw-rw-r--  2.0 unx     1900 b- defN 24-Mar-06 16:39 topmost/models/hierarchical/TraCo/CDDecoder.py
--rw-rw-r--  2.0 unx     1952 b- defN 24-Mar-08 07:17 topmost/models/hierarchical/TraCo/TPD.py
--rw-rw-r--  2.0 unx     4262 b- defN 24-Mar-08 07:17 topmost/models/hierarchical/TraCo/TraCo.py
--rw-rw-r--  2.0 unx        0 b- defN 24-Mar-08 04:46 topmost/models/hierarchical/TraCo/__init__.py
--rw-rw-r--  2.0 unx      485 b- defN 24-Mar-06 16:16 topmost/models/hierarchical/TraCo/utils.py
--rw-rw-r--  2.0 unx       41 b- defN 23-Sep-09 07:49 topmost/preprocessing/__init__.py
--rw-rw-r--  2.0 unx    12646 b- defN 24-May-23 17:06 topmost/preprocessing/preprocessing.py
--rw-rw-r--  2.0 unx      580 b- defN 24-Mar-13 10:24 topmost/trainers/__init__.py
--rw-rw-r--  2.0 unx      920 b- defN 24-Mar-13 10:48 topmost/trainers/basic/BERTopic_trainer.py
--rw-rw-r--  2.0 unx     2437 b- defN 24-Mar-06 13:07 topmost/trainers/basic/LDA_trainer.py
--rw-rw-r--  2.0 unx     2421 b- defN 24-Mar-06 13:07 topmost/trainers/basic/NMF_trainer.py
--rw-rw-r--  2.0 unx        0 b- defN 23-Sep-08 14:34 topmost/trainers/basic/__init__.py
--rw-rw-r--  2.0 unx     3214 b- defN 23-Sep-18 13:04 topmost/trainers/basic/basic_contextual_trainer.py
--rw-rw-r--  2.0 unx     3551 b- defN 24-Mar-07 13:34 topmost/trainers/basic/basic_trainer.py
--rw-rw-r--  2.0 unx        0 b- defN 23-Sep-08 14:34 topmost/trainers/crosslingual/__init__.py
--rw-rw-r--  2.0 unx     4428 b- defN 24-Mar-07 13:53 topmost/trainers/crosslingual/crosslingual_trainer.py
--rw-rw-r--  2.0 unx     4167 b- defN 24-Mar-06 13:07 topmost/trainers/dynamic/DTM_trainer.py
--rw-rw-r--  2.0 unx        0 b- defN 23-Sep-08 14:34 topmost/trainers/dynamic/__init__.py
--rw-rw-r--  2.0 unx     3714 b- defN 24-Mar-07 13:42 topmost/trainers/dynamic/dynamic_trainer.py
--rw-rw-r--  2.0 unx     2373 b- defN 24-Mar-06 13:07 topmost/trainers/hierarchical/HDP_trainer.py
--rw-rw-r--  2.0 unx        0 b- defN 23-Sep-08 14:34 topmost/trainers/hierarchical/__init__.py
--rw-rw-r--  2.0 unx     4402 b- defN 24-Mar-07 11:57 topmost/trainers/hierarchical/hierarchical_trainer.py
--rw-rw-r--  2.0 unx        0 b- defN 23-Sep-08 14:34 topmost/utils/__init__.py
--rw-rw-r--  2.0 unx      390 b- defN 24-Mar-06 13:05 topmost/utils/basic_utils.py
--rw-rw-r--  2.0 unx      390 b- defN 23-Sep-08 14:34 topmost/utils/static_utils.py
--rw-rw-r--  2.0 unx    11358 b- defN 24-May-23 17:25 topmost-0.0.2.dist-info/LICENSE
--rw-rw-r--  2.0 unx    16609 b- defN 24-May-23 17:25 topmost-0.0.2.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 24-May-23 17:25 topmost-0.0.2.dist-info/WHEEL
--rw-rw-r--  2.0 unx        8 b- defN 24-May-23 17:25 topmost-0.0.2.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     8084 b- defN 24-May-23 17:25 topmost-0.0.2.dist-info/RECORD
-86 files, 229548 bytes uncompressed, 67070 bytes compressed:  70.8%
+Zip file size: 77298 bytes, number of entries: 84
+-rw-rw-r--  2.0 unx      117 b- defN 24-May-31 03:51 topmost/__init__.py
+-rw-rw-r--  2.0 unx      276 b- defN 24-May-31 03:51 topmost/data/__init__.py
+-rw-rw-r--  2.0 unx     3785 b- defN 24-May-31 03:51 topmost/data/basic_dataset_handler.py
+-rw-rw-r--  2.0 unx     5009 b- defN 24-May-31 03:51 topmost/data/crosslingual_dataset_handler.py
+-rw-rw-r--  2.0 unx      827 b- defN 24-May-31 03:51 topmost/data/download.py
+-rw-rw-r--  2.0 unx     1673 b- defN 24-May-31 03:51 topmost/data/download_20ng.py
+-rw-rw-r--  2.0 unx     3669 b- defN 24-May-31 03:51 topmost/data/dynamic_dataset_handler.py
+-rw-rw-r--  2.0 unx      952 b- defN 24-May-31 03:51 topmost/data/file_utils.py
+-rw-rw-r--  2.0 unx      552 b- defN 24-May-31 03:51 topmost/evaluations/__init__.py
+-rw-rw-r--  2.0 unx     1104 b- defN 24-May-31 03:51 topmost/evaluations/build_hierarchy.py
+-rw-rw-r--  2.0 unx     1986 b- defN 24-May-31 03:51 topmost/evaluations/classification.py
+-rw-rw-r--  2.0 unx     1297 b- defN 24-May-31 03:51 topmost/evaluations/clustering.py
+-rw-rw-r--  2.0 unx     9263 b- defN 24-May-31 03:51 topmost/evaluations/hierarchy_quality.py
+-rw-rw-r--  2.0 unx     1402 b- defN 24-May-31 03:51 topmost/evaluations/topic_coherence.py
+-rw-rw-r--  2.0 unx     1767 b- defN 24-May-31 03:51 topmost/evaluations/topic_diversity.py
+-rw-rw-r--  2.0 unx     1326 b- defN 24-May-31 03:51 topmost/models/Encoder.py
+-rw-rw-r--  2.0 unx      551 b- defN 24-May-31 03:51 topmost/models/__init__.py
+-rw-rw-r--  2.0 unx     3917 b- defN 24-May-31 03:51 topmost/models/basic/CombinedTM.py
+-rw-rw-r--  2.0 unx     3758 b- defN 24-May-31 03:51 topmost/models/basic/DecTM.py
+-rw-rw-r--  2.0 unx     2512 b- defN 24-May-31 03:51 topmost/models/basic/ETM.py
+-rw-rw-r--  2.0 unx     3646 b- defN 24-May-31 03:51 topmost/models/basic/ProdLDA.py
+-rw-rw-r--  2.0 unx        0 b- defN 24-May-31 03:51 topmost/models/basic/__init__.py
+-rw-rw-r--  2.0 unx     1503 b- defN 24-May-31 03:51 topmost/models/basic/ECRTM/ECR.py
+-rw-rw-r--  2.0 unx     4506 b- defN 24-May-31 03:51 topmost/models/basic/ECRTM/ECRTM.py
+-rw-rw-r--  2.0 unx        0 b- defN 24-May-31 03:51 topmost/models/basic/ECRTM/__init__.py
+-rw-rw-r--  2.0 unx     2253 b- defN 24-May-31 03:51 topmost/models/basic/NSTM/NSTM.py
+-rw-rw-r--  2.0 unx        0 b- defN 24-May-31 03:51 topmost/models/basic/NSTM/__init__.py
+-rw-rw-r--  2.0 unx      850 b- defN 24-May-31 03:51 topmost/models/basic/NSTM/auto_diff_sinkhorn.py
+-rw-rw-r--  2.0 unx     2941 b- defN 24-May-31 03:51 topmost/models/basic/TSCTM/TSC.py
+-rw-rw-r--  2.0 unx     2575 b- defN 24-May-31 03:51 topmost/models/basic/TSCTM/TSCTM.py
+-rw-rw-r--  2.0 unx     1675 b- defN 24-May-31 03:51 topmost/models/basic/TSCTM/TopicDistQuant.py
+-rw-rw-r--  2.0 unx        0 b- defN 24-May-31 03:51 topmost/models/basic/TSCTM/__init__.py
+-rw-rw-r--  2.0 unx     4478 b- defN 24-May-31 03:51 topmost/models/crosslingual/NMTM.py
+-rw-rw-r--  2.0 unx        0 b- defN 24-May-31 03:51 topmost/models/crosslingual/__init__.py
+-rw-rw-r--  2.0 unx     3546 b- defN 24-May-31 03:51 topmost/models/crosslingual/InfoCTM/InfoCTM.py
+-rw-rw-r--  2.0 unx     3628 b- defN 24-May-31 03:51 topmost/models/crosslingual/InfoCTM/TAMI.py
+-rw-rw-r--  2.0 unx        0 b- defN 24-May-31 03:51 topmost/models/crosslingual/InfoCTM/__init__.py
+-rw-rw-r--  2.0 unx     9766 b- defN 24-May-31 03:51 topmost/models/dynamic/DETM.py
+-rw-rw-r--  2.0 unx        0 b- defN 24-May-31 03:51 topmost/models/dynamic/__init__.py
+-rw-rw-r--  2.0 unx     4346 b- defN 24-May-31 03:51 topmost/models/dynamic/CFDTM/CFDTM.py
+-rw-rw-r--  2.0 unx     2365 b- defN 24-May-31 03:51 topmost/models/dynamic/CFDTM/ETC.py
+-rw-rw-r--  2.0 unx     1635 b- defN 24-May-31 03:51 topmost/models/dynamic/CFDTM/UWE.py
+-rw-rw-r--  2.0 unx        0 b- defN 24-May-31 03:51 topmost/models/dynamic/CFDTM/__init__.py
+-rw-rw-r--  2.0 unx        0 b- defN 24-May-31 03:51 topmost/models/hierarchical/ProGBN.py
+-rw-rw-r--  2.0 unx        0 b- defN 24-May-31 03:51 topmost/models/hierarchical/__init__.py
+-rw-rw-r--  2.0 unx     5234 b- defN 24-May-31 03:51 topmost/models/hierarchical/HyperMiner/HyperMiner.py
+-rw-rw-r--  2.0 unx        0 b- defN 24-May-31 03:51 topmost/models/hierarchical/HyperMiner/__init__.py
+-rw-rw-r--  2.0 unx      107 b- defN 24-May-31 03:51 topmost/models/hierarchical/HyperMiner/manifolds/__init__.py
+-rw-rw-r--  2.0 unx     6632 b- defN 24-May-31 03:51 topmost/models/hierarchical/HyperMiner/manifolds/base.py
+-rw-rw-r--  2.0 unx     1386 b- defN 24-May-31 03:51 topmost/models/hierarchical/HyperMiner/manifolds/euclidean.py
+-rw-rw-r--  2.0 unx     5117 b- defN 24-May-31 03:51 topmost/models/hierarchical/HyperMiner/manifolds/math_util.py
+-rw-rw-r--  2.0 unx     4403 b- defN 24-May-31 03:51 topmost/models/hierarchical/HyperMiner/manifolds/poincare.py
+-rw-rw-r--  2.0 unx     7821 b- defN 24-May-31 03:51 topmost/models/hierarchical/SawETM/SawETM.py
+-rw-rw-r--  2.0 unx        0 b- defN 24-May-31 03:51 topmost/models/hierarchical/SawETM/__init__.py
+-rw-rw-r--  2.0 unx     1392 b- defN 24-May-31 03:51 topmost/models/hierarchical/SawETM/block.py
+-rw-rw-r--  2.0 unx     1900 b- defN 24-May-31 03:51 topmost/models/hierarchical/TraCo/CDDecoder.py
+-rw-rw-r--  2.0 unx     1952 b- defN 24-May-31 03:51 topmost/models/hierarchical/TraCo/TPD.py
+-rw-rw-r--  2.0 unx     4262 b- defN 24-May-31 03:51 topmost/models/hierarchical/TraCo/TraCo.py
+-rw-rw-r--  2.0 unx        0 b- defN 24-May-31 03:51 topmost/models/hierarchical/TraCo/__init__.py
+-rw-rw-r--  2.0 unx      485 b- defN 24-May-31 03:51 topmost/models/hierarchical/TraCo/utils.py
+-rw-rw-r--  2.0 unx       41 b- defN 24-May-31 03:51 topmost/preprocessing/__init__.py
+-rw-rw-r--  2.0 unx    13268 b- defN 24-May-31 03:51 topmost/preprocessing/preprocessing.py
+-rw-rw-r--  2.0 unx      580 b- defN 24-May-31 03:51 topmost/trainers/__init__.py
+-rw-rw-r--  2.0 unx      920 b- defN 24-May-31 03:51 topmost/trainers/basic/BERTopic_trainer.py
+-rw-rw-r--  2.0 unx     2440 b- defN 24-May-31 03:51 topmost/trainers/basic/LDA_trainer.py
+-rw-rw-r--  2.0 unx     2424 b- defN 24-May-31 03:51 topmost/trainers/basic/NMF_trainer.py
+-rw-rw-r--  2.0 unx        0 b- defN 24-May-31 03:51 topmost/trainers/basic/__init__.py
+-rw-rw-r--  2.0 unx     3858 b- defN 24-May-31 03:51 topmost/trainers/basic/basic_trainer.py
+-rw-rw-r--  2.0 unx        0 b- defN 24-May-31 03:51 topmost/trainers/crosslingual/__init__.py
+-rw-rw-r--  2.0 unx     4736 b- defN 24-May-31 03:51 topmost/trainers/crosslingual/crosslingual_trainer.py
+-rw-rw-r--  2.0 unx     4111 b- defN 24-May-31 03:51 topmost/trainers/dynamic/DTM_trainer.py
+-rw-rw-r--  2.0 unx        0 b- defN 24-May-31 03:51 topmost/trainers/dynamic/__init__.py
+-rw-rw-r--  2.0 unx     4074 b- defN 24-May-31 03:51 topmost/trainers/dynamic/dynamic_trainer.py
+-rw-rw-r--  2.0 unx     2375 b- defN 24-May-31 03:51 topmost/trainers/hierarchical/HDP_trainer.py
+-rw-rw-r--  2.0 unx        0 b- defN 24-May-31 03:51 topmost/trainers/hierarchical/__init__.py
+-rw-rw-r--  2.0 unx     4761 b- defN 24-May-31 03:51 topmost/trainers/hierarchical/hierarchical_trainer.py
+-rw-rw-r--  2.0 unx        0 b- defN 24-May-31 03:51 topmost/utils/__init__.py
+-rw-rw-r--  2.0 unx      854 b- defN 24-May-31 03:51 topmost/utils/logger.py
+-rw-rw-r--  2.0 unx      390 b- defN 24-May-31 03:51 topmost/utils/static_utils.py
+-rw-rw-r--  2.0 unx    11358 b- defN 24-May-31 04:23 topmost-0.0.3.dist-info/LICENSE
+-rw-rw-r--  2.0 unx    16667 b- defN 24-May-31 04:23 topmost-0.0.3.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 24-May-31 04:23 topmost-0.0.3.dist-info/WHEEL
+-rw-rw-r--  2.0 unx        8 b- defN 24-May-31 04:23 topmost-0.0.3.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     7856 b- defN 24-May-31 04:23 topmost-0.0.3.dist-info/RECORD
+84 files, 216960 bytes uncompressed, 64506 bytes compressed:  70.3%
```

## zipnote {}

```diff
@@ -39,17 +39,14 @@
 
 Filename: topmost/evaluations/topic_coherence.py
 Comment: 
 
 Filename: topmost/evaluations/topic_diversity.py
 Comment: 
 
-Filename: topmost/models/Encoder copy.py
-Comment: 
-
 Filename: topmost/models/Encoder.py
 Comment: 
 
 Filename: topmost/models/__init__.py
 Comment: 
 
 Filename: topmost/models/basic/CombinedTM.py
@@ -63,17 +60,14 @@
 
 Filename: topmost/models/basic/ProdLDA.py
 Comment: 
 
 Filename: topmost/models/basic/__init__.py
 Comment: 
 
-Filename: topmost/models/basic/ECRTM/CombinedECRTM.py
-Comment: 
-
 Filename: topmost/models/basic/ECRTM/ECR.py
 Comment: 
 
 Filename: topmost/models/basic/ECRTM/ECRTM.py
 Comment: 
 
 Filename: topmost/models/basic/ECRTM/__init__.py
@@ -117,14 +111,26 @@
 
 Filename: topmost/models/dynamic/DETM.py
 Comment: 
 
 Filename: topmost/models/dynamic/__init__.py
 Comment: 
 
+Filename: topmost/models/dynamic/CFDTM/CFDTM.py
+Comment: 
+
+Filename: topmost/models/dynamic/CFDTM/ETC.py
+Comment: 
+
+Filename: topmost/models/dynamic/CFDTM/UWE.py
+Comment: 
+
+Filename: topmost/models/dynamic/CFDTM/__init__.py
+Comment: 
+
 Filename: topmost/models/hierarchical/ProGBN.py
 Comment: 
 
 Filename: topmost/models/hierarchical/__init__.py
 Comment: 
 
 Filename: topmost/models/hierarchical/HyperMiner/HyperMiner.py
@@ -144,23 +150,14 @@
 
 Filename: topmost/models/hierarchical/HyperMiner/manifolds/math_util.py
 Comment: 
 
 Filename: topmost/models/hierarchical/HyperMiner/manifolds/poincare.py
 Comment: 
 
-Filename: topmost/models/hierarchical/ProGBN/ProGBN.py
-Comment: 
-
-Filename: topmost/models/hierarchical/ProGBN/__init__.py
-Comment: 
-
-Filename: topmost/models/hierarchical/ProGBN/utils.py
-Comment: 
-
 Filename: topmost/models/hierarchical/SawETM/SawETM.py
 Comment: 
 
 Filename: topmost/models/hierarchical/SawETM/__init__.py
 Comment: 
 
 Filename: topmost/models/hierarchical/SawETM/block.py
@@ -198,17 +195,14 @@
 
 Filename: topmost/trainers/basic/NMF_trainer.py
 Comment: 
 
 Filename: topmost/trainers/basic/__init__.py
 Comment: 
 
-Filename: topmost/trainers/basic/basic_contextual_trainer.py
-Comment: 
-
 Filename: topmost/trainers/basic/basic_trainer.py
 Comment: 
 
 Filename: topmost/trainers/crosslingual/__init__.py
 Comment: 
 
 Filename: topmost/trainers/crosslingual/crosslingual_trainer.py
@@ -231,29 +225,29 @@
 
 Filename: topmost/trainers/hierarchical/hierarchical_trainer.py
 Comment: 
 
 Filename: topmost/utils/__init__.py
 Comment: 
 
-Filename: topmost/utils/basic_utils.py
+Filename: topmost/utils/logger.py
 Comment: 
 
 Filename: topmost/utils/static_utils.py
 Comment: 
 
-Filename: topmost-0.0.2.dist-info/LICENSE
+Filename: topmost-0.0.3.dist-info/LICENSE
 Comment: 
 
-Filename: topmost-0.0.2.dist-info/METADATA
+Filename: topmost-0.0.3.dist-info/METADATA
 Comment: 
 
-Filename: topmost-0.0.2.dist-info/WHEEL
+Filename: topmost-0.0.3.dist-info/WHEEL
 Comment: 
 
-Filename: topmost-0.0.2.dist-info/top_level.txt
+Filename: topmost-0.0.3.dist-info/top_level.txt
 Comment: 
 
-Filename: topmost-0.0.2.dist-info/RECORD
+Filename: topmost-0.0.3.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## topmost/data/basic_dataset_handler.py

```diff
@@ -10,17 +10,17 @@
 def load_contextual_embed(texts, device, model_name="all-mpnet-base-v2", show_progress_bar=True):
     model = SentenceTransformer(model_name, device=device)
     embeddings = model.encode(texts, show_progress_bar=show_progress_bar)
     return embeddings
 
 
 class RawDatasetHandler:
-    def __init__(self, docs, preprocessing, batch_size=200, device='cpu', as_tensor=False, contextual_embed=False):
+    def __init__(self, docs, preprocessing, batch_size=200, device='cpu', as_tensor=False, contextual_embed=False, pretrained_WE=True):
 
-        rst = preprocessing.preprocess(docs)
+        rst = preprocessing.preprocess(docs, pretrained_WE=pretrained_WE)
         self.train_data = rst['train_bow']
         self.train_texts = rst['train_texts']
         self.vocab = rst['vocab']
 
         self.vocab_size = len(self.vocab)
 
         if contextual_embed:
@@ -41,18 +41,18 @@
         # test_bow: Nxv
         # word_emeddings: VxD
         # vocab: V, ordered by word id.
 
         self.load_data(dataset_dir, read_labels)
         self.vocab_size = len(self.vocab)
 
-        print("===>train_size: ", self.train_bow.shape[0])
-        print("===>test_size: ", self.test_bow.shape[0])
-        print("===>vocab_size: ", self.vocab_size)
-        print("===>average length: {:.3f}".format(self.train_bow.sum(1).sum() / self.train_bow.shape[0]))
+        print("train_size: ", self.train_bow.shape[0])
+        print("test_size: ", self.test_bow.shape[0])
+        print("vocab_size: ", self.vocab_size)
+        print("average length: {:.3f}".format(self.train_bow.sum(1).sum() / self.train_bow.shape[0]))
 
         if contextual_embed:
             self.train_contextual_embed = load_contextual_embed(self.train_texts, device)
             self.test_contextual_embed = load_contextual_embed(self.test_texts, device)
             self.contextual_embed_size = self.train_contextual_embed.shape[1]
 
         if as_tensor:
```

## topmost/data/download_20ng.py

```diff
@@ -18,17 +18,17 @@
     for group in groups.keys():
         if len(groups[group]) > 1:
             for subset in ['train', 'test', 'all']:
                 download_articles(group, groups[group], subset, output_dir)
 
 
 def download_articles(name, categories, subset, output_dir):
-    print("===>name: ", name)
-    print("===>categories: ", categories)
-    print("===>subset: ", subset)
+    print("name: ", name)
+    print("categories: ", categories)
+    print("subset: ", subset)
 
     data = []
     print("Downloading articles")
     newsgroups_data = fetch_20newsgroups(subset=subset, categories=categories, remove=())
 
     for i in range(len(newsgroups_data['data'])):
         line = newsgroups_data['data'][i]
```

## topmost/data/dynamic_dataset_handler.py

```diff
@@ -32,19 +32,19 @@
         self.load_data(dataset_dir, read_labels)
 
         self.vocab_size = len(self.vocab)
         self.train_size = len(self.train_bow)
         self.num_times = len(np.unique(self.train_times))
         self.train_time_wordfreq = self.get_time_wordfreq(self.train_bow, self.train_times)
 
-        print('===>Info: all train size: ', len(self.train_bow))
-        print('===>Info: all test size: ', len(self.test_bow))
-        print('===>Info: all vocab size: ', len(self.vocab))
-        print('===>Info: average length: {:.3f}'.format(self.train_bow.sum(1).mean().item()))
-        print('===>Info: num of each time slice: ', self.num_times, np.bincount(self.train_times))
+        print('all train size: ', len(self.train_bow))
+        print('all test size: ', len(self.test_bow))
+        print('all vocab size: ', len(self.vocab))
+        print('average length: {:.3f}'.format(self.train_bow.sum(1).mean().item()))
+        print('num of each time slice: ', self.num_times, np.bincount(self.train_times))
 
         if as_tensor:
             self.train_bow = torch.from_numpy(self.train_bow).float().to(device)
             self.test_bow = torch.from_numpy(self.test_bow).float().to(device)
             self.train_times = torch.from_numpy(self.train_times).long().to(device)
             self.test_times = torch.from_numpy(self.test_times).long().to(device)
             self.train_time_wordfreq = torch.from_numpy(self.train_time_wordfreq).float().to(device)
@@ -63,14 +63,16 @@
         self.test_texts = file_utils.read_text(f'{path}/test_texts.txt')
 
         self.train_times = np.loadtxt(f'{path}/train_times.txt').astype('int32')
         self.test_times = np.loadtxt(f'{path}/test_times.txt').astype('int32')
 
         self.vocab = file_utils.read_text(f'{path}/vocab.txt')
 
+        self.pretrained_WE = scipy.sparse.load_npz(f'{path}/word_embeddings.npz').toarray().astype('float32')
+
         if read_labels:
             self.train_labels = np.loadtxt(f'{path}/train_labels.txt').astype('int32')
             self.test_labels = np.loadtxt(f'{path}/test_labels.txt').astype('int32')
 
     # word frequency at each time slice.
     def get_time_wordfreq(self, bow, times):
         train_time_wordfreq = np.zeros((self.num_times, self.vocab_size))
```

## topmost/evaluations/__init__.py

```diff
@@ -1,14 +1,15 @@
 from .topic_diversity import compute_topic_diversity
 from .topic_diversity import multiaspect_topic_diversity
+from .topic_diversity import dynamic_TD
 
 from .clustering import evaluate_clustering
 from .clustering import hierarchical_clustering
 
 from .classification import evaluate_classification
 from .classification import crosslingual_classification
 from .classification import hierarchical_classification
 
-from .topic_coherence import compute_dynamic_TC
+from .topic_coherence import dynamic_TC
 from .topic_coherence import compute_topic_coherence
 
 from .hierarchy_quality import hierarchy_quality
```

## topmost/evaluations/topic_coherence.py

```diff
@@ -17,23 +17,23 @@
     cm = CoherenceModel(texts=split_reference_corpus, dictionary=dictionary, topics=split_top_words, topn=num_top_words, coherence=cv_type)
     cv_per_topic = cm.get_coherence_per_topic()
     score = np.mean(cv_per_topic)
 
     return score
 
 
-def compute_dynamic_TC(train_texts, train_times, vocab, top_words_list, cv_type='c_v'):
+def dynamic_TC(train_texts, train_times, vocab, top_words_list, cv_type='c_v', verbose=False):
     cv_score_list = list()
 
-    for time in tqdm(range(len(top_words_list))):
+    for time, top_words in tqdm(enumerate(top_words_list)):
         # use the texts of each time slice as the reference corpus.
         idx = np.where(train_times == time)[0]
         reference_corpus = [train_texts[i] for i in idx]
 
-        # use the the topics at the time slice
-        top_words = top_words_list[time]
+        # use the topics at a time slice
         cv_score = compute_topic_coherence(reference_corpus, vocab, top_words, cv_type)
         cv_score_list.append(cv_score)
 
-    print("===>CV score list: ", cv_score_list)
+    if verbose:
+        print(f"dynamic TC list: {cv_score_list}")
 
     return np.mean(cv_score_list)
```

## topmost/evaluations/topic_diversity.py

```diff
@@ -1,9 +1,11 @@
 import numpy as np
 from sklearn.feature_extraction.text import CountVectorizer
+from collections import Counter
+from tqdm import tqdm
 
 
 def compute_TD(texts):
     K = len(texts)
     T = len(texts[0].split())
     vectorizer = CountVectorizer(tokenizer=lambda x: x.split())
     counter = vectorizer.fit_transform(texts).toarray()
@@ -22,7 +24,40 @@
 def multiaspect_topic_diversity(top_words, _type="TD"):
     TD_list = list()
     for level_top_words in top_words:
         TD = compute_topic_diversity(level_top_words, _type)
         TD_list.append(TD)
 
     return np.mean(TD_list)
+
+
+def _time_dynamic_TD(topics, time_vocab):
+    num_associated_words = 0.
+    T = len(topics[0].split())
+    flatten_topic_words = [word for topic_words in topics for word in topic_words.split()]
+    counter = Counter(flatten_topic_words)
+
+    # for word in np.sort(list(set(flatten_topic_words))):
+    for word in np.sort(flatten_topic_words):
+        if (counter[word] == 1) and word in time_vocab:
+            num_associated_words += 1
+
+    return num_associated_words / (len(topics) * T)
+
+
+def dynamic_TD(top_words, train_bow, train_times, vocab, verbose=False):
+    TD_list = list()
+
+    time_idx = np.sort(np.unique(train_times))
+
+    for time in tqdm(time_idx):
+        doc_idx = np.where(train_times == time)[0]
+        time_vocab_idx = np.nonzero(train_bow[doc_idx].sum(0))[0]
+        time_vocab = np.asarray(vocab)[time_vocab_idx]
+
+        topics = top_words[time]
+        TD_list.append(_time_dynamic_TD(topics, time_vocab))
+
+    if verbose:
+        print(f"dynamic TD list: {TD_list}")
+
+    return np.mean(TD_list)
```

## topmost/models/__init__.py

```diff
@@ -6,11 +6,12 @@
 from .basic.TSCTM.TSCTM import TSCTM
 from .basic.ECRTM.ECRTM import ECRTM
 
 from .crosslingual.NMTM import NMTM
 from .crosslingual.InfoCTM.InfoCTM import InfoCTM
 
 from .dynamic.DETM import DETM
+from .dynamic.CFDTM.CFDTM import CFDTM
 
 from .hierarchical.SawETM.SawETM import SawETM
 from .hierarchical.HyperMiner.HyperMiner import HyperMiner
 from .hierarchical.TraCo.TraCo import TraCo
```

## topmost/preprocessing/preprocessing.py

```diff
@@ -9,33 +9,38 @@
 from collections import Counter
 import numpy as np
 import scipy.sparse
 from tqdm import tqdm
 from sklearn.feature_extraction.text import CountVectorizer
 
 from topmost.data import file_utils
+from topmost.utils.logger import Logger
+
+
+logger = Logger("WARNING")
 
 
 # compile some regexes
 punct_chars = list(set(string.punctuation) - set("'"))
 punct_chars.sort()
 punctuation = ''.join(punct_chars)
 replace = re.compile('[%s]' % re.escape(punctuation))
 alpha = re.compile('^[a-zA-Z_]+$')
 alpha_or_num = re.compile('^[a-zA-Z_]+|[0-9_]+$')
 alphanum = re.compile('^[a-zA-Z0-9_]+$')
 
 
 def get_stopwords(stopwords):
     if stopwords is None:
-        stopwords = []
-    if isinstance(stopwords, list):
-        stopword_set = stopwords
+        from gensim.parsing.preprocessing import STOPWORDS
+        stopword_set = STOPWORDS
+    elif isinstance(stopwords, list):
+        stopword_set = set(stopwords)
     elif isinstance(stopwords, str):
-        stopword_set = file_utils.read_text(stopwords)
+        stopword_set = set(file_utils.read_text(stopwords))
     else:
         raise NotImplementedError(stopwords)
 
     return stopword_set
 
 
 class Tokenizer:
@@ -114,26 +119,42 @@
     num_found = 0
 
     try:
         key_word_list = glove_vectors.index_to_key
     except:
         key_word_list = glove_vectors.index2word
 
-    for i, word in enumerate(tqdm(vocab, desc="===>making word embeddings")):
+    for i, word in enumerate(tqdm(vocab, desc="make word embeddings")):
         if word in key_word_list:
             word_embeddings[i] = glove_vectors[word]
             num_found += 1
 
-    print(f'===> number of found embeddings: {num_found}/{len(vocab)}')
+    logger.info(f'number of found embeddings: {num_found}/{len(vocab)}')
 
     return scipy.sparse.csr_matrix(word_embeddings)
 
 
 class Preprocessing:
-    def __init__(self, tokenizer=None, test_sample_size=None, test_p=0.2, stopwords=None, min_doc_count=0, max_doc_freq=1.0, keep_num=False, keep_alphanum=False, strip_html=False, no_lower=False, min_length=3, min_term=1, vocab_size=None, seed=42):
+    def __init__(self,
+                 tokenizer=None,
+                 test_sample_size=None,
+                 test_p=0.2,
+                 stopwords=None,
+                 min_doc_count=0,
+                 max_doc_freq=1.0,
+                 keep_num=False,
+                 keep_alphanum=False,
+                 strip_html=False,
+                 no_lower=False,
+                 min_length=3,
+                 min_term=0,
+                 vocab_size=None,
+                 seed=42,
+                 verbose=True
+                ):
         """
         Args:
             test_sample_size:
                 Size of the test set.
             test_p:
                 Proportion of the test set. This helps sample the train set based on the size of the test set.
             stopwords:
@@ -169,35 +190,40 @@
         self.seed = seed
 
         if tokenizer is not None:
             self.tokenizer = tokenizer
         else:
             self.tokenizer = Tokenizer(stopwords, keep_num, keep_alphanum, strip_html, no_lower, min_length).tokenize
 
+        if verbose:
+            logger.set_level("DEBUG")
+        else:
+            logger.set_level("WARNING")
+
     def parse(self, texts, vocab):
         if not isinstance(texts, list):
             texts = [texts]
 
         vocab_set = set(vocab)
         parsed_texts = list()
-        for i, text in enumerate(tqdm(texts, desc="===>parse texts")):
+        for i, text in enumerate(tqdm(texts, desc="parse texts")):
             tokens = self.tokenizer(text)
             tokens = [t for t in tokens if t in vocab_set]
             parsed_texts.append(' '.join(tokens))
 
         vectorizer = CountVectorizer(vocabulary=vocab, tokenizer=lambda x: x.split())
         bow = vectorizer.fit_transform(parsed_texts)
         bow = bow.toarray()
         return parsed_texts, bow
 
     def preprocess_jsonlist(self, dataset_dir, label_name=None):
         train_items = file_utils.read_jsonlist(os.path.join(dataset_dir, 'train.jsonlist'))
         test_items = file_utils.read_jsonlist(os.path.join(dataset_dir, 'test.jsonlist'))
 
-        print(f"Found training documents {len(train_items)} testing documents {len(test_items)}")
+        logger.info(f"Found training documents {len(train_items)} testing documents {len(test_items)}")
 
         raw_train_texts = []
         train_labels = []
         raw_test_texts = []
         test_labels = []
 
         for item in train_items:
@@ -219,42 +245,42 @@
     def convert_labels(self, train_labels, test_labels):
         if train_labels is not None:
             label_list = list(set(train_labels))
             label_list.sort()
             n_labels = len(label_list)
             label2id = dict(zip(label_list, range(n_labels)))
 
-            print("label2id: ", label2id)
+            logger.info(f"label2id: {label2id}")
 
             train_labels = [label2id[label] for label in train_labels]
 
             if test_labels is not None:
                 test_labels = [label2id[label] for label in test_labels]
 
         return train_labels, test_labels
 
-    def preprocess(self, raw_train_texts, train_labels=None, raw_test_texts=None, test_labels=None):
+    def preprocess(self, raw_train_texts, train_labels=None, raw_test_texts=None, test_labels=None, pretrained_WE=True):
         np.random.seed(self.seed)
 
         train_texts = list()
         test_texts = list()
         word_counts = Counter()
         doc_counts_counter = Counter()
 
         train_labels, test_labels = self.convert_labels(train_labels, test_labels)
 
-        for text in tqdm(raw_train_texts, desc="===>parse train texts"):
+        for text in tqdm(raw_train_texts, desc="parse train texts"):
             tokens = self.tokenizer(text)
             word_counts.update(tokens)
             doc_counts_counter.update(set(tokens))
             parsed_text = ' '.join(tokens)
             train_texts.append(parsed_text)
 
         if raw_test_texts:
-            for text in tqdm(raw_test_texts, desc="===>parse test texts"):
+            for text in tqdm(raw_test_texts, desc="parse test texts"):
                 tokens = self.tokenizer(text)
                 word_counts.update(tokens)
                 doc_counts_counter.update(set(tokens))
                 parsed_text = ' '.join(tokens)
                 test_texts.append(parsed_text)
 
         words, doc_counts = zip(*doc_counts_counter.most_common())
@@ -273,52 +299,54 @@
 
         if raw_test_texts is not None:
             test_idx = [i for i, text in enumerate(test_texts) if len(text.split()) >= self.min_term]
             test_idx = np.asarray(test_idx)
 
         # randomly sample
         if self.test_sample_size:
-            print("===>sample train and test sets...")
+            logger.info("sample train and test sets...")
 
             train_num = len(train_idx)
             test_num = len(test_idx)
             test_sample_size = min(test_num, self.test_sample_size)
             train_sample_size = int((test_sample_size / self.test_p) * (1 - self.test_p))
             if train_sample_size > train_num:
                 test_sample_size = int((train_num / (1 - self.test_p)) * self.test_p)
                 train_sample_size = train_num
 
             train_idx = train_idx[np.sort(np.random.choice(train_num, train_sample_size, replace=False))]
             test_idx = test_idx[np.sort(np.random.choice(test_num, test_sample_size, replace=False))]
 
-            print(f"===>sampled train size: {len(train_idx)}")
-            print(f"===>sampled train size: {len(test_idx)}")
+            logger.info(f"sampled train size: {len(train_idx)}")
+            logger.info(f"sampled train size: {len(test_idx)}")
 
         train_texts, train_bow = self.parse(np.asarray(train_texts)[train_idx].tolist(), vocab)
 
         rst = {
             'vocab': vocab,
             'train_bow': train_bow,
             'train_texts': train_texts,
-            'word_embeddings': make_word_embeddings(vocab)
         }
 
         if train_labels is not None:
             rst['train_labels'] = np.asarray(train_labels)[train_idx]
 
-        print(f"Real vocab size: {len(vocab)}")
-        print(f"Real training size: {len(train_texts)} \t avg length: {rst['train_bow'].sum() / len(train_texts):.3f}")
+        logger.info(f"Real vocab size: {len(vocab)}")
+        logger.info(f"Real training size: {len(train_texts)} \t avg length: {rst['train_bow'].sum() / len(train_texts):.3f}")
 
         if raw_test_texts is not None:
             rst['test_texts'], rst['test_bow'] = self.parse(np.asarray(test_texts)[test_idx].tolist(), vocab)
 
             if test_labels is not None:
                 rst['test_labels'] = np.asarray(test_labels)[test_idx]
 
-            print(f"Real testing size: {len(rst['test_texts'])} \t avg length: {rst['test_bow'].sum() / len(rst['test_texts']):.3f}")
+            logger.info(f"Real testing size: {len(rst['test_texts'])} \t avg length: {rst['test_bow'].sum() / len(rst['test_texts']):.3f}")
+
+        if pretrained_WE:
+            rst['word_embeddings'] = make_word_embeddings(vocab)
 
         return rst
 
     def save(self, output_dir, vocab, train_texts, train_bow, word_embeddings, train_labels=None, test_texts=None, test_bow=None, test_labels=None):
         file_utils.make_dir(output_dir)
 
         file_utils.save_text(vocab, f"{output_dir}/vocab.txt")
```

## topmost/trainers/basic/LDA_trainer.py

```diff
@@ -1,10 +1,10 @@
 import gensim
 from gensim.models import LdaModel
-from topmost.utils import basic_utils
+from topmost.utils import static_utils
 
 
 class LDAGensimTrainer:
     def __init__(self, dataset, num_topics=50, max_iter=1, alpha="symmetric", eta=None):
         self.dataset = dataset
         self.num_topics = num_topics
         self.vocab_size = dataset.vocab_size
@@ -33,15 +33,15 @@
         return theta
 
     def export_beta(self):
         return self.model.get_topics()
 
     def export_top_words(self, num_top=15):
         beta = self.export_beta()
-        top_words = basic_utils.print_topic_words(beta, vocab=self.dataset.vocab, num_top=num_top)
+        top_words = static_utils.print_topic_words(beta, vocab=self.dataset.vocab, num_top=num_top)
         return top_words
 
     def export_theta(self):
         train_theta = self.test(self.dataset.train_bow)
         test_theta = self.test(self.dataset.test_bow)
         return train_theta, test_theta
 
@@ -60,14 +60,14 @@
         return self.model.transform(bow.astype('int64'))
 
     def export_beta(self):
         return self.model.components_
 
     def export_top_words(self, num_top=15):
         beta = self.export_beta()
-        top_words = basic_utils.print_topic_words(beta, vocab=self.dataset.vocab, num_top=num_top)
+        top_words = static_utils.print_topic_words(beta, vocab=self.dataset.vocab, num_top=num_top)
         return top_words
 
     def export_theta(self):
         train_theta = self.test(self.dataset.train_bow)
         test_theta = self.test(self.dataset.test_bow)
         return train_theta, test_theta
```

## topmost/trainers/basic/NMF_trainer.py

```diff
@@ -1,10 +1,10 @@
 import gensim
 from gensim.models import nmf
-from topmost.utils import basic_utils
+from topmost.utils import static_utils
 
 
 class NMFGensimTrainer:
     def __init__(self, dataset_handler, vocab_size, num_topics=50, max_iter=1):
         self.dataset_handler = dataset_handler
         self.num_topics = num_topics
         self.vocab_size = vocab_size
@@ -30,15 +30,15 @@
         return theta
 
     def export_beta(self):
         return self.model.get_topics()
 
     def export_top_words(self, num_top=15):
         beta = self.export_beta()
-        top_words = basic_utils.print_topic_words(beta, vocab=self.dataset_handler.vocab, num_top=num_top)
+        top_words = static_utils.print_topic_words(beta, vocab=self.dataset_handler.vocab, num_top=num_top)
         return top_words
 
     def export_theta(self):
         train_theta = self.test(self.dataset_handler.train_bow)
         test_theta = self.test(self.dataset_handler.test_bow)
         return train_theta, test_theta
 
@@ -57,14 +57,14 @@
         return self.model.transform(bow.astype('int64'))
 
     def export_beta(self):
         return self.model.components_
 
     def export_top_words(self, num_top=15):
         beta = self.export_beta()
-        top_words = basic_utils.print_topic_words(beta, vocab=self.dataset_handler.vocab, num_top=num_top)
+        top_words = static_utils.print_topic_words(beta, vocab=self.dataset_handler.vocab, num_top=num_top)
         return top_words
 
     def export_theta(self):
         train_theta = self.test(self.dataset_handler.train_bow)
         test_theta = self.test(self.dataset_handler.test_bow)
         return train_theta, test_theta
```

## topmost/trainers/basic/basic_trainer.py

```diff
@@ -1,25 +1,45 @@
 import numpy as np
 from tqdm import tqdm
+from collections import defaultdict
+
 import torch
 from torch.optim.lr_scheduler import StepLR
-from collections import defaultdict
 from topmost.utils import static_utils
+from topmost.utils.logger import Logger
+
+
+logger = Logger("WARNING")
 
 
 class BasicTrainer:
-    def __init__(self, model, epochs=200, learning_rate=0.002, batch_size=200, lr_scheduler=None, lr_step_size=125, log_interval=5):
+    def __init__(self,
+                 model,
+                 epochs=200,
+                 learning_rate=0.002,
+                 batch_size=200,
+                 lr_scheduler=None,
+                 lr_step_size=125,
+                 log_interval=5,
+                 verbose=False
+                ):
+
         self.model = model
         self.epochs = epochs
         self.learning_rate = learning_rate
         self.batch_size = batch_size
         self.lr_scheduler = lr_scheduler
         self.lr_step_size = lr_step_size
         self.log_interval = log_interval
 
+        if verbose:
+            logger.set_level("DEBUG")
+        else:
+            logger.set_level("WARNING")
+
     def make_optimizer(self,):
         args_dict = {
             'params': self.model.parameters(),
             'lr': self.learning_rate,
         }
 
         optimizer = torch.optim.Adam(**args_dict)
@@ -28,26 +48,26 @@
     def make_lr_scheduler(self, optimizer):
         if self.lr_scheduler == "StepLR":
             lr_scheduler = StepLR(optimizer, step_size=self.lr_step_size, gamma=0.5, verbose=False)
         else:
             raise NotImplementedError(self.lr_scheduler)
         return lr_scheduler
 
-    def fit_transform(self, dataset_handler, num_top_words=15, verbose=False):
-        self.train(dataset_handler, verbose)
+    def fit_transform(self, dataset_handler, num_top_words=15):
+        self.train(dataset_handler)
         top_words = self.export_top_words(dataset_handler.vocab, num_top_words)
         train_theta = self.test(dataset_handler.train_data)
 
         return top_words, train_theta
 
-    def train(self, dataset_handler, verbose=False):
+    def train(self, dataset_handler):
         optimizer = self.make_optimizer()
 
         if self.lr_scheduler:
-            print("===>using lr_scheduler")
+            logger.info("use lr_scheduler")
             lr_scheduler = self.make_lr_scheduler(optimizer)
 
         data_size = len(dataset_handler.train_dataloader.dataset)
 
         for epoch in tqdm(range(1, self.epochs + 1)):
             self.model.train()
             loss_rst_dict = defaultdict(float)
@@ -63,20 +83,20 @@
 
                 for key in rst_dict:
                     loss_rst_dict[key] += rst_dict[key] * len(batch_data)
 
             if self.lr_scheduler:
                 lr_scheduler.step()
 
-            if verbose and epoch % self.log_interval == 0:
+            if epoch % self.log_interval == 0:
                 output_log = f'Epoch: {epoch:03d}'
                 for key in loss_rst_dict:
                     output_log += f' {key}: {loss_rst_dict[key] / data_size :.3f}'
 
-                print(output_log)
+                logger.info(output_log)
 
     def test(self, input_data):
         data_size = input_data.shape[0]
         theta = list()
         all_idx = torch.split(torch.arange(data_size), self.batch_size)
 
         with torch.no_grad():
```

## topmost/trainers/crosslingual/crosslingual_trainer.py

```diff
@@ -1,52 +1,71 @@
 import torch
 from torch.optim.lr_scheduler import StepLR
 from collections import defaultdict
 import numpy as np
 from tqdm import tqdm
 from topmost.utils import static_utils
+from topmost.utils.logger import Logger
+
+
+logger = Logger("WARNING")
 
 
 class CrosslingualTrainer:
-    def __init__(self, model, epochs=500, learning_rate=0.002, batch_size=200, lr_scheduler=None, lr_step_size=125, log_interval=5):
+    def __init__(self,
+                 model,
+                 epochs=500,
+                 learning_rate=0.002,
+                 batch_size=200,
+                 lr_scheduler=None,
+                 lr_step_size=125,
+                 log_interval=5,
+                 verbose=False
+                ):
+
         self.model = model
         self.epochs = epochs
         self.learning_rate = learning_rate
         self.batch_size = batch_size
         self.lr_scheduler = lr_scheduler
         self.lr_step_size = lr_step_size
         self.log_interval = log_interval
 
+        if verbose:
+            logger.set_level("DEBUG")
+        else:
+            logger.set_level("WARNING")
+
     def make_optimizer(self):
         args_dict = {
             'params': self.model.parameters(),
             'lr': self.learning_rate
         }
 
         optimizer = torch.optim.Adam(**args_dict)
         return optimizer
 
     def make_lr_scheduler(self, optimizer):
         if self.lr_scheduler == 'StepLR':
-            print("===>using lr_scheduler")
+            logger.info("using lr_scheduler")
             lr_scheduler = StepLR(optimizer, step_size=self.lr_step_size, gamma=0.5, verbose=False)
         else:
             raise NotImplementedError(self.lr_scheduler)
 
         return lr_scheduler
 
-    def fit_transform(self, dataset_handler, num_top_words=15, verbose=False):
-        self.train(dataset_handler, verbose)
+    def fit_transform(self, dataset_handler, num_top_words=15):
+        self.train(dataset_handler)
         top_words_en, top_words_cn = self.export_top_words(dataset_handler.vocab_en, dataset_handler.vocab_cn, num_top_words)
         train_theta_en, train_theta_cn = self.test(dataset_handler.train_bow_en, dataset_handler.train_bow_cn)
 
         return top_words_en, top_words_cn, train_theta_en, train_theta_cn
 
 
-    def train(self, dataset_handler, verbose=False):
+    def train(self, dataset_handler):
         data_size = len(dataset_handler.train_dataloader.dataset)
         optimizer = self.make_optimizer()
 
         if self.lr_scheduler:
             lr_scheduler = self.make_lr_scheduler(optimizer)
 
         for epoch in tqdm(range(1, self.epochs + 1)):
@@ -71,20 +90,20 @@
                 optimizer.zero_grad()
                 batch_loss.backward()
                 optimizer.step()
 
             if self.lr_scheduler:
                 lr_scheduler.step()
 
-            if verbose and epoch % self.log_interval == 0:
+            if epoch % self.log_interval == 0:
                 output_log = f'Epoch: {epoch:03d}'
                 for key in loss_rst_dict:
                     output_log += f' {key}: {loss_rst_dict[key] / data_size :.3f}'
 
-                print(output_log)
+                logger.info(output_log)
 
     def get_theta(self, bow, lang):
         theta_list = list()
         data_size = bow.shape[0]
         all_idx = torch.split(torch.arange(data_size,), self.batch_size)
         with torch.no_grad():
             self.model.eval()
```

## topmost/trainers/dynamic/DTM_trainer.py

```diff
@@ -1,14 +1,14 @@
 import gensim
 import numpy as np
 from gensim.models import ldaseqmodel
 from tqdm import tqdm
 import datetime
 from multiprocessing.pool import Pool
-from topmost.utils import basic_utils
+from topmost.utils import static_utils
 
 
 def work(arguments):
     model, docs = arguments
     theta_list = list()
     for doc in tqdm(docs):
         theta_list.append(model[doc])
@@ -56,15 +56,14 @@
         # bow = dataset.bow.cpu().numpy()
         # times = dataset.times.cpu().numpy()
         corpus = gensim.matutils.Dense2Corpus(bow, documents_columns=False)
 
         num_workers = 20
         split_idx_list = np.array_split(np.arange(len(bow)), num_workers)
         worker_size_list = [len(x) for x in split_idx_list]
-        print("===>worker_size_list: ", worker_size_list)
 
         worker_id = 0
         docs_list = [list() for i in range(num_workers)]
         for i, doc in enumerate(corpus):
             docs_list[worker_id].append(doc)
             if len(docs_list[worker_id]) >= worker_size_list[worker_id]:
                 worker_id += 1
@@ -110,15 +109,15 @@
         beta = beta / beta.sum(-1, keepdims=True)
         return beta
 
     def export_top_words(self, num_top=15):
         beta = self.export_beta()
         top_words_list = list()
         for time in range(beta.shape[0]):
-            top_words = basic_utils.print_topic_words(beta[time], vocab=self.dataset_handler.vocab, num_top=num_top)
+            top_words = static_utils.print_topic_words(beta[time], vocab=self.dataset_handler.vocab, num_top=num_top)
             top_words_list.append(top_words)
         return top_words_list
 
     def export_theta(self):
         train_theta = self.get_theta()
         test_theta = self.test(self.dataset_handler.test_bow)
         return train_theta, test_theta
```

## topmost/trainers/dynamic/dynamic_trainer.py

```diff
@@ -1,25 +1,45 @@
 import numpy as np
-import torch
-from torch.optim.lr_scheduler import StepLR
 from tqdm import tqdm
 from collections import defaultdict
+
+import torch
+from torch.optim.lr_scheduler import StepLR
 from topmost.utils import static_utils
+from topmost.utils.logger import Logger
+
+
+logger = Logger("WARNING")
 
 
 class DynamicTrainer:
-    def __init__(self, model, epochs=200, learning_rate=0.002, batch_size=200, lr_scheduler=None, lr_step_size=125, log_interval=5):
+    def __init__(self,
+                 model,
+                 epochs=200,
+                 learning_rate=0.002,
+                 batch_size=200,
+                 lr_scheduler=None,
+                 lr_step_size=125,
+                 log_interval=5,
+                 verbose=False
+                ):
+
         self.model = model
         self.epochs = epochs
         self.learning_rate = learning_rate
         self.batch_size = batch_size
         self.lr_scheduler = lr_scheduler
         self.lr_step_size = lr_step_size
         self.log_interval = log_interval
 
+        if verbose:
+            logger.set_level("DEBUG")
+        else:
+            logger.set_level("WARNING")
+
     def make_optimizer(self,):
         args_dict = {
             'params': self.model.parameters(),
             'lr': self.learning_rate,
         }
 
         optimizer = torch.optim.Adam(**args_dict)
@@ -36,15 +56,15 @@
 
         return top_words, train_theta
 
     def train(self, dataset_handler, verbose=False):
         optimizer = self.make_optimizer()
 
         if self.lr_scheduler:
-            print("===>using lr_scheduler")
+            logger.info("using lr_scheduler")
             lr_scheduler = self.make_lr_scheduler(optimizer)
 
         data_size = len(dataset_handler.train_dataloader.dataset)
 
         for epoch in tqdm(range(1, self.epochs + 1)):
             self.model.train()
             loss_rst_dict = defaultdict(float)
@@ -65,15 +85,15 @@
                 lr_scheduler.step()
 
             if verbose and epoch % self.log_interval == 0:
                 output_log = f'Epoch: {epoch:03d}'
                 for key in loss_rst_dict:
                     output_log += f' {key}: {loss_rst_dict[key] / data_size :.3f}'
 
-                print(output_log)
+                logger.info(output_log)
 
     def test(self, bow, times):
         data_size = bow.shape[0]
         theta = list()
         all_idx = torch.split(torch.arange(data_size), self.batch_size)
 
         with torch.no_grad():
```

## topmost/trainers/hierarchical/HDP_trainer.py

```diff
@@ -1,14 +1,14 @@
 """
 https://radimrehurek.com/gensim/models/hdpmodel.html
 """
 
 import gensim
 from gensim.models import HdpModel
-from topmost.utils import basic_utils
+from topmost.utils import static_utils
 
 
 class HDPGensimTrainer:
     def __init__(self, dataset, max_chunks=None, max_time=None, chunksize=256, kappa=1.0, tau=64.0, K=15, T=150, alpha=1, gamma=1, eta=0.01, scale=1.0, var_converge=0.0001):
         self.dataset = dataset
         self.vocab_size = dataset.vocab_size
         self.max_chunks = max_chunks
@@ -58,14 +58,14 @@
         return theta
 
     def export_beta(self):
         return self.model.get_topics()
 
     def export_top_words(self, num_top_words=15):
         beta = self.export_beta()
-        top_words = basic_utils.print_topic_words(beta, vocab=self.dataset.vocab, num_top_words=num_top_words)
+        top_words = static_utils.print_topic_words(beta, vocab=self.dataset.vocab, num_top_words=num_top_words)
         return top_words
 
     def export_theta(self):
         train_theta = self.test(self.dataset.train_bow)
         test_theta = self.test(self.dataset.test_bow)
         return train_theta, test_theta
```

## topmost/trainers/hierarchical/hierarchical_trainer.py

```diff
@@ -1,30 +1,49 @@
 import numpy as np
+from tqdm import tqdm
+from collections import defaultdict
+
 import torch
 from torch.optim.lr_scheduler import StepLR
-from collections import defaultdict
-from tqdm import tqdm
 from topmost.utils import static_utils
+from topmost.utils.logger import Logger
+
+
+logger = Logger("WARNING")
 
 
 # transform tensor list to numpy list
 def to_nparray(tensor_list):
     return np.asarray([item.detach().cpu().numpy() for item in tensor_list], dtype=object)
 
 
 class HierarchicalTrainer:
-    def __init__(self, model, epochs=200, learning_rate=0.002, batch_size=200, lr_scheduler=None, lr_step_size=125, log_interval=5):
+    def __init__(self,
+                 model,
+                 epochs=200,
+                 learning_rate=0.002,
+                 batch_size=200,
+                 lr_scheduler=None,
+                 lr_step_size=125,
+                 log_interval=5,
+                 verbose=False
+                ):
         self.model = model
         self.epochs = epochs
         self.learning_rate = learning_rate
         self.batch_size = batch_size
         self.lr_scheduler = lr_scheduler
         self.lr_step_size = lr_step_size
         self.log_interval = log_interval
 
+        if verbose:
+            logger.set_level("DEBUG")
+        else:
+            logger.set_level("WARNING")
+
     def make_optimizer(self,):
         args_dict = {
             'params': self.model.parameters(),
             'lr': self.learning_rate,
         }
 
         optimizer = torch.optim.Adam(**args_dict)
@@ -41,15 +60,15 @@
 
         return top_words, train_theta
 
     def train(self, dataset_handler, verbose=False):
         optimizer = self.make_optimizer()
 
         if self.lr_scheduler:
-            print("===>using lr_scheduler")
+            logger.info("using lr_scheduler")
             lr_scheduler = self.make_lr_scheduler(optimizer)
 
         data_size = len(dataset_handler.train_dataloader.dataset)
 
         for epoch in tqdm(range(1, self.epochs + 1), leave=False):
             self.model.train()
             loss_rst_dict = defaultdict(float)
@@ -70,15 +89,15 @@
                 lr_scheduler.step()
 
             if verbose and epoch % self.log_interval == 0:
                 output_log = f'Epoch: {epoch:03d}'
                 for key in loss_rst_dict:
                     output_log += f' {key}: {loss_rst_dict[key] / data_size :.3f}'
 
-                print(output_log)
+                logger.info(output_log)
 
     def test(self, bow):
         data_size = bow.shape[0]
 
         num_topics_list = self.model.num_topics_list
         theta_list = [list() for _ in range(len(num_topics_list))]
         all_idx = torch.split(torch.arange(data_size), self.batch_size)
```

## Comparing `topmost/models/basic/ECRTM/CombinedECRTM.py` & `topmost/models/dynamic/CFDTM/CFDTM.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,125 +1,127 @@
 import numpy as np
 import torch
-from torch import nn
+import torch.nn as nn
 import torch.nn.functional as F
-from .ECR import ECR
 
+from .ETC import ETC
+from .UWE import UWE
+from topmost.models.Encoder import MLPEncoder
 
-class CombinedECRTM(nn.Module):
+
+class CFDTM(nn.Module):
     '''
-        Effective Neural Topic Modeling with Embedding Clustering Regularization. ICML 2023
+    Modeling Dynamic Topics in Chain-Free Fashion by Evolution-Tracking Contrastive Learning and Unassociated Word Exclusion. ACL 2024 Findings
 
-        Xiaobao Wu, Xinshuai Dong, Thong Thanh Nguyen, Anh Tuan Luu.
+    Xiaobao Wu, Xinshuai Dong, Liangming Pan, Thong Nguyen, Anh Tuan Luu.
     '''
-    def __init__(self, vocab_size, contextual_embed_size, num_topics=50, en_units=200, dropout=0., pretrained_WE=None, embed_size=200, beta_temp=0.2, weight_loss_ECR=100.0, sinkhorn_alpha=20.0, sinkhorn_max_iter=1000):
+
+    def __init__(self,
+                 vocab_size,
+                 train_time_wordfreq,
+                 num_times,
+                 pretrained_WE=None,
+                 num_topics=50,
+                 en_units=100,
+                 temperature=0.1,
+                 beta_temp=1.0,
+                 weight_neg=1.0e+7,
+                 weight_pos=1.0e+1,
+                 weight_UWE=1.0e+3,
+                 neg_topk=15,
+                 dropout=0.,
+                 embed_size=200
+                ):
         super().__init__()
 
         self.num_topics = num_topics
         self.beta_temp = beta_temp
-        self.vocab_size = vocab_size
+        self.train_time_wordfreq = train_time_wordfreq
+        self.encoder = MLPEncoder(vocab_size, num_topics, en_units, dropout)
 
         self.a = 1 * np.ones((1, num_topics)).astype(np.float32)
         self.mu2 = nn.Parameter(torch.as_tensor((np.log(self.a).T - np.mean(np.log(self.a), 1)).T))
         self.var2 = nn.Parameter(torch.as_tensor((((1.0 / self.a) * (1 - (2.0 / num_topics))).T + (1.0 / (num_topics * num_topics)) * np.sum(1.0 / self.a, 1)).T))
 
         self.mu2.requires_grad = False
         self.var2.requires_grad = False
 
-        self.fc11 = nn.Linear(vocab_size + contextual_embed_size, en_units)
-        self.fc12 = nn.Linear(en_units, en_units)
-        self.fc21 = nn.Linear(en_units, num_topics)
-        self.fc22 = nn.Linear(en_units, num_topics)
-        self.fc1_dropout = nn.Dropout(dropout)
-        self.theta_dropout = nn.Dropout(dropout)
-
-        self.mean_bn = nn.BatchNorm1d(num_topics)
-        self.mean_bn.weight.requires_grad = False
-        self.logvar_bn = nn.BatchNorm1d(num_topics)
-        self.logvar_bn.weight.requires_grad = False
-        self.decoder_bn = nn.BatchNorm1d(vocab_size, affine=True)
-        self.decoder_bn.weight.requires_grad = False
+        self.decoder_bn = nn.BatchNorm1d(vocab_size, affine=False)
+
+        if pretrained_WE is None:
+            self.word_embeddings  = nn.init.trunc_normal_(torch.empty(vocab_size, embed_size), std=0.1)
+            self.word_embeddings = nn.Parameter(F.normalize(self.word_embeddings))
 
-        if pretrained_WE is not None:
-            self.word_embeddings = torch.from_numpy(pretrained_WE).float()
         else:
-            self.word_embeddings = nn.init.trunc_normal_(torch.empty(vocab_size, embed_size))
-        self.word_embeddings = nn.Parameter(F.normalize(self.word_embeddings))
+            self.word_embeddings = nn.Parameter(torch.from_numpy(pretrained_WE).float())
 
-        self.topic_embeddings = torch.empty((num_topics, self.word_embeddings.shape[1]))
-        nn.init.trunc_normal_(self.topic_embeddings, std=0.1)
-        self.topic_embeddings = nn.Parameter(F.normalize(self.topic_embeddings))
+        # topic_embeddings: TxKxD
+        self.topic_embeddings = nn.init.xavier_normal_(torch.zeros(num_topics, self.word_embeddings.shape[1])).repeat(num_times, 1, 1)
+        self.topic_embeddings = nn.Parameter(self.topic_embeddings)
 
-        self.ECR = ECR(weight_loss_ECR, sinkhorn_alpha, sinkhorn_max_iter)
+        self.ETC = ETC(num_times, temperature, weight_neg, weight_pos)
+        self.UWE = UWE(self.ETC, num_times, temperature, weight_UWE, neg_topk)
 
     def get_beta(self):
-        dist = self.pairwise_euclidean_distance(self.topic_embeddings, self.word_embeddings)
-        beta = F.softmax(-dist / self.beta_temp, dim=0)
-        return beta
-
-    def reparameterize(self, mu, logvar):
-        if self.training:
-            std = torch.exp(0.5 * logvar)
-            eps = torch.randn_like(std)
-            return mu + (eps * std)
-        else:
-            return mu
-
-    def encode(self, input):
-        e1 = F.softplus(self.fc11(input))
-        e1 = F.softplus(self.fc12(e1))
-        e1 = self.fc1_dropout(e1)
-        mu = self.mean_bn(self.fc21(e1))
-        logvar = self.logvar_bn(self.fc22(e1))
-        z = self.reparameterize(mu, logvar)
-        theta = F.softmax(z, dim=1)
+        dist = self.pairwise_euclidean_dist(F.normalize(self.topic_embeddings, dim=-1), F.normalize(self.word_embeddings, dim=-1))
+        beta = F.softmax(-dist / self.beta_temp, dim=1)
 
-        loss_KL = self.compute_loss_KL(mu, logvar)
+        return beta
 
-        return theta, loss_KL
+    def pairwise_euclidean_dist(self, x, y):
+        cost = torch.sum(x ** 2, axis=-1, keepdim=True) + torch.sum(y ** 2, axis=-1) - 2 * torch.matmul(x, y.t())
+        return cost
 
-    def get_theta(self, input):
-        theta, loss_KL = self.encode(input)
+    def get_theta(self, x, times=None):
+        theta, mu, logvar = self.encoder(x)
         if self.training:
-            return theta, loss_KL
-        else:
-            return theta
+            return theta, mu, logvar
+
+        return theta
 
-    def compute_loss_KL(self, mu, logvar):
+    def get_KL(self, mu, logvar):
         var = logvar.exp()
         var_division = var / self.var2
         diff = mu - self.mu2
         diff_term = diff * diff / self.var2
         logvar_division = self.var2.log() - logvar
-        # KLD: N*K
         KLD = 0.5 * ((var_division + diff_term + logvar_division).sum(axis=1) - self.num_topics)
-        KLD = KLD.mean()
-        return KLD
 
-    def get_loss_ECR(self):
-        cost = self.pairwise_euclidean_distance(self.topic_embeddings, self.word_embeddings)
-        loss_ECR = self.ECR(cost)
-        return loss_ECR
+        return KLD.mean()
 
-    def pairwise_euclidean_distance(self, x, y):
-        cost = torch.sum(x ** 2, axis=1, keepdim=True) + torch.sum(y ** 2, dim=1) - 2 * torch.matmul(x, y.t())
-        return cost
+    def get_NLL(self, theta, beta, x, recon_x=None):
+        if recon_x is None:
+            recon_x = self.decode(theta, beta)
+        recon_loss = -(x * recon_x.log()).sum(axis=1)
 
-    def forward(self, input):
-        theta, loss_KL = self.encode(input)
-        beta = self.get_beta()
+        return recon_loss
+
+    def decode(self, theta, beta):
+        d1 = F.softmax(self.decoder_bn(torch.bmm(theta.unsqueeze(1), beta).squeeze(1)), dim=-1)
+        return d1
 
-        recon = F.softmax(self.decoder_bn(torch.matmul(theta, beta)), dim=-1)
-        recon_loss = -(input[:, :self.vocab_size] * recon.log()).sum(axis=1).mean()
+    def forward(self, x, times):
+        loss = 0.
+
+        theta, mu, logvar = self.get_theta(x)
+        kl_theta = self.get_KL(mu, logvar)
+
+        loss += kl_theta
+
+        beta = self.get_beta()
+        time_index_beta = beta[times]
+        recon_x = self.decode(theta, time_index_beta)
+        NLL = self.get_NLL(theta, time_index_beta, x, recon_x)
+        NLL = NLL.mean()
+        loss += NLL
 
-        loss_TM = recon_loss + loss_KL
+        loss_ETC = self.ETC(self.topic_embeddings)
+        loss += loss_ETC
 
-        loss_ECR = self.get_loss_ECR()
-        loss = loss_TM + loss_ECR
+        loss_UWE = self.UWE(self.train_time_wordfreq, beta, self.topic_embeddings, self.word_embeddings)
+        loss += loss_UWE
 
         rst_dict = {
             'loss': loss,
-            'loss_TM': loss_TM,
-            'loss_ECR': loss_ECR
         }
 
         return rst_dict
```

## Comparing `topmost-0.0.2.dist-info/LICENSE` & `topmost-0.0.3.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `topmost-0.0.2.dist-info/METADATA` & `topmost-0.0.3.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: topmost
-Version: 0.0.2
+Version: 0.0.3
 Summary: Towards the Topmost: A Topic Modeling System Tookit
 Home-page: https://github.com/bobxwu/topmost
 Author: Xiaobao Wu
 Author-email: xiaobao002@e.ntu.edu.sg
 License: Apache 2.0 License
 Keywords: toolkit,topic model,neural topic model
 Classifier: Development Status :: 3 - Alpha
@@ -13,21 +13,23 @@
 Classifier: Programming Language :: Python :: 3.7
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Operating System :: MacOS :: MacOS X
 Classifier: Operating System :: Microsoft :: Windows
 Description-Content-Type: text/x-rst
 License-File: LICENSE
-Requires-Dist: torch (==1.13.1)
-Requires-Dist: torchvision (>=0.14.1)
-Requires-Dist: gensim (>=4.2.0)
-Requires-Dist: scikit-learn (>=0.24.2)
-Requires-Dist: tqdm
+Requires-Dist: numpy <1.27.0
+Requires-Dist: scipy <=1.10.1
+Requires-Dist: torch >=1.13.1
+Requires-Dist: torchvision >=0.14.1
+Requires-Dist: gensim >=4.2.0
+Requires-Dist: scikit-learn >=0.24.2
+Requires-Dist: tqdm >=4.66.0
 Requires-Dist: python-Levenshtein
-Requires-Dist: bertopic (>=0.15.0)
+Requires-Dist: bertopic >=0.15.0
 
 |topmost-logo| TopMost
 =================================
 
 .. |topmost-logo| image:: docs/source/_static/topmost-logo.png
     :width: 38
```

## Comparing `topmost-0.0.2.dist-info/RECORD` & `topmost-0.0.3.dist-info/RECORD`

 * *Files 10% similar despite different names*

```diff
@@ -1,31 +1,29 @@
 topmost/__init__.py,sha256=d_oJW-0EVhW7rLcqTOEWt4lBx7MDVQxp_JNMQtCUu5w,117
 topmost/data/__init__.py,sha256=q3j4dEmTa1cU3NhoOGmyG6z8H3ztOmLvNq5sV6TJBGg,276
-topmost/data/basic_dataset_handler.py,sha256=CxGrxJxgAfp6532MnuXonmUkFMicupxkW5zJByva0CY,3752
+topmost/data/basic_dataset_handler.py,sha256=PJm-MnZ-xXh_lz48GOv32dIbFD8zn3NSofie8HY4NSE,3785
 topmost/data/crosslingual_dataset_handler.py,sha256=tZjqnu_1Rc_t7YJlLYZdKnFpzsvQjguxxU-lQabWSig,5009
 topmost/data/download.py,sha256=UXHuPSbOGh3oR3tOwhFzrfAFa2SPJQTmLTzOOASs3zc,827
-topmost/data/download_20ng.py,sha256=j3FIBSfd8wQ0DLIRqRCJpx-PWID59BpWbfGvkxZMc4Y,1685
-topmost/data/dynamic_dataset_handler.py,sha256=prU6mFdfhNGXhRm9eapNWOobp2dXqdu3UIAak798abU,3608
+topmost/data/download_20ng.py,sha256=3HjrrLD-2O9lP5GFLzVX-M_dg0MuAK_ft-S6fYJx71g,1673
+topmost/data/dynamic_dataset_handler.py,sha256=PrxSSoAQWlj2Dlb-sMAYuVTC8fSkX0aFVghcmHJMaCM,3669
 topmost/data/file_utils.py,sha256=NtaPLWk7e-fKa6QIgol2yNdUIewyGfMifY62oKnYJxw,952
-topmost/evaluations/__init__.py,sha256=1CfWWc6ormJItxopdcSUZYSQSck2lzCcRvUpZwmwGmU,520
+topmost/evaluations/__init__.py,sha256=ILsCmiwTdg9xUIhPDG_Fja9zi_1A1VfJrgtNHU4WJEQ,552
 topmost/evaluations/build_hierarchy.py,sha256=1x5E6fIR0vzhUqkDA-SBvCHcLsixCTjK-jHFeUASyfM,1104
 topmost/evaluations/classification.py,sha256=_fk3_jnjTCZJa9tmUxrSenCwMTXOgGSu5MJmkoB15tw,1986
 topmost/evaluations/clustering.py,sha256=s7UT1_m8DrX-BzFGGhQRdnK_zBPWwj_kNLg6IK-zZ-o,1297
 topmost/evaluations/hierarchy_quality.py,sha256=GMKZyhPYMfmBua41DyzQKaOIpuv-eKsqlq7q4NTWqc0,9263
-topmost/evaluations/topic_coherence.py,sha256=Kk6EojxO9jLU-RIlt42o1lRrN3EpP2A8oGS9oszLlho,1413
-topmost/evaluations/topic_diversity.py,sha256=E8ooUHHWwWV2Qj2Iiz501LfI5pYtbXoVRY2Z79Wtc8E,686
-topmost/models/Encoder copy.py,sha256=Cb1FagwiuCKeDWVRngQKeMnmJ4Yp5kDJ4yYD56hj4Ic,1373
+topmost/evaluations/topic_coherence.py,sha256=cXsVP3GI2T_Nxtf8EyTBoLpNuZO_Mv8skFZZhEpT4FI,1402
+topmost/evaluations/topic_diversity.py,sha256=RKdWst8qi4P45T7nSuL7BH8Ycfv6Q8aWwvN-4vpKI-c,1767
 topmost/models/Encoder.py,sha256=PDcZI_qf2PyATaEhaj8D9CZdWEf8VbVWmn0E6gj1a-A,1326
-topmost/models/__init__.py,sha256=m4l_sXkE2bFwcLZ0zDedK4DyaA2ukHhIgjNiTpCyxvM,512
+topmost/models/__init__.py,sha256=apxb9RNxDK0Z8hEjGqp6wwBHHmZruppu2jR2rQPdRPY,551
 topmost/models/basic/CombinedTM.py,sha256=mzRVnwPpkJD6VsXuporkEgPhxlMaEoUvPtJtAYQYFNQ,3917
 topmost/models/basic/DecTM.py,sha256=dYnkVd8YPJ9oVjJR6C43neqfEwFbXP3RWlAn6wkC2tw,3758
 topmost/models/basic/ETM.py,sha256=IecGChTLYJDll--JKIA5pEUB80kBEEeX3uKgZKjbvaE,2512
 topmost/models/basic/ProdLDA.py,sha256=Z8hCY48VktzdFoYMXJqQfUa-Y5JG_oKieJtaqv-vJ6I,3646
 topmost/models/basic/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-topmost/models/basic/ECRTM/CombinedECRTM.py,sha256=WzsSQJ0Lz19kpavKS8nJJcNeUxv7j-J_kV1HjrxTHhA,4619
 topmost/models/basic/ECRTM/ECR.py,sha256=AYA6F1DpQcMAkGp-CUqZr7WBJEAqEaVPrOvlVsavVU4,1503
 topmost/models/basic/ECRTM/ECRTM.py,sha256=jSv3k417Y-smWwI-Y4Ay10v0qL0H42URLPDazOWMPpk,4506
 topmost/models/basic/ECRTM/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 topmost/models/basic/NSTM/NSTM.py,sha256=MKa0EuqQZ6WKGnGklDn4h5Sr080KPB5wnpAqC7R59hs,2253
 topmost/models/basic/NSTM/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 topmost/models/basic/NSTM/auto_diff_sinkhorn.py,sha256=g-Ooiy0mpbQwMeVmfDMPcsRc7QGuwkPgZl6Nkg4aUc8,850
 topmost/models/basic/TSCTM/TSC.py,sha256=WnZS5_G2bxOb4XU1YCCWrVjvxnIuKPZohMnkkCRJ4Ro,2941
@@ -35,52 +33,52 @@
 topmost/models/crosslingual/NMTM.py,sha256=MMYD6rs_lMsBoGEUtI2FTip1CvGASe2WCwPfKsGsWdU,4478
 topmost/models/crosslingual/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 topmost/models/crosslingual/InfoCTM/InfoCTM.py,sha256=NnjiBvTthkSebS0LHw2x9L9C-cOpNZamw-Ere2V1L7w,3546
 topmost/models/crosslingual/InfoCTM/TAMI.py,sha256=nPWPcz1FMJ-Dw8i4XF0evpdrxdZWMfQWOcal3rg9UAc,3628
 topmost/models/crosslingual/InfoCTM/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 topmost/models/dynamic/DETM.py,sha256=AGlr1neUFOUrcikE6K6xmSqpZvIcrqGzsY-DQU9h_v4,9766
 topmost/models/dynamic/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+topmost/models/dynamic/CFDTM/CFDTM.py,sha256=l5GZP8s3f3xXPXxi_t0G3MF_gAn7XIhOSBEigkBw4xQ,4346
+topmost/models/dynamic/CFDTM/ETC.py,sha256=lT9L6mE8LHDn5A5ejMYscVlpKG_nytH3ULhCpg5XZSw,2365
+topmost/models/dynamic/CFDTM/UWE.py,sha256=QpeM-sTac1KkXW0UdSCoS8SULZ8Go12R2PiqRwlNJTk,1635
+topmost/models/dynamic/CFDTM/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 topmost/models/hierarchical/ProGBN.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 topmost/models/hierarchical/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 topmost/models/hierarchical/HyperMiner/HyperMiner.py,sha256=aBDXZgV2BG7KR6sNGcnSYRXfmIvfaTi3JLImo01exts,5234
 topmost/models/hierarchical/HyperMiner/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 topmost/models/hierarchical/HyperMiner/manifolds/__init__.py,sha256=Ch1JFMW3NUrifGjGPoWR3IAusBjQo6tqHnPpKAJINw8,107
 topmost/models/hierarchical/HyperMiner/manifolds/base.py,sha256=QpmqCfJI1B1eEfNwPxuuFjSaQI5nQY976a0N6xJhZm4,6632
 topmost/models/hierarchical/HyperMiner/manifolds/euclidean.py,sha256=_hAAEzaXSJYsr_SXglGHMJqYvqoB83s_SISgEeksjpA,1386
 topmost/models/hierarchical/HyperMiner/manifolds/math_util.py,sha256=Is5U0bWo5w1Xi-fty2KJ97--eYdlh7RQN-b3X4HEmeY,5117
 topmost/models/hierarchical/HyperMiner/manifolds/poincare.py,sha256=a6DtPcl88vvKHE7W5EupW5wVaUynKraZy2eKK4DO8KU,4403
-topmost/models/hierarchical/ProGBN/ProGBN.py,sha256=66-7k12K8EoGR7OjCFWtV3JGYGpTx8jEYZ0oLcx8kPg,10196
-topmost/models/hierarchical/ProGBN/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-topmost/models/hierarchical/ProGBN/utils.py,sha256=yO_j3eBXI7MakXj9ZYpymc8PBunnurCmqGUpDOM8k28,4957
 topmost/models/hierarchical/SawETM/SawETM.py,sha256=7FcnvPq_C8qZzNCfk-scx-kZe5mp0oh74jJZdtoVK6o,7821
 topmost/models/hierarchical/SawETM/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 topmost/models/hierarchical/SawETM/block.py,sha256=zfOCpW0MZczAGma-x6YHA5juFwwxSGy9FCWs2Z_T0NM,1392
 topmost/models/hierarchical/TraCo/CDDecoder.py,sha256=YzDlmXqtn6pD6zfQwC9qgHnyu1zLIFTuTJWyi_Fxa7A,1900
 topmost/models/hierarchical/TraCo/TPD.py,sha256=NS0-Jqs7MtVJaNW3iy7QS2BHm_9Y5-iOwHJWUp9xkK4,1952
 topmost/models/hierarchical/TraCo/TraCo.py,sha256=g1in457vCQLWvFlVMwZp2MoYJT1CijasAe3zjhcYQho,4262
 topmost/models/hierarchical/TraCo/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 topmost/models/hierarchical/TraCo/utils.py,sha256=lwj-BAv7NssHVRqoVm47DuD3BJV79-v9EmMjtlDJ15g,485
 topmost/preprocessing/__init__.py,sha256=7XpnLXPEubaZurrigk9l28fmZalMj6VXJKQ7xqsCU_g,41
-topmost/preprocessing/preprocessing.py,sha256=qtvYV9Zi9rBdRoNz2L51gIrrqE_OrWjRYC8Pe_gCg7M,12646
+topmost/preprocessing/preprocessing.py,sha256=KOLtYid71Vuw9imoxo9JaEW55d8Nj_u9hczI2Q9qXC0,13268
 topmost/trainers/__init__.py,sha256=BzvDldLbY87SSfJiBUc6b10oG2bw3vpL7BSS2ZKqBDo,580
 topmost/trainers/basic/BERTopic_trainer.py,sha256=0loBNOoBp2a6Eh3ntOA_NQn3_cN6f0F9AV0pkZps8TM,920
-topmost/trainers/basic/LDA_trainer.py,sha256=9q3dtPogwSVEFwo-Yl1eAx9qVd1A8YejKaXOHk30ygU,2437
-topmost/trainers/basic/NMF_trainer.py,sha256=ExA_hR68tGInR7OCAi3FBCR5X_LOMwv4TxjNnw8P6jk,2421
+topmost/trainers/basic/LDA_trainer.py,sha256=M8U20b7A5WUGDH1YWH13P3DkwLkJ4rUi2Jyy4-ow12A,2440
+topmost/trainers/basic/NMF_trainer.py,sha256=nokrOSCaqzyEoacMzEQ-dUwNn0ivLJfsub4lkTN-Ow0,2424
 topmost/trainers/basic/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-topmost/trainers/basic/basic_contextual_trainer.py,sha256=zRw2tS6VHAw2hr0rotw0qJNcObDI3gGirWND0MAFJis,3214
-topmost/trainers/basic/basic_trainer.py,sha256=eFQC6OFBEjZKfvFb4bl3uv5USQxIqDj04Bh2wJt1VF8,3551
+topmost/trainers/basic/basic_trainer.py,sha256=ocCA4fJt6QV5U-_2E8AW-tL2K_pjhSvLpaN8s6hFr14,3858
 topmost/trainers/crosslingual/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-topmost/trainers/crosslingual/crosslingual_trainer.py,sha256=NVOVEFYlalNlCtXekr_ftQ28ySRScU8yISNykGMAzgs,4428
-topmost/trainers/dynamic/DTM_trainer.py,sha256=KDk_oA2DBIRd0wl0vRbfUJeU53kRJAor_srpngMJuHI,4167
+topmost/trainers/crosslingual/crosslingual_trainer.py,sha256=HIoZgMKAIR1p2N8Zjs8xKz7aAoDYjptdk-vnbuRQDxc,4736
+topmost/trainers/dynamic/DTM_trainer.py,sha256=vDXdiYHk0yhfTk4mHKcP-AaJRFxNCkLDQ9PV35M1QH4,4111
 topmost/trainers/dynamic/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-topmost/trainers/dynamic/dynamic_trainer.py,sha256=YJg9ubeoJHdTvo6txaJio93ALfH9qriZ0-3r9WGE_gg,3714
-topmost/trainers/hierarchical/HDP_trainer.py,sha256=G0iTBDNgRH24YJbBEAbBafDY8Hq_qD0T1vfK2l1juEM,2373
+topmost/trainers/dynamic/dynamic_trainer.py,sha256=UBebzyh_y5Hn8rGNFSP-7GskTcaBXtMN7oHUlgMWa08,4074
+topmost/trainers/hierarchical/HDP_trainer.py,sha256=ZIxmHgamyYVAbd-JSMNxss_RERIfT9IRmUfk7KXr7_A,2375
 topmost/trainers/hierarchical/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-topmost/trainers/hierarchical/hierarchical_trainer.py,sha256=jjiboubbbt_H1NMZGsYJGXHXO51zKHqN1e9PzqaVHss,4402
+topmost/trainers/hierarchical/hierarchical_trainer.py,sha256=_mAfD9LZU4lhjAuG3EnX7FzysWX60cRvUChi8ysIm68,4761
 topmost/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-topmost/utils/basic_utils.py,sha256=Zavmrraj0TNvVmkTzOqSpCMOz5CF4-J0nMsuWalTi88,390
+topmost/utils/logger.py,sha256=Twa40u38yZTB1i94YVEthhqajTMIIt2rUwtnfofe9cw,854
 topmost/utils/static_utils.py,sha256=Zavmrraj0TNvVmkTzOqSpCMOz5CF4-J0nMsuWalTi88,390
-topmost-0.0.2.dist-info/LICENSE,sha256=yVuuHRzgI17MzTVgt3LsHvuX80innw--CmNPDCzO_iw,11358
-topmost-0.0.2.dist-info/METADATA,sha256=XxRjx10EtCOOQ1imMxUTBtNwLjDytFhHqX0BZMwHEQA,16609
-topmost-0.0.2.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-topmost-0.0.2.dist-info/top_level.txt,sha256=LGxnB073Vb97ZHW3Iz16d0q991aBULtbPFpwmYKqVME,8
-topmost-0.0.2.dist-info/RECORD,,
+topmost-0.0.3.dist-info/LICENSE,sha256=yVuuHRzgI17MzTVgt3LsHvuX80innw--CmNPDCzO_iw,11358
+topmost-0.0.3.dist-info/METADATA,sha256=DeBs9EA51O76zPR86CvGQIafLiuBGBoaJGB5U8nOJZE,16667
+topmost-0.0.3.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+topmost-0.0.3.dist-info/top_level.txt,sha256=LGxnB073Vb97ZHW3Iz16d0q991aBULtbPFpwmYKqVME,8
+topmost-0.0.3.dist-info/RECORD,,
```

