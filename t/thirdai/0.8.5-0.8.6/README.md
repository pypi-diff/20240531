# Comparing `tmp/thirdai-0.8.5-cp39-cp39-win_amd64.whl.zip` & `tmp/thirdai-0.8.6-cp39-cp39-macosx_11_0_arm64.whl.zip`

## zipinfo {}

```diff
@@ -1,163 +1,135 @@
-Zip file size: 5330522 bytes, number of entries: 161
-drwxrwxrwx  2.0 fat        0 b- stor 24-May-23 11:25 thirdai/
-drwxrwxrwx  2.0 fat        0 b- stor 24-May-23 11:25 thirdai-0.8.5.dist-info/
-drwxrwxrwx  2.0 fat        0 b- stor 24-May-23 11:25 thirdai.libs/
-drwxrwxrwx  2.0 fat        0 b- stor 24-May-23 11:25 thirdai/bolt/
-drwxrwxrwx  2.0 fat        0 b- stor 24-May-23 11:25 thirdai/data/
-drwxrwxrwx  2.0 fat        0 b- stor 24-May-23 11:25 thirdai/dataset/
-drwxrwxrwx  2.0 fat        0 b- stor 24-May-23 11:25 thirdai/demos/
-drwxrwxrwx  2.0 fat        0 b- stor 24-May-23 11:25 thirdai/gen/
-drwxrwxrwx  2.0 fat        0 b- stor 24-May-23 11:25 thirdai/neural_db/
-drwxrwxrwx  2.0 fat        0 b- stor 24-May-23 11:25 thirdai/neural_db_v2/
-drwxrwxrwx  2.0 fat        0 b- stor 24-May-23 11:25 thirdai/telemetry/
-drwxrwxrwx  2.0 fat        0 b- stor 24-May-23 11:25 thirdai/_deps/
-drwxrwxrwx  2.0 fat        0 b- stor 24-May-23 11:25 thirdai/_distributed_bolt/
--rw-rw-rw-  2.0 fat      145 b- defN 24-May-23 11:25 thirdai/deployment.py
--rw-rw-rw-  2.0 fat      139 b- defN 24-May-23 11:25 thirdai/distributed_bolt.py
--rw-rw-rw-  2.0 fat     1312 b- defN 24-May-23 11:25 thirdai/embeddings.py
--rw-rw-rw-  2.0 fat      136 b- defN 24-May-23 11:25 thirdai/hashing.py
--rw-rw-rw-  2.0 fat      195 b- defN 24-May-23 11:25 thirdai/licensing.py
--rw-rw-rw-  2.0 fat     1277 b- defN 24-May-23 11:25 thirdai/search.py
--rw-rw-rw-  2.0 fat     1308 b- defN 24-May-23 11:25 thirdai/_download.py
--rw-rw-rw-  2.0 fat 13161472 b- defN 24-May-23 11:25 thirdai/_thirdai.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat     2817 b- defN 24-May-23 11:25 thirdai/__init__.py
--rw-rw-rw-  2.0 fat     2340 b- defN 24-May-23 11:25 thirdai/bolt/ner_modifications.py
--rw-rw-rw-  2.0 fat    17208 b- defN 24-May-23 11:25 thirdai/bolt/seismic_modifications.py
--rw-rw-rw-  2.0 fat    10707 b- defN 24-May-23 11:25 thirdai/bolt/udt_docs.py
--rw-rw-rw-  2.0 fat    14291 b- defN 24-May-23 11:25 thirdai/bolt/udt_modifications.py
--rw-rw-rw-  2.0 fat      583 b- defN 24-May-23 11:25 thirdai/bolt/__init__.py
--rw-rw-rw-  2.0 fat     1953 b- defN 24-May-23 11:25 thirdai/data/column_map_utils.py
--rw-rw-rw-  2.0 fat     2474 b- defN 24-May-23 11:25 thirdai/data/get_udt_columns.py
--rw-rw-rw-  2.0 fat     5748 b- defN 24-May-23 11:25 thirdai/data/type_inference.py
--rw-rw-rw-  2.0 fat      525 b- defN 24-May-23 11:25 thirdai/data/__init__.py
--rw-rw-rw-  2.0 fat     2592 b- defN 24-May-23 11:25 thirdai/dataset/bolt_ner_data_source.py
--rw-rw-rw-  2.0 fat     3305 b- defN 24-May-23 11:25 thirdai/dataset/csv_data_source.py
--rw-rw-rw-  2.0 fat     1105 b- defN 24-May-23 11:25 thirdai/dataset/data_source.py
--rw-rw-rw-  2.0 fat     2428 b- defN 24-May-23 11:25 thirdai/dataset/llm_data_source.py
--rw-rw-rw-  2.0 fat     1542 b- defN 24-May-23 11:25 thirdai/dataset/parquet_data_source.py
--rw-rw-rw-  2.0 fat     2398 b- defN 24-May-23 11:25 thirdai/dataset/ray_data_source.py
--rw-rw-rw-  2.0 fat      404 b- defN 24-May-23 11:25 thirdai/dataset/__init__.py
--rw-rw-rw-  2.0 fat     8758 b- defN 24-May-23 11:25 thirdai/demos/beir_download_utils.py
--rw-rw-rw-  2.0 fat    24702 b- defN 24-May-23 11:25 thirdai/demos/download_datasets.py
--rw-rw-rw-  2.0 fat      520 b- defN 24-May-23 11:25 thirdai/demos/download_tokenizer_vocabs.py
--rw-rw-rw-  2.0 fat       76 b- defN 24-May-23 11:25 thirdai/demos/__init__.py
--rw-rw-rw-  2.0 fat     1265 b- defN 24-May-23 11:25 thirdai/gen/questions.py
--rw-rw-rw-  2.0 fat       26 b- defN 24-May-23 11:25 thirdai/gen/__init__.py
-drwxrwxrwx  2.0 fat        0 b- stor 24-May-23 11:25 thirdai/neural_db/models/
-drwxrwxrwx  2.0 fat        0 b- stor 24-May-23 11:25 thirdai/neural_db/model_bazaar/
-drwxrwxrwx  2.0 fat        0 b- stor 24-May-23 11:25 thirdai/neural_db/parsing_utils/
-drwxrwxrwx  2.0 fat        0 b- stor 24-May-23 11:25 thirdai/neural_db/trainer/
--rw-rw-rw-  2.0 fat     6881 b- defN 24-May-23 11:25 thirdai/neural_db/connectors.py
--rw-rw-rw-  2.0 fat     9130 b- defN 24-May-23 11:25 thirdai/neural_db/constraint_matcher.py
--rw-rw-rw-  2.0 fat    93810 b- defN 24-May-23 11:25 thirdai/neural_db/documents.py
--rw-rw-rw-  2.0 fat     2703 b- defN 24-May-23 11:25 thirdai/neural_db/inverted_index.py
--rw-rw-rw-  2.0 fat     3619 b- defN 24-May-23 11:25 thirdai/neural_db/loggers.py
--rw-rw-rw-  2.0 fat    44815 b- defN 24-May-23 11:25 thirdai/neural_db/neural_db.py
--rw-rw-rw-  2.0 fat      677 b- defN 24-May-23 11:25 thirdai/neural_db/question_generation.py
--rw-rw-rw-  2.0 fat     5145 b- defN 24-May-23 11:25 thirdai/neural_db/savable_state.py
--rw-rw-rw-  2.0 fat    10446 b- defN 24-May-23 11:25 thirdai/neural_db/sharded_documents.py
--rw-rw-rw-  2.0 fat     1718 b- defN 24-May-23 11:25 thirdai/neural_db/sql_helpers.py
--rw-rw-rw-  2.0 fat     9886 b- defN 24-May-23 11:25 thirdai/neural_db/supervised_datasource.py
--rw-rw-rw-  2.0 fat     8126 b- defN 24-May-23 11:25 thirdai/neural_db/table.py
--rw-rw-rw-  2.0 fat      959 b- defN 24-May-23 11:25 thirdai/neural_db/teachers.py
--rw-rw-rw-  2.0 fat     3844 b- defN 24-May-23 11:25 thirdai/neural_db/utils.py
--rw-rw-rw-  2.0 fat      737 b- defN 24-May-23 11:25 thirdai/neural_db/__init__.py
--rw-rw-rw-  2.0 fat     3418 b- defN 24-May-23 11:25 thirdai/neural_db/models/finetunable_retriever.py
--rw-rw-rw-  2.0 fat    27554 b- defN 24-May-23 11:25 thirdai/neural_db/models/mach.py
--rw-rw-rw-  2.0 fat     1474 b- defN 24-May-23 11:25 thirdai/neural_db/models/mach_defaults.py
--rw-rw-rw-  2.0 fat    28166 b- defN 24-May-23 11:25 thirdai/neural_db/models/mach_mixture_model.py
--rw-rw-rw-  2.0 fat      575 b- defN 24-May-23 11:25 thirdai/neural_db/models/models.py
--rw-rw-rw-  2.0 fat     5583 b- defN 24-May-23 11:25 thirdai/neural_db/models/model_interface.py
--rw-rw-rw-  2.0 fat     7846 b- defN 24-May-23 11:25 thirdai/neural_db/models/multi_mach.py
--rw-rw-rw-  2.0 fat       24 b- defN 24-May-23 11:25 thirdai/neural_db/models/__init__.py
--rw-rw-rw-  2.0 fat    19936 b- defN 24-May-23 11:25 thirdai/neural_db/model_bazaar/bazaar_base.py
--rw-rw-rw-  2.0 fat    36069 b- defN 24-May-23 11:25 thirdai/neural_db/model_bazaar/bazaar_client.py
--rw-rw-rw-  2.0 fat     4003 b- defN 24-May-23 11:25 thirdai/neural_db/model_bazaar/utils.py
--rw-rw-rw-  2.0 fat       88 b- defN 24-May-23 11:25 thirdai/neural_db/model_bazaar/__init__.py
--rw-rw-rw-  2.0 fat     1476 b- defN 24-May-23 11:25 thirdai/neural_db/parsing_utils/doc_parse.py
--rw-rw-rw-  2.0 fat     6055 b- defN 24-May-23 11:25 thirdai/neural_db/parsing_utils/pdf_parse.py
--rw-rw-rw-  2.0 fat    11499 b- defN 24-May-23 11:25 thirdai/neural_db/parsing_utils/sliding_pdf_parse.py
--rw-rw-rw-  2.0 fat     7460 b- defN 24-May-23 11:25 thirdai/neural_db/parsing_utils/unstructured_parse.py
--rw-rw-rw-  2.0 fat     4064 b- defN 24-May-23 11:25 thirdai/neural_db/parsing_utils/url_parse.py
--rw-rw-rw-  2.0 fat     2778 b- defN 24-May-23 11:25 thirdai/neural_db/parsing_utils/utils.py
--rw-rw-rw-  2.0 fat       45 b- defN 24-May-23 11:25 thirdai/neural_db/parsing_utils/__init__.py
--rw-rw-rw-  2.0 fat     4013 b- defN 24-May-23 11:25 thirdai/neural_db/trainer/checkpoint_config.py
--rw-rw-rw-  2.0 fat     7964 b- defN 24-May-23 11:25 thirdai/neural_db/trainer/training_data_manager.py
--rw-rw-rw-  2.0 fat    11920 b- defN 24-May-23 11:25 thirdai/neural_db/trainer/training_progress_manager.py
--rw-rw-rw-  2.0 fat     7825 b- defN 24-May-23 11:25 thirdai/neural_db/trainer/training_progress_tracker.py
--rw-rw-rw-  2.0 fat        0 b- defN 24-May-23 11:25 thirdai/neural_db/trainer/__init__.py
-drwxrwxrwx  2.0 fat        0 b- stor 24-May-23 11:25 thirdai/neural_db_v2/chunk_stores/
-drwxrwxrwx  2.0 fat        0 b- stor 24-May-23 11:25 thirdai/neural_db_v2/core/
-drwxrwxrwx  2.0 fat        0 b- stor 24-May-23 11:25 thirdai/neural_db_v2/documents/
-drwxrwxrwx  2.0 fat        0 b- stor 24-May-23 11:25 thirdai/neural_db_v2/retrievers/
-drwxrwxrwx  2.0 fat        0 b- stor 24-May-23 11:25 thirdai/neural_db_v2/supervised/
-drwxrwxrwx  2.0 fat        0 b- stor 24-May-23 11:25 thirdai/neural_db_v2/utils/
--rw-rw-rw-  2.0 fat     5460 b- defN 24-May-23 11:25 thirdai/neural_db_v2/neural_db.py
--rw-rw-rw-  2.0 fat      234 b- defN 24-May-23 11:25 thirdai/neural_db_v2/__init__.py
--rw-rw-rw-  2.0 fat     2171 b- defN 24-May-23 11:25 thirdai/neural_db_v2/chunk_stores/constraints.py
--rw-rw-rw-  2.0 fat     6128 b- defN 24-May-23 11:25 thirdai/neural_db_v2/chunk_stores/pandas_chunk_store.py
--rw-rw-rw-  2.0 fat    12976 b- defN 24-May-23 11:25 thirdai/neural_db_v2/chunk_stores/sqlite_chunk_store.py
--rw-rw-rw-  2.0 fat      100 b- defN 24-May-23 11:25 thirdai/neural_db_v2/chunk_stores/__init__.py
--rw-rw-rw-  2.0 fat     2128 b- defN 24-May-23 11:25 thirdai/neural_db_v2/core/chunk_store.py
--rw-rw-rw-  2.0 fat      234 b- defN 24-May-23 11:25 thirdai/neural_db_v2/core/documents.py
--rw-rw-rw-  2.0 fat     1891 b- defN 24-May-23 11:25 thirdai/neural_db_v2/core/retriever.py
--rw-rw-rw-  2.0 fat      823 b- defN 24-May-23 11:25 thirdai/neural_db_v2/core/supervised.py
--rw-rw-rw-  2.0 fat     6352 b- defN 24-May-23 11:25 thirdai/neural_db_v2/core/types.py
--rw-rw-rw-  2.0 fat        0 b- defN 24-May-23 11:25 thirdai/neural_db_v2/core/__init__.py
--rw-rw-rw-  2.0 fat     2478 b- defN 24-May-23 11:25 thirdai/neural_db_v2/documents/csv.py
--rw-rw-rw-  2.0 fat     1153 b- defN 24-May-23 11:25 thirdai/neural_db_v2/documents/docx.py
--rw-rw-rw-  2.0 fat     1309 b- defN 24-May-23 11:25 thirdai/neural_db_v2/documents/in_memory_text.py
--rw-rw-rw-  2.0 fat     2304 b- defN 24-May-23 11:25 thirdai/neural_db_v2/documents/pdf.py
--rw-rw-rw-  2.0 fat     2361 b- defN 24-May-23 11:25 thirdai/neural_db_v2/documents/unstructured.py
--rw-rw-rw-  2.0 fat     1417 b- defN 24-May-23 11:25 thirdai/neural_db_v2/documents/url.py
--rw-rw-rw-  2.0 fat      840 b- defN 24-May-23 11:25 thirdai/neural_db_v2/documents/utils.py
--rw-rw-rw-  2.0 fat      365 b- defN 24-May-23 11:25 thirdai/neural_db_v2/documents/__init__.py
--rw-rw-rw-  2.0 fat     2018 b- defN 24-May-23 11:25 thirdai/neural_db_v2/retrievers/finetunable_retriever.py
--rw-rw-rw-  2.0 fat     6804 b- defN 24-May-23 11:25 thirdai/neural_db_v2/retrievers/mach.py
--rw-rw-rw-  2.0 fat      109 b- defN 24-May-23 11:25 thirdai/neural_db_v2/retrievers/__init__.py
--rw-rw-rw-  2.0 fat     1133 b- defN 24-May-23 11:25 thirdai/neural_db_v2/supervised/csv_supervised.py
--rw-rw-rw-  2.0 fat      729 b- defN 24-May-23 11:25 thirdai/neural_db_v2/supervised/in_memory_supervised.py
--rw-rw-rw-  2.0 fat       97 b- defN 24-May-23 11:25 thirdai/neural_db_v2/supervised/__init__.py
--rw-rw-rw-  2.0 fat        0 b- defN 24-May-23 11:25 thirdai/neural_db_v2/utils/__init__.py
--rw-rw-rw-  2.0 fat     5263 b- defN 24-May-23 11:25 thirdai/telemetry/telemetry_daemon.py
--rw-rw-rw-  2.0 fat     5939 b- defN 24-May-23 11:25 thirdai/telemetry/telemetry_start_and_stop.py
--rw-rw-rw-  2.0 fat       51 b- defN 24-May-23 11:25 thirdai/telemetry/__init__.py
-drwxrwxrwx  2.0 fat        0 b- stor 24-May-23 11:25 thirdai/_deps/ColBERT/
--rw-rw-rw-  2.0 fat        0 b- defN 24-May-23 11:25 thirdai/_deps/__init__.py
-drwxrwxrwx  2.0 fat        0 b- stor 24-May-23 11:25 thirdai/_deps/ColBERT/colbertconfig/
-drwxrwxrwx  2.0 fat        0 b- stor 24-May-23 11:25 thirdai/_deps/ColBERT/colbertmodeling/
-drwxrwxrwx  2.0 fat        0 b- stor 24-May-23 11:25 thirdai/_deps/ColBERT/colbertutils/
--rw-rw-rw-  2.0 fat        0 b- defN 24-May-23 11:25 thirdai/_deps/ColBERT/__init__.py
--rw-rw-rw-  2.0 fat     2118 b- defN 24-May-23 11:25 thirdai/_deps/ColBERT/colbertconfig/base_config.py
--rw-rw-rw-  2.0 fat      314 b- defN 24-May-23 11:25 thirdai/_deps/ColBERT/colbertconfig/config.py
--rw-rw-rw-  2.0 fat     2321 b- defN 24-May-23 11:25 thirdai/_deps/ColBERT/colbertconfig/core_config.py
--rw-rw-rw-  2.0 fat     3887 b- defN 24-May-23 11:25 thirdai/_deps/ColBERT/colbertconfig/settings.py
--rw-rw-rw-  2.0 fat       48 b- defN 24-May-23 11:25 thirdai/_deps/ColBERT/colbertconfig/__init__.py
-drwxrwxrwx  2.0 fat        0 b- stor 24-May-23 11:25 thirdai/_deps/ColBERT/colbertmodeling/tokenization/
--rw-rw-rw-  2.0 fat     1268 b- defN 24-May-23 11:25 thirdai/_deps/ColBERT/colbertmodeling/base_colbert.py
--rw-rw-rw-  2.0 fat     1418 b- defN 24-May-23 11:25 thirdai/_deps/ColBERT/colbertmodeling/checkpoint.py
--rw-rw-rw-  2.0 fat     2277 b- defN 24-May-23 11:25 thirdai/_deps/ColBERT/colbertmodeling/colbert.py
--rw-rw-rw-  2.0 fat     2175 b- defN 24-May-23 11:25 thirdai/_deps/ColBERT/colbertmodeling/hf_colbert.py
--rw-rw-rw-  2.0 fat        0 b- defN 24-May-23 11:25 thirdai/_deps/ColBERT/colbertmodeling/__init__.py
--rw-rw-rw-  2.0 fat     2064 b- defN 24-May-23 11:25 thirdai/_deps/ColBERT/colbertmodeling/tokenization/doc_tokenization.py
--rw-rw-rw-  2.0 fat     3295 b- defN 24-May-23 11:25 thirdai/_deps/ColBERT/colbertmodeling/tokenization/query_tokenization.py
--rw-rw-rw-  2.0 fat      168 b- defN 24-May-23 11:25 thirdai/_deps/ColBERT/colbertmodeling/tokenization/__init__.py
--rw-rw-rw-  2.0 fat      319 b- defN 24-May-23 11:25 thirdai/_deps/ColBERT/colbertutils/utils.py
--rw-rw-rw-  2.0 fat        0 b- defN 24-May-23 11:25 thirdai/_deps/ColBERT/colbertutils/__init__.py
-drwxrwxrwx  2.0 fat        0 b- stor 24-May-23 11:25 thirdai/_distributed_bolt/ray_trainer/
--rw-rw-rw-  2.0 fat     3817 b- defN 24-May-23 11:25 thirdai/_distributed_bolt/distributed.py
--rw-rw-rw-  2.0 fat      950 b- defN 24-May-23 11:25 thirdai/_distributed_bolt/utils.py
--rw-rw-rw-  2.0 fat      309 b- defN 24-May-23 11:25 thirdai/_distributed_bolt/__init__.py
--rw-rw-rw-  2.0 fat     3248 b- defN 24-May-23 11:25 thirdai/_distributed_bolt/ray_trainer/bolt_checkpoint.py
--rw-rw-rw-  2.0 fat     3450 b- defN 24-May-23 11:25 thirdai/_distributed_bolt/ray_trainer/bolt_trainer.py
--rw-rw-rw-  2.0 fat      732 b- defN 24-May-23 11:25 thirdai/_distributed_bolt/ray_trainer/train_loop_utils.py
--rw-rw-rw-  2.0 fat        0 b- defN 24-May-23 11:25 thirdai/_distributed_bolt/ray_trainer/__init__.py
--rw-rw-rw-  2.0 fat      394 b- defN 24-May-23 11:25 thirdai-0.8.5.dist-info/DELVEWHEEL
--rw-rw-rw-  2.0 fat    16919 b- defN 24-May-23 11:25 thirdai-0.8.5.dist-info/LICENSE.txt
--rw-rw-rw-  2.0 fat     7359 b- defN 24-May-23 11:25 thirdai-0.8.5.dist-info/METADATA
--rw-rw-rw-  2.0 fat    12567 b- defN 24-May-23 11:25 thirdai-0.8.5.dist-info/RECORD
--rw-rw-rw-  2.0 fat        8 b- defN 24-May-23 11:25 thirdai-0.8.5.dist-info/top_level.txt
--rw-rw-rw-  2.0 fat      100 b- defN 24-May-23 11:25 thirdai-0.8.5.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       54 b- defN 24-May-23 11:25 thirdai.libs/.load-order-thirdai-0.8.5
--rw-rw-rw-  2.0 fat   772016 b- defN 24-Jan-19 20:38 thirdai.libs/libomp140.x86_64-05cfcc04b0c6de37e0a3c60db4df4e20.dll
-161 files, 14592544 bytes uncompressed, 5306514 bytes compressed:  63.6%
+Zip file size: 6757527 bytes, number of entries: 133
+-rw-rw-r--  2.0 unx    12767 b- defN 24-May-30 22:50 thirdai-0.8.6.dist-info/RECORD
+-rw-r--r--  2.0 unx      108 b- defN 24-May-31 03:49 thirdai-0.8.6.dist-info/WHEEL
+-rw-r--r--  2.0 unx        8 b- defN 24-May-31 03:49 thirdai-0.8.6.dist-info/top_level.txt
+-rw-r--r--  2.0 unx    16709 b- defN 24-May-31 03:49 thirdai-0.8.6.dist-info/LICENSE.txt
+-rw-r--r--  2.0 unx     7359 b- defN 24-May-31 03:49 thirdai-0.8.6.dist-info/METADATA
+-rw-r--r--  2.0 unx      140 b- defN 24-May-17 19:53 thirdai/deployment.py
+-rwxr-xr-x  2.0 unx 10870960 b- defN 24-May-30 22:50 thirdai/_thirdai.cpython-39-darwin.so
+-rw-r--r--  2.0 unx     1400 b- defN 24-May-17 19:53 thirdai/__init__.py
+-rw-r--r--  2.0 unx     1267 b- defN 24-May-17 19:53 thirdai/_download.py
+-rw-r--r--  2.0 unx     1273 b- defN 24-May-17 19:53 thirdai/embeddings.py
+-rw-r--r--  2.0 unx     1236 b- defN 24-May-17 19:53 thirdai/search.py
+-rw-r--r--  2.0 unx      187 b- defN 24-May-17 19:53 thirdai/licensing.py
+-rw-r--r--  2.0 unx      131 b- defN 24-May-17 19:53 thirdai/hashing.py
+-rw-r--r--  2.0 unx      134 b- defN 24-May-17 19:53 thirdai/distributed_bolt.py
+-rw-r--r--  2.0 unx     5373 b- defN 24-May-30 20:04 thirdai/neural_db_v2/neural_db.py
+-rw-r--r--  2.0 unx      221 b- defN 24-May-17 19:53 thirdai/neural_db_v2/__init__.py
+-rw-r--r--  2.0 unx      145 b- defN 24-May-30 20:04 thirdai/neural_db_v2/retrievers/__init__.py
+-rw-r--r--  2.0 unx     4566 b- defN 24-May-30 20:04 thirdai/neural_db_v2/retrievers/mach_ensemble.py
+-rw-r--r--  2.0 unx     1960 b- defN 24-May-17 19:53 thirdai/neural_db_v2/retrievers/finetunable_retriever.py
+-rw-r--r--  2.0 unx     6813 b- defN 24-May-30 20:04 thirdai/neural_db_v2/retrievers/mach.py
+-rw-r--r--  2.0 unx      224 b- defN 24-May-17 19:53 thirdai/neural_db_v2/core/documents.py
+-rw-r--r--  2.0 unx     1839 b- defN 24-May-30 20:04 thirdai/neural_db_v2/core/retriever.py
+-rw-r--r--  2.0 unx        0 b- defN 24-May-17 19:53 thirdai/neural_db_v2/core/__init__.py
+-rw-r--r--  2.0 unx     6144 b- defN 24-May-17 19:53 thirdai/neural_db_v2/core/types.py
+-rw-r--r--  2.0 unx      796 b- defN 24-May-17 19:53 thirdai/neural_db_v2/core/supervised.py
+-rw-r--r--  2.0 unx     2060 b- defN 24-May-17 19:53 thirdai/neural_db_v2/core/chunk_store.py
+-rw-r--r--  2.0 unx        0 b- defN 24-May-17 19:53 thirdai/neural_db_v2/utils/__init__.py
+-rw-r--r--  2.0 unx       95 b- defN 24-May-17 19:53 thirdai/neural_db_v2/supervised/__init__.py
+-rw-r--r--  2.0 unx     1093 b- defN 24-May-17 19:53 thirdai/neural_db_v2/supervised/csv_supervised.py
+-rw-r--r--  2.0 unx      706 b- defN 24-May-17 19:53 thirdai/neural_db_v2/supervised/in_memory_supervised.py
+-rw-r--r--  2.0 unx     5948 b- defN 24-May-17 19:53 thirdai/neural_db_v2/chunk_stores/pandas_chunk_store.py
+-rw-r--r--  2.0 unx       98 b- defN 24-May-17 19:53 thirdai/neural_db_v2/chunk_stores/__init__.py
+-rw-r--r--  2.0 unx    12610 b- defN 24-May-17 19:53 thirdai/neural_db_v2/chunk_stores/sqlite_chunk_store.py
+-rw-r--r--  2.0 unx     2099 b- defN 24-May-17 19:53 thirdai/neural_db_v2/chunk_stores/constraints.py
+-rw-r--r--  2.0 unx     1114 b- defN 24-May-17 19:53 thirdai/neural_db_v2/documents/docx.py
+-rw-r--r--  2.0 unx     2274 b- defN 24-May-17 19:53 thirdai/neural_db_v2/documents/unstructured.py
+-rw-r--r--  2.0 unx     1265 b- defN 24-May-17 19:53 thirdai/neural_db_v2/documents/in_memory_text.py
+-rw-r--r--  2.0 unx      353 b- defN 24-May-17 19:53 thirdai/neural_db_v2/documents/__init__.py
+-rw-r--r--  2.0 unx     2236 b- defN 24-May-17 19:53 thirdai/neural_db_v2/documents/pdf.py
+-rw-r--r--  2.0 unx     1369 b- defN 24-May-17 19:53 thirdai/neural_db_v2/documents/url.py
+-rw-r--r--  2.0 unx      811 b- defN 24-May-17 19:53 thirdai/neural_db_v2/documents/utils.py
+-rw-r--r--  2.0 unx     2398 b- defN 24-May-17 19:53 thirdai/neural_db_v2/documents/csv.py
+-rw-r--r--  2.0 unx     1647 b- defN 24-May-17 19:53 thirdai/neural_db/sql_helpers.py
+-rw-r--r--  2.0 unx     6680 b- defN 24-May-17 19:53 thirdai/neural_db/connectors.py
+-rw-r--r--  2.0 unx    43752 b- defN 24-May-21 21:21 thirdai/neural_db/neural_db.py
+-rw-r--r--  2.0 unx     9633 b- defN 24-May-17 19:53 thirdai/neural_db/supervised_datasource.py
+-rw-r--r--  2.0 unx     4988 b- defN 24-May-21 21:21 thirdai/neural_db/savable_state.py
+-rw-r--r--  2.0 unx     3474 b- defN 24-May-17 19:53 thirdai/neural_db/loggers.py
+-rw-r--r--  2.0 unx      915 b- defN 24-May-21 21:21 thirdai/neural_db/teachers.py
+-rw-r--r--  2.0 unx    91379 b- defN 24-May-30 20:04 thirdai/neural_db/documents.py
+-rw-r--r--  2.0 unx     2613 b- defN 24-May-17 22:16 thirdai/neural_db/inverted_index.py
+-rw-r--r--  2.0 unx      708 b- defN 24-May-17 19:53 thirdai/neural_db/__init__.py
+-rw-r--r--  2.0 unx      658 b- defN 24-May-17 19:53 thirdai/neural_db/question_generation.py
+-rw-r--r--  2.0 unx     3711 b- defN 24-May-17 19:53 thirdai/neural_db/utils.py
+-rw-r--r--  2.0 unx    10200 b- defN 24-May-17 19:53 thirdai/neural_db/sharded_documents.py
+-rw-r--r--  2.0 unx     7869 b- defN 24-May-17 19:53 thirdai/neural_db/table.py
+-rw-r--r--  2.0 unx     8876 b- defN 24-May-17 19:53 thirdai/neural_db/constraint_matcher.py
+-rw-r--r--  2.0 unx      564 b- defN 24-May-21 21:21 thirdai/neural_db/models/models.py
+-rw-r--r--  2.0 unx       23 b- defN 24-May-17 19:53 thirdai/neural_db/models/__init__.py
+-rw-r--r--  2.0 unx     1422 b- defN 24-May-17 19:53 thirdai/neural_db/models/mach_defaults.py
+-rw-r--r--  2.0 unx     3313 b- defN 24-May-21 21:21 thirdai/neural_db/models/finetunable_retriever.py
+-rw-r--r--  2.0 unx     7619 b- defN 24-May-21 21:21 thirdai/neural_db/models/multi_mach.py
+-rw-r--r--  2.0 unx     5392 b- defN 24-May-21 21:21 thirdai/neural_db/models/model_interface.py
+-rw-r--r--  2.0 unx    26811 b- defN 24-May-21 21:21 thirdai/neural_db/models/mach.py
+-rw-r--r--  2.0 unx    27476 b- defN 24-May-21 21:21 thirdai/neural_db/models/mach_mixture_model.py
+-rw-r--r--  2.0 unx    19358 b- defN 24-May-17 19:53 thirdai/neural_db/model_bazaar/bazaar_base.py
+-rw-r--r--  2.0 unx    35091 b- defN 24-May-21 20:27 thirdai/neural_db/model_bazaar/bazaar_client.py
+-rw-r--r--  2.0 unx       86 b- defN 24-May-17 19:53 thirdai/neural_db/model_bazaar/__init__.py
+-rw-r--r--  2.0 unx     3858 b- defN 24-May-17 19:53 thirdai/neural_db/model_bazaar/utils.py
+-rw-r--r--  2.0 unx     7228 b- defN 24-May-17 19:53 thirdai/neural_db/parsing_utils/unstructured_parse.py
+-rw-r--r--  2.0 unx       44 b- defN 24-May-17 19:53 thirdai/neural_db/parsing_utils/__init__.py
+-rw-r--r--  2.0 unx     5866 b- defN 24-May-17 19:53 thirdai/neural_db/parsing_utils/pdf_parse.py
+-rw-r--r--  2.0 unx     3937 b- defN 24-May-17 19:53 thirdai/neural_db/parsing_utils/url_parse.py
+-rw-r--r--  2.0 unx     2678 b- defN 24-May-17 19:53 thirdai/neural_db/parsing_utils/utils.py
+-rw-r--r--  2.0 unx    11144 b- defN 24-May-17 19:53 thirdai/neural_db/parsing_utils/sliding_pdf_parse.py
+-rw-r--r--  2.0 unx     1430 b- defN 24-May-17 19:53 thirdai/neural_db/parsing_utils/doc_parse.py
+-rw-r--r--  2.0 unx     7590 b- defN 24-May-17 19:53 thirdai/neural_db/trainer/training_progress_tracker.py
+-rw-r--r--  2.0 unx     3922 b- defN 24-May-17 19:53 thirdai/neural_db/trainer/checkpoint_config.py
+-rw-r--r--  2.0 unx        0 b- defN 24-May-17 19:53 thirdai/neural_db/trainer/__init__.py
+-rw-r--r--  2.0 unx    11622 b- defN 24-May-17 19:53 thirdai/neural_db/trainer/training_progress_manager.py
+-rw-r--r--  2.0 unx     7755 b- defN 24-May-17 19:53 thirdai/neural_db/trainer/training_data_manager.py
+-rw-r--r--  2.0 unx      561 b- defN 24-May-17 22:16 thirdai/bolt/__init__.py
+-rw-r--r--  2.0 unx    10517 b- defN 24-May-17 19:53 thirdai/bolt/udt_docs.py
+-rw-r--r--  2.0 unx     2271 b- defN 24-May-17 22:16 thirdai/bolt/ner_modifications.py
+-rw-r--r--  2.0 unx    16696 b- defN 24-May-17 19:53 thirdai/bolt/seismic_modifications.py
+-rw-r--r--  2.0 unx    13874 b- defN 24-May-17 19:53 thirdai/bolt/udt_modifications.py
+-rw-r--r--  2.0 unx     1068 b- defN 24-May-17 19:53 thirdai/dataset/data_source.py
+-rw-r--r--  2.0 unx      391 b- defN 24-May-17 19:53 thirdai/dataset/__init__.py
+-rw-r--r--  2.0 unx     2340 b- defN 24-May-17 19:53 thirdai/dataset/ray_data_source.py
+-rw-r--r--  2.0 unx     2515 b- defN 24-May-17 22:16 thirdai/dataset/bolt_ner_data_source.py
+-rw-r--r--  2.0 unx     3218 b- defN 24-May-17 19:53 thirdai/dataset/csv_data_source.py
+-rw-r--r--  2.0 unx     1500 b- defN 24-May-17 19:53 thirdai/dataset/parquet_data_source.py
+-rw-r--r--  2.0 unx     2359 b- defN 24-May-17 19:53 thirdai/dataset/llm_data_source.py
+-rw-r--r--  2.0 unx     1230 b- defN 24-May-17 19:53 thirdai/gen/questions.py
+-rw-r--r--  2.0 unx       25 b- defN 24-May-17 19:53 thirdai/gen/__init__.py
+-r--r--r--  2.0 unx   837536 b- defN 24-May-30 22:50 thirdai/.dylibs/libssl.3.dylib
+-r--r--r--  2.0 unx   822960 b- defN 24-May-30 22:50 thirdai/.dylibs/libomp.dylib
+-rw-r--r--  2.0 unx  4200880 b- defN 24-May-30 22:50 thirdai/.dylibs/libcrypto.3.dylib
+-rw-r--r--  2.0 unx      302 b- defN 24-May-17 19:53 thirdai/_distributed_bolt/__init__.py
+-rw-r--r--  2.0 unx     3693 b- defN 24-May-17 19:53 thirdai/_distributed_bolt/distributed.py
+-rw-r--r--  2.0 unx      914 b- defN 24-May-17 19:53 thirdai/_distributed_bolt/utils.py
+-rw-r--r--  2.0 unx      704 b- defN 24-May-17 19:53 thirdai/_distributed_bolt/ray_trainer/train_loop_utils.py
+-rw-r--r--  2.0 unx     3146 b- defN 24-May-17 19:53 thirdai/_distributed_bolt/ray_trainer/bolt_checkpoint.py
+-rw-r--r--  2.0 unx        0 b- defN 24-May-17 19:53 thirdai/_distributed_bolt/ray_trainer/__init__.py
+-rw-r--r--  2.0 unx     3367 b- defN 24-May-17 19:53 thirdai/_distributed_bolt/ray_trainer/bolt_trainer.py
+-rw-r--r--  2.0 unx        0 b- defN 24-May-17 19:53 thirdai/_deps/__init__.py
+-rw-r--r--  2.0 unx        0 b- defN 24-May-17 19:53 thirdai/_deps/ColBERT/__init__.py
+-rw-r--r--  2.0 unx        0 b- defN 24-May-17 19:53 thirdai/_deps/ColBERT/colbertutils/__init__.py
+-rw-r--r--  2.0 unx      306 b- defN 24-May-17 19:53 thirdai/_deps/ColBERT/colbertutils/utils.py
+-rw-r--r--  2.0 unx      296 b- defN 24-May-17 19:53 thirdai/_deps/ColBERT/colbertconfig/config.py
+-rw-r--r--  2.0 unx     2047 b- defN 24-May-17 19:53 thirdai/_deps/ColBERT/colbertconfig/base_config.py
+-rw-r--r--  2.0 unx     2237 b- defN 24-May-17 19:53 thirdai/_deps/ColBERT/colbertconfig/core_config.py
+-rw-r--r--  2.0 unx       46 b- defN 24-May-17 19:53 thirdai/_deps/ColBERT/colbertconfig/__init__.py
+-rw-r--r--  2.0 unx     3729 b- defN 24-May-17 19:53 thirdai/_deps/ColBERT/colbertconfig/settings.py
+-rw-r--r--  2.0 unx     1372 b- defN 24-May-17 19:53 thirdai/_deps/ColBERT/colbertmodeling/checkpoint.py
+-rw-r--r--  2.0 unx     1223 b- defN 24-May-17 19:53 thirdai/_deps/ColBERT/colbertmodeling/base_colbert.py
+-rw-r--r--  2.0 unx        0 b- defN 24-May-17 19:53 thirdai/_deps/ColBERT/colbertmodeling/__init__.py
+-rw-r--r--  2.0 unx     2111 b- defN 24-May-17 19:53 thirdai/_deps/ColBERT/colbertmodeling/hf_colbert.py
+-rw-r--r--  2.0 unx     2202 b- defN 24-May-17 19:53 thirdai/_deps/ColBERT/colbertmodeling/colbert.py
+-rw-r--r--  2.0 unx     1997 b- defN 24-May-17 19:53 thirdai/_deps/ColBERT/colbertmodeling/tokenization/doc_tokenization.py
+-rw-r--r--  2.0 unx     3187 b- defN 24-May-17 19:53 thirdai/_deps/ColBERT/colbertmodeling/tokenization/query_tokenization.py
+-rw-r--r--  2.0 unx      166 b- defN 24-May-17 19:53 thirdai/_deps/ColBERT/colbertmodeling/tokenization/__init__.py
+-rw-r--r--  2.0 unx     5792 b- defN 24-May-17 19:53 thirdai/telemetry/telemetry_start_and_stop.py
+-rw-r--r--  2.0 unx     5122 b- defN 24-May-17 19:53 thirdai/telemetry/telemetry_daemon.py
+-rw-r--r--  2.0 unx       50 b- defN 24-May-17 19:53 thirdai/telemetry/__init__.py
+-rw-r--r--  2.0 unx     5589 b- defN 24-May-17 19:53 thirdai/data/type_inference.py
+-rw-r--r--  2.0 unx      507 b- defN 24-May-17 19:53 thirdai/data/__init__.py
+-rw-r--r--  2.0 unx     1905 b- defN 24-May-17 19:53 thirdai/data/column_map_utils.py
+-rw-r--r--  2.0 unx     2408 b- defN 24-May-17 19:53 thirdai/data/get_udt_columns.py
+-rw-r--r--  2.0 unx      504 b- defN 24-May-17 19:53 thirdai/demos/download_tokenizer_vocabs.py
+-rw-r--r--  2.0 unx       74 b- defN 24-May-17 19:53 thirdai/demos/__init__.py
+-rw-r--r--  2.0 unx    23993 b- defN 24-May-21 03:15 thirdai/demos/download_datasets.py
+-rw-r--r--  2.0 unx     8505 b- defN 24-May-17 19:53 thirdai/demos/beir_download_utils.py
+133 files, 17377056 bytes uncompressed, 6737093 bytes compressed:  61.2%
```

## zipnote {}

```diff
@@ -1,484 +1,400 @@
-Filename: thirdai/
+Filename: thirdai-0.8.6.dist-info/RECORD
 Comment: 
 
-Filename: thirdai-0.8.5.dist-info/
+Filename: thirdai-0.8.6.dist-info/WHEEL
 Comment: 
 
-Filename: thirdai.libs/
+Filename: thirdai-0.8.6.dist-info/top_level.txt
 Comment: 
 
-Filename: thirdai/bolt/
+Filename: thirdai-0.8.6.dist-info/LICENSE.txt
 Comment: 
 
-Filename: thirdai/data/
+Filename: thirdai-0.8.6.dist-info/METADATA
 Comment: 
 
-Filename: thirdai/dataset/
-Comment: 
-
-Filename: thirdai/demos/
+Filename: thirdai/deployment.py
 Comment: 
 
-Filename: thirdai/gen/
+Filename: thirdai/_thirdai.cpython-39-darwin.so
 Comment: 
 
-Filename: thirdai/neural_db/
+Filename: thirdai/__init__.py
 Comment: 
 
-Filename: thirdai/neural_db_v2/
+Filename: thirdai/_download.py
 Comment: 
 
-Filename: thirdai/telemetry/
+Filename: thirdai/embeddings.py
 Comment: 
 
-Filename: thirdai/_deps/
+Filename: thirdai/search.py
 Comment: 
 
-Filename: thirdai/_distributed_bolt/
+Filename: thirdai/licensing.py
 Comment: 
 
-Filename: thirdai/deployment.py
+Filename: thirdai/hashing.py
 Comment: 
 
 Filename: thirdai/distributed_bolt.py
 Comment: 
 
-Filename: thirdai/embeddings.py
+Filename: thirdai/neural_db_v2/neural_db.py
 Comment: 
 
-Filename: thirdai/hashing.py
+Filename: thirdai/neural_db_v2/__init__.py
 Comment: 
 
-Filename: thirdai/licensing.py
+Filename: thirdai/neural_db_v2/retrievers/__init__.py
 Comment: 
 
-Filename: thirdai/search.py
+Filename: thirdai/neural_db_v2/retrievers/mach_ensemble.py
 Comment: 
 
-Filename: thirdai/_download.py
+Filename: thirdai/neural_db_v2/retrievers/finetunable_retriever.py
 Comment: 
 
-Filename: thirdai/_thirdai.cp39-win_amd64.pyd
+Filename: thirdai/neural_db_v2/retrievers/mach.py
 Comment: 
 
-Filename: thirdai/__init__.py
+Filename: thirdai/neural_db_v2/core/documents.py
 Comment: 
 
-Filename: thirdai/bolt/ner_modifications.py
+Filename: thirdai/neural_db_v2/core/retriever.py
 Comment: 
 
-Filename: thirdai/bolt/seismic_modifications.py
+Filename: thirdai/neural_db_v2/core/__init__.py
 Comment: 
 
-Filename: thirdai/bolt/udt_docs.py
+Filename: thirdai/neural_db_v2/core/types.py
 Comment: 
 
-Filename: thirdai/bolt/udt_modifications.py
+Filename: thirdai/neural_db_v2/core/supervised.py
 Comment: 
 
-Filename: thirdai/bolt/__init__.py
+Filename: thirdai/neural_db_v2/core/chunk_store.py
 Comment: 
 
-Filename: thirdai/data/column_map_utils.py
+Filename: thirdai/neural_db_v2/utils/__init__.py
 Comment: 
 
-Filename: thirdai/data/get_udt_columns.py
+Filename: thirdai/neural_db_v2/supervised/__init__.py
 Comment: 
 
-Filename: thirdai/data/type_inference.py
+Filename: thirdai/neural_db_v2/supervised/csv_supervised.py
 Comment: 
 
-Filename: thirdai/data/__init__.py
+Filename: thirdai/neural_db_v2/supervised/in_memory_supervised.py
 Comment: 
 
-Filename: thirdai/dataset/bolt_ner_data_source.py
+Filename: thirdai/neural_db_v2/chunk_stores/pandas_chunk_store.py
 Comment: 
 
-Filename: thirdai/dataset/csv_data_source.py
+Filename: thirdai/neural_db_v2/chunk_stores/__init__.py
 Comment: 
 
-Filename: thirdai/dataset/data_source.py
+Filename: thirdai/neural_db_v2/chunk_stores/sqlite_chunk_store.py
 Comment: 
 
-Filename: thirdai/dataset/llm_data_source.py
+Filename: thirdai/neural_db_v2/chunk_stores/constraints.py
 Comment: 
 
-Filename: thirdai/dataset/parquet_data_source.py
+Filename: thirdai/neural_db_v2/documents/docx.py
 Comment: 
 
-Filename: thirdai/dataset/ray_data_source.py
+Filename: thirdai/neural_db_v2/documents/unstructured.py
 Comment: 
 
-Filename: thirdai/dataset/__init__.py
+Filename: thirdai/neural_db_v2/documents/in_memory_text.py
 Comment: 
 
-Filename: thirdai/demos/beir_download_utils.py
+Filename: thirdai/neural_db_v2/documents/__init__.py
 Comment: 
 
-Filename: thirdai/demos/download_datasets.py
+Filename: thirdai/neural_db_v2/documents/pdf.py
 Comment: 
 
-Filename: thirdai/demos/download_tokenizer_vocabs.py
+Filename: thirdai/neural_db_v2/documents/url.py
 Comment: 
 
-Filename: thirdai/demos/__init__.py
+Filename: thirdai/neural_db_v2/documents/utils.py
 Comment: 
 
-Filename: thirdai/gen/questions.py
+Filename: thirdai/neural_db_v2/documents/csv.py
 Comment: 
 
-Filename: thirdai/gen/__init__.py
+Filename: thirdai/neural_db/sql_helpers.py
 Comment: 
 
-Filename: thirdai/neural_db/models/
+Filename: thirdai/neural_db/connectors.py
 Comment: 
 
-Filename: thirdai/neural_db/model_bazaar/
+Filename: thirdai/neural_db/neural_db.py
 Comment: 
 
-Filename: thirdai/neural_db/parsing_utils/
+Filename: thirdai/neural_db/supervised_datasource.py
 Comment: 
 
-Filename: thirdai/neural_db/trainer/
+Filename: thirdai/neural_db/savable_state.py
 Comment: 
 
-Filename: thirdai/neural_db/connectors.py
+Filename: thirdai/neural_db/loggers.py
 Comment: 
 
-Filename: thirdai/neural_db/constraint_matcher.py
+Filename: thirdai/neural_db/teachers.py
 Comment: 
 
 Filename: thirdai/neural_db/documents.py
 Comment: 
 
 Filename: thirdai/neural_db/inverted_index.py
 Comment: 
 
-Filename: thirdai/neural_db/loggers.py
-Comment: 
-
-Filename: thirdai/neural_db/neural_db.py
+Filename: thirdai/neural_db/__init__.py
 Comment: 
 
 Filename: thirdai/neural_db/question_generation.py
 Comment: 
 
-Filename: thirdai/neural_db/savable_state.py
+Filename: thirdai/neural_db/utils.py
 Comment: 
 
 Filename: thirdai/neural_db/sharded_documents.py
 Comment: 
 
-Filename: thirdai/neural_db/sql_helpers.py
-Comment: 
-
-Filename: thirdai/neural_db/supervised_datasource.py
-Comment: 
-
 Filename: thirdai/neural_db/table.py
 Comment: 
 
-Filename: thirdai/neural_db/teachers.py
-Comment: 
-
-Filename: thirdai/neural_db/utils.py
-Comment: 
-
-Filename: thirdai/neural_db/__init__.py
+Filename: thirdai/neural_db/constraint_matcher.py
 Comment: 
 
-Filename: thirdai/neural_db/models/finetunable_retriever.py
+Filename: thirdai/neural_db/models/models.py
 Comment: 
 
-Filename: thirdai/neural_db/models/mach.py
+Filename: thirdai/neural_db/models/__init__.py
 Comment: 
 
 Filename: thirdai/neural_db/models/mach_defaults.py
 Comment: 
 
-Filename: thirdai/neural_db/models/mach_mixture_model.py
+Filename: thirdai/neural_db/models/finetunable_retriever.py
 Comment: 
 
-Filename: thirdai/neural_db/models/models.py
+Filename: thirdai/neural_db/models/multi_mach.py
 Comment: 
 
 Filename: thirdai/neural_db/models/model_interface.py
 Comment: 
 
-Filename: thirdai/neural_db/models/multi_mach.py
+Filename: thirdai/neural_db/models/mach.py
 Comment: 
 
-Filename: thirdai/neural_db/models/__init__.py
+Filename: thirdai/neural_db/models/mach_mixture_model.py
 Comment: 
 
 Filename: thirdai/neural_db/model_bazaar/bazaar_base.py
 Comment: 
 
 Filename: thirdai/neural_db/model_bazaar/bazaar_client.py
 Comment: 
 
-Filename: thirdai/neural_db/model_bazaar/utils.py
-Comment: 
-
 Filename: thirdai/neural_db/model_bazaar/__init__.py
 Comment: 
 
-Filename: thirdai/neural_db/parsing_utils/doc_parse.py
+Filename: thirdai/neural_db/model_bazaar/utils.py
 Comment: 
 
-Filename: thirdai/neural_db/parsing_utils/pdf_parse.py
+Filename: thirdai/neural_db/parsing_utils/unstructured_parse.py
 Comment: 
 
-Filename: thirdai/neural_db/parsing_utils/sliding_pdf_parse.py
+Filename: thirdai/neural_db/parsing_utils/__init__.py
 Comment: 
 
-Filename: thirdai/neural_db/parsing_utils/unstructured_parse.py
+Filename: thirdai/neural_db/parsing_utils/pdf_parse.py
 Comment: 
 
 Filename: thirdai/neural_db/parsing_utils/url_parse.py
 Comment: 
 
 Filename: thirdai/neural_db/parsing_utils/utils.py
 Comment: 
 
-Filename: thirdai/neural_db/parsing_utils/__init__.py
-Comment: 
-
-Filename: thirdai/neural_db/trainer/checkpoint_config.py
-Comment: 
-
-Filename: thirdai/neural_db/trainer/training_data_manager.py
+Filename: thirdai/neural_db/parsing_utils/sliding_pdf_parse.py
 Comment: 
 
-Filename: thirdai/neural_db/trainer/training_progress_manager.py
+Filename: thirdai/neural_db/parsing_utils/doc_parse.py
 Comment: 
 
 Filename: thirdai/neural_db/trainer/training_progress_tracker.py
 Comment: 
 
-Filename: thirdai/neural_db/trainer/__init__.py
-Comment: 
-
-Filename: thirdai/neural_db_v2/chunk_stores/
-Comment: 
-
-Filename: thirdai/neural_db_v2/core/
-Comment: 
-
-Filename: thirdai/neural_db_v2/documents/
-Comment: 
-
-Filename: thirdai/neural_db_v2/retrievers/
-Comment: 
-
-Filename: thirdai/neural_db_v2/supervised/
-Comment: 
-
-Filename: thirdai/neural_db_v2/utils/
-Comment: 
-
-Filename: thirdai/neural_db_v2/neural_db.py
-Comment: 
-
-Filename: thirdai/neural_db_v2/__init__.py
-Comment: 
-
-Filename: thirdai/neural_db_v2/chunk_stores/constraints.py
-Comment: 
-
-Filename: thirdai/neural_db_v2/chunk_stores/pandas_chunk_store.py
+Filename: thirdai/neural_db/trainer/checkpoint_config.py
 Comment: 
 
-Filename: thirdai/neural_db_v2/chunk_stores/sqlite_chunk_store.py
+Filename: thirdai/neural_db/trainer/__init__.py
 Comment: 
 
-Filename: thirdai/neural_db_v2/chunk_stores/__init__.py
+Filename: thirdai/neural_db/trainer/training_progress_manager.py
 Comment: 
 
-Filename: thirdai/neural_db_v2/core/chunk_store.py
+Filename: thirdai/neural_db/trainer/training_data_manager.py
 Comment: 
 
-Filename: thirdai/neural_db_v2/core/documents.py
+Filename: thirdai/bolt/__init__.py
 Comment: 
 
-Filename: thirdai/neural_db_v2/core/retriever.py
+Filename: thirdai/bolt/udt_docs.py
 Comment: 
 
-Filename: thirdai/neural_db_v2/core/supervised.py
+Filename: thirdai/bolt/ner_modifications.py
 Comment: 
 
-Filename: thirdai/neural_db_v2/core/types.py
+Filename: thirdai/bolt/seismic_modifications.py
 Comment: 
 
-Filename: thirdai/neural_db_v2/core/__init__.py
+Filename: thirdai/bolt/udt_modifications.py
 Comment: 
 
-Filename: thirdai/neural_db_v2/documents/csv.py
+Filename: thirdai/dataset/data_source.py
 Comment: 
 
-Filename: thirdai/neural_db_v2/documents/docx.py
+Filename: thirdai/dataset/__init__.py
 Comment: 
 
-Filename: thirdai/neural_db_v2/documents/in_memory_text.py
+Filename: thirdai/dataset/ray_data_source.py
 Comment: 
 
-Filename: thirdai/neural_db_v2/documents/pdf.py
+Filename: thirdai/dataset/bolt_ner_data_source.py
 Comment: 
 
-Filename: thirdai/neural_db_v2/documents/unstructured.py
+Filename: thirdai/dataset/csv_data_source.py
 Comment: 
 
-Filename: thirdai/neural_db_v2/documents/url.py
+Filename: thirdai/dataset/parquet_data_source.py
 Comment: 
 
-Filename: thirdai/neural_db_v2/documents/utils.py
+Filename: thirdai/dataset/llm_data_source.py
 Comment: 
 
-Filename: thirdai/neural_db_v2/documents/__init__.py
+Filename: thirdai/gen/questions.py
 Comment: 
 
-Filename: thirdai/neural_db_v2/retrievers/finetunable_retriever.py
+Filename: thirdai/gen/__init__.py
 Comment: 
 
-Filename: thirdai/neural_db_v2/retrievers/mach.py
+Filename: thirdai/.dylibs/libssl.3.dylib
 Comment: 
 
-Filename: thirdai/neural_db_v2/retrievers/__init__.py
+Filename: thirdai/.dylibs/libomp.dylib
 Comment: 
 
-Filename: thirdai/neural_db_v2/supervised/csv_supervised.py
+Filename: thirdai/.dylibs/libcrypto.3.dylib
 Comment: 
 
-Filename: thirdai/neural_db_v2/supervised/in_memory_supervised.py
+Filename: thirdai/_distributed_bolt/__init__.py
 Comment: 
 
-Filename: thirdai/neural_db_v2/supervised/__init__.py
+Filename: thirdai/_distributed_bolt/distributed.py
 Comment: 
 
-Filename: thirdai/neural_db_v2/utils/__init__.py
+Filename: thirdai/_distributed_bolt/utils.py
 Comment: 
 
-Filename: thirdai/telemetry/telemetry_daemon.py
+Filename: thirdai/_distributed_bolt/ray_trainer/train_loop_utils.py
 Comment: 
 
-Filename: thirdai/telemetry/telemetry_start_and_stop.py
+Filename: thirdai/_distributed_bolt/ray_trainer/bolt_checkpoint.py
 Comment: 
 
-Filename: thirdai/telemetry/__init__.py
+Filename: thirdai/_distributed_bolt/ray_trainer/__init__.py
 Comment: 
 
-Filename: thirdai/_deps/ColBERT/
+Filename: thirdai/_distributed_bolt/ray_trainer/bolt_trainer.py
 Comment: 
 
 Filename: thirdai/_deps/__init__.py
 Comment: 
 
-Filename: thirdai/_deps/ColBERT/colbertconfig/
+Filename: thirdai/_deps/ColBERT/__init__.py
 Comment: 
 
-Filename: thirdai/_deps/ColBERT/colbertmodeling/
+Filename: thirdai/_deps/ColBERT/colbertutils/__init__.py
 Comment: 
 
-Filename: thirdai/_deps/ColBERT/colbertutils/
+Filename: thirdai/_deps/ColBERT/colbertutils/utils.py
 Comment: 
 
-Filename: thirdai/_deps/ColBERT/__init__.py
+Filename: thirdai/_deps/ColBERT/colbertconfig/config.py
 Comment: 
 
 Filename: thirdai/_deps/ColBERT/colbertconfig/base_config.py
 Comment: 
 
-Filename: thirdai/_deps/ColBERT/colbertconfig/config.py
-Comment: 
-
 Filename: thirdai/_deps/ColBERT/colbertconfig/core_config.py
 Comment: 
 
-Filename: thirdai/_deps/ColBERT/colbertconfig/settings.py
-Comment: 
-
 Filename: thirdai/_deps/ColBERT/colbertconfig/__init__.py
 Comment: 
 
-Filename: thirdai/_deps/ColBERT/colbertmodeling/tokenization/
+Filename: thirdai/_deps/ColBERT/colbertconfig/settings.py
 Comment: 
 
-Filename: thirdai/_deps/ColBERT/colbertmodeling/base_colbert.py
+Filename: thirdai/_deps/ColBERT/colbertmodeling/checkpoint.py
 Comment: 
 
-Filename: thirdai/_deps/ColBERT/colbertmodeling/checkpoint.py
+Filename: thirdai/_deps/ColBERT/colbertmodeling/base_colbert.py
 Comment: 
 
-Filename: thirdai/_deps/ColBERT/colbertmodeling/colbert.py
+Filename: thirdai/_deps/ColBERT/colbertmodeling/__init__.py
 Comment: 
 
 Filename: thirdai/_deps/ColBERT/colbertmodeling/hf_colbert.py
 Comment: 
 
-Filename: thirdai/_deps/ColBERT/colbertmodeling/__init__.py
+Filename: thirdai/_deps/ColBERT/colbertmodeling/colbert.py
 Comment: 
 
 Filename: thirdai/_deps/ColBERT/colbertmodeling/tokenization/doc_tokenization.py
 Comment: 
 
 Filename: thirdai/_deps/ColBERT/colbertmodeling/tokenization/query_tokenization.py
 Comment: 
 
 Filename: thirdai/_deps/ColBERT/colbertmodeling/tokenization/__init__.py
 Comment: 
 
-Filename: thirdai/_deps/ColBERT/colbertutils/utils.py
-Comment: 
-
-Filename: thirdai/_deps/ColBERT/colbertutils/__init__.py
-Comment: 
-
-Filename: thirdai/_distributed_bolt/ray_trainer/
-Comment: 
-
-Filename: thirdai/_distributed_bolt/distributed.py
-Comment: 
-
-Filename: thirdai/_distributed_bolt/utils.py
-Comment: 
-
-Filename: thirdai/_distributed_bolt/__init__.py
-Comment: 
-
-Filename: thirdai/_distributed_bolt/ray_trainer/bolt_checkpoint.py
-Comment: 
-
-Filename: thirdai/_distributed_bolt/ray_trainer/bolt_trainer.py
+Filename: thirdai/telemetry/telemetry_start_and_stop.py
 Comment: 
 
-Filename: thirdai/_distributed_bolt/ray_trainer/train_loop_utils.py
+Filename: thirdai/telemetry/telemetry_daemon.py
 Comment: 
 
-Filename: thirdai/_distributed_bolt/ray_trainer/__init__.py
+Filename: thirdai/telemetry/__init__.py
 Comment: 
 
-Filename: thirdai-0.8.5.dist-info/DELVEWHEEL
+Filename: thirdai/data/type_inference.py
 Comment: 
 
-Filename: thirdai-0.8.5.dist-info/LICENSE.txt
+Filename: thirdai/data/__init__.py
 Comment: 
 
-Filename: thirdai-0.8.5.dist-info/METADATA
+Filename: thirdai/data/column_map_utils.py
 Comment: 
 
-Filename: thirdai-0.8.5.dist-info/RECORD
+Filename: thirdai/data/get_udt_columns.py
 Comment: 
 
-Filename: thirdai-0.8.5.dist-info/top_level.txt
+Filename: thirdai/demos/download_tokenizer_vocabs.py
 Comment: 
 
-Filename: thirdai-0.8.5.dist-info/WHEEL
+Filename: thirdai/demos/__init__.py
 Comment: 
 
-Filename: thirdai.libs/.load-order-thirdai-0.8.5
+Filename: thirdai/demos/download_datasets.py
 Comment: 
 
-Filename: thirdai.libs/libomp140.x86_64-05cfcc04b0c6de37e0a3c60db4df4e20.dll
+Filename: thirdai/demos/beir_download_utils.py
 Comment: 
 
 Zip file comment:
```

## filetype from file(1)

```diff
@@ -1 +1 @@
-Zip archive data, at least v2.0 to extract, compression method=store
+Zip archive data, at least v2.0 to extract, compression method=deflate
```

## thirdai/deployment.py

 * *Ordering differences only*

```diff
@@ -1,5 +1,5 @@
-import thirdai._thirdai.deployment
-from thirdai._thirdai.deployment import *
-
-__all__ = []
-__all__.extend(dir(thirdai._thirdai.deployment))
+import thirdai._thirdai.deployment
+from thirdai._thirdai.deployment import *
+
+__all__ = []
+__all__.extend(dir(thirdai._thirdai.deployment))
```

## thirdai/distributed_bolt.py

 * *Ordering differences only*

```diff
@@ -1,5 +1,5 @@
-import thirdai._distributed_bolt
-from thirdai._distributed_bolt import *
-
-__all__ = []
-__all__.extend(dir(thirdai._distributed_bolt))
+import thirdai._distributed_bolt
+from thirdai._distributed_bolt import *
+
+__all__ = []
+__all__.extend(dir(thirdai._distributed_bolt))
```

## thirdai/embeddings.py

 * *Ordering differences only*

```diff
@@ -1,39 +1,39 @@
-try:
-    import numpy as np
-    import torch
-    import transformers
-except ImportError as e:
-    print(
-        "The embeddings package requires the PyTorch, transformers, and numpy "
-        "packages. Please install these before importing the embeddings "
-        "package by e.g. running `pip3 install torch transformers numpy`."
-    )
-    raise e
-
-from thirdai._deps.ColBERT.colbertmodeling.checkpoint import Checkpoint
-from thirdai._download import ensure_targz_installed
-
-MSMARCO_URL = "https://www.dropbox.com/s/s02nev64icelbkr/msmarco.tar.gz?dl=0"
-MSMARCO_DIR_NAME = "msmarco"
-
-
-class DocSearchModel:
-    def __init__(
-        self, local_path=None, download_metadata=(MSMARCO_URL, MSMARCO_DIR_NAME)
-    ):
-        if not local_path:
-            local_path, _ = ensure_targz_installed(
-                download_url=download_metadata[0],
-                unzipped_dir_name=download_metadata[1],
-            )
-        self.checkpoint = Checkpoint(str(local_path)).cpu()
-        self.centroids = np.load(f"{local_path}/centroids.npy")
-
-    def encodeQuery(self, query):
-        return self.checkpoint.queryFromText([query])[0]
-
-    def encodeDocs(self, docs):
-        return self.checkpoint.docFromText(docs)
-
-    def getCentroids(self):
-        return self.centroids
+try:
+    import numpy as np
+    import torch
+    import transformers
+except ImportError as e:
+    print(
+        "The embeddings package requires the PyTorch, transformers, and numpy "
+        "packages. Please install these before importing the embeddings "
+        "package by e.g. running `pip3 install torch transformers numpy`."
+    )
+    raise e
+
+from thirdai._deps.ColBERT.colbertmodeling.checkpoint import Checkpoint
+from thirdai._download import ensure_targz_installed
+
+MSMARCO_URL = "https://www.dropbox.com/s/s02nev64icelbkr/msmarco.tar.gz?dl=0"
+MSMARCO_DIR_NAME = "msmarco"
+
+
+class DocSearchModel:
+    def __init__(
+        self, local_path=None, download_metadata=(MSMARCO_URL, MSMARCO_DIR_NAME)
+    ):
+        if not local_path:
+            local_path, _ = ensure_targz_installed(
+                download_url=download_metadata[0],
+                unzipped_dir_name=download_metadata[1],
+            )
+        self.checkpoint = Checkpoint(str(local_path)).cpu()
+        self.centroids = np.load(f"{local_path}/centroids.npy")
+
+    def encodeQuery(self, query):
+        return self.checkpoint.queryFromText([query])[0]
+
+    def encodeDocs(self, docs):
+        return self.checkpoint.docFromText(docs)
+
+    def getCentroids(self):
+        return self.centroids
```

## thirdai/hashing.py

 * *Ordering differences only*

```diff
@@ -1,5 +1,5 @@
-import thirdai._thirdai.hashing
-from thirdai._thirdai.hashing import *
-
-__all__ = []
-__all__.extend(dir(thirdai._thirdai.hashing))
+import thirdai._thirdai.hashing
+from thirdai._thirdai.hashing import *
+
+__all__ = []
+__all__.extend(dir(thirdai._thirdai.hashing))
```

## thirdai/licensing.py

 * *Ordering differences only*

```diff
@@ -1,8 +1,8 @@
-try:
-    import thirdai._thirdai.licensing
-    from thirdai._thirdai.licensing import *
-
-    __all__ = []
-    __all__.extend(dir(thirdai._thirdai.licensing))
-except ImportError:
-    pass
+try:
+    import thirdai._thirdai.licensing
+    from thirdai._thirdai.licensing import *
+
+    __all__ = []
+    __all__.extend(dir(thirdai._thirdai.licensing))
+except ImportError:
+    pass
```

## thirdai/search.py

 * *Ordering differences only*

```diff
@@ -1,41 +1,41 @@
-import thirdai._thirdai.search
-from thirdai._thirdai.search import *
-
-__all__ = []
-__all__.extend(dir(thirdai._thirdai.search))
-
-
-class EasyQA:
-    """
-    A simple QA system for if you are indexing less than ~100,000 documents.
-    """
-
-    def __init__(self):
-        pass
-
-    def index(self, id_passage_pairs):
-        from thirdai.embeddings import DocSearchModel
-        from thirdai.search import DocRetrieval
-        from tqdm import tqdm
-
-        self.embedding_model = DocSearchModel()
-        reduced_centroids = self.embedding_model.getCentroids()[
-            : max(len(id_passage_pairs) // 1000, 1)
-        ]
-        self.index = DocRetrieval(
-            dense_input_dimension=128,
-            num_tables=16,
-            hashes_per_table=6,
-            centroids=reduced_centroids,
-        )
-        for doc_id, doc_text in tqdm(id_passage_pairs):
-            embedding = self.embedding_model.encodeDocs([doc_text])[0]
-            self.index.add_doc(
-                doc_id=doc_id, doc_text=doc_text, doc_embeddings=embedding
-            )
-
-        return self
-
-    def query(self, question, top_k=1):
-        embedding = self.embedding_model.encodeQuery(question)
-        return self.index.query(embedding, top_k=top_k)
+import thirdai._thirdai.search
+from thirdai._thirdai.search import *
+
+__all__ = []
+__all__.extend(dir(thirdai._thirdai.search))
+
+
+class EasyQA:
+    """
+    A simple QA system for if you are indexing less than ~100,000 documents.
+    """
+
+    def __init__(self):
+        pass
+
+    def index(self, id_passage_pairs):
+        from thirdai.embeddings import DocSearchModel
+        from thirdai.search import DocRetrieval
+        from tqdm import tqdm
+
+        self.embedding_model = DocSearchModel()
+        reduced_centroids = self.embedding_model.getCentroids()[
+            : max(len(id_passage_pairs) // 1000, 1)
+        ]
+        self.index = DocRetrieval(
+            dense_input_dimension=128,
+            num_tables=16,
+            hashes_per_table=6,
+            centroids=reduced_centroids,
+        )
+        for doc_id, doc_text in tqdm(id_passage_pairs):
+            embedding = self.embedding_model.encodeDocs([doc_text])[0]
+            self.index.add_doc(
+                doc_id=doc_id, doc_text=doc_text, doc_embeddings=embedding
+            )
+
+        return self
+
+    def query(self, question, top_k=1):
+        embedding = self.embedding_model.encodeQuery(question)
+        return self.index.query(embedding, top_k=top_k)
```

## thirdai/_download.py

 * *Ordering differences only*

```diff
@@ -1,41 +1,41 @@
-import os
-import pathlib
-
-CACHE_DIR = pathlib.Path.home() / ".cache" / "thirdai"
-
-
-def _unzip_targz(targz_path, destination_parent_folder):
-    os.system(f"tar -xzf {targz_path} -C {destination_parent_folder}")
-
-
-def _download_file(url, download_path):
-    os.system(f"curl -L {url} -o {download_path}")
-
-
-def ensure_targz_installed(download_url, unzipped_dir_name):
-    global CACHE_DIR
-    CACHE_DIR.mkdir(parents=True, exist_ok=True)
-
-    unzipped_dir_path = CACHE_DIR / unzipped_dir_name
-    targz_download_path = CACHE_DIR / "temp.tar.gz"
-
-    cached = False
-    if not unzipped_dir_path.exists():
-        print(
-            f"Downloading {download_url}, which will be unzipped and saved in directory {unzipped_dir_path}"
-        )
-
-        _download_file(download_url, download_path=targz_download_path)
-        _unzip_targz(targz_download_path, destination_parent_folder=CACHE_DIR)
-        targz_download_path.unlink()
-
-        if not unzipped_dir_path.exists():
-            raise ValueError(
-                "Unzipped directory name was different than anticipated, caching will not work as expected."
-            )
-
-    else:
-        cached = True
-        print(f"{unzipped_dir_path} already exists, skipping download.")
-
-    return unzipped_dir_path, cached
+import os
+import pathlib
+
+CACHE_DIR = pathlib.Path.home() / ".cache" / "thirdai"
+
+
+def _unzip_targz(targz_path, destination_parent_folder):
+    os.system(f"tar -xzf {targz_path} -C {destination_parent_folder}")
+
+
+def _download_file(url, download_path):
+    os.system(f"curl -L {url} -o {download_path}")
+
+
+def ensure_targz_installed(download_url, unzipped_dir_name):
+    global CACHE_DIR
+    CACHE_DIR.mkdir(parents=True, exist_ok=True)
+
+    unzipped_dir_path = CACHE_DIR / unzipped_dir_name
+    targz_download_path = CACHE_DIR / "temp.tar.gz"
+
+    cached = False
+    if not unzipped_dir_path.exists():
+        print(
+            f"Downloading {download_url}, which will be unzipped and saved in directory {unzipped_dir_path}"
+        )
+
+        _download_file(download_url, download_path=targz_download_path)
+        _unzip_targz(targz_download_path, destination_parent_folder=CACHE_DIR)
+        targz_download_path.unlink()
+
+        if not unzipped_dir_path.exists():
+            raise ValueError(
+                "Unzipped directory name was different than anticipated, caching will not work as expected."
+            )
+
+    else:
+        cached = True
+        print(f"{unzipped_dir_path} already exists, skipping download.")
+
+    return unzipped_dir_path, cached
```

## thirdai/__init__.py

```diff
@@ -1,81 +1,53 @@
-"""The ThirdAI Python package"""
-
-
-# start delvewheel patch
-def _delvewheel_patch_1_6_0():
-    import ctypes
-    import os
-    import platform
-    import sys
-    libs_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir, 'thirdai.libs'))
-    is_conda_cpython = platform.python_implementation() == 'CPython' and (hasattr(ctypes.pythonapi, 'Anaconda_GetVersion') or 'packaged by conda-forge' in sys.version)
-    if sys.version_info[:2] >= (3, 8) and not is_conda_cpython or sys.version_info[:2] >= (3, 10):
-        if os.path.isdir(libs_dir):
-            os.add_dll_directory(libs_dir)
-    else:
-        load_order_filepath = os.path.join(libs_dir, '.load-order-thirdai-0.8.5')
-        if os.path.isfile(load_order_filepath):
-            with open(os.path.join(libs_dir, '.load-order-thirdai-0.8.5')) as file:
-                load_order = file.read().split()
-            for lib in load_order:
-                lib_path = os.path.join(os.path.join(libs_dir, lib))
-                kernel32 = ctypes.WinDLL('kernel32', use_last_error=True)
-                if os.path.isfile(lib_path) and not kernel32.LoadLibraryExW(ctypes.c_wchar_p(lib_path), None, 0x00000008):
-                    raise OSError('Error loading {}; {}'.format(lib, ctypes.FormatError(ctypes.get_last_error())))
-
-
-_delvewheel_patch_1_6_0()
-del _delvewheel_patch_1_6_0
-# end delvewheel patch
-
-__all__ = [
-    "bolt",
-    "search",
-    "dataset",
-    "data",
-    "hashing",
-    "distributed_bolt",
-    "licensing",
-    "demos",
-    "gen",
-    "telemetry",
-    "set_global_num_threads",
-    "logging",
-]
-
-# Include these so we can use them just by import the top level.
-import thirdai.bolt as bolt
-import thirdai.data as data
-import thirdai.dataset as dataset
-import thirdai.demos as demos
-import thirdai.gen as gen
-import thirdai.hashing as hashing
-import thirdai.licensing as licensing
-import thirdai.search as search
-import thirdai.telemetry as telemetry
-
-# Relay __version__ from C++
-from thirdai._thirdai import __version__, logging, set_seed
-
-try:
-    from thirdai._thirdai import set_global_num_threads
-
-    __all__.extend(["set_global_num_threads"])
-except ImportError:
-    pass
-
-# ray's grcpio dependency installation is not trivial on
-# Apple Mac M1 Silicon and requires conda.
-#
-# See:
-# [1] https://github.com/grpc/grpc/issues/25082,
-# [2] https://docs.ray.io/en/latest/ray-overview/installation.html#m1-mac-apple-silicon-support
-# For the time being users are expected to explictly import the package.
-#
-# TODO(pratkpranav): Uncomment the following when this issue is solved upstream.
-# import thirdai.distributed_bolt
-
-
-# Don't import this or include it in __all__ for now because it requires
-# pytorch + transformers.
-# import thirdai.embeddings
+"""The ThirdAI Python package"""
+
+__all__ = [
+    "bolt",
+    "search",
+    "dataset",
+    "data",
+    "hashing",
+    "distributed_bolt",
+    "licensing",
+    "demos",
+    "gen",
+    "telemetry",
+    "set_global_num_threads",
+    "logging",
+]
+
+# Include these so we can use them just by import the top level.
+import thirdai.bolt as bolt
+import thirdai.data as data
+import thirdai.dataset as dataset
+import thirdai.demos as demos
+import thirdai.gen as gen
+import thirdai.hashing as hashing
+import thirdai.licensing as licensing
+import thirdai.search as search
+import thirdai.telemetry as telemetry
+
+# Relay __version__ from C++
+from thirdai._thirdai import __version__, logging, set_seed
+
+try:
+    from thirdai._thirdai import set_global_num_threads
+
+    __all__.extend(["set_global_num_threads"])
+except ImportError:
+    pass
+
+# ray's grcpio dependency installation is not trivial on
+# Apple Mac M1 Silicon and requires conda.
+#
+# See:
+# [1] https://github.com/grpc/grpc/issues/25082,
+# [2] https://docs.ray.io/en/latest/ray-overview/installation.html#m1-mac-apple-silicon-support
+# For the time being users are expected to explictly import the package.
+#
+# TODO(pratkpranav): Uncomment the following when this issue is solved upstream.
+# import thirdai.distributed_bolt
+
+
+# Don't import this or include it in __all__ for now because it requires
+# pytorch + transformers.
+# import thirdai.embeddings
```

## thirdai/bolt/ner_modifications.py

 * *Ordering differences only*

```diff
@@ -1,69 +1,69 @@
-from typing import Dict, List, Optional, Tuple
-
-import thirdai
-import thirdai._thirdai.bolt as bolt
-from thirdai.dataset.bolt_ner_data_source import NerDataSource
-
-
-def modify_ner():
-    original_train = bolt.NER.train
-    original_get_tags = bolt.NER.get_ner_tags
-
-    def wrapped_train(
-        self,
-        filename,
-        learning_rate: float = 1e-3,
-        epochs: int = 5,
-        batch_size: Optional[int] = 2000,
-        train_metrics: List[str] = ["loss"],
-        validation_file: Optional[str] = None,
-        val_metrics: List[str] = [],
-    ):
-        train_data_source = NerDataSource(
-            model_type=self.type(),
-            tokens_column=self.tokens_column(),
-            tags_column=self.tags_column(),
-            file_path=filename,
-        )
-
-        if validation_file:
-            validation_data_source = NerDataSource(
-                model_type=self.type(),
-                tokens_column=self.tokens_column(),
-                tags_column=self.tags_column(),
-                file_path=validation_file,
-            )
-        else:
-            validation_data_source = None
-
-        return original_train(
-            self,
-            train_data=train_data_source,
-            learning_rate=learning_rate,
-            epochs=epochs,
-            batch_size=batch_size,
-            train_metrics=train_metrics,
-            val_data=validation_data_source,
-            val_metrics=val_metrics,
-        )
-
-    def wrapped_predict_batch(self, tokens: List[List[str]], top_k: int = 1):
-        inference_source = NerDataSource(self.type())
-        featurized_tokens = inference_source.inference_featurizer(tokens)
-        return original_get_tags(self, featurized_tokens, top_k)
-
-    def wrapped_predict(self, tokens: List[str], top_k: int = 1):
-        inference_source = NerDataSource(self.type())
-        featurized_tokens = inference_source.inference_featurizer([tokens])
-        return original_get_tags(self, featurized_tokens, top_k)[0]
-
-    delattr(bolt.NER, "get_ner_tags")
-    delattr(bolt.NER, "train")
-
-    bolt.NER.train = wrapped_train
-    bolt.NER.predict_batch = wrapped_predict_batch
-    bolt.NER.predict = wrapped_predict
-
-    bolt.NER.train_on_data_source = original_train
-
-    bolt.UniversalDeepTransformer.NER = bolt.NER
+from typing import Dict, List, Optional, Tuple
+
+import thirdai
+import thirdai._thirdai.bolt as bolt
+from thirdai.dataset.bolt_ner_data_source import NerDataSource
+
+
+def modify_ner():
+    original_train = bolt.NER.train
+    original_get_tags = bolt.NER.get_ner_tags
+
+    def wrapped_train(
+        self,
+        filename,
+        learning_rate: float = 1e-3,
+        epochs: int = 5,
+        batch_size: Optional[int] = 2000,
+        train_metrics: List[str] = ["loss"],
+        validation_file: Optional[str] = None,
+        val_metrics: List[str] = [],
+    ):
+        train_data_source = NerDataSource(
+            model_type=self.type(),
+            tokens_column=self.tokens_column(),
+            tags_column=self.tags_column(),
+            file_path=filename,
+        )
+
+        if validation_file:
+            validation_data_source = NerDataSource(
+                model_type=self.type(),
+                tokens_column=self.tokens_column(),
+                tags_column=self.tags_column(),
+                file_path=validation_file,
+            )
+        else:
+            validation_data_source = None
+
+        return original_train(
+            self,
+            train_data=train_data_source,
+            learning_rate=learning_rate,
+            epochs=epochs,
+            batch_size=batch_size,
+            train_metrics=train_metrics,
+            val_data=validation_data_source,
+            val_metrics=val_metrics,
+        )
+
+    def wrapped_predict_batch(self, tokens: List[List[str]], top_k: int = 1):
+        inference_source = NerDataSource(self.type())
+        featurized_tokens = inference_source.inference_featurizer(tokens)
+        return original_get_tags(self, featurized_tokens, top_k)
+
+    def wrapped_predict(self, tokens: List[str], top_k: int = 1):
+        inference_source = NerDataSource(self.type())
+        featurized_tokens = inference_source.inference_featurizer([tokens])
+        return original_get_tags(self, featurized_tokens, top_k)[0]
+
+    delattr(bolt.NER, "get_ner_tags")
+    delattr(bolt.NER, "train")
+
+    bolt.NER.train = wrapped_train
+    bolt.NER.predict_batch = wrapped_predict_batch
+    bolt.NER.predict = wrapped_predict
+
+    bolt.NER.train_on_data_source = original_train
+
+    bolt.UniversalDeepTransformer.NER = bolt.NER
```

## thirdai/bolt/seismic_modifications.py

 * *Ordering differences only*

```diff
@@ -1,512 +1,512 @@
-import os
-import time
-from pathlib import Path
-
-import numpy as np
-import pandas as pd
-import scipy.ndimage
-import thirdai
-import thirdai._thirdai.bolt as bolt
-import torch
-import torch.nn.functional as F
-from torch.utils.data import DataLoader, Dataset, default_collate
-
-
-class UnsupervisedSubcubeDataset(Dataset):
-    def __init__(self, subcube_directory, subcube_files, blur_subcube_fraction=0.0):
-        self.subcube_directory = subcube_directory
-        self.subcube_files = subcube_files
-        self.blur_subcube_fraction = blur_subcube_fraction
-
-    def __len__(self):
-        return len(self.subcube_files)
-
-    def __getitem__(self, index):
-        filename = self.subcube_files[index]
-        metadata = UnsupervisedSubcubeDataset.parse_metadata(Path(filename).stem)
-        subcube = np.load(os.path.join(self.subcube_directory, filename))
-        subcube = subcube.astype(np.float32)
-        if self.blur_subcube_fraction > 0:
-            r = np.random.rand()
-            if r < (self.blur_subcube_fraction / 2):
-                subcube = UnsupervisedSubcubeDataset.median_blur(subcube)
-            elif r < self.blur_subcube_fraction:
-                subcube = UnsupervisedSubcubeDataset.gaussian_blur(subcube)
-
-        return subcube, metadata
-
-    @staticmethod
-    def parse_metadata(metadata):
-        volume, x, y, z = metadata.split("_")
-        return (volume, int(x), int(y), int(z))
-
-    @staticmethod
-    def median_blur(subcube):
-        return scipy.ndimage.median_filter(subcube, size=3)
-
-    @staticmethod
-    def gaussian_blur(subcube):
-        blur = np.random.choice(np.arange(1.55, 1.95, 0.15))
-        return scipy.ndimage.gaussian_filter(subcube, sigma=blur).astype(np.float32)
-
-
-class ClassificationSubcubeDataset(Dataset):
-    def __init__(self, sample_index: pd.DataFrame):
-        if (
-            "labels" not in sample_index.columns
-            or "subcube" not in sample_index.columns
-        ):
-            raise ValueError(
-                "Expected sample index to contain the columns 'labels' and 'subcube'."
-            )
-        self.sample_index = sample_index
-
-    def __len__(self):
-        return len(self.sample_index)
-
-    def __getitem__(self, index):
-        subcube_path = self.sample_index["subcube"].iloc[index]
-        subcube = np.load(subcube_path).astype(np.float32)
-        labels = self.sample_index["labels"].iloc[index]
-        return subcube, labels
-
-
-def collate_fn(batch):
-    data, metadata = zip(*batch)
-    return default_collate(data), metadata
-
-
-def convert_to_patches(subcubes, expected_subcube_shape, patch_shape, max_pool=None):
-    if subcubes.shape[1:] != expected_subcube_shape:
-        raise ValueError(
-            f"Expected subcubes with shape {expected_subcube_shape}. But received subcubes with shape {subcubes.shape[1:]}"
-        )
-
-    pd_x, pd_y, pd_z = patch_shape
-    if max_pool:
-        # Unsqueeze/squeeze are to add/remove the 'channels' dimension
-        subcubes = F.max_pool3d(
-            subcubes.unsqueeze(1), kernel_size=max_pool, stride=max_pool
-        )
-        subcubes = subcubes.squeeze_(1)
-        # Scale the patch dim since pooling is applied first.
-        pd_x //= max_pool[0]
-        pd_y //= max_pool[1]
-        pd_z //= max_pool[2]
-
-    n_cubes, x, y, z = subcubes.shape
-    assert x % pd_x == 0
-    assert y % pd_y == 0
-    assert z % pd_z == 0
-
-    pd_flat = pd_x * pd_y * pd_z
-    n_patches = (x * y * z) // pd_flat
-
-    patches = torch.reshape(
-        subcubes, (n_cubes, x // pd_x, pd_x, y // pd_y, pd_y, z // pd_z, pd_z)
-    )
-    patches = torch.permute(patches, (0, 1, 3, 5, 2, 4, 6))
-
-    patches = torch.reshape(patches, (n_cubes, n_patches, pd_flat))
-
-    return patches.numpy()
-
-
-def get_rank_and_world_size():
-    from ray import train
-
-    rank = train.get_context().get_world_rank()
-    world_size = train.get_context().get_world_size()
-    return rank, world_size
-
-
-def subcube_range_for_worker(n_subcubes: int):
-    rank, world_size = get_rank_and_world_size()
-
-    subcubes_for_worker = n_subcubes // world_size
-    if rank < (n_subcubes % world_size):
-        subcubes_for_worker += 1
-
-    offset = (n_subcubes // world_size * rank) + min(n_subcubes % world_size, rank)
-
-    return offset, offset + subcubes_for_worker
-
-
-def log(msg):
-    thirdai.logging.info(msg)
-    print(msg, flush=True)
-
-
-class TimedIterator:
-    def __init__(self, obj):
-        self.iter = iter(obj)
-
-    def __iter__(self):
-        return self
-
-    def __next__(self):
-        start = time.perf_counter()
-        out = next(self.iter)
-        end = time.perf_counter()
-        log(f"Loaded {len(out[0])} subcubes in {end-start} seconds.")
-        return out
-
-
-def train_seismic_model(
-    seismic_model,
-    dataset: Dataset,
-    learning_rate: float,
-    epochs: int,
-    batch_size: int,
-    checkpoint_dir: str = None,
-    checkpoint_interval: int = 1000,
-    log_interval=20,
-    validation_fn=None,
-    max_data_in_memory=30,  # In Gb
-    comm=None,
-):
-    callbacks = []
-    if checkpoint_dir:
-        if not os.path.exists(checkpoint_dir):
-            os.makedirs(checkpoint_dir)
-        callbacks = [
-            bolt.seismic.Checkpoint(
-                seismic_model=seismic_model,
-                checkpoint_dir=checkpoint_dir,
-                interval=checkpoint_interval,
-            )
-        ]
-
-    # Number of bytes per subcube
-    subcube_size = np.prod(seismic_model.subcube_shape) * 4
-    # Load less than 30Gb of subcubes
-    n_subcubes_per_chunk = min(
-        int((10**9) * max_data_in_memory / subcube_size), len(dataset)
-    )
-
-    data_loader = DataLoader(
-        dataset=dataset,
-        batch_size=n_subcubes_per_chunk,
-        shuffle=True,
-        num_workers=2,
-        collate_fn=collate_fn,
-    )
-
-    output_metrics = {"epoch_times": [], "train_loss": []}
-
-    for epoch in range(epochs):
-        epoch_start = time.perf_counter()
-
-        for subcubes, label_or_metadata in TimedIterator(data_loader):
-            patch_start = time.perf_counter()
-
-            subcubes = convert_to_patches(
-                subcubes=subcubes,
-                expected_subcube_shape=seismic_model.subcube_shape,
-                patch_shape=seismic_model.patch_shape,
-                max_pool=seismic_model.max_pool,
-            )
-
-            patch_end = time.perf_counter()
-
-            log(
-                f"Converted {subcubes.shape[0]} subcubes to patches in {patch_end - patch_start} seconds.",
-            )
-
-            metrics = seismic_model.train_on_patches(
-                subcubes,
-                # We call this label or metadata becuase in supervised training this will contain
-                # the labels, but in unsupervised training we just use it to pass in metadata
-                # about the subcube. Doing it this way saves having to duplicate a lot of code for
-                # this method.
-                label_or_metadata,
-                learning_rate=learning_rate,
-                batch_size=batch_size,
-                callbacks=callbacks,
-                log_interval=log_interval,
-                comm=comm,
-            )
-
-        epoch_end = time.perf_counter()
-
-        epoch_time = epoch_end - epoch_start
-        output_metrics["epoch_times"].append(epoch_time)
-        train_loss = metrics["train_loss"][-1]
-        output_metrics["train_loss"].append(train_loss)
-
-        log(
-            f"train | completed epoch {epoch} | train_loss={train_loss} | time={epoch_time} "
-        )
-
-        if validation_fn:
-            validation_fn(seismic_model)
-
-        if checkpoint_dir:
-            seismic_model.save(os.path.join(checkpoint_dir, f"epoch_{epoch}_end"))
-
-    return output_metrics
-
-
-def train_embedding_model(
-    self,
-    subcube_directory: str,
-    learning_rate: float,
-    epochs: int,
-    batch_size: int,
-    checkpoint_dir: str = None,
-    checkpoint_interval: int = 1000,
-    log_interval=20,
-    validation_fn=None,
-    blur_subcubes_fraction=0.0,
-    max_data_in_memory=30,  # In Gb
-    comm=None,
-):
-    subcube_files = [
-        file for file in os.listdir(subcube_directory) if file.endswith(".npy")
-    ]
-
-    if not subcube_files:
-        raise ValueError(f"Could not find any .npy files in {subcube_directory}.")
-
-    if comm:
-        # For distributed training give each worker a seperate partition of the subcubes.
-        worker_start, worker_end = subcube_range_for_worker(len(subcube_files))
-        subcube_files = subcube_files[worker_start:worker_end]
-
-    dataset = UnsupervisedSubcubeDataset(
-        subcube_directory=subcube_directory,
-        subcube_files=subcube_files,
-        blur_subcube_fraction=blur_subcubes_fraction,
-    )
-
-    return train_seismic_model(
-        seismic_model=self,
-        dataset=dataset,
-        learning_rate=learning_rate,
-        epochs=epochs,
-        batch_size=batch_size,
-        checkpoint_dir=checkpoint_dir,
-        checkpoint_interval=checkpoint_interval,
-        log_interval=log_interval,
-        validation_fn=validation_fn,
-        max_data_in_memory=max_data_in_memory,
-        comm=comm,
-    )
-
-
-def train_classifier(
-    self,
-    sample_index_file: str,
-    learning_rate: float,
-    epochs: int,
-    batch_size: int,
-    checkpoint_dir: str = None,
-    checkpoint_interval: int = 1000,
-    log_interval=20,
-    validation_fn=None,
-    blur_subcubes_fraction=0.0,  # Unused
-    max_data_in_memory=30,  # In Gb
-    comm=None,
-):
-    sample_index = pd.read_csv(sample_index_file)
-    sample_index = sample_index.sample(frac=1.0)
-    if sample_index["labels"].dtype == object:
-        sample_index["labels"] = sample_index["labels"].apply(
-            lambda x: list(map(int, x.split(" ")))
-        )
-    elif sample_index["labels"].dtype == int:
-        sample_index["labels"] = sample_index["labels"].apply(lambda x: [x])
-
-    if comm:
-        if not sample_index["subcube"].apply(os.path.isabs).all():
-            raise ValueError(
-                "Subcube files in sample index must be specified as absolute paths for distributed training so that they can be accessed from each worker."
-            )
-
-    if comm:
-        # For distributed training give each worker a seperate partition of the subcubes.
-        worker_start, worker_end = subcube_range_for_worker(len(sample_index))
-        sample_index = sample_index.iloc[worker_start:worker_end]
-
-    dataset = ClassificationSubcubeDataset(sample_index=sample_index)
-
-    return train_seismic_model(
-        seismic_model=self,
-        dataset=dataset,
-        learning_rate=learning_rate,
-        epochs=epochs,
-        batch_size=batch_size,
-        checkpoint_dir=checkpoint_dir,
-        checkpoint_interval=checkpoint_interval,
-        log_interval=log_interval,
-        validation_fn=validation_fn,
-        max_data_in_memory=max_data_in_memory,
-        comm=comm,
-    )
-
-
-def train_distributed(
-    self,
-    data_path: str,
-    learning_rate: float,
-    epochs: int,
-    batch_size: int,
-    run_config,
-    scaling_config,
-    log_file: str,
-    checkpoint_dir: str,
-    log_interval: int = 20,
-    checkpoint_interval: int = 1000,
-    validation_fn=None,
-    blur_subcubes_fraction=0.0,
-    max_data_in_memory=30,  # In Gb
-    communication_backend: str = "gloo",
-):
-    import ray
-    import thirdai.distributed_bolt as dist
-    from ray.train.torch import TorchConfig
-
-    from .._distributed_bolt.distributed import Communication
-
-    def train_loop_per_worker(config):
-        import ray
-        from ray import train
-
-        rank, world_size = get_rank_and_world_size()
-
-        config["licensing_lambda"]()
-
-        log_file = config["log_file"]
-        if rank != 0:
-            log_file += f".worker_{rank}"
-        thirdai.logging.setup(log_to_stderr=False, path=log_file, level="info")
-
-        model = ray.get(config["model_ref"])
-
-        metrics = model.train(
-            config["data_path"],
-            learning_rate=config["learning_rate"],
-            epochs=config["epochs"],
-            batch_size=config["batch_size"] // world_size,
-            checkpoint_dir=config["checkpoint_dir"] if rank == 0 else None,
-            checkpoint_interval=config["checkpoint_interval"],
-            log_interval=config["log_interval"],
-            validation_fn=config["validation_fn"] if rank == 0 else None,
-            blur_subcubes_fraction=config["blur_subcubes_fraction"],
-            max_data_in_memory=config["max_data_in_memory"],
-            comm=Communication(),
-        )
-
-        checkpoint = None
-        if rank == 0:
-            checkpoint = dist.BoltCheckPoint.from_model(model.model)
-
-        train.report(metrics=metrics, checkpoint=checkpoint)
-
-    config = {
-        "model_ref": ray.put(self),
-        "data_path": os.path.abspath(data_path),
-        "learning_rate": learning_rate,
-        "epochs": epochs,
-        "batch_size": batch_size,
-        "log_file": os.path.abspath(log_file),
-        "log_interval": log_interval,
-        "checkpoint_dir": os.path.abspath(checkpoint_dir),
-        "checkpoint_interval": checkpoint_interval,
-        "validation_fn": validation_fn,
-        "blur_subcubes_fraction": blur_subcubes_fraction,
-        "max_data_in_memory": max_data_in_memory,
-    }
-
-    license_state = thirdai._thirdai.licensing._get_license_state()
-    licensing_lambda = lambda: thirdai._thirdai.licensing._set_license_state(
-        license_state
-    )
-    config["licensing_lambda"] = licensing_lambda
-
-    trainer = dist.BoltTrainer(
-        train_loop_per_worker=train_loop_per_worker,
-        train_loop_config=config,
-        scaling_config=scaling_config,
-        backend_config=TorchConfig(backend=communication_backend),
-        run_config=run_config,
-    )
-
-    result = trainer.fit()
-
-    self.model = dist.BoltCheckPoint.get_model(result.checkpoint)
-
-
-def subcube_embeddings(seismic_model, subcubes, sparse_inference=False):
-    subcubes = convert_to_patches(
-        torch.from_numpy(subcubes),
-        expected_subcube_shape=seismic_model.subcube_shape,
-        patch_shape=seismic_model.patch_shape,
-        max_pool=seismic_model.max_pool,
-    )
-    return seismic_model.embeddings_for_patches(subcubes, sparse_inference)
-
-
-def forward_finetuning(seismic_model, subcubes):
-    subcubes = convert_to_patches(
-        subcubes,
-        expected_subcube_shape=seismic_model.subcube_shape,
-        patch_shape=seismic_model.patch_shape,
-        max_pool=seismic_model.max_pool,
-    )
-    out = seismic_model.forward_finetuning(subcubes)
-    out = torch.from_numpy(out)
-    out.requires_grad = True
-    return out
-
-
-def backpropagate_finetuning(seismic_model, grads):
-    # Bolt takes optimizer steps in the direction of the gradients, torch takes
-    # steps opposite the direction of the gradient.
-    seismic_model.backpropagate_finetuning((-grads).numpy())
-
-
-def classifier_predict(seismic_classifier, subcubes, sparse_inference=False):
-    subcubes = convert_to_patches(
-        torch.from_numpy(subcubes),
-        expected_subcube_shape=seismic_classifier.subcube_shape,
-        patch_shape=seismic_classifier.patch_shape,
-        max_pool=seismic_classifier.max_pool,
-    )
-
-    return seismic_classifier.predictions_for_patches(subcubes, sparse_inference)
-
-
-def score_subcubes(
-    seismic_model, directory, target_subcube="tgt.npy", sparse_inference=False
-):
-    files = [file for file in os.listdir(directory) if file.endswith(".npy")]
-    if target_subcube not in files:
-        raise ValueError(f"Expected unable to find {target_subcube} in {directory}.")
-    files.remove(target_subcube)
-    target = np.load(os.path.join(directory, target_subcube))
-    candidates = [np.load(os.path.join(directory, file)) for file in files]
-
-    # Feed in as a batch for best parallelism.
-    embs = seismic_model.embeddings(
-        np.stack([target] + candidates), sparse_inference=sparse_inference
-    )
-
-    cosine_sims = np.matmul(embs[1:], embs[0])  # The fist embedding is the target.
-    magnitudes = np.linalg.norm(embs, axis=1, ord=2)
-    cosine_sims /= magnitudes[1:]  # The magnitude of the candidate embeddings.
-    cosine_sims /= magnitudes[0]  # The magnitude of the target embedding.
-
-    return sorted(list(zip(files, cosine_sims)), key=lambda x: x[1], reverse=True)
-
-
-def modify_seismic():
-    bolt.seismic.SeismicBase.train_distributed = train_distributed
-    bolt.seismic.SeismicBase.embeddings = subcube_embeddings
-    bolt.seismic.SeismicBase.score_subcubes = score_subcubes
-
-    bolt.seismic.SeismicEmbedding.train = train_embedding_model
-    bolt.seismic.SeismicEmbedding.forward = forward_finetuning
-    bolt.seismic.SeismicEmbedding.backpropagate = backpropagate_finetuning
-    bolt.seismic.SeismicClassifier.train = train_classifier
-    bolt.seismic.SeismicClassifier.predict = classifier_predict
+import os
+import time
+from pathlib import Path
+
+import numpy as np
+import pandas as pd
+import scipy.ndimage
+import thirdai
+import thirdai._thirdai.bolt as bolt
+import torch
+import torch.nn.functional as F
+from torch.utils.data import DataLoader, Dataset, default_collate
+
+
+class UnsupervisedSubcubeDataset(Dataset):
+    def __init__(self, subcube_directory, subcube_files, blur_subcube_fraction=0.0):
+        self.subcube_directory = subcube_directory
+        self.subcube_files = subcube_files
+        self.blur_subcube_fraction = blur_subcube_fraction
+
+    def __len__(self):
+        return len(self.subcube_files)
+
+    def __getitem__(self, index):
+        filename = self.subcube_files[index]
+        metadata = UnsupervisedSubcubeDataset.parse_metadata(Path(filename).stem)
+        subcube = np.load(os.path.join(self.subcube_directory, filename))
+        subcube = subcube.astype(np.float32)
+        if self.blur_subcube_fraction > 0:
+            r = np.random.rand()
+            if r < (self.blur_subcube_fraction / 2):
+                subcube = UnsupervisedSubcubeDataset.median_blur(subcube)
+            elif r < self.blur_subcube_fraction:
+                subcube = UnsupervisedSubcubeDataset.gaussian_blur(subcube)
+
+        return subcube, metadata
+
+    @staticmethod
+    def parse_metadata(metadata):
+        volume, x, y, z = metadata.split("_")
+        return (volume, int(x), int(y), int(z))
+
+    @staticmethod
+    def median_blur(subcube):
+        return scipy.ndimage.median_filter(subcube, size=3)
+
+    @staticmethod
+    def gaussian_blur(subcube):
+        blur = np.random.choice(np.arange(1.55, 1.95, 0.15))
+        return scipy.ndimage.gaussian_filter(subcube, sigma=blur).astype(np.float32)
+
+
+class ClassificationSubcubeDataset(Dataset):
+    def __init__(self, sample_index: pd.DataFrame):
+        if (
+            "labels" not in sample_index.columns
+            or "subcube" not in sample_index.columns
+        ):
+            raise ValueError(
+                "Expected sample index to contain the columns 'labels' and 'subcube'."
+            )
+        self.sample_index = sample_index
+
+    def __len__(self):
+        return len(self.sample_index)
+
+    def __getitem__(self, index):
+        subcube_path = self.sample_index["subcube"].iloc[index]
+        subcube = np.load(subcube_path).astype(np.float32)
+        labels = self.sample_index["labels"].iloc[index]
+        return subcube, labels
+
+
+def collate_fn(batch):
+    data, metadata = zip(*batch)
+    return default_collate(data), metadata
+
+
+def convert_to_patches(subcubes, expected_subcube_shape, patch_shape, max_pool=None):
+    if subcubes.shape[1:] != expected_subcube_shape:
+        raise ValueError(
+            f"Expected subcubes with shape {expected_subcube_shape}. But received subcubes with shape {subcubes.shape[1:]}"
+        )
+
+    pd_x, pd_y, pd_z = patch_shape
+    if max_pool:
+        # Unsqueeze/squeeze are to add/remove the 'channels' dimension
+        subcubes = F.max_pool3d(
+            subcubes.unsqueeze(1), kernel_size=max_pool, stride=max_pool
+        )
+        subcubes = subcubes.squeeze_(1)
+        # Scale the patch dim since pooling is applied first.
+        pd_x //= max_pool[0]
+        pd_y //= max_pool[1]
+        pd_z //= max_pool[2]
+
+    n_cubes, x, y, z = subcubes.shape
+    assert x % pd_x == 0
+    assert y % pd_y == 0
+    assert z % pd_z == 0
+
+    pd_flat = pd_x * pd_y * pd_z
+    n_patches = (x * y * z) // pd_flat
+
+    patches = torch.reshape(
+        subcubes, (n_cubes, x // pd_x, pd_x, y // pd_y, pd_y, z // pd_z, pd_z)
+    )
+    patches = torch.permute(patches, (0, 1, 3, 5, 2, 4, 6))
+
+    patches = torch.reshape(patches, (n_cubes, n_patches, pd_flat))
+
+    return patches.numpy()
+
+
+def get_rank_and_world_size():
+    from ray import train
+
+    rank = train.get_context().get_world_rank()
+    world_size = train.get_context().get_world_size()
+    return rank, world_size
+
+
+def subcube_range_for_worker(n_subcubes: int):
+    rank, world_size = get_rank_and_world_size()
+
+    subcubes_for_worker = n_subcubes // world_size
+    if rank < (n_subcubes % world_size):
+        subcubes_for_worker += 1
+
+    offset = (n_subcubes // world_size * rank) + min(n_subcubes % world_size, rank)
+
+    return offset, offset + subcubes_for_worker
+
+
+def log(msg):
+    thirdai.logging.info(msg)
+    print(msg, flush=True)
+
+
+class TimedIterator:
+    def __init__(self, obj):
+        self.iter = iter(obj)
+
+    def __iter__(self):
+        return self
+
+    def __next__(self):
+        start = time.perf_counter()
+        out = next(self.iter)
+        end = time.perf_counter()
+        log(f"Loaded {len(out[0])} subcubes in {end-start} seconds.")
+        return out
+
+
+def train_seismic_model(
+    seismic_model,
+    dataset: Dataset,
+    learning_rate: float,
+    epochs: int,
+    batch_size: int,
+    checkpoint_dir: str = None,
+    checkpoint_interval: int = 1000,
+    log_interval=20,
+    validation_fn=None,
+    max_data_in_memory=30,  # In Gb
+    comm=None,
+):
+    callbacks = []
+    if checkpoint_dir:
+        if not os.path.exists(checkpoint_dir):
+            os.makedirs(checkpoint_dir)
+        callbacks = [
+            bolt.seismic.Checkpoint(
+                seismic_model=seismic_model,
+                checkpoint_dir=checkpoint_dir,
+                interval=checkpoint_interval,
+            )
+        ]
+
+    # Number of bytes per subcube
+    subcube_size = np.prod(seismic_model.subcube_shape) * 4
+    # Load less than 30Gb of subcubes
+    n_subcubes_per_chunk = min(
+        int((10**9) * max_data_in_memory / subcube_size), len(dataset)
+    )
+
+    data_loader = DataLoader(
+        dataset=dataset,
+        batch_size=n_subcubes_per_chunk,
+        shuffle=True,
+        num_workers=2,
+        collate_fn=collate_fn,
+    )
+
+    output_metrics = {"epoch_times": [], "train_loss": []}
+
+    for epoch in range(epochs):
+        epoch_start = time.perf_counter()
+
+        for subcubes, label_or_metadata in TimedIterator(data_loader):
+            patch_start = time.perf_counter()
+
+            subcubes = convert_to_patches(
+                subcubes=subcubes,
+                expected_subcube_shape=seismic_model.subcube_shape,
+                patch_shape=seismic_model.patch_shape,
+                max_pool=seismic_model.max_pool,
+            )
+
+            patch_end = time.perf_counter()
+
+            log(
+                f"Converted {subcubes.shape[0]} subcubes to patches in {patch_end - patch_start} seconds.",
+            )
+
+            metrics = seismic_model.train_on_patches(
+                subcubes,
+                # We call this label or metadata becuase in supervised training this will contain
+                # the labels, but in unsupervised training we just use it to pass in metadata
+                # about the subcube. Doing it this way saves having to duplicate a lot of code for
+                # this method.
+                label_or_metadata,
+                learning_rate=learning_rate,
+                batch_size=batch_size,
+                callbacks=callbacks,
+                log_interval=log_interval,
+                comm=comm,
+            )
+
+        epoch_end = time.perf_counter()
+
+        epoch_time = epoch_end - epoch_start
+        output_metrics["epoch_times"].append(epoch_time)
+        train_loss = metrics["train_loss"][-1]
+        output_metrics["train_loss"].append(train_loss)
+
+        log(
+            f"train | completed epoch {epoch} | train_loss={train_loss} | time={epoch_time} "
+        )
+
+        if validation_fn:
+            validation_fn(seismic_model)
+
+        if checkpoint_dir:
+            seismic_model.save(os.path.join(checkpoint_dir, f"epoch_{epoch}_end"))
+
+    return output_metrics
+
+
+def train_embedding_model(
+    self,
+    subcube_directory: str,
+    learning_rate: float,
+    epochs: int,
+    batch_size: int,
+    checkpoint_dir: str = None,
+    checkpoint_interval: int = 1000,
+    log_interval=20,
+    validation_fn=None,
+    blur_subcubes_fraction=0.0,
+    max_data_in_memory=30,  # In Gb
+    comm=None,
+):
+    subcube_files = [
+        file for file in os.listdir(subcube_directory) if file.endswith(".npy")
+    ]
+
+    if not subcube_files:
+        raise ValueError(f"Could not find any .npy files in {subcube_directory}.")
+
+    if comm:
+        # For distributed training give each worker a seperate partition of the subcubes.
+        worker_start, worker_end = subcube_range_for_worker(len(subcube_files))
+        subcube_files = subcube_files[worker_start:worker_end]
+
+    dataset = UnsupervisedSubcubeDataset(
+        subcube_directory=subcube_directory,
+        subcube_files=subcube_files,
+        blur_subcube_fraction=blur_subcubes_fraction,
+    )
+
+    return train_seismic_model(
+        seismic_model=self,
+        dataset=dataset,
+        learning_rate=learning_rate,
+        epochs=epochs,
+        batch_size=batch_size,
+        checkpoint_dir=checkpoint_dir,
+        checkpoint_interval=checkpoint_interval,
+        log_interval=log_interval,
+        validation_fn=validation_fn,
+        max_data_in_memory=max_data_in_memory,
+        comm=comm,
+    )
+
+
+def train_classifier(
+    self,
+    sample_index_file: str,
+    learning_rate: float,
+    epochs: int,
+    batch_size: int,
+    checkpoint_dir: str = None,
+    checkpoint_interval: int = 1000,
+    log_interval=20,
+    validation_fn=None,
+    blur_subcubes_fraction=0.0,  # Unused
+    max_data_in_memory=30,  # In Gb
+    comm=None,
+):
+    sample_index = pd.read_csv(sample_index_file)
+    sample_index = sample_index.sample(frac=1.0)
+    if sample_index["labels"].dtype == object:
+        sample_index["labels"] = sample_index["labels"].apply(
+            lambda x: list(map(int, x.split(" ")))
+        )
+    elif sample_index["labels"].dtype == int:
+        sample_index["labels"] = sample_index["labels"].apply(lambda x: [x])
+
+    if comm:
+        if not sample_index["subcube"].apply(os.path.isabs).all():
+            raise ValueError(
+                "Subcube files in sample index must be specified as absolute paths for distributed training so that they can be accessed from each worker."
+            )
+
+    if comm:
+        # For distributed training give each worker a seperate partition of the subcubes.
+        worker_start, worker_end = subcube_range_for_worker(len(sample_index))
+        sample_index = sample_index.iloc[worker_start:worker_end]
+
+    dataset = ClassificationSubcubeDataset(sample_index=sample_index)
+
+    return train_seismic_model(
+        seismic_model=self,
+        dataset=dataset,
+        learning_rate=learning_rate,
+        epochs=epochs,
+        batch_size=batch_size,
+        checkpoint_dir=checkpoint_dir,
+        checkpoint_interval=checkpoint_interval,
+        log_interval=log_interval,
+        validation_fn=validation_fn,
+        max_data_in_memory=max_data_in_memory,
+        comm=comm,
+    )
+
+
+def train_distributed(
+    self,
+    data_path: str,
+    learning_rate: float,
+    epochs: int,
+    batch_size: int,
+    run_config,
+    scaling_config,
+    log_file: str,
+    checkpoint_dir: str,
+    log_interval: int = 20,
+    checkpoint_interval: int = 1000,
+    validation_fn=None,
+    blur_subcubes_fraction=0.0,
+    max_data_in_memory=30,  # In Gb
+    communication_backend: str = "gloo",
+):
+    import ray
+    import thirdai.distributed_bolt as dist
+    from ray.train.torch import TorchConfig
+
+    from .._distributed_bolt.distributed import Communication
+
+    def train_loop_per_worker(config):
+        import ray
+        from ray import train
+
+        rank, world_size = get_rank_and_world_size()
+
+        config["licensing_lambda"]()
+
+        log_file = config["log_file"]
+        if rank != 0:
+            log_file += f".worker_{rank}"
+        thirdai.logging.setup(log_to_stderr=False, path=log_file, level="info")
+
+        model = ray.get(config["model_ref"])
+
+        metrics = model.train(
+            config["data_path"],
+            learning_rate=config["learning_rate"],
+            epochs=config["epochs"],
+            batch_size=config["batch_size"] // world_size,
+            checkpoint_dir=config["checkpoint_dir"] if rank == 0 else None,
+            checkpoint_interval=config["checkpoint_interval"],
+            log_interval=config["log_interval"],
+            validation_fn=config["validation_fn"] if rank == 0 else None,
+            blur_subcubes_fraction=config["blur_subcubes_fraction"],
+            max_data_in_memory=config["max_data_in_memory"],
+            comm=Communication(),
+        )
+
+        checkpoint = None
+        if rank == 0:
+            checkpoint = dist.BoltCheckPoint.from_model(model.model)
+
+        train.report(metrics=metrics, checkpoint=checkpoint)
+
+    config = {
+        "model_ref": ray.put(self),
+        "data_path": os.path.abspath(data_path),
+        "learning_rate": learning_rate,
+        "epochs": epochs,
+        "batch_size": batch_size,
+        "log_file": os.path.abspath(log_file),
+        "log_interval": log_interval,
+        "checkpoint_dir": os.path.abspath(checkpoint_dir),
+        "checkpoint_interval": checkpoint_interval,
+        "validation_fn": validation_fn,
+        "blur_subcubes_fraction": blur_subcubes_fraction,
+        "max_data_in_memory": max_data_in_memory,
+    }
+
+    license_state = thirdai._thirdai.licensing._get_license_state()
+    licensing_lambda = lambda: thirdai._thirdai.licensing._set_license_state(
+        license_state
+    )
+    config["licensing_lambda"] = licensing_lambda
+
+    trainer = dist.BoltTrainer(
+        train_loop_per_worker=train_loop_per_worker,
+        train_loop_config=config,
+        scaling_config=scaling_config,
+        backend_config=TorchConfig(backend=communication_backend),
+        run_config=run_config,
+    )
+
+    result = trainer.fit()
+
+    self.model = dist.BoltCheckPoint.get_model(result.checkpoint)
+
+
+def subcube_embeddings(seismic_model, subcubes, sparse_inference=False):
+    subcubes = convert_to_patches(
+        torch.from_numpy(subcubes),
+        expected_subcube_shape=seismic_model.subcube_shape,
+        patch_shape=seismic_model.patch_shape,
+        max_pool=seismic_model.max_pool,
+    )
+    return seismic_model.embeddings_for_patches(subcubes, sparse_inference)
+
+
+def forward_finetuning(seismic_model, subcubes):
+    subcubes = convert_to_patches(
+        subcubes,
+        expected_subcube_shape=seismic_model.subcube_shape,
+        patch_shape=seismic_model.patch_shape,
+        max_pool=seismic_model.max_pool,
+    )
+    out = seismic_model.forward_finetuning(subcubes)
+    out = torch.from_numpy(out)
+    out.requires_grad = True
+    return out
+
+
+def backpropagate_finetuning(seismic_model, grads):
+    # Bolt takes optimizer steps in the direction of the gradients, torch takes
+    # steps opposite the direction of the gradient.
+    seismic_model.backpropagate_finetuning((-grads).numpy())
+
+
+def classifier_predict(seismic_classifier, subcubes, sparse_inference=False):
+    subcubes = convert_to_patches(
+        torch.from_numpy(subcubes),
+        expected_subcube_shape=seismic_classifier.subcube_shape,
+        patch_shape=seismic_classifier.patch_shape,
+        max_pool=seismic_classifier.max_pool,
+    )
+
+    return seismic_classifier.predictions_for_patches(subcubes, sparse_inference)
+
+
+def score_subcubes(
+    seismic_model, directory, target_subcube="tgt.npy", sparse_inference=False
+):
+    files = [file for file in os.listdir(directory) if file.endswith(".npy")]
+    if target_subcube not in files:
+        raise ValueError(f"Expected unable to find {target_subcube} in {directory}.")
+    files.remove(target_subcube)
+    target = np.load(os.path.join(directory, target_subcube))
+    candidates = [np.load(os.path.join(directory, file)) for file in files]
+
+    # Feed in as a batch for best parallelism.
+    embs = seismic_model.embeddings(
+        np.stack([target] + candidates), sparse_inference=sparse_inference
+    )
+
+    cosine_sims = np.matmul(embs[1:], embs[0])  # The fist embedding is the target.
+    magnitudes = np.linalg.norm(embs, axis=1, ord=2)
+    cosine_sims /= magnitudes[1:]  # The magnitude of the candidate embeddings.
+    cosine_sims /= magnitudes[0]  # The magnitude of the target embedding.
+
+    return sorted(list(zip(files, cosine_sims)), key=lambda x: x[1], reverse=True)
+
+
+def modify_seismic():
+    bolt.seismic.SeismicBase.train_distributed = train_distributed
+    bolt.seismic.SeismicBase.embeddings = subcube_embeddings
+    bolt.seismic.SeismicBase.score_subcubes = score_subcubes
+
+    bolt.seismic.SeismicEmbedding.train = train_embedding_model
+    bolt.seismic.SeismicEmbedding.forward = forward_finetuning
+    bolt.seismic.SeismicEmbedding.backpropagate = backpropagate_finetuning
+    bolt.seismic.SeismicClassifier.train = train_classifier
+    bolt.seismic.SeismicClassifier.predict = classifier_predict
```

## thirdai/bolt/udt_docs.py

 * *Ordering differences only*

```diff
@@ -1,190 +1,190 @@
-udt_train_doc = """
-    Trains a UniversalDeepTransformer (UDT) on a given dataset using a file on disk
-    or in a cloud storage bucket, such as s3 or google cloud storage (GCS). If the
-    file is on S3, it should be in the normal s3 form, i.e. s3://bucket/path/to/key.
-    For files in GCS, the path should have the form gcs://bucket/path/to/filename.
-    We currently support csv and parquet format files. If the file is parquet, it 
-    should end in .parquet or .pqt. Otherwise, we will assume it is a csv file.
-
-    Args:
-        filename (str): Path to the dataset file. It Can be a path to a file on
-            disk or an S3 or GCS resource identifier. If the file is on s3 or GCS,
-            regular credentials files will be required for authentication. 
-        learning_rate (float): Optional, uses default if not provided.
-        epochs (int): Optional, uses default if not provided.
-        validation (Optional[bolt.Validation]): This is an optional parameter that 
-            specifies a validation dataset, metrics, and interval to use during 
-            training.
-        batch_size (Option[int]): This is an optional parameter indicating which batch
-            size to use for training. If not specified, the batch size will be autotuned.
-        max_in_memory_batches (Option[int]): The maximum number of batches to load in
-            memory at a given time. If this is specified then the dataset will be processed
-            in a streaming fashion.
-        verbose (bool): Optional, defaults to True. Controls if additional information 
-            is printed during training.
-        callbacks (List[bolt.train.callbacks.Callback]): List of callbacks to use during 
-            training. 
-        metrics (List[str]): List of metrics to compute during training. These are
-            logged if logging is enabled, and are accessible by any callbacks. 
-        logging_interval (Optional[int]): How frequently to log training metrics,
-            represents the number of batches between logging metrics. If not specified 
-            logging is done at the end of each epoch. 
-
-    Returns:
-        (Dict[str, List[float]]): 
-        The train method returns a dictionary providing the values of any metrics 
-        computed during training. The format is: {"name of metric": [list of values]}.
-
-    Examples:
-        >>> model.train(
-                filename="./train_file", epochs=5, learning_rate=0.01, max_in_memory_batches=12
-            )
-        >>> model.train(
-                filename="s3://bucket/path/to/key"
-            )
-
-    Notes:
-        - If temporal tracking relationships are provided, UDT can make better 
-          predictions by taking temporal context into account. For example, UDT may 
-          keep track of the last few movies that a user has watched to better 
-          recommend the next movie. `model.train()` automatically updates UDT's 
-          temporal context.
-        - If the prediction task is binary classification then the model will attempt 
-          to find an optimal threshold for predictions that will be used if `return_predicted_class=True`
-          is passed to calls to evaluate, predict, and predict_batch. The optimal threshold
-          will be selected based on what threshold maximizes the first validation metric
-          on the validation data. If no validation data or metrics are passed in then 
-          it will use the first 100 batches of the training data and the first training
-          metric. If there is also no training metrics then it will not choose a prediction
-          threshold. 
-"""
-
-udt_train_on_datasource_doc = """
-Same as train except for arg `filename` is replaced by an arg `data_source` which accepts an DataSource object.
-"""
-
-udt_eval_doc = """
-    Evaluates the UniversalDeepTransformer (UDT) on the given dataset and returns a 
-    numpy array of the activations. We currently support csv and parquet format 
-    files. If the file is parquet, it should end in .parquet or .pqt. Otherwise, 
-    we will assume it is a csv file.
-
-    Args:
-        filename (str): Path to the dataset file. Like train, this can be a path
-            to a local file or a path to a file that lives in an s3 or google cloud
-            storage (GCS) bucket. 
-        metrics (List[str]): List of metrics to compute during evaluation. 
-        use_sparse_inference (bool): Optional, defaults to False, determines if 
-            sparse inference is used during evaluation. 
-        verbose (bool): Optional, defaults to True. Controls if additional information 
-            is printed during training.
-        top_k (Optional[int]): Optional, defaults to None. This parameter is only used 
-            for query reformulation model to deterimine how many candidates to select
-            before computing evaluation metrics.
-
-    Returns:
-        (Dict[str, float]): 
-        Returns a list of values for the specified metrics, keyed by the metric names.
-
-    Examples:
-        >>> metrics = model.evaluate(filename="./test_file", metrics=["categorical_accuracy"])
-
-    Notes: 
-        - If temporal tracking relationships are provided, UDT can make better predictions 
-          by taking temporal context into account. For example, UDT may keep track of 
-          the last few movies that a user has watched to better recommend the next movie.
-          `model.evaluate()` automatically updates UDT's temporal context.
- """
-
-udt_eval_on_data_source_doc = """
-Same as evaluate except for arg `filename` is replaced by an arg `data_source` which accepts an DataSource object.
-"""
-
-udt_cold_start_doc = """
-    This method will perform cold start pretraining for UDT. This is a type of 
-    pretraining for text classification models that is especially useful for query 
-    to product recommendation models. It requires that the model takes in a single 
-    text input and has a categorical/multi-categorical output.
-
-    The cold start pretraining typically takes in an unsupervised dataset of objects
-    where each object corresponds to one or more columns of textual metadata. This could 
-    be something like a product catalog (with product ids as objects, and titles, 
-    descriptions, and tags as metadata). The goal with cold start is to pre-train UDT
-    on unsupervised data so in the future it may be able to answer text search queries 
-    and return the relevant objects. The dataset it takes in should be a csv file that
-    gives a class id column and some number of text columns, where for a given row 
-    the text is related to the class also specified by that row.
-
-    You may cold_start the model and train with supervised data afterwards, typically
-    leading to faster convergence on the supervised data.
-
-    Args:
-        filename (str): Path to the dataset used for pretraining.
-        strong_column_names (List[str]): The strong column names indicate which 
-            text columns are most closely related to the output class. In this 
-            case closely related means that all of the words in the text are useful
-            in identifying the output class in that row. For example in the 
-            case of a product catalog then a strong column could be the full title 
-            of the product.
-        weak_column_names (List[str]): The weak column names indicate which text 
-            columns are either more loosely related to the output class. In 
-            this case loosely related means that parts of the text are useful in 
-            identifying the output class, but there may also be parts of the 
-            text that contain more generic words or phrases that don't have as high 
-            of a correlation. For example in a product catalog the description of
-            the product could be a weak column because while there is a correlation,
-            parts of the description may be fairly similar between products or be
-            too general to completly identify which products the correspond to.
-        learning_rate (float): Optional, uses default if not provided.
-        epochs (int): Optional, uses default if not provided.
-        batch_size (Option[int]): This is an optional parameter indicating which batch
-            size to use for training. If not specified, the batch size will be autotuned.
-        metrics (List[str]): List of metrics to compute during training. These are
-            logged if logging is enabled, and are accessible by any callbacks. 
-        validation (Optional[bolt.Validation]): This is an optional parameter that 
-            specifies a validation dataset, metrics, and interval to use during 
-            training.
-        callbacks (List[bolt.train.callbacks.Callback]): List of callbacks to use during 
-            training. 
-        max_in_memory_batches (Option[int]): The maximum number of batches to load in
-            memory at a given time. If this is specified then the dataset will be processed
-            in a streaming fashion.
-        verbose (bool): Optional, defaults to True. Controls if additional information 
-            is printed during training.
-        logging_interval (Optional[int]): How frequently to log training metrics,
-            represents the number of batches between logging metrics. If not specified 
-            logging is done at the end of each epoch. 
-
-    Returns:
-        (Dict[str, List[float]]): 
-        The train method returns a dictionary providing the values of any metrics 
-        computed during training. The format is: {"name of metric": [list of values]}.
-
-    Examples:
-        >>> model = bolt.UniversalDeepTransformer(
-                data_types={
-                    "query": bolt.types.text(), 
-                    "product": bolt.types.categorical(),
-                }
-                target="product",
-                n_target_classes=1000,
-                integer_target=True,
-            )
-        >>> model.cold_start(
-                filename="product_catalog.csv",
-                strong_column_names=["title"],
-                weak_column_names=["description", "bullet_points"],
-                learning_rate=0.001,
-                epochs=5,
-                metrics=["f_measure(0.95)"]
-            )
-        >>> model.train(
-                train_filename=supervised_query_product_data,
-            )
-        >>> result = model.predict({"QUERY": query})
-
-"""
-
-udt_cold_start_on_data_source_doc = """
-Same as evaluate except for arg `filename` is replaced by an arg `data_source` which accepts an DataSource object.
-"""
+udt_train_doc = """
+    Trains a UniversalDeepTransformer (UDT) on a given dataset using a file on disk
+    or in a cloud storage bucket, such as s3 or google cloud storage (GCS). If the
+    file is on S3, it should be in the normal s3 form, i.e. s3://bucket/path/to/key.
+    For files in GCS, the path should have the form gcs://bucket/path/to/filename.
+    We currently support csv and parquet format files. If the file is parquet, it 
+    should end in .parquet or .pqt. Otherwise, we will assume it is a csv file.
+
+    Args:
+        filename (str): Path to the dataset file. It Can be a path to a file on
+            disk or an S3 or GCS resource identifier. If the file is on s3 or GCS,
+            regular credentials files will be required for authentication. 
+        learning_rate (float): Optional, uses default if not provided.
+        epochs (int): Optional, uses default if not provided.
+        validation (Optional[bolt.Validation]): This is an optional parameter that 
+            specifies a validation dataset, metrics, and interval to use during 
+            training.
+        batch_size (Option[int]): This is an optional parameter indicating which batch
+            size to use for training. If not specified, the batch size will be autotuned.
+        max_in_memory_batches (Option[int]): The maximum number of batches to load in
+            memory at a given time. If this is specified then the dataset will be processed
+            in a streaming fashion.
+        verbose (bool): Optional, defaults to True. Controls if additional information 
+            is printed during training.
+        callbacks (List[bolt.train.callbacks.Callback]): List of callbacks to use during 
+            training. 
+        metrics (List[str]): List of metrics to compute during training. These are
+            logged if logging is enabled, and are accessible by any callbacks. 
+        logging_interval (Optional[int]): How frequently to log training metrics,
+            represents the number of batches between logging metrics. If not specified 
+            logging is done at the end of each epoch. 
+
+    Returns:
+        (Dict[str, List[float]]): 
+        The train method returns a dictionary providing the values of any metrics 
+        computed during training. The format is: {"name of metric": [list of values]}.
+
+    Examples:
+        >>> model.train(
+                filename="./train_file", epochs=5, learning_rate=0.01, max_in_memory_batches=12
+            )
+        >>> model.train(
+                filename="s3://bucket/path/to/key"
+            )
+
+    Notes:
+        - If temporal tracking relationships are provided, UDT can make better 
+          predictions by taking temporal context into account. For example, UDT may 
+          keep track of the last few movies that a user has watched to better 
+          recommend the next movie. `model.train()` automatically updates UDT's 
+          temporal context.
+        - If the prediction task is binary classification then the model will attempt 
+          to find an optimal threshold for predictions that will be used if `return_predicted_class=True`
+          is passed to calls to evaluate, predict, and predict_batch. The optimal threshold
+          will be selected based on what threshold maximizes the first validation metric
+          on the validation data. If no validation data or metrics are passed in then 
+          it will use the first 100 batches of the training data and the first training
+          metric. If there is also no training metrics then it will not choose a prediction
+          threshold. 
+"""
+
+udt_train_on_datasource_doc = """
+Same as train except for arg `filename` is replaced by an arg `data_source` which accepts an DataSource object.
+"""
+
+udt_eval_doc = """
+    Evaluates the UniversalDeepTransformer (UDT) on the given dataset and returns a 
+    numpy array of the activations. We currently support csv and parquet format 
+    files. If the file is parquet, it should end in .parquet or .pqt. Otherwise, 
+    we will assume it is a csv file.
+
+    Args:
+        filename (str): Path to the dataset file. Like train, this can be a path
+            to a local file or a path to a file that lives in an s3 or google cloud
+            storage (GCS) bucket. 
+        metrics (List[str]): List of metrics to compute during evaluation. 
+        use_sparse_inference (bool): Optional, defaults to False, determines if 
+            sparse inference is used during evaluation. 
+        verbose (bool): Optional, defaults to True. Controls if additional information 
+            is printed during training.
+        top_k (Optional[int]): Optional, defaults to None. This parameter is only used 
+            for query reformulation model to deterimine how many candidates to select
+            before computing evaluation metrics.
+
+    Returns:
+        (Dict[str, float]): 
+        Returns a list of values for the specified metrics, keyed by the metric names.
+
+    Examples:
+        >>> metrics = model.evaluate(filename="./test_file", metrics=["categorical_accuracy"])
+
+    Notes: 
+        - If temporal tracking relationships are provided, UDT can make better predictions 
+          by taking temporal context into account. For example, UDT may keep track of 
+          the last few movies that a user has watched to better recommend the next movie.
+          `model.evaluate()` automatically updates UDT's temporal context.
+ """
+
+udt_eval_on_data_source_doc = """
+Same as evaluate except for arg `filename` is replaced by an arg `data_source` which accepts an DataSource object.
+"""
+
+udt_cold_start_doc = """
+    This method will perform cold start pretraining for UDT. This is a type of 
+    pretraining for text classification models that is especially useful for query 
+    to product recommendation models. It requires that the model takes in a single 
+    text input and has a categorical/multi-categorical output.
+
+    The cold start pretraining typically takes in an unsupervised dataset of objects
+    where each object corresponds to one or more columns of textual metadata. This could 
+    be something like a product catalog (with product ids as objects, and titles, 
+    descriptions, and tags as metadata). The goal with cold start is to pre-train UDT
+    on unsupervised data so in the future it may be able to answer text search queries 
+    and return the relevant objects. The dataset it takes in should be a csv file that
+    gives a class id column and some number of text columns, where for a given row 
+    the text is related to the class also specified by that row.
+
+    You may cold_start the model and train with supervised data afterwards, typically
+    leading to faster convergence on the supervised data.
+
+    Args:
+        filename (str): Path to the dataset used for pretraining.
+        strong_column_names (List[str]): The strong column names indicate which 
+            text columns are most closely related to the output class. In this 
+            case closely related means that all of the words in the text are useful
+            in identifying the output class in that row. For example in the 
+            case of a product catalog then a strong column could be the full title 
+            of the product.
+        weak_column_names (List[str]): The weak column names indicate which text 
+            columns are either more loosely related to the output class. In 
+            this case loosely related means that parts of the text are useful in 
+            identifying the output class, but there may also be parts of the 
+            text that contain more generic words or phrases that don't have as high 
+            of a correlation. For example in a product catalog the description of
+            the product could be a weak column because while there is a correlation,
+            parts of the description may be fairly similar between products or be
+            too general to completly identify which products the correspond to.
+        learning_rate (float): Optional, uses default if not provided.
+        epochs (int): Optional, uses default if not provided.
+        batch_size (Option[int]): This is an optional parameter indicating which batch
+            size to use for training. If not specified, the batch size will be autotuned.
+        metrics (List[str]): List of metrics to compute during training. These are
+            logged if logging is enabled, and are accessible by any callbacks. 
+        validation (Optional[bolt.Validation]): This is an optional parameter that 
+            specifies a validation dataset, metrics, and interval to use during 
+            training.
+        callbacks (List[bolt.train.callbacks.Callback]): List of callbacks to use during 
+            training. 
+        max_in_memory_batches (Option[int]): The maximum number of batches to load in
+            memory at a given time. If this is specified then the dataset will be processed
+            in a streaming fashion.
+        verbose (bool): Optional, defaults to True. Controls if additional information 
+            is printed during training.
+        logging_interval (Optional[int]): How frequently to log training metrics,
+            represents the number of batches between logging metrics. If not specified 
+            logging is done at the end of each epoch. 
+
+    Returns:
+        (Dict[str, List[float]]): 
+        The train method returns a dictionary providing the values of any metrics 
+        computed during training. The format is: {"name of metric": [list of values]}.
+
+    Examples:
+        >>> model = bolt.UniversalDeepTransformer(
+                data_types={
+                    "query": bolt.types.text(), 
+                    "product": bolt.types.categorical(),
+                }
+                target="product",
+                n_target_classes=1000,
+                integer_target=True,
+            )
+        >>> model.cold_start(
+                filename="product_catalog.csv",
+                strong_column_names=["title"],
+                weak_column_names=["description", "bullet_points"],
+                learning_rate=0.001,
+                epochs=5,
+                metrics=["f_measure(0.95)"]
+            )
+        >>> model.train(
+                train_filename=supervised_query_product_data,
+            )
+        >>> result = model.predict({"QUERY": query})
+
+"""
+
+udt_cold_start_on_data_source_doc = """
+Same as evaluate except for arg `filename` is replaced by an arg `data_source` which accepts an DataSource object.
+"""
```

## thirdai/bolt/udt_modifications.py

 * *Ordering differences only*

```diff
@@ -1,417 +1,417 @@
-from typing import Dict, List, Optional, Tuple
-
-import pandas as pd
-import thirdai
-import thirdai._thirdai.bolt as bolt
-import thirdai._thirdai.data as data
-import thirdai._thirdai.dataset as dataset
-
-from .udt_docs import *
-
-
-def _create_parquet_source(path):
-    return thirdai.dataset.ParquetSource(parquet_path=path)
-
-
-def _create_data_source(path):
-    """
-    Reading data from S3 and GCS assumes that the credentials are already
-    set. For S3, pandas.read_csv method in the data loader will look for
-    credentials in ~/.aws/credentials while for GCS the path will be assumed to be
-    ~/.config/gcloud/credentials or ~/.config/gcloud/application_default_credentials.json.
-    """
-
-    # This also handles parquet on s3, so it comes before the general s3 and gcs
-    # handling and file handling below which assume the target files are
-    # CSVs.
-    if path.endswith(".parquet") or path.endswith(".pqt"):
-        return _create_parquet_source(path)
-
-    if path.startswith("s3://") or path.startswith("gcs://"):
-        return thirdai.dataset.CSVDataSource(
-            storage_path=path,
-        )
-
-    return thirdai.dataset.FileDataSource(path)
-
-
-def _process_validation_and_options(
-    validation: Optional[bolt.Validation] = None,
-    batch_size: Optional[int] = None,
-    max_in_memory_batches: Optional[int] = None,
-    verbose: bool = True,
-    logging_interval: Optional[int] = None,
-    shuffle_reservoir_size: int = 64000,
-):
-    train_options = bolt.TrainOptions()
-
-    train_options.batch_size = batch_size
-    train_options.max_in_memory_batches = max_in_memory_batches
-    train_options.verbose = verbose
-    train_options.logging_interval = logging_interval
-    train_options.shuffle_config = dataset.ShuffleConfig(
-        min_vecs_in_buffer=shuffle_reservoir_size
-    )
-
-    if validation:
-        val_data = _create_data_source(validation.filename)
-        train_options.steps_per_validation = validation.steps_per_validation
-        train_options.sparse_validation = validation.sparse_validation
-
-        return val_data, validation.metrics, train_options
-
-    return None, [], train_options
-
-
-def modify_udt():
-    original_train = bolt.UniversalDeepTransformer.train
-    original_evaluate = bolt.UniversalDeepTransformer.evaluate
-    original_cold_start = bolt.UniversalDeepTransformer.cold_start
-
-    def wrapped_train(
-        self,
-        filename: str,
-        learning_rate: float = 0.001,
-        epochs: int = 5,
-        validation: Optional[bolt.Validation] = None,
-        batch_size: Optional[int] = None,
-        max_in_memory_batches: Optional[int] = None,
-        verbose: bool = True,
-        callbacks: List[bolt.train.callbacks.Callback] = [],
-        metrics: List[str] = [],
-        logging_interval: Optional[int] = None,
-        shuffle_reservoir_size: int = 64000,
-        comm=None,
-        **kwargs
-    ):
-        data_source = _create_data_source(filename)
-
-        val_data, val_metrics, train_options = _process_validation_and_options(
-            validation=validation,
-            batch_size=batch_size,
-            max_in_memory_batches=max_in_memory_batches,
-            verbose=verbose,
-            logging_interval=logging_interval,
-            shuffle_reservoir_size=shuffle_reservoir_size,
-        )
-
-        return original_train(
-            self,
-            data=data_source,
-            learning_rate=learning_rate,
-            epochs=epochs,
-            train_metrics=metrics,
-            val_data=val_data,
-            val_metrics=val_metrics,
-            callbacks=callbacks,
-            options=train_options,
-            comm=comm,
-            **kwargs
-        )
-
-    def wrapped_train_on_data_source(
-        self,
-        data_source: dataset.DataSource,
-        learning_rate: float = 0.001,
-        epochs: int = 3,
-        batch_size: Optional[int] = None,
-        max_in_memory_batches: Optional[int] = None,
-        verbose: bool = True,
-        callbacks: List[bolt.train.callbacks.Callback] = [],
-        metrics: List[str] = [],
-        logging_interval: Optional[int] = None,
-        comm=None,
-        **kwargs
-    ):
-        val_data, val_metrics, train_options = _process_validation_and_options(
-            validation=None,
-            batch_size=batch_size,
-            max_in_memory_batches=max_in_memory_batches,
-            verbose=verbose,
-            logging_interval=logging_interval,
-        )
-
-        return original_train(
-            self,
-            data=data_source,
-            learning_rate=learning_rate,
-            epochs=epochs,
-            train_metrics=metrics,
-            val_data=val_data,
-            val_metrics=val_metrics,
-            callbacks=callbacks,
-            options=train_options,
-            comm=comm,
-            **kwargs
-        )
-
-    def wrapped_evaluate(
-        self,
-        filename: str,
-        metrics: List[str] = [],
-        use_sparse_inference: bool = False,
-        verbose: bool = True,
-        **kwargs
-    ):
-        data_source = _create_data_source(filename)
-
-        return original_evaluate(
-            self,
-            data=data_source,
-            metrics=metrics,
-            sparse_inference=use_sparse_inference,
-            verbose=verbose,
-            **kwargs
-        )
-
-    def wrapped_evaluate_on_data_source(
-        self,
-        data_source: dataset.DataSource,
-        metrics: List[str] = [],
-        use_sparse_inference: bool = False,
-        verbose: bool = True,
-        **kwargs
-    ):
-        return original_evaluate(
-            self,
-            data=data_source,
-            metrics=metrics,
-            sparse_inference=use_sparse_inference,
-            verbose=verbose,
-            **kwargs
-        )
-
-    def wrapped_cold_start(
-        self,
-        filename: str,
-        strong_column_names: List[str],
-        weak_column_names: List[str],
-        variable_length: Optional[
-            data.transformations.VariableLengthConfig
-        ] = data.transformations.VariableLengthConfig(),
-        learning_rate: float = 0.001,
-        epochs: int = 5,
-        batch_size: int = None,
-        metrics: List[str] = [],
-        validation: Optional[bolt.Validation] = None,
-        callbacks: List[bolt.train.callbacks.Callback] = [],
-        max_in_memory_batches: Optional[int] = None,
-        verbose: bool = True,
-        logging_interval: Optional[int] = None,
-        comm=None,
-        shuffle_reservoir_size: int = 64000,
-        **kwargs
-    ):
-        data_source = _create_data_source(filename)
-
-        val_data, val_metrics, train_options = _process_validation_and_options(
-            validation=validation,
-            batch_size=batch_size,
-            max_in_memory_batches=max_in_memory_batches,
-            verbose=verbose,
-            logging_interval=logging_interval,
-            shuffle_reservoir_size=shuffle_reservoir_size,
-        )
-
-        return original_cold_start(
-            self,
-            data=data_source,
-            strong_column_names=strong_column_names,
-            weak_column_names=weak_column_names,
-            variable_length=variable_length,
-            learning_rate=learning_rate,
-            epochs=epochs,
-            train_metrics=metrics,
-            val_data=val_data,
-            val_metrics=val_metrics,
-            callbacks=callbacks,
-            options=train_options,
-            comm=comm,
-            **kwargs
-        )
-
-    def wrapped_cold_start_on_data_source(
-        self,
-        data_source: dataset.DataSource,
-        strong_column_names: List[str],
-        weak_column_names: List[str],
-        variable_length: Optional[
-            data.transformations.VariableLengthConfig
-        ] = data.transformations.VariableLengthConfig(),
-        learning_rate: float = 0.001,
-        epochs: int = 5,
-        batch_size: int = None,
-        metrics: List[str] = [],
-        callbacks: List[bolt.train.callbacks.Callback] = [],
-        max_in_memory_batches: Optional[int] = None,
-        verbose: bool = True,
-        logging_interval: Optional[int] = None,
-        comm=None,
-        **kwargs
-    ):
-        val_data, val_metrics, train_options = _process_validation_and_options(
-            validation=None,
-            batch_size=batch_size,
-            max_in_memory_batches=max_in_memory_batches,
-            verbose=verbose,
-            logging_interval=logging_interval,
-        )
-
-        return original_cold_start(
-            self,
-            data=data_source,
-            strong_column_names=strong_column_names,
-            weak_column_names=weak_column_names,
-            variable_length=variable_length,
-            learning_rate=learning_rate,
-            epochs=epochs,
-            train_metrics=metrics,
-            val_data=val_data,
-            val_metrics=val_metrics,
-            callbacks=callbacks,
-            options=train_options,
-            comm=comm,
-            **kwargs
-        )
-
-    delattr(bolt.UniversalDeepTransformer, "train")
-    delattr(bolt.UniversalDeepTransformer, "evaluate")
-    delattr(bolt.UniversalDeepTransformer, "cold_start")
-
-    bolt.UniversalDeepTransformer.train = wrapped_train
-    bolt.UniversalDeepTransformer.train.__doc__ = udt_train_doc
-    bolt.UniversalDeepTransformer.evaluate = wrapped_evaluate
-    bolt.UniversalDeepTransformer.evaluate.__doc__ = udt_eval_doc
-    bolt.UniversalDeepTransformer.cold_start = wrapped_cold_start
-    bolt.UniversalDeepTransformer.cold_start.__doc__ = udt_cold_start_doc
-
-    bolt.UniversalDeepTransformer.train_on_data_source = wrapped_train_on_data_source
-    bolt.UniversalDeepTransformer.train_on_data_source.__doc__ = (
-        udt_train_on_datasource_doc
-    )
-    bolt.UniversalDeepTransformer.evaluate_on_data_source = (
-        wrapped_evaluate_on_data_source
-    )
-    bolt.UniversalDeepTransformer.evaluate_on_data_source.__doc__ = (
-        udt_eval_on_data_source_doc
-    )
-    bolt.UniversalDeepTransformer.cold_start_on_data_source = (
-        wrapped_cold_start_on_data_source
-    )
-    bolt.UniversalDeepTransformer.cold_start_on_data_source.__doc__ = (
-        udt_cold_start_on_data_source_doc
-    )
-
-
-def modify_mach_udt():
-    original_introduce_documents = bolt.UniversalDeepTransformer.introduce_documents
-
-    def wrapped_introduce_documents(
-        self,
-        filename: str,
-        strong_column_names: List[str],
-        weak_column_names: List[str],
-        num_buckets_to_sample: Optional[int] = None,
-        num_random_hashes: int = 0,
-        load_balancing: bool = False,
-        fast_approximation: bool = False,
-        verbose: bool = True,
-    ):
-        data_source = _create_data_source(filename)
-
-        return original_introduce_documents(
-            self,
-            data_source,
-            strong_column_names,
-            weak_column_names,
-            num_buckets_to_sample,
-            num_random_hashes,
-            load_balancing,
-            fast_approximation,
-            verbose,
-        )
-
-    delattr(bolt.UniversalDeepTransformer, "introduce_documents")
-
-    bolt.UniversalDeepTransformer.introduce_documents = wrapped_introduce_documents
-    bolt.UniversalDeepTransformer.introduce_documents_on_data_source = (
-        original_introduce_documents
-    )
-
-    def wrapped_associate_train(
-        self,
-        filename: str,
-        source_target_samples: List[Tuple[Dict[str, str], Dict[str, str]]],
-        n_buckets: int,
-        n_association_samples: int = 3,
-        learning_rate: float = 0.001,
-        epochs: int = 3,
-        metrics: List[str] = [],
-        batch_size: int = None,
-        verbose=True,
-    ):
-        return self.associate_train_data_source(
-            balancing_data=_create_data_source(filename),
-            source_target_samples=source_target_samples,
-            n_buckets=n_buckets,
-            n_association_samples=n_association_samples,
-            learning_rate=learning_rate,
-            epochs=epochs,
-            metrics=metrics,
-            options=_process_validation_and_options(
-                batch_size=batch_size,
-                verbose=verbose,
-            )[-1],
-        )
-
-    def wrapped_associate_cold_start(
-        self,
-        filename: str,
-        strong_column_names: List[str],
-        weak_column_names: List[str],
-        source_target_samples: List[Tuple[Dict[str, str], Dict[str, str]]],
-        n_buckets: int,
-        n_association_samples: int = 3,
-        learning_rate: float = 0.001,
-        epochs: int = 3,
-        metrics: List[str] = [],
-        batch_size: int = None,
-        verbose=True,
-    ):
-        return self.associate_cold_start_data_source(
-            balancing_data=_create_data_source(filename),
-            strong_column_names=strong_column_names,
-            weak_column_names=weak_column_names,
-            source_target_samples=source_target_samples,
-            n_buckets=n_buckets,
-            n_association_samples=n_association_samples,
-            learning_rate=learning_rate,
-            epochs=epochs,
-            metrics=metrics,
-            options=_process_validation_and_options(
-                batch_size=batch_size,
-                verbose=verbose,
-            )[-1],
-        )
-
-    bolt.UniversalDeepTransformer.associate_train = wrapped_associate_train
-    bolt.UniversalDeepTransformer.associate_cold_start = wrapped_associate_cold_start
-
-
-def modify_graph_udt():
-    original_index_nodes_method = bolt.UniversalDeepTransformer.index_nodes
-
-    def wrapped_index_nodes(self, filename: str):
-        data_source = _create_data_source(filename)
-
-        original_index_nodes_method(self, data_source)
-
-    delattr(bolt.UniversalDeepTransformer, "index_nodes")
-
-    bolt.UniversalDeepTransformer.index_nodes = wrapped_index_nodes
-    bolt.UniversalDeepTransformer.index_nodes.__doc__ = (
-        original_index_nodes_method.__doc__
-    )
-    bolt.UniversalDeepTransformer.index_nodes_on_data_source = (
-        original_index_nodes_method
-    )
+from typing import Dict, List, Optional, Tuple
+
+import pandas as pd
+import thirdai
+import thirdai._thirdai.bolt as bolt
+import thirdai._thirdai.data as data
+import thirdai._thirdai.dataset as dataset
+
+from .udt_docs import *
+
+
+def _create_parquet_source(path):
+    return thirdai.dataset.ParquetSource(parquet_path=path)
+
+
+def _create_data_source(path):
+    """
+    Reading data from S3 and GCS assumes that the credentials are already
+    set. For S3, pandas.read_csv method in the data loader will look for
+    credentials in ~/.aws/credentials while for GCS the path will be assumed to be
+    ~/.config/gcloud/credentials or ~/.config/gcloud/application_default_credentials.json.
+    """
+
+    # This also handles parquet on s3, so it comes before the general s3 and gcs
+    # handling and file handling below which assume the target files are
+    # CSVs.
+    if path.endswith(".parquet") or path.endswith(".pqt"):
+        return _create_parquet_source(path)
+
+    if path.startswith("s3://") or path.startswith("gcs://"):
+        return thirdai.dataset.CSVDataSource(
+            storage_path=path,
+        )
+
+    return thirdai.dataset.FileDataSource(path)
+
+
+def _process_validation_and_options(
+    validation: Optional[bolt.Validation] = None,
+    batch_size: Optional[int] = None,
+    max_in_memory_batches: Optional[int] = None,
+    verbose: bool = True,
+    logging_interval: Optional[int] = None,
+    shuffle_reservoir_size: int = 64000,
+):
+    train_options = bolt.TrainOptions()
+
+    train_options.batch_size = batch_size
+    train_options.max_in_memory_batches = max_in_memory_batches
+    train_options.verbose = verbose
+    train_options.logging_interval = logging_interval
+    train_options.shuffle_config = dataset.ShuffleConfig(
+        min_vecs_in_buffer=shuffle_reservoir_size
+    )
+
+    if validation:
+        val_data = _create_data_source(validation.filename)
+        train_options.steps_per_validation = validation.steps_per_validation
+        train_options.sparse_validation = validation.sparse_validation
+
+        return val_data, validation.metrics, train_options
+
+    return None, [], train_options
+
+
+def modify_udt():
+    original_train = bolt.UniversalDeepTransformer.train
+    original_evaluate = bolt.UniversalDeepTransformer.evaluate
+    original_cold_start = bolt.UniversalDeepTransformer.cold_start
+
+    def wrapped_train(
+        self,
+        filename: str,
+        learning_rate: float = 0.001,
+        epochs: int = 5,
+        validation: Optional[bolt.Validation] = None,
+        batch_size: Optional[int] = None,
+        max_in_memory_batches: Optional[int] = None,
+        verbose: bool = True,
+        callbacks: List[bolt.train.callbacks.Callback] = [],
+        metrics: List[str] = [],
+        logging_interval: Optional[int] = None,
+        shuffle_reservoir_size: int = 64000,
+        comm=None,
+        **kwargs
+    ):
+        data_source = _create_data_source(filename)
+
+        val_data, val_metrics, train_options = _process_validation_and_options(
+            validation=validation,
+            batch_size=batch_size,
+            max_in_memory_batches=max_in_memory_batches,
+            verbose=verbose,
+            logging_interval=logging_interval,
+            shuffle_reservoir_size=shuffle_reservoir_size,
+        )
+
+        return original_train(
+            self,
+            data=data_source,
+            learning_rate=learning_rate,
+            epochs=epochs,
+            train_metrics=metrics,
+            val_data=val_data,
+            val_metrics=val_metrics,
+            callbacks=callbacks,
+            options=train_options,
+            comm=comm,
+            **kwargs
+        )
+
+    def wrapped_train_on_data_source(
+        self,
+        data_source: dataset.DataSource,
+        learning_rate: float = 0.001,
+        epochs: int = 3,
+        batch_size: Optional[int] = None,
+        max_in_memory_batches: Optional[int] = None,
+        verbose: bool = True,
+        callbacks: List[bolt.train.callbacks.Callback] = [],
+        metrics: List[str] = [],
+        logging_interval: Optional[int] = None,
+        comm=None,
+        **kwargs
+    ):
+        val_data, val_metrics, train_options = _process_validation_and_options(
+            validation=None,
+            batch_size=batch_size,
+            max_in_memory_batches=max_in_memory_batches,
+            verbose=verbose,
+            logging_interval=logging_interval,
+        )
+
+        return original_train(
+            self,
+            data=data_source,
+            learning_rate=learning_rate,
+            epochs=epochs,
+            train_metrics=metrics,
+            val_data=val_data,
+            val_metrics=val_metrics,
+            callbacks=callbacks,
+            options=train_options,
+            comm=comm,
+            **kwargs
+        )
+
+    def wrapped_evaluate(
+        self,
+        filename: str,
+        metrics: List[str] = [],
+        use_sparse_inference: bool = False,
+        verbose: bool = True,
+        **kwargs
+    ):
+        data_source = _create_data_source(filename)
+
+        return original_evaluate(
+            self,
+            data=data_source,
+            metrics=metrics,
+            sparse_inference=use_sparse_inference,
+            verbose=verbose,
+            **kwargs
+        )
+
+    def wrapped_evaluate_on_data_source(
+        self,
+        data_source: dataset.DataSource,
+        metrics: List[str] = [],
+        use_sparse_inference: bool = False,
+        verbose: bool = True,
+        **kwargs
+    ):
+        return original_evaluate(
+            self,
+            data=data_source,
+            metrics=metrics,
+            sparse_inference=use_sparse_inference,
+            verbose=verbose,
+            **kwargs
+        )
+
+    def wrapped_cold_start(
+        self,
+        filename: str,
+        strong_column_names: List[str],
+        weak_column_names: List[str],
+        variable_length: Optional[
+            data.transformations.VariableLengthConfig
+        ] = data.transformations.VariableLengthConfig(),
+        learning_rate: float = 0.001,
+        epochs: int = 5,
+        batch_size: int = None,
+        metrics: List[str] = [],
+        validation: Optional[bolt.Validation] = None,
+        callbacks: List[bolt.train.callbacks.Callback] = [],
+        max_in_memory_batches: Optional[int] = None,
+        verbose: bool = True,
+        logging_interval: Optional[int] = None,
+        comm=None,
+        shuffle_reservoir_size: int = 64000,
+        **kwargs
+    ):
+        data_source = _create_data_source(filename)
+
+        val_data, val_metrics, train_options = _process_validation_and_options(
+            validation=validation,
+            batch_size=batch_size,
+            max_in_memory_batches=max_in_memory_batches,
+            verbose=verbose,
+            logging_interval=logging_interval,
+            shuffle_reservoir_size=shuffle_reservoir_size,
+        )
+
+        return original_cold_start(
+            self,
+            data=data_source,
+            strong_column_names=strong_column_names,
+            weak_column_names=weak_column_names,
+            variable_length=variable_length,
+            learning_rate=learning_rate,
+            epochs=epochs,
+            train_metrics=metrics,
+            val_data=val_data,
+            val_metrics=val_metrics,
+            callbacks=callbacks,
+            options=train_options,
+            comm=comm,
+            **kwargs
+        )
+
+    def wrapped_cold_start_on_data_source(
+        self,
+        data_source: dataset.DataSource,
+        strong_column_names: List[str],
+        weak_column_names: List[str],
+        variable_length: Optional[
+            data.transformations.VariableLengthConfig
+        ] = data.transformations.VariableLengthConfig(),
+        learning_rate: float = 0.001,
+        epochs: int = 5,
+        batch_size: int = None,
+        metrics: List[str] = [],
+        callbacks: List[bolt.train.callbacks.Callback] = [],
+        max_in_memory_batches: Optional[int] = None,
+        verbose: bool = True,
+        logging_interval: Optional[int] = None,
+        comm=None,
+        **kwargs
+    ):
+        val_data, val_metrics, train_options = _process_validation_and_options(
+            validation=None,
+            batch_size=batch_size,
+            max_in_memory_batches=max_in_memory_batches,
+            verbose=verbose,
+            logging_interval=logging_interval,
+        )
+
+        return original_cold_start(
+            self,
+            data=data_source,
+            strong_column_names=strong_column_names,
+            weak_column_names=weak_column_names,
+            variable_length=variable_length,
+            learning_rate=learning_rate,
+            epochs=epochs,
+            train_metrics=metrics,
+            val_data=val_data,
+            val_metrics=val_metrics,
+            callbacks=callbacks,
+            options=train_options,
+            comm=comm,
+            **kwargs
+        )
+
+    delattr(bolt.UniversalDeepTransformer, "train")
+    delattr(bolt.UniversalDeepTransformer, "evaluate")
+    delattr(bolt.UniversalDeepTransformer, "cold_start")
+
+    bolt.UniversalDeepTransformer.train = wrapped_train
+    bolt.UniversalDeepTransformer.train.__doc__ = udt_train_doc
+    bolt.UniversalDeepTransformer.evaluate = wrapped_evaluate
+    bolt.UniversalDeepTransformer.evaluate.__doc__ = udt_eval_doc
+    bolt.UniversalDeepTransformer.cold_start = wrapped_cold_start
+    bolt.UniversalDeepTransformer.cold_start.__doc__ = udt_cold_start_doc
+
+    bolt.UniversalDeepTransformer.train_on_data_source = wrapped_train_on_data_source
+    bolt.UniversalDeepTransformer.train_on_data_source.__doc__ = (
+        udt_train_on_datasource_doc
+    )
+    bolt.UniversalDeepTransformer.evaluate_on_data_source = (
+        wrapped_evaluate_on_data_source
+    )
+    bolt.UniversalDeepTransformer.evaluate_on_data_source.__doc__ = (
+        udt_eval_on_data_source_doc
+    )
+    bolt.UniversalDeepTransformer.cold_start_on_data_source = (
+        wrapped_cold_start_on_data_source
+    )
+    bolt.UniversalDeepTransformer.cold_start_on_data_source.__doc__ = (
+        udt_cold_start_on_data_source_doc
+    )
+
+
+def modify_mach_udt():
+    original_introduce_documents = bolt.UniversalDeepTransformer.introduce_documents
+
+    def wrapped_introduce_documents(
+        self,
+        filename: str,
+        strong_column_names: List[str],
+        weak_column_names: List[str],
+        num_buckets_to_sample: Optional[int] = None,
+        num_random_hashes: int = 0,
+        load_balancing: bool = False,
+        fast_approximation: bool = False,
+        verbose: bool = True,
+    ):
+        data_source = _create_data_source(filename)
+
+        return original_introduce_documents(
+            self,
+            data_source,
+            strong_column_names,
+            weak_column_names,
+            num_buckets_to_sample,
+            num_random_hashes,
+            load_balancing,
+            fast_approximation,
+            verbose,
+        )
+
+    delattr(bolt.UniversalDeepTransformer, "introduce_documents")
+
+    bolt.UniversalDeepTransformer.introduce_documents = wrapped_introduce_documents
+    bolt.UniversalDeepTransformer.introduce_documents_on_data_source = (
+        original_introduce_documents
+    )
+
+    def wrapped_associate_train(
+        self,
+        filename: str,
+        source_target_samples: List[Tuple[Dict[str, str], Dict[str, str]]],
+        n_buckets: int,
+        n_association_samples: int = 3,
+        learning_rate: float = 0.001,
+        epochs: int = 3,
+        metrics: List[str] = [],
+        batch_size: int = None,
+        verbose=True,
+    ):
+        return self.associate_train_data_source(
+            balancing_data=_create_data_source(filename),
+            source_target_samples=source_target_samples,
+            n_buckets=n_buckets,
+            n_association_samples=n_association_samples,
+            learning_rate=learning_rate,
+            epochs=epochs,
+            metrics=metrics,
+            options=_process_validation_and_options(
+                batch_size=batch_size,
+                verbose=verbose,
+            )[-1],
+        )
+
+    def wrapped_associate_cold_start(
+        self,
+        filename: str,
+        strong_column_names: List[str],
+        weak_column_names: List[str],
+        source_target_samples: List[Tuple[Dict[str, str], Dict[str, str]]],
+        n_buckets: int,
+        n_association_samples: int = 3,
+        learning_rate: float = 0.001,
+        epochs: int = 3,
+        metrics: List[str] = [],
+        batch_size: int = None,
+        verbose=True,
+    ):
+        return self.associate_cold_start_data_source(
+            balancing_data=_create_data_source(filename),
+            strong_column_names=strong_column_names,
+            weak_column_names=weak_column_names,
+            source_target_samples=source_target_samples,
+            n_buckets=n_buckets,
+            n_association_samples=n_association_samples,
+            learning_rate=learning_rate,
+            epochs=epochs,
+            metrics=metrics,
+            options=_process_validation_and_options(
+                batch_size=batch_size,
+                verbose=verbose,
+            )[-1],
+        )
+
+    bolt.UniversalDeepTransformer.associate_train = wrapped_associate_train
+    bolt.UniversalDeepTransformer.associate_cold_start = wrapped_associate_cold_start
+
+
+def modify_graph_udt():
+    original_index_nodes_method = bolt.UniversalDeepTransformer.index_nodes
+
+    def wrapped_index_nodes(self, filename: str):
+        data_source = _create_data_source(filename)
+
+        original_index_nodes_method(self, data_source)
+
+    delattr(bolt.UniversalDeepTransformer, "index_nodes")
+
+    bolt.UniversalDeepTransformer.index_nodes = wrapped_index_nodes
+    bolt.UniversalDeepTransformer.index_nodes.__doc__ = (
+        original_index_nodes_method.__doc__
+    )
+    bolt.UniversalDeepTransformer.index_nodes_on_data_source = (
+        original_index_nodes_method
+    )
```

## thirdai/bolt/__init__.py

 * *Ordering differences only*

```diff
@@ -1,22 +1,22 @@
-import thirdai._thirdai.bolt
-from thirdai._thirdai.bolt import *
-
-from .ner_modifications import modify_ner
-from .udt_modifications import modify_graph_udt, modify_mach_udt, modify_udt
-
-modify_udt()
-modify_graph_udt()
-modify_mach_udt()
-modify_ner()
-
-try:
-    # This is to prevent errors if torch or scipy are not installed.
-    # They are used in the seismic model but not the main thirdai package.
-    from .seismic_modifications import modify_seismic
-
-    modify_seismic()
-except ImportError:
-    pass
-
-__all__ = []
-__all__.extend(dir(thirdai._thirdai.bolt))
+import thirdai._thirdai.bolt
+from thirdai._thirdai.bolt import *
+
+from .ner_modifications import modify_ner
+from .udt_modifications import modify_graph_udt, modify_mach_udt, modify_udt
+
+modify_udt()
+modify_graph_udt()
+modify_mach_udt()
+modify_ner()
+
+try:
+    # This is to prevent errors if torch or scipy are not installed.
+    # They are used in the seismic model but not the main thirdai package.
+    from .seismic_modifications import modify_seismic
+
+    modify_seismic()
+except ImportError:
+    pass
+
+__all__ = []
+__all__.extend(dir(thirdai._thirdai.bolt))
```

## thirdai/data/column_map_utils.py

 * *Ordering differences only*

```diff
@@ -1,48 +1,48 @@
-from abc import ABC, abstractmethod
-from typing import Optional
-
-import numpy as np
-from thirdai._thirdai.data import ColumnMap, columns
-
-
-class ColumnMapGenerator(ABC):
-    @abstractmethod
-    def next() -> Optional[ColumnMap]:
-        pass
-
-    @abstractmethod
-    def restart() -> None:
-        pass
-
-
-def _is_string_column(column):
-    return all([isinstance(s, str) for s in column])
-
-
-def pandas_to_columnmap(df, dense_int_cols=set(), int_col_dims={}):
-    """
-    Converts a pandas dataframe to a ColumnMap object. This method assumes that
-    integer type columns are sparse. If you want to force an integer column to
-    be dense, pass the name of the column as an element of the dense_int_cols
-    set. This method will also assume that integer columns are
-    non-concatenatable (i.e. they have None for the dim), but you can explicitly
-    pass in the actual range in the int_col_dims dictionary with the key of the
-    column name . Finally, note that the pandas array should have valid headers,
-    as these will be the names of the column in the ColumnMap.
-    """
-    column_map = {}
-    for column_name in df:
-        column_np = df[column_name].to_numpy()
-        if np.issubdtype(column_np.dtype, np.floating) or column_name in dense_int_cols:
-            column_map[column_name] = columns.DecimalColumn(data=column_np)
-        elif np.issubdtype(column_np.dtype, np.integer):
-            dim = int_col_dims[column_name] if column_name in int_col_dims else None
-            column_map[column_name] = columns.TokenColumn(data=column_np, dim=dim)
-        elif _is_string_column(column_np):
-            column_map[column_name] = columns.StringColumn(data=column_np)
-        else:
-            raise ValueError(
-                f"All columns must be either an integer, float, or string type, but column {column_name} was none of these types."
-            )
-
-    return ColumnMap(column_map)
+from abc import ABC, abstractmethod
+from typing import Optional
+
+import numpy as np
+from thirdai._thirdai.data import ColumnMap, columns
+
+
+class ColumnMapGenerator(ABC):
+    @abstractmethod
+    def next() -> Optional[ColumnMap]:
+        pass
+
+    @abstractmethod
+    def restart() -> None:
+        pass
+
+
+def _is_string_column(column):
+    return all([isinstance(s, str) for s in column])
+
+
+def pandas_to_columnmap(df, dense_int_cols=set(), int_col_dims={}):
+    """
+    Converts a pandas dataframe to a ColumnMap object. This method assumes that
+    integer type columns are sparse. If you want to force an integer column to
+    be dense, pass the name of the column as an element of the dense_int_cols
+    set. This method will also assume that integer columns are
+    non-concatenatable (i.e. they have None for the dim), but you can explicitly
+    pass in the actual range in the int_col_dims dictionary with the key of the
+    column name . Finally, note that the pandas array should have valid headers,
+    as these will be the names of the column in the ColumnMap.
+    """
+    column_map = {}
+    for column_name in df:
+        column_np = df[column_name].to_numpy()
+        if np.issubdtype(column_np.dtype, np.floating) or column_name in dense_int_cols:
+            column_map[column_name] = columns.DecimalColumn(data=column_np)
+        elif np.issubdtype(column_np.dtype, np.integer):
+            dim = int_col_dims[column_name] if column_name in int_col_dims else None
+            column_map[column_name] = columns.TokenColumn(data=column_np, dim=dim)
+        elif _is_string_column(column_np):
+            column_map[column_name] = columns.StringColumn(data=column_np)
+        else:
+            raise ValueError(
+                f"All columns must be either an integer, float, or string type, but column {column_name} was none of these types."
+            )
+
+    return ColumnMap(column_map)
```

## thirdai/data/get_udt_columns.py

 * *Ordering differences only*

```diff
@@ -1,66 +1,66 @@
-from typing import Dict
-
-import pandas as pd
-import thirdai._thirdai.bolt as bolt
-
-from .type_inference import semantic_type_inference
-
-
-def get_udt_col_types(
-    filename: str, n_rows: int = 1e6
-) -> Dict[str, bolt.types.ColumnType]:
-    """Returns a best guess for the types and metadata of the columns of the
-            input file.
-
-    Args:
-        filename (str): Path to a csv or a parquet stored locally or on aws/gcp/etc
-            (anything that can be read by pandas.read_csv or pandas.read_parquet).
-
-    Returns:
-        (Dict[str, bolt.types.ColumnType]):
-        A map from column name to our best guess for ColumnType.
-    """
-    column_types = semantic_type_inference(filename)
-
-    try:
-        if filename.endswith(".pqt") or filename.endswith(".parquet"):
-            df = pd.read_parquet(filename)
-        else:
-            df = pd.read_csv(filename, nrows=n_rows, low_memory=False)
-    except:
-        raise ValueError(
-            "UDT currently supports all files that can be read using "
-            "pandas.read_parquet (for .pqt or .parquet files) or "
-            "pandas.read_csv (for all other files). Please convert your files "
-            "to one of the supported formats."
-        )
-
-    udt_column_types = {}
-
-    for col_name, type_info in column_types.items():
-        if col_name not in df.columns:
-            raise ValueError(
-                f"column_type map contains column: {col_name} not in dataframe."
-            )
-        col_type = type_info["type"]
-
-        if col_type == "text":
-            udt_column_types[col_name] = bolt.types.text()
-        elif col_type == "categorical":
-            udt_column_types[col_name] = bolt.types.categorical()
-        elif col_type == "multi-categorical":
-            udt_column_types[col_name] = bolt.types.categorical(
-                delimiter=type_info["delimiter"]
-            )
-        elif col_type == "numerical":
-            min_val = df[col_name].min()
-            max_val = df[col_name].max()
-            udt_column_types[col_name] = bolt.types.numerical(range=(min_val, max_val))
-        elif col_type == "datetime":
-            udt_column_types[col_name] = bolt.types.date()
-        else:
-            raise ValueError(
-                f"Received invalid column type: {col_type}. Supports 'text', 'categorical', 'multi-categorical', 'numerical', and 'datetime'."
-            )
-
-    return udt_column_types
+from typing import Dict
+
+import pandas as pd
+import thirdai._thirdai.bolt as bolt
+
+from .type_inference import semantic_type_inference
+
+
+def get_udt_col_types(
+    filename: str, n_rows: int = 1e6
+) -> Dict[str, bolt.types.ColumnType]:
+    """Returns a best guess for the types and metadata of the columns of the
+            input file.
+
+    Args:
+        filename (str): Path to a csv or a parquet stored locally or on aws/gcp/etc
+            (anything that can be read by pandas.read_csv or pandas.read_parquet).
+
+    Returns:
+        (Dict[str, bolt.types.ColumnType]):
+        A map from column name to our best guess for ColumnType.
+    """
+    column_types = semantic_type_inference(filename)
+
+    try:
+        if filename.endswith(".pqt") or filename.endswith(".parquet"):
+            df = pd.read_parquet(filename)
+        else:
+            df = pd.read_csv(filename, nrows=n_rows, low_memory=False)
+    except:
+        raise ValueError(
+            "UDT currently supports all files that can be read using "
+            "pandas.read_parquet (for .pqt or .parquet files) or "
+            "pandas.read_csv (for all other files). Please convert your files "
+            "to one of the supported formats."
+        )
+
+    udt_column_types = {}
+
+    for col_name, type_info in column_types.items():
+        if col_name not in df.columns:
+            raise ValueError(
+                f"column_type map contains column: {col_name} not in dataframe."
+            )
+        col_type = type_info["type"]
+
+        if col_type == "text":
+            udt_column_types[col_name] = bolt.types.text()
+        elif col_type == "categorical":
+            udt_column_types[col_name] = bolt.types.categorical()
+        elif col_type == "multi-categorical":
+            udt_column_types[col_name] = bolt.types.categorical(
+                delimiter=type_info["delimiter"]
+            )
+        elif col_type == "numerical":
+            min_val = df[col_name].min()
+            max_val = df[col_name].max()
+            udt_column_types[col_name] = bolt.types.numerical(range=(min_val, max_val))
+        elif col_type == "datetime":
+            udt_column_types[col_name] = bolt.types.date()
+        else:
+            raise ValueError(
+                f"Received invalid column type: {col_type}. Supports 'text', 'categorical', 'multi-categorical', 'numerical', and 'datetime'."
+            )
+
+    return udt_column_types
```

## thirdai/data/type_inference.py

 * *Ordering differences only*

```diff
@@ -1,159 +1,159 @@
-from typing import Any, Dict, List, Optional, Tuple
-
-import pandas as pd
-
-_TEXT_DELIMITER = " "
-_CATEGORICAL_DELIMITERS = [";", ":", "-", "\t", "|"]
-# If the most occurring delimiter would cause there to be more than _DELIMITER_RATIO_THRESHOLD
-# items per line, that is enough to assume that these are valid splits and that
-# the column is indeed multi-categorical (or text)
-_DELIMITER_RATIO_THRESHOLD = 1.5
-
-
-# Returns the average number of entries per row there would be if the passed in
-# delimiter was the actual delimiter
-def _get_delimiter_ratio(column: pd.Series, delimiter: str) -> float:
-    count = sum(column.apply(lambda entry: entry.strip().count(delimiter))) + len(
-        column
-    )
-    return count / len(column)
-
-
-# Returns a tuple where the first item is whether the column is multi-categorical
-# and the second is the column's delimiter if it is multi-categorical and None
-# otherwise.
-def _is_multi_categorical(column: pd.Series) -> Tuple[bool, Optional[str]]:
-    ratios = {
-        delimiter: _get_delimiter_ratio(column, delimiter)
-        for delimiter in _CATEGORICAL_DELIMITERS
-    }
-
-    most_occurring_delimiter = max(ratios, key=lambda entry: ratios[entry])
-    most_occurring_ratio = ratios[most_occurring_delimiter]
-
-    if most_occurring_ratio >= _DELIMITER_RATIO_THRESHOLD:
-        return True, most_occurring_delimiter
-
-    return False, None
-
-
-def _is_text_col(column: pd.Series) -> bool:
-    space_ratio = _get_delimiter_ratio(column, delimiter=_TEXT_DELIMITER)
-    return space_ratio > _DELIMITER_RATIO_THRESHOLD
-
-
-def _is_datetime_col(column: pd.Series) -> bool:
-    try:
-        pd.to_datetime(column)
-        return True
-    except Exception as e:
-        return False
-
-
-def _is_float_col(column: pd.Series):
-    try:
-        converted_col = pd.to_numeric(column)
-        return converted_col.dtype == "float64"
-    except:
-        return False
-
-
-def _is_int_col(column: pd.Series):
-    try:
-        converted_col = pd.to_numeric(column)
-        return converted_col.dtype == "int64"
-    except:
-        return False
-
-
-def _infer_col_type(column: pd.Series) -> Dict[str, str]:
-    # Since Pandas reads blank values as NA in read_csv, this will drop missing
-    # values, thus allowing us to do type inference correctly.
-    column = column.dropna()
-
-    if len(column) < 2:
-        raise ValueError(
-            f"Column {column} has less than 2 non-missing values so we cannot do type inference"
-        )
-
-    if _is_float_col(column):
-        return {"type": "numerical"}
-
-    if _is_int_col(column):
-        return {"type": "categorical"}
-
-    if _is_datetime_col(column):
-        return {"type": "datetime"}
-
-    if _is_text_col(column):
-        return {"type": "text"}
-
-    is_multi_categorical, delimiter = _is_multi_categorical(column)
-    if is_multi_categorical:
-        assert delimiter is not None
-        return {
-            "type": "multi-categorical",
-            "delimiter": delimiter,
-        }
-
-    return {"type": "categorical"}
-
-
-def semantic_type_inference(
-    filename: str, nrows: int = 100, min_rows_allowed: int = 3
-) -> Dict[str, Dict[str, str]]:
-    """Tries to parse the given filename as a csv and then infer the type of each
-    column.
-
-    Args:
-        filename: The filename to use for type inference. The file should be
-            a CSV with no extra whitespace between items.
-
-        nrows: The number of rows of the file to use to infer column types. If
-            the file has less than nrows number of rows, all rows will be used.
-
-        min_rows_allowed: The minimum number of rows we allow for inferring
-            types. If we find less than min_rows_allowed rows, we will throw a
-            ValueError.
-
-    Returns:
-        A map from column name to a metadata dictionary. One of the items in the
-        metadata dictionary will be {"type": X}, where X is one of ["categorical",
-        "multi-categorical", "datetime", "numerical", "text"]. Types that are
-        "multi-categorical" will have an additional item in the dictionary
-        specifying the estimate delimiter. Refer to the infer_types_test.py files
-        for full examples with expected inputs and outputs.
-    """
-    try:
-        if filename.endswith(".pqt") or filename.endswith(".parquet"):
-            df = pd.read_parquet(filename)
-        else:
-            # We force dtype=object so that int and string columns are treated correctly
-            # even with missing values (we will later drop the missing values, which
-            # get converted to NAs during read_csv, and then convert to the correct
-            # more specific type).
-            df = pd.read_csv(filename, nrows=nrows, dtype=object)
-    except:
-        raise ValueError(
-            "UDT currently supports all files that can be read using "
-            "pandas.read_parquet (for .pqt or .parquet files) or "
-            "pandas.read_csv (for all other files). Please convert your files "
-            "to one of the supported formats."
-        )
-
-    if len(df) < min_rows_allowed:
-        raise ValueError(
-            f"Parsed csv {filename} must have at least {min_rows_allowed} rows, but we found only {len(df)} rows."
-        )
-
-    semantic_types = {}
-    for column_name in df:
-        # Mypy says this can happen sometimes, but I'm not sure when.
-        if isinstance(column_name, float):
-            raise ValueError(
-                f"All columns should have valid names, but found a column with float name {column_name}"
-            )
-
-        semantic_types[column_name] = _infer_col_type(df[column_name])
-
-    return semantic_types
+from typing import Any, Dict, List, Optional, Tuple
+
+import pandas as pd
+
+_TEXT_DELIMITER = " "
+_CATEGORICAL_DELIMITERS = [";", ":", "-", "\t", "|"]
+# If the most occurring delimiter would cause there to be more than _DELIMITER_RATIO_THRESHOLD
+# items per line, that is enough to assume that these are valid splits and that
+# the column is indeed multi-categorical (or text)
+_DELIMITER_RATIO_THRESHOLD = 1.5
+
+
+# Returns the average number of entries per row there would be if the passed in
+# delimiter was the actual delimiter
+def _get_delimiter_ratio(column: pd.Series, delimiter: str) -> float:
+    count = sum(column.apply(lambda entry: entry.strip().count(delimiter))) + len(
+        column
+    )
+    return count / len(column)
+
+
+# Returns a tuple where the first item is whether the column is multi-categorical
+# and the second is the column's delimiter if it is multi-categorical and None
+# otherwise.
+def _is_multi_categorical(column: pd.Series) -> Tuple[bool, Optional[str]]:
+    ratios = {
+        delimiter: _get_delimiter_ratio(column, delimiter)
+        for delimiter in _CATEGORICAL_DELIMITERS
+    }
+
+    most_occurring_delimiter = max(ratios, key=lambda entry: ratios[entry])
+    most_occurring_ratio = ratios[most_occurring_delimiter]
+
+    if most_occurring_ratio >= _DELIMITER_RATIO_THRESHOLD:
+        return True, most_occurring_delimiter
+
+    return False, None
+
+
+def _is_text_col(column: pd.Series) -> bool:
+    space_ratio = _get_delimiter_ratio(column, delimiter=_TEXT_DELIMITER)
+    return space_ratio > _DELIMITER_RATIO_THRESHOLD
+
+
+def _is_datetime_col(column: pd.Series) -> bool:
+    try:
+        pd.to_datetime(column)
+        return True
+    except Exception as e:
+        return False
+
+
+def _is_float_col(column: pd.Series):
+    try:
+        converted_col = pd.to_numeric(column)
+        return converted_col.dtype == "float64"
+    except:
+        return False
+
+
+def _is_int_col(column: pd.Series):
+    try:
+        converted_col = pd.to_numeric(column)
+        return converted_col.dtype == "int64"
+    except:
+        return False
+
+
+def _infer_col_type(column: pd.Series) -> Dict[str, str]:
+    # Since Pandas reads blank values as NA in read_csv, this will drop missing
+    # values, thus allowing us to do type inference correctly.
+    column = column.dropna()
+
+    if len(column) < 2:
+        raise ValueError(
+            f"Column {column} has less than 2 non-missing values so we cannot do type inference"
+        )
+
+    if _is_float_col(column):
+        return {"type": "numerical"}
+
+    if _is_int_col(column):
+        return {"type": "categorical"}
+
+    if _is_datetime_col(column):
+        return {"type": "datetime"}
+
+    if _is_text_col(column):
+        return {"type": "text"}
+
+    is_multi_categorical, delimiter = _is_multi_categorical(column)
+    if is_multi_categorical:
+        assert delimiter is not None
+        return {
+            "type": "multi-categorical",
+            "delimiter": delimiter,
+        }
+
+    return {"type": "categorical"}
+
+
+def semantic_type_inference(
+    filename: str, nrows: int = 100, min_rows_allowed: int = 3
+) -> Dict[str, Dict[str, str]]:
+    """Tries to parse the given filename as a csv and then infer the type of each
+    column.
+
+    Args:
+        filename: The filename to use for type inference. The file should be
+            a CSV with no extra whitespace between items.
+
+        nrows: The number of rows of the file to use to infer column types. If
+            the file has less than nrows number of rows, all rows will be used.
+
+        min_rows_allowed: The minimum number of rows we allow for inferring
+            types. If we find less than min_rows_allowed rows, we will throw a
+            ValueError.
+
+    Returns:
+        A map from column name to a metadata dictionary. One of the items in the
+        metadata dictionary will be {"type": X}, where X is one of ["categorical",
+        "multi-categorical", "datetime", "numerical", "text"]. Types that are
+        "multi-categorical" will have an additional item in the dictionary
+        specifying the estimate delimiter. Refer to the infer_types_test.py files
+        for full examples with expected inputs and outputs.
+    """
+    try:
+        if filename.endswith(".pqt") or filename.endswith(".parquet"):
+            df = pd.read_parquet(filename)
+        else:
+            # We force dtype=object so that int and string columns are treated correctly
+            # even with missing values (we will later drop the missing values, which
+            # get converted to NAs during read_csv, and then convert to the correct
+            # more specific type).
+            df = pd.read_csv(filename, nrows=nrows, dtype=object)
+    except:
+        raise ValueError(
+            "UDT currently supports all files that can be read using "
+            "pandas.read_parquet (for .pqt or .parquet files) or "
+            "pandas.read_csv (for all other files). Please convert your files "
+            "to one of the supported formats."
+        )
+
+    if len(df) < min_rows_allowed:
+        raise ValueError(
+            f"Parsed csv {filename} must have at least {min_rows_allowed} rows, but we found only {len(df)} rows."
+        )
+
+    semantic_types = {}
+    for column_name in df:
+        # Mypy says this can happen sometimes, but I'm not sure when.
+        if isinstance(column_name, float):
+            raise ValueError(
+                f"All columns should have valid names, but found a column with float name {column_name}"
+            )
+
+        semantic_types[column_name] = _infer_col_type(df[column_name])
+
+    return semantic_types
```

## thirdai/data/__init__.py

 * *Ordering differences only*

```diff
@@ -1,18 +1,18 @@
-import thirdai._thirdai.data
-from thirdai._thirdai.data import *
-
-from .column_map_utils import ColumnMapGenerator, pandas_to_columnmap
-from .get_udt_columns import get_udt_col_types
-from .type_inference import _CATEGORICAL_DELIMITERS, semantic_type_inference
-
-__all__ = []
-__all__.extend(dir(thirdai._thirdai.data))
-__all__.extend(
-    [
-        "ColumnMapGenerator",
-        "pandas_to_columnmap",
-        "semantic_type_inference",
-        "_CATEGORICAL_DELIMITERS",
-        "get_udt_col_types",
-    ]
-)
+import thirdai._thirdai.data
+from thirdai._thirdai.data import *
+
+from .column_map_utils import ColumnMapGenerator, pandas_to_columnmap
+from .get_udt_columns import get_udt_col_types
+from .type_inference import _CATEGORICAL_DELIMITERS, semantic_type_inference
+
+__all__ = []
+__all__.extend(dir(thirdai._thirdai.data))
+__all__.extend(
+    [
+        "ColumnMapGenerator",
+        "pandas_to_columnmap",
+        "semantic_type_inference",
+        "_CATEGORICAL_DELIMITERS",
+        "get_udt_col_types",
+    ]
+)
```

## thirdai/dataset/bolt_ner_data_source.py

 * *Ordering differences only*

```diff
@@ -1,77 +1,77 @@
-import json
-
-from thirdai.dataset.data_source import PyDataSource
-
-
-def tokenize_text(tokenizer, text):
-    tokens = tokenizer.encode(text)
-    return " ".join(map(str, tokens))
-
-
-class NerDataSource(PyDataSource):
-    def __init__(
-        self, model_type, tokens_column=None, tags_column=None, file_path=None
-    ):
-        PyDataSource.__init__(self)
-
-        self.file_path = file_path
-
-        self.tokens_column = tokens_column
-        self.tags_column = tags_column
-        self.pretrained = model_type == "bolt_ner"
-
-        if self.pretrained:
-            try:
-                from transformers import GPT2Tokenizer
-
-                self.tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
-            except ImportError:
-                raise ImportError(
-                    "transformers library is not installed. Please install it to use LLMDataSource."
-                )
-
-        self.restart()
-
-    def _get_line_iterator(self):
-
-        if self.file_path is None:
-            raise ValueError(
-                "The file path is None. Please provide a valid file path to access the data source."
-            )
-
-        if self.tags_column is None or self.tokens_column is None:
-            raise ValueError(
-                "Cannot load a datasource with either 'tokens_column' or 'tags_column' set to None. Please provide valid column names."
-            )
-
-        with open(self.file_path, "r") as file:
-            for line in file:
-
-                json_obj = json.loads(line.strip())
-                if not all(
-                    column in json_obj
-                    for column in [self.tags_column, self.tokens_column]
-                ):
-                    raise ValueError(
-                        f"{self.tags_column} or {self.tokens_column} doesn't exist in the column, line: {line}"
-                    )
-                if self.pretrained:
-                    json_obj[self.tokens_column] = [
-                        tokenize_text(self.tokenizer, token)
-                        for token in json_obj[self.tokens_column]
-                    ]
-
-                data = json.dumps(json_obj)
-
-                yield data
-
-    def inference_featurizer(self, sentence_tokens_list):
-        if self.pretrained:
-            return [
-                [tokenize_text(self.tokenizer, token) for token in sentence_tokens]
-                for sentence_tokens in sentence_tokens_list
-            ]
-        return sentence_tokens_list
-
-    def resource_name(self) -> str:
-        return self.file_path
+import json
+
+from thirdai.dataset.data_source import PyDataSource
+
+
+def tokenize_text(tokenizer, text):
+    tokens = tokenizer.encode(text)
+    return " ".join(map(str, tokens))
+
+
+class NerDataSource(PyDataSource):
+    def __init__(
+        self, model_type, tokens_column=None, tags_column=None, file_path=None
+    ):
+        PyDataSource.__init__(self)
+
+        self.file_path = file_path
+
+        self.tokens_column = tokens_column
+        self.tags_column = tags_column
+        self.pretrained = model_type == "bolt_ner"
+
+        if self.pretrained:
+            try:
+                from transformers import GPT2Tokenizer
+
+                self.tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
+            except ImportError:
+                raise ImportError(
+                    "transformers library is not installed. Please install it to use LLMDataSource."
+                )
+
+        self.restart()
+
+    def _get_line_iterator(self):
+
+        if self.file_path is None:
+            raise ValueError(
+                "The file path is None. Please provide a valid file path to access the data source."
+            )
+
+        if self.tags_column is None or self.tokens_column is None:
+            raise ValueError(
+                "Cannot load a datasource with either 'tokens_column' or 'tags_column' set to None. Please provide valid column names."
+            )
+
+        with open(self.file_path, "r") as file:
+            for line in file:
+
+                json_obj = json.loads(line.strip())
+                if not all(
+                    column in json_obj
+                    for column in [self.tags_column, self.tokens_column]
+                ):
+                    raise ValueError(
+                        f"{self.tags_column} or {self.tokens_column} doesn't exist in the column, line: {line}"
+                    )
+                if self.pretrained:
+                    json_obj[self.tokens_column] = [
+                        tokenize_text(self.tokenizer, token)
+                        for token in json_obj[self.tokens_column]
+                    ]
+
+                data = json.dumps(json_obj)
+
+                yield data
+
+    def inference_featurizer(self, sentence_tokens_list):
+        if self.pretrained:
+            return [
+                [tokenize_text(self.tokenizer, token) for token in sentence_tokens]
+                for sentence_tokens in sentence_tokens_list
+            ]
+        return sentence_tokens_list
+
+    def resource_name(self) -> str:
+        return self.file_path
```

## thirdai/dataset/csv_data_source.py

 * *Ordering differences only*

```diff
@@ -1,87 +1,87 @@
-import os
-from io import BytesIO
-from typing import List, Optional
-from urllib.parse import urlparse
-
-import pandas as pd
-from thirdai.dataset.data_source import PyDataSource
-
-
-class CSVDataSource(PyDataSource):
-    """CSV data source that can be used to load from a cloud
-    storage instance such as s3 and GCS.
-
-    Args:
-        storage_path: Path to the CSV file.
-        batch_size: Batch size
-        gcs_credentials_path: Path to a file containing GCS credentials.
-            This is typically a credentials file. For the authorization
-            protocol to work, the credentials file must contain a project ID,
-            client E-mail, a token URI and a private key.
-
-    Note: To read a file from s3, Pandas will expect a credentials file
-        containing an AWS access key id and an AWS secret key located at
-        ~/aws/credentials. For GCS, the gcloud CLI typically stores the
-        credentials file in locations, such as ~/.config/gcloud/credentials
-        or ~/.config/gcloud/application_default_credentials.json.
-        https://gcsfs.readthedocs.io/en/latest/api.html
-    """
-
-    DEFAULT_CHUNK_SIZE = 1000
-
-    # These are provided here since pandas.read_csv does not implicitly use these
-    # paths although google cloud recommends storing credentials in either one of them.
-    # It is only for s3 that pandas.read_csv implicitly searches for a ~/.aws/credentials file.
-    FIRST_DEFAULT_GCS_CREDS_PATH = (
-        "~/.config/gcloud/application_default_credentials.json"
-    )
-    SECOND_DEFAULT_GCS_CREDS_PATH = "~/.config/gcloud/credentials"
-
-    def __init__(
-        self,
-        storage_path: str,
-        gcs_credentials_path: str = None,
-    ) -> None:
-        PyDataSource.__init__(self)
-
-        if gcs_credentials_path:
-            # Pandas requires the GCS file system in order
-            # to authenticate a read request from a GCS bucket
-            import gcsfs
-
-        self._storage_path = storage_path
-        self._gcs_credentials = gcs_credentials_path
-
-        token = None
-        if gcs_credentials_path:
-            token = gcs_credentials_path
-        else:
-            if os.path.exists(self.FIRST_DEFAULT_GCS_CREDS_PATH):
-                token = self.FIRST_DEFAULT_GCS_CREDS_PATH
-            elif os.path.exists(self.SECOND_DEFAULT_GCS_CREDS_PATH):
-                token = self.SECOND_DEFAULT_GCS_CREDS_PATH
-
-        self._storage_options = {"token": token}
-
-        parsed_path = urlparse(self._storage_path, allow_fragments=False)
-        self._cloud_instance_type = parsed_path.scheme
-        self.restart()
-
-    def _get_line_iterator(self):
-        if self._cloud_instance_type not in ["s3", "gcs"]:
-            raise ValueError(
-                f"Invalid data storage path starting with {self._storage_path}"
-            )
-
-        for chunk in pd.read_csv(
-            self._storage_path,
-            chunksize=self.DEFAULT_CHUNK_SIZE,
-            storage_options=self._storage_options,
-            dtype="object",
-            header=None,
-        ):
-            for i in range(len(chunk)):
-                yield chunk.iloc[i : i + 1].to_csv(header=None, index=None).strip("\n")
-
-    def resource_name(self) -> str:
-        return self._storage_path
+import os
+from io import BytesIO
+from typing import List, Optional
+from urllib.parse import urlparse
+
+import pandas as pd
+from thirdai.dataset.data_source import PyDataSource
+
+
+class CSVDataSource(PyDataSource):
+    """CSV data source that can be used to load from a cloud
+    storage instance such as s3 and GCS.
+
+    Args:
+        storage_path: Path to the CSV file.
+        batch_size: Batch size
+        gcs_credentials_path: Path to a file containing GCS credentials.
+            This is typically a credentials file. For the authorization
+            protocol to work, the credentials file must contain a project ID,
+            client E-mail, a token URI and a private key.
+
+    Note: To read a file from s3, Pandas will expect a credentials file
+        containing an AWS access key id and an AWS secret key located at
+        ~/aws/credentials. For GCS, the gcloud CLI typically stores the
+        credentials file in locations, such as ~/.config/gcloud/credentials
+        or ~/.config/gcloud/application_default_credentials.json.
+        https://gcsfs.readthedocs.io/en/latest/api.html
+    """
+
+    DEFAULT_CHUNK_SIZE = 1000
+
+    # These are provided here since pandas.read_csv does not implicitly use these
+    # paths although google cloud recommends storing credentials in either one of them.
+    # It is only for s3 that pandas.read_csv implicitly searches for a ~/.aws/credentials file.
+    FIRST_DEFAULT_GCS_CREDS_PATH = (
+        "~/.config/gcloud/application_default_credentials.json"
+    )
+    SECOND_DEFAULT_GCS_CREDS_PATH = "~/.config/gcloud/credentials"
+
+    def __init__(
+        self,
+        storage_path: str,
+        gcs_credentials_path: str = None,
+    ) -> None:
+        PyDataSource.__init__(self)
+
+        if gcs_credentials_path:
+            # Pandas requires the GCS file system in order
+            # to authenticate a read request from a GCS bucket
+            import gcsfs
+
+        self._storage_path = storage_path
+        self._gcs_credentials = gcs_credentials_path
+
+        token = None
+        if gcs_credentials_path:
+            token = gcs_credentials_path
+        else:
+            if os.path.exists(self.FIRST_DEFAULT_GCS_CREDS_PATH):
+                token = self.FIRST_DEFAULT_GCS_CREDS_PATH
+            elif os.path.exists(self.SECOND_DEFAULT_GCS_CREDS_PATH):
+                token = self.SECOND_DEFAULT_GCS_CREDS_PATH
+
+        self._storage_options = {"token": token}
+
+        parsed_path = urlparse(self._storage_path, allow_fragments=False)
+        self._cloud_instance_type = parsed_path.scheme
+        self.restart()
+
+    def _get_line_iterator(self):
+        if self._cloud_instance_type not in ["s3", "gcs"]:
+            raise ValueError(
+                f"Invalid data storage path starting with {self._storage_path}"
+            )
+
+        for chunk in pd.read_csv(
+            self._storage_path,
+            chunksize=self.DEFAULT_CHUNK_SIZE,
+            storage_options=self._storage_options,
+            dtype="object",
+            header=None,
+        ):
+            for i in range(len(chunk)):
+                yield chunk.iloc[i : i + 1].to_csv(header=None, index=None).strip("\n")
+
+    def resource_name(self) -> str:
+        return self._storage_path
```

## thirdai/dataset/data_source.py

 * *Ordering differences only*

```diff
@@ -1,37 +1,37 @@
-from typing import List, Optional
-
-from thirdai._thirdai.dataset import DataSource
-
-
-class PyDataSource(DataSource):
-    """Base class for DataSources Implemented in Python.
-    Implements common methods `next_batch`, `next_line`, and `restart`.
-    Concrete implementations must implement `_get_line_iterator` and
-    `resource_name`.
-    """
-
-    def __init__(self):
-        DataSource.__init__(self)
-
-    def _get_line_iterator(self):
-        raise NotImplementedError()
-
-    def resource_name(self) -> str:
-        raise NotImplementedError()
-
-    def next_batch(self, target_batch_size) -> Optional[List[str]]:
-        lines = []
-        while len(lines) < target_batch_size:
-            next_line = self.next_line()
-            if next_line == None:
-                break
-            lines.append(next_line)
-
-        return lines if len(lines) else None
-
-    def next_line(self) -> Optional[str]:
-        next_line = next(self._line_iterator, None)
-        return next_line
-
-    def restart(self) -> None:
-        self._line_iterator = self._get_line_iterator()
+from typing import List, Optional
+
+from thirdai._thirdai.dataset import DataSource
+
+
+class PyDataSource(DataSource):
+    """Base class for DataSources Implemented in Python.
+    Implements common methods `next_batch`, `next_line`, and `restart`.
+    Concrete implementations must implement `_get_line_iterator` and
+    `resource_name`.
+    """
+
+    def __init__(self):
+        DataSource.__init__(self)
+
+    def _get_line_iterator(self):
+        raise NotImplementedError()
+
+    def resource_name(self) -> str:
+        raise NotImplementedError()
+
+    def next_batch(self, target_batch_size) -> Optional[List[str]]:
+        lines = []
+        while len(lines) < target_batch_size:
+            next_line = self.next_line()
+            if next_line == None:
+                break
+            lines.append(next_line)
+
+        return lines if len(lines) else None
+
+    def next_line(self) -> Optional[str]:
+        next_line = next(self._line_iterator, None)
+        return next_line
+
+    def restart(self) -> None:
+        self._line_iterator = self._get_line_iterator()
```

## thirdai/dataset/llm_data_source.py

 * *Ordering differences only*

```diff
@@ -1,69 +1,69 @@
-import json
-
-from thirdai.dataset.data_source import PyDataSource
-
-
-def tokenize_and_dump_json(tokenizer, json_obj):
-    json_obj["target"] = tokenize_text(tokenizer, json_obj["target"])
-    if "context" in json_obj:
-        json_obj["context"] = tokenize_text(tokenizer, json_obj["context"])
-    if "prompt" in json_obj:
-        json_obj["prompt"] = tokenize_text(tokenizer, json_obj["prompt"])
-    return json.dumps(json_obj)
-
-
-def tokenize_text(tokenizer, text):
-    tokens = tokenizer.encode(text)
-    return " ".join(map(str, tokens))
-
-
-class LLMDataSource(PyDataSource):
-    def __init__(self, file_path):
-        self.file_path = file_path
-        try:
-            from transformers import GPT2Tokenizer
-        except ImportError:
-            raise ImportError(
-                "transformers library is not installed. Please install it to use LLMDataSource."
-            )
-        self.tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
-        PyDataSource.__init__(self)
-        self.restart()
-
-    def _get_line_iterator(self):
-        with open(self.file_path, "r") as file:
-            for line in file:
-                json_obj = json.loads(line.strip())
-                tokenized_json_obj = tokenize_and_dump_json(self.tokenizer, json_obj)
-                yield tokenized_json_obj
-
-    def resource_name(self) -> str:
-        return self.file_path
-
-
-class RayTextDataSource(PyDataSource):
-    def __init__(self, ray_dataset, should_tokenize=False):
-        PyDataSource.__init__(self)
-        self.ray_dataset = ray_dataset
-        self.should_tokenize = should_tokenize
-        try:
-            import ray
-            from transformers import GPT2Tokenizer
-        except ImportError:
-            raise ImportError(
-                "This class requires both the 'ray' and 'transformers' libraries. Please ensure they are installed."
-            )
-        if self.should_tokenize:
-            self.tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
-        self.restart()
-
-    def _get_line_iterator(self):
-        for row in self.ray_dataset.iter_rows():
-            text = row["text"]
-            if self.should_tokenize:
-                json_obj = json.loads(text.strip())
-                text = tokenize_and_dump_json(self.tokenizer, json_obj)
-            yield text
-
-    def resource_name(self) -> str:
-        return f"ray-dataset-sources"
+import json
+
+from thirdai.dataset.data_source import PyDataSource
+
+
+def tokenize_and_dump_json(tokenizer, json_obj):
+    json_obj["target"] = tokenize_text(tokenizer, json_obj["target"])
+    if "context" in json_obj:
+        json_obj["context"] = tokenize_text(tokenizer, json_obj["context"])
+    if "prompt" in json_obj:
+        json_obj["prompt"] = tokenize_text(tokenizer, json_obj["prompt"])
+    return json.dumps(json_obj)
+
+
+def tokenize_text(tokenizer, text):
+    tokens = tokenizer.encode(text)
+    return " ".join(map(str, tokens))
+
+
+class LLMDataSource(PyDataSource):
+    def __init__(self, file_path):
+        self.file_path = file_path
+        try:
+            from transformers import GPT2Tokenizer
+        except ImportError:
+            raise ImportError(
+                "transformers library is not installed. Please install it to use LLMDataSource."
+            )
+        self.tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
+        PyDataSource.__init__(self)
+        self.restart()
+
+    def _get_line_iterator(self):
+        with open(self.file_path, "r") as file:
+            for line in file:
+                json_obj = json.loads(line.strip())
+                tokenized_json_obj = tokenize_and_dump_json(self.tokenizer, json_obj)
+                yield tokenized_json_obj
+
+    def resource_name(self) -> str:
+        return self.file_path
+
+
+class RayTextDataSource(PyDataSource):
+    def __init__(self, ray_dataset, should_tokenize=False):
+        PyDataSource.__init__(self)
+        self.ray_dataset = ray_dataset
+        self.should_tokenize = should_tokenize
+        try:
+            import ray
+            from transformers import GPT2Tokenizer
+        except ImportError:
+            raise ImportError(
+                "This class requires both the 'ray' and 'transformers' libraries. Please ensure they are installed."
+            )
+        if self.should_tokenize:
+            self.tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
+        self.restart()
+
+    def _get_line_iterator(self):
+        for row in self.ray_dataset.iter_rows():
+            text = row["text"]
+            if self.should_tokenize:
+                json_obj = json.loads(text.strip())
+                text = tokenize_and_dump_json(self.tokenizer, json_obj)
+            yield text
+
+    def resource_name(self) -> str:
+        return f"ray-dataset-sources"
```

## thirdai/dataset/parquet_data_source.py

 * *Ordering differences only*

```diff
@@ -1,42 +1,42 @@
-import io
-
-from thirdai.dataset.data_source import PyDataSource
-
-
-class ParquetSource(PyDataSource):
-    def __init__(self, parquet_path):
-        PyDataSource.__init__(self)
-
-        # By importing here, we make it so that pyarrow isn't a dependency.
-        # If pyarrow isn't installed and you try to read a parquet, this will
-        # throw an import error. This is standard practice, see e.g.
-        # https://pandas.pydata.org/docs/getting_started/install.html
-        import pyarrow.parquet as pq
-
-        self._parquet_path = parquet_path
-        self._parquet_table = pq.read_table(parquet_path)
-        self.restart()
-
-    def _get_line_iterator(self):
-        from pyarrow import csv
-
-        # This loop is pretty hacky. Basically, as a quick overview, we loop
-        # through the parquet file line by line, write each line to an in memory
-        # buffer as a two line csv (header and single line of data) using the
-        # pyarrow csv writer, and then yield string versions of the data line
-        # (and header line if it's the first line)
-        first = True
-        for single_line_batch in self._parquet_table.to_batches(1):
-            buf = io.BytesIO()
-            csv.write_csv(single_line_batch, buf)
-            buf.seek(0)
-
-            header, data_line, *_ = buf.read().decode().split("\n")
-
-            if first:
-                yield header
-                first = False
-            yield data_line
-
-    def resource_name(self):
-        return self._parquet_path
+import io
+
+from thirdai.dataset.data_source import PyDataSource
+
+
+class ParquetSource(PyDataSource):
+    def __init__(self, parquet_path):
+        PyDataSource.__init__(self)
+
+        # By importing here, we make it so that pyarrow isn't a dependency.
+        # If pyarrow isn't installed and you try to read a parquet, this will
+        # throw an import error. This is standard practice, see e.g.
+        # https://pandas.pydata.org/docs/getting_started/install.html
+        import pyarrow.parquet as pq
+
+        self._parquet_path = parquet_path
+        self._parquet_table = pq.read_table(parquet_path)
+        self.restart()
+
+    def _get_line_iterator(self):
+        from pyarrow import csv
+
+        # This loop is pretty hacky. Basically, as a quick overview, we loop
+        # through the parquet file line by line, write each line to an in memory
+        # buffer as a two line csv (header and single line of data) using the
+        # pyarrow csv writer, and then yield string versions of the data line
+        # (and header line if it's the first line)
+        first = True
+        for single_line_batch in self._parquet_table.to_batches(1):
+            buf = io.BytesIO()
+            csv.write_csv(single_line_batch, buf)
+            buf.seek(0)
+
+            header, data_line, *_ = buf.read().decode().split("\n")
+
+            if first:
+                yield header
+                first = False
+            yield data_line
+
+    def resource_name(self):
+        return self._parquet_path
```

## thirdai/dataset/ray_data_source.py

 * *Ordering differences only*

```diff
@@ -1,58 +1,58 @@
-import pandas as pd
-from thirdai.dataset.data_source import PyDataSource
-
-
-class RayCsvDataSource(PyDataSource):
-    """
-    RayCsvDataSource ingests ray datasets during distributed training.
-    Using this ideally we should be able to load data from any of
-    the sources mentioned here https://docs.ray.io/en/latest/data/loading-data.html
-    which includes, parquet, s3, gcs, dask, spark, sql etc. It should work
-    out of the box for single amchine training too.
-    """
-
-    DEFAULT_CHUNK_SIZE = 1000
-
-    def __init__(self, ray_dataset, model_target_column, document_target_column):
-        PyDataSource.__init__(self)
-        self.ray_dataset = ray_dataset
-        self.model_target_column = model_target_column
-        self.document_target_column = document_target_column
-        self.restart()
-        try:
-            import ray
-        except ImportError:
-            raise ImportError(
-                "ray is not installed. Please install it to use RayCsvDataSource."
-            )
-
-    def _get_line_iterator(self):
-        # return the header first
-        column_names = self.ray_dataset.schema().names
-        data_dict = {}
-        for column_name in column_names:
-            if column_name == self.document_target_column:
-                data_dict[self.model_target_column] = [self.model_target_column]
-            else:
-                data_dict[column_name] = [column_name]
-
-        yield pd.DataFrame(data_dict).to_csv(index=None, header=None)
-        # return row-by-row data
-        for chunk in self.ray_dataset.iter_batches(
-            batch_size=self.DEFAULT_CHUNK_SIZE, batch_format="pandas"
-        ):
-            for i in range(len(chunk)):
-                # TODO(pratik): Ray dataset lacks built-in column renaming support.
-                # Reference: https://docs.ray.io/en/latest/data/api/dataset.html#basic-transformations
-                # Only add/drop column options available, but they could be memory-intensive.
-                yield (
-                    chunk.iloc[i : i + 1]
-                    .rename(
-                        columns={self.document_target_column: self.model_target_column}
-                    )
-                    .to_csv(header=None, index=None)
-                    .strip("\n")
-                )
-
-    def resource_name(self) -> str:
-        return f"ray-dataset-sources"
+import pandas as pd
+from thirdai.dataset.data_source import PyDataSource
+
+
+class RayCsvDataSource(PyDataSource):
+    """
+    RayCsvDataSource ingests ray datasets during distributed training.
+    Using this ideally we should be able to load data from any of
+    the sources mentioned here https://docs.ray.io/en/latest/data/loading-data.html
+    which includes, parquet, s3, gcs, dask, spark, sql etc. It should work
+    out of the box for single amchine training too.
+    """
+
+    DEFAULT_CHUNK_SIZE = 1000
+
+    def __init__(self, ray_dataset, model_target_column, document_target_column):
+        PyDataSource.__init__(self)
+        self.ray_dataset = ray_dataset
+        self.model_target_column = model_target_column
+        self.document_target_column = document_target_column
+        self.restart()
+        try:
+            import ray
+        except ImportError:
+            raise ImportError(
+                "ray is not installed. Please install it to use RayCsvDataSource."
+            )
+
+    def _get_line_iterator(self):
+        # return the header first
+        column_names = self.ray_dataset.schema().names
+        data_dict = {}
+        for column_name in column_names:
+            if column_name == self.document_target_column:
+                data_dict[self.model_target_column] = [self.model_target_column]
+            else:
+                data_dict[column_name] = [column_name]
+
+        yield pd.DataFrame(data_dict).to_csv(index=None, header=None)
+        # return row-by-row data
+        for chunk in self.ray_dataset.iter_batches(
+            batch_size=self.DEFAULT_CHUNK_SIZE, batch_format="pandas"
+        ):
+            for i in range(len(chunk)):
+                # TODO(pratik): Ray dataset lacks built-in column renaming support.
+                # Reference: https://docs.ray.io/en/latest/data/api/dataset.html#basic-transformations
+                # Only add/drop column options available, but they could be memory-intensive.
+                yield (
+                    chunk.iloc[i : i + 1]
+                    .rename(
+                        columns={self.document_target_column: self.model_target_column}
+                    )
+                    .to_csv(header=None, index=None)
+                    .strip("\n")
+                )
+
+    def resource_name(self) -> str:
+        return f"ray-dataset-sources"
```

## thirdai/dataset/__init__.py

 * *Ordering differences only*

```diff
@@ -1,13 +1,13 @@
-import time
-
-import thirdai._thirdai.dataset
-from thirdai._thirdai.dataset import *
-
-__all__ = []
-__all__.extend(dir(thirdai._thirdai.dataset))
-
-from .bolt_ner_data_source import NerDataSource
-from .csv_data_source import CSVDataSource
-from .llm_data_source import LLMDataSource, RayTextDataSource
-from .parquet_data_source import ParquetSource
-from .ray_data_source import RayCsvDataSource
+import time
+
+import thirdai._thirdai.dataset
+from thirdai._thirdai.dataset import *
+
+__all__ = []
+__all__.extend(dir(thirdai._thirdai.dataset))
+
+from .bolt_ner_data_source import NerDataSource
+from .csv_data_source import CSVDataSource
+from .llm_data_source import LLMDataSource, RayTextDataSource
+from .parquet_data_source import ParquetSource
+from .ray_data_source import RayCsvDataSource
```

## thirdai/demos/beir_download_utils.py

 * *Ordering differences only*

```diff
@@ -1,253 +1,253 @@
-import csv
-import json
-import logging
-import os
-import zipfile
-from typing import Dict, Tuple
-
-import requests
-from tqdm.autonotebook import tqdm
-
-logger = logging.getLogger(__name__)
-
-
-def write_unsupervised_file(corpus, data_path):
-    unsup_file = data_path + "/unsupervised.csv"
-    with open(unsup_file, "w") as fw:
-        header = "DOC_ID,TITLE,TEXT\n"
-        fw.write(header)
-        count = 0
-        for key in corpus:
-            title = corpus[key]["title"].replace(",", " ")
-            title = title.replace("\r", " ")
-            title = title.replace("\n", " ")
-            title = " " if title == "" else title
-            text = corpus[key]["text"].replace(",", " ")
-            text = text.replace("\r", " ")
-            text = text.replace("\n", " ")
-            text = " " if text == "" else text
-            fw.write(str(count) + "," + title + "," + text + "\n")
-            count += 1
-
-
-def remap_doc_ids(corpus):
-    doc_ids_to_integers = {}
-    count = 0
-    for key in corpus:
-        doc_ids_to_integers[key] = count
-        count += 1
-    return doc_ids_to_integers
-
-
-def remap_query_answers(qrels, doc_ids_to_integers):
-    new_qrels = {}
-    for key in qrels:
-        output = {}
-        for doc_id in qrels[key]:
-            if qrels[key][doc_id] > 0:
-                output[str(doc_ids_to_integers[doc_id])] = qrels[key][doc_id]
-        new_qrels[key] = output
-    return new_qrels
-
-
-def write_supervised_file(queries, answers, data_path, filename):
-    sup_train_file = data_path + "/" + filename
-    with open(sup_train_file, "w") as fw:
-        fw.write("QUERY,DOC_ID\n")
-
-        for key in queries:
-            query = queries[key].replace(",", " ")
-            doc_ids = ":".join(list(answers[key].keys()))
-            fw.write(query + "," + doc_ids + "\n")
-
-
-#############################################################################
-# Everything below this was taken from https://github.com/beir-cellar/beir
-# They have an Apache 2.0 license so it should be good for commercial and private
-# use. We include their logic here so as to not add the additional dependency
-#############################################################################
-
-
-def download_url(url: str, save_path: str, chunk_size: int = 1024):
-    """Download url with progress bar using tqdm
-    https://stackoverflow.com/questions/15644964/python-progress-bar-and-downloads
-
-    Args:
-        url (str): downloadable url
-        save_path (str): local path to save the downloaded file
-        chunk_size (int, optional): chunking of files. Defaults to 1024.
-    """
-    r = requests.get(url, stream=True)
-    total = int(r.headers.get("Content-Length", 0))
-    with open(save_path, "wb") as fd, tqdm(
-        desc=save_path,
-        total=total,
-        unit="iB",
-        unit_scale=True,
-        unit_divisor=chunk_size,
-    ) as bar:
-        for data in r.iter_content(chunk_size=chunk_size):
-            size = fd.write(data)
-            bar.update(size)
-
-
-def unzip(zip_file: str, out_dir: str):
-    zip_ = zipfile.ZipFile(zip_file, "r")
-    zip_.extractall(path=out_dir)
-    zip_.close()
-
-
-def download_and_unzip(url: str, out_dir: str, chunk_size: int = 1024) -> str:
-    os.makedirs(out_dir, exist_ok=True)
-    dataset = url.split("/")[-1]
-    zip_file = os.path.join(out_dir, dataset)
-
-    if not os.path.isfile(zip_file):
-        print(f"Downloading {dataset} ...")
-        download_url(url, zip_file, chunk_size)
-
-    if not os.path.isdir(zip_file.replace(".zip", "")):
-        print(f"Unzipping {dataset} ...")
-        unzip(zip_file, out_dir)
-
-    return os.path.join(out_dir, dataset.replace(".zip", ""))
-
-
-class GenericDataLoader:
-    def __init__(
-        self,
-        data_folder: str = None,
-        prefix: str = None,
-        corpus_file: str = "corpus.jsonl",
-        query_file: str = "queries.jsonl",
-        qrels_folder: str = "qrels",
-        qrels_file: str = "",
-    ):
-        self.corpus = {}
-        self.queries = {}
-        self.qrels = {}
-
-        if prefix:
-            query_file = prefix + "-" + query_file
-            qrels_folder = prefix + "-" + qrels_folder
-
-        self.corpus_file = (
-            os.path.join(data_folder, corpus_file) if data_folder else corpus_file
-        )
-        self.query_file = (
-            os.path.join(data_folder, query_file) if data_folder else query_file
-        )
-        self.qrels_folder = (
-            os.path.join(data_folder, qrels_folder) if data_folder else None
-        )
-        self.qrels_file = qrels_file
-
-    @staticmethod
-    def check(fIn: str, ext: str):
-        if not os.path.exists(fIn):
-            raise ValueError(
-                "File {} not present! Please provide accurate file.".format(fIn)
-            )
-
-        if not fIn.endswith(ext):
-            raise ValueError(
-                "File {} must be present with extension {}".format(fIn, ext)
-            )
-
-    def load_custom(
-        self,
-    ) -> Tuple[Dict[str, Dict[str, str]], Dict[str, str], Dict[str, Dict[str, int]]]:
-
-        self.check(fIn=self.corpus_file, ext="jsonl")
-        self.check(fIn=self.query_file, ext="jsonl")
-        self.check(fIn=self.qrels_file, ext="tsv")
-
-        if not len(self.corpus):
-            logger.info("Loading Corpus...")
-            self._load_corpus()
-            logger.info("Loaded %d Documents.", len(self.corpus))
-            logger.info("Doc Example: %s", list(self.corpus.values())[0])
-
-        if not len(self.queries):
-            logger.info("Loading Queries...")
-            self._load_queries()
-
-        if os.path.exists(self.qrels_file):
-            self._load_qrels()
-            self.queries = {qid: self.queries[qid] for qid in self.qrels}
-            logger.info("Loaded %d Queries.", len(self.queries))
-            logger.info("Query Example: %s", list(self.queries.values())[0])
-
-        return self.corpus, self.queries, self.qrels
-
-    def load(
-        self, split="test"
-    ) -> Tuple[Dict[str, Dict[str, str]], Dict[str, str], Dict[str, Dict[str, int]]]:
-
-        self.qrels_file = os.path.join(self.qrels_folder, split + ".tsv")
-        self.check(fIn=self.corpus_file, ext="jsonl")
-        self.check(fIn=self.query_file, ext="jsonl")
-        self.check(fIn=self.qrels_file, ext="tsv")
-
-        if not len(self.corpus):
-            logger.info("Loading Corpus...")
-            self._load_corpus()
-            logger.info("Loaded %d %s Documents.", len(self.corpus), split.upper())
-            logger.info("Doc Example: %s", list(self.corpus.values())[0])
-
-        if not len(self.queries):
-            logger.info("Loading Queries...")
-            self._load_queries()
-
-        if os.path.exists(self.qrels_file):
-            self._load_qrels()
-            self.queries = {qid: self.queries[qid] for qid in self.qrels}
-            logger.info("Loaded %d %s Queries.", len(self.queries), split.upper())
-            logger.info("Query Example: %s", list(self.queries.values())[0])
-
-        return self.corpus, self.queries, self.qrels
-
-    def load_corpus(self) -> Dict[str, Dict[str, str]]:
-
-        self.check(fIn=self.corpus_file, ext="jsonl")
-
-        if not len(self.corpus):
-            logger.info("Loading Corpus...")
-            self._load_corpus()
-            logger.info("Loaded %d Documents.", len(self.corpus))
-            logger.info("Doc Example: %s", list(self.corpus.values())[0])
-
-        return self.corpus
-
-    def _load_corpus(self):
-        num_lines = sum(1 for i in open(self.corpus_file, "rb"))
-        with open(self.corpus_file, encoding="utf8") as fIn:
-            for line in tqdm(fIn, total=num_lines):
-                line = json.loads(line)
-                self.corpus[line.get("_id")] = {
-                    "text": line.get("text"),
-                    "title": line.get("title"),
-                }
-
-    def _load_queries(self):
-        with open(self.query_file, encoding="utf8") as fIn:
-            for line in fIn:
-                line = json.loads(line)
-                self.queries[line.get("_id")] = line.get("text")
-
-    def _load_qrels(self):
-        reader = csv.reader(
-            open(self.qrels_file, encoding="utf-8"),
-            delimiter="\t",
-            quoting=csv.QUOTE_MINIMAL,
-        )
-
-        next(reader)
-
-        for id, row in enumerate(reader):
-            query_id, corpus_id, score = row[0], row[1], int(row[2])
-
-            if query_id not in self.qrels:
-                self.qrels[query_id] = {corpus_id: score}
-            else:
-                self.qrels[query_id][corpus_id] = score
+import csv
+import json
+import logging
+import os
+import zipfile
+from typing import Dict, Tuple
+
+import requests
+from tqdm.autonotebook import tqdm
+
+logger = logging.getLogger(__name__)
+
+
+def write_unsupervised_file(corpus, data_path):
+    unsup_file = data_path + "/unsupervised.csv"
+    with open(unsup_file, "w") as fw:
+        header = "DOC_ID,TITLE,TEXT\n"
+        fw.write(header)
+        count = 0
+        for key in corpus:
+            title = corpus[key]["title"].replace(",", " ")
+            title = title.replace("\r", " ")
+            title = title.replace("\n", " ")
+            title = " " if title == "" else title
+            text = corpus[key]["text"].replace(",", " ")
+            text = text.replace("\r", " ")
+            text = text.replace("\n", " ")
+            text = " " if text == "" else text
+            fw.write(str(count) + "," + title + "," + text + "\n")
+            count += 1
+
+
+def remap_doc_ids(corpus):
+    doc_ids_to_integers = {}
+    count = 0
+    for key in corpus:
+        doc_ids_to_integers[key] = count
+        count += 1
+    return doc_ids_to_integers
+
+
+def remap_query_answers(qrels, doc_ids_to_integers):
+    new_qrels = {}
+    for key in qrels:
+        output = {}
+        for doc_id in qrels[key]:
+            if qrels[key][doc_id] > 0:
+                output[str(doc_ids_to_integers[doc_id])] = qrels[key][doc_id]
+        new_qrels[key] = output
+    return new_qrels
+
+
+def write_supervised_file(queries, answers, data_path, filename):
+    sup_train_file = data_path + "/" + filename
+    with open(sup_train_file, "w") as fw:
+        fw.write("QUERY,DOC_ID\n")
+
+        for key in queries:
+            query = queries[key].replace(",", " ")
+            doc_ids = ":".join(list(answers[key].keys()))
+            fw.write(query + "," + doc_ids + "\n")
+
+
+#############################################################################
+# Everything below this was taken from https://github.com/beir-cellar/beir
+# They have an Apache 2.0 license so it should be good for commercial and private
+# use. We include their logic here so as to not add the additional dependency
+#############################################################################
+
+
+def download_url(url: str, save_path: str, chunk_size: int = 1024):
+    """Download url with progress bar using tqdm
+    https://stackoverflow.com/questions/15644964/python-progress-bar-and-downloads
+
+    Args:
+        url (str): downloadable url
+        save_path (str): local path to save the downloaded file
+        chunk_size (int, optional): chunking of files. Defaults to 1024.
+    """
+    r = requests.get(url, stream=True)
+    total = int(r.headers.get("Content-Length", 0))
+    with open(save_path, "wb") as fd, tqdm(
+        desc=save_path,
+        total=total,
+        unit="iB",
+        unit_scale=True,
+        unit_divisor=chunk_size,
+    ) as bar:
+        for data in r.iter_content(chunk_size=chunk_size):
+            size = fd.write(data)
+            bar.update(size)
+
+
+def unzip(zip_file: str, out_dir: str):
+    zip_ = zipfile.ZipFile(zip_file, "r")
+    zip_.extractall(path=out_dir)
+    zip_.close()
+
+
+def download_and_unzip(url: str, out_dir: str, chunk_size: int = 1024) -> str:
+    os.makedirs(out_dir, exist_ok=True)
+    dataset = url.split("/")[-1]
+    zip_file = os.path.join(out_dir, dataset)
+
+    if not os.path.isfile(zip_file):
+        print(f"Downloading {dataset} ...")
+        download_url(url, zip_file, chunk_size)
+
+    if not os.path.isdir(zip_file.replace(".zip", "")):
+        print(f"Unzipping {dataset} ...")
+        unzip(zip_file, out_dir)
+
+    return os.path.join(out_dir, dataset.replace(".zip", ""))
+
+
+class GenericDataLoader:
+    def __init__(
+        self,
+        data_folder: str = None,
+        prefix: str = None,
+        corpus_file: str = "corpus.jsonl",
+        query_file: str = "queries.jsonl",
+        qrels_folder: str = "qrels",
+        qrels_file: str = "",
+    ):
+        self.corpus = {}
+        self.queries = {}
+        self.qrels = {}
+
+        if prefix:
+            query_file = prefix + "-" + query_file
+            qrels_folder = prefix + "-" + qrels_folder
+
+        self.corpus_file = (
+            os.path.join(data_folder, corpus_file) if data_folder else corpus_file
+        )
+        self.query_file = (
+            os.path.join(data_folder, query_file) if data_folder else query_file
+        )
+        self.qrels_folder = (
+            os.path.join(data_folder, qrels_folder) if data_folder else None
+        )
+        self.qrels_file = qrels_file
+
+    @staticmethod
+    def check(fIn: str, ext: str):
+        if not os.path.exists(fIn):
+            raise ValueError(
+                "File {} not present! Please provide accurate file.".format(fIn)
+            )
+
+        if not fIn.endswith(ext):
+            raise ValueError(
+                "File {} must be present with extension {}".format(fIn, ext)
+            )
+
+    def load_custom(
+        self,
+    ) -> Tuple[Dict[str, Dict[str, str]], Dict[str, str], Dict[str, Dict[str, int]]]:
+
+        self.check(fIn=self.corpus_file, ext="jsonl")
+        self.check(fIn=self.query_file, ext="jsonl")
+        self.check(fIn=self.qrels_file, ext="tsv")
+
+        if not len(self.corpus):
+            logger.info("Loading Corpus...")
+            self._load_corpus()
+            logger.info("Loaded %d Documents.", len(self.corpus))
+            logger.info("Doc Example: %s", list(self.corpus.values())[0])
+
+        if not len(self.queries):
+            logger.info("Loading Queries...")
+            self._load_queries()
+
+        if os.path.exists(self.qrels_file):
+            self._load_qrels()
+            self.queries = {qid: self.queries[qid] for qid in self.qrels}
+            logger.info("Loaded %d Queries.", len(self.queries))
+            logger.info("Query Example: %s", list(self.queries.values())[0])
+
+        return self.corpus, self.queries, self.qrels
+
+    def load(
+        self, split="test"
+    ) -> Tuple[Dict[str, Dict[str, str]], Dict[str, str], Dict[str, Dict[str, int]]]:
+
+        self.qrels_file = os.path.join(self.qrels_folder, split + ".tsv")
+        self.check(fIn=self.corpus_file, ext="jsonl")
+        self.check(fIn=self.query_file, ext="jsonl")
+        self.check(fIn=self.qrels_file, ext="tsv")
+
+        if not len(self.corpus):
+            logger.info("Loading Corpus...")
+            self._load_corpus()
+            logger.info("Loaded %d %s Documents.", len(self.corpus), split.upper())
+            logger.info("Doc Example: %s", list(self.corpus.values())[0])
+
+        if not len(self.queries):
+            logger.info("Loading Queries...")
+            self._load_queries()
+
+        if os.path.exists(self.qrels_file):
+            self._load_qrels()
+            self.queries = {qid: self.queries[qid] for qid in self.qrels}
+            logger.info("Loaded %d %s Queries.", len(self.queries), split.upper())
+            logger.info("Query Example: %s", list(self.queries.values())[0])
+
+        return self.corpus, self.queries, self.qrels
+
+    def load_corpus(self) -> Dict[str, Dict[str, str]]:
+
+        self.check(fIn=self.corpus_file, ext="jsonl")
+
+        if not len(self.corpus):
+            logger.info("Loading Corpus...")
+            self._load_corpus()
+            logger.info("Loaded %d Documents.", len(self.corpus))
+            logger.info("Doc Example: %s", list(self.corpus.values())[0])
+
+        return self.corpus
+
+    def _load_corpus(self):
+        num_lines = sum(1 for i in open(self.corpus_file, "rb"))
+        with open(self.corpus_file, encoding="utf8") as fIn:
+            for line in tqdm(fIn, total=num_lines):
+                line = json.loads(line)
+                self.corpus[line.get("_id")] = {
+                    "text": line.get("text"),
+                    "title": line.get("title"),
+                }
+
+    def _load_queries(self):
+        with open(self.query_file, encoding="utf8") as fIn:
+            for line in fIn:
+                line = json.loads(line)
+                self.queries[line.get("_id")] = line.get("text")
+
+    def _load_qrels(self):
+        reader = csv.reader(
+            open(self.qrels_file, encoding="utf-8"),
+            delimiter="\t",
+            quoting=csv.QUOTE_MINIMAL,
+        )
+
+        next(reader)
+
+        for id, row in enumerate(reader):
+            query_id, corpus_id, score = row[0], row[1], int(row[2])
+
+            if query_id not in self.qrels:
+                self.qrels[query_id] = {corpus_id: score}
+            else:
+                self.qrels[query_id][corpus_id] = score
```

## thirdai/demos/download_datasets.py

 * *Ordering differences only*

```diff
@@ -1,709 +1,709 @@
-import json
-import math
-import os
-import random
-import zipfile
-
-import numpy as np
-import pandas as pd
-import requests
-from thirdai._thirdai import bolt
-
-from .beir_download_utils import (
-    GenericDataLoader,
-    download_and_unzip,
-    remap_doc_ids,
-    remap_query_answers,
-    write_supervised_file,
-    write_unsupervised_file,
-)
-
-
-def download_file(url, output_path):
-    response = requests.get(url, stream=True)
-    response.raise_for_status()
-    with open(output_path, "wb") as file:
-        for chunk in response.iter_content(chunk_size=8192):
-            file.write(chunk)
-
-
-def _download_dataset(url, zip_file, check_existence, output_dir):
-    if not os.path.exists(zip_file):
-        download_file(url, zip_file)
-
-    if any([not os.path.exists(must_exist) for must_exist in check_existence]):
-        with zipfile.ZipFile(zip_file, "r") as zip_ref:
-            zip_ref.extractall(output_dir)
-
-
-def _create_inference_samples(filename, label_col):
-    df = pd.read_csv(filename)
-    inference_samples = []
-    for _, row in df.iterrows():
-        sample = dict(row)
-        label = sample[label_col]
-        del sample[label_col]
-        sample = {x: str(y) for x, y in sample.items()}
-        inference_samples.append((sample, label))
-    return inference_samples
-
-
-def to_udt_input_batch(dataframe):
-    return [
-        {col_name: str(col_value) for col_name, col_value in record.items()}
-        for record in dataframe.to_dict(orient="records")
-    ]
-
-
-def download_movielens():
-    MOVIELENS_1M_URL = "https://files.grouplens.org/datasets/movielens/ml-1m.zip"
-    MOVIELENS_ZIP = "./movielens.zip"
-    MOVIELENS_DIR = "./movielens"
-    RATINGS_FILE = MOVIELENS_DIR + "/ml-1m/ratings.dat"
-    MOVIE_TITLES = MOVIELENS_DIR + "/ml-1m/movies.dat"
-    TRAIN_FILE = "./movielens_train.csv"
-    TEST_FILE = "./movielens_test.csv"
-    SPLIT = 0.9
-    INFERENCE_BATCH_SIZE = 5
-
-    _download_dataset(
-        url=MOVIELENS_1M_URL,
-        zip_file=MOVIELENS_ZIP,
-        check_existence=[RATINGS_FILE, MOVIE_TITLES],
-        output_dir=MOVIELENS_DIR,
-    )
-
-    df = pd.read_csv(RATINGS_FILE, header=None, delimiter="::", engine="python")
-    df.columns = ["userId", "movieId", "rating", "timestamp"]
-    df["timestamp"] = pd.to_datetime(df["timestamp"], unit="s")
-
-    movies_df = pd.read_csv(
-        MOVIE_TITLES,
-        header=None,
-        delimiter="::",
-        engine="python",
-        encoding="ISO-8859-1",
-    )
-    movies_df.columns = ["movieId", "movieTitle", "genre"]
-    movies_df["movieTitle"] = movies_df["movieTitle"].apply(
-        lambda x: x.replace(",", "")
-    )
-
-    df = pd.merge(df, movies_df, on="movieId")
-    df = df[["userId", "movieTitle", "timestamp"]]
-    df = df.sort_values("timestamp")
-
-    n_train_samples = int(SPLIT * len(df))
-    train_df = df.iloc[:n_train_samples]
-    test_df = df.iloc[n_train_samples:]
-    train_df.to_csv(TRAIN_FILE, index=False)
-    test_df.to_csv(TEST_FILE, index=False)
-
-    index_batch = to_udt_input_batch(df.iloc[:INFERENCE_BATCH_SIZE])
-    inference_batch = to_udt_input_batch(
-        df.iloc[:INFERENCE_BATCH_SIZE][["userId", "timestamp"]]
-    )
-
-    return TRAIN_FILE, TEST_FILE, inference_batch, index_batch
-
-
-def download_criteo():
-    CRITEO_URL = "http://go.criteo.net/criteo-research-kaggle-display-advertising-challenge-dataset.tar.gz"
-    CRITEO_ZIP = "./criteo.tar.gz"
-    CRITEO_DIR = "./criteo"
-    MAIN_FILE = CRITEO_DIR + "/train.txt"
-    CREATED_TRAIN_FILE = "./criteo/train_udt.csv"
-    CREATED_TEST_FILE = "./criteo/test_udt.csv"
-
-    os.makedirs("./criteo", exist_ok=True)
-
-    if not os.path.exists(CRITEO_ZIP):
-        print(
-            f"Downloading from {CRITEO_URL}. This can take 20-40 minutes depending on"
-            " the Criteo server."
-        )
-        os.system(f"wget -t inf -c {CRITEO_URL} -O {CRITEO_ZIP}")
-
-    if not os.path.exists(MAIN_FILE):
-        print("Extracting files. This can take up to 10 minutes.")
-        os.system(f"tar -xvzf {CRITEO_ZIP} -C {CRITEO_DIR}")
-
-    df = pd.read_csv(MAIN_FILE, delimiter="\t", header=None)
-    n_train = int(0.8 * df.shape[0])
-    header = (
-        ["label"]
-        + [f"num_{i}" for i in range(1, 14)]
-        + [f"cat_{i}" for i in range(1, 27)]
-    )
-
-    print("Processing the dataset (this will take about 6-7 mins).")
-    min_vals = df.iloc[:, 1:14].min()
-    df.iloc[:, 1:14] = np.round(np.log(df.iloc[:, 1:14] - min_vals + 1), 2)
-    min_vals = np.float32(df.iloc[:, 1:14].min())
-    max_vals = np.float32(df.iloc[:, 1:14].max())
-    y = np.float32(df.iloc[:, 0])
-    n_unique_classes = list(df.iloc[:, 14:].nunique())
-
-    y_train = y[:n_train]
-    y_test = y[n_train:]
-
-    if not os.path.exists(CREATED_TRAIN_FILE) or not os.path.exists(CREATED_TEST_FILE):
-        print("saving the train and test datasets (this will take about 10 mins)")
-        df[:n_train].to_csv(CREATED_TRAIN_FILE, header=header, index=False)
-        df[n_train:].to_csv(CREATED_TEST_FILE, header=header, index=False)
-
-    df_sample = df.iloc[n_train : n_train + 2]
-    df_sample = df_sample.fillna("")
-    sample_batch = [
-        {header[i]: str(df_sample.iloc[0, i]) for i in range(1, 40)}
-    ]  # first sample
-    sample_batch.append(
-        {header[i]: str(df_sample.iloc[1, i]) for i in range(1, 40)}
-    )  # second sample
-
-    return (
-        CREATED_TRAIN_FILE,
-        CREATED_TEST_FILE,
-        y_train,
-        y_test,
-        min_vals,
-        max_vals,
-        n_unique_classes,
-        sample_batch,
-    )
-
-
-def prep_fraud_dataset(dataset_path, seed=42):
-    df = pd.read_csv(dataset_path)
-    df["amount"] = (df["oldbalanceOrg"] - df["newbalanceOrig"]).abs()
-
-    def upsample(df):
-        fraud_samples = df[df["isFraud"] == 1]
-        upsampling_ratio = 5
-        for i in range(upsampling_ratio):
-            df = pd.concat([df, fraud_samples], axis=0)
-        return df
-
-    df = upsample(df)
-
-    df = df.sample(frac=1, random_state=seed)
-
-    SPLIT = 0.8
-    n_train_samples = int(SPLIT * len(df))
-    train_df = df.iloc[:n_train_samples]
-    test_df = df.iloc[n_train_samples:]
-
-    train_filename = "fraud_detection/new_train.csv"
-    test_filename = "fraud_detection/new_test.csv"
-
-    train_df.to_csv(train_filename, index=False)
-    test_df.to_csv(test_filename, index=False)
-
-    INFERENCE_BATCH_SIZE = 5
-    inference_batch = to_udt_input_batch(
-        df.iloc[:INFERENCE_BATCH_SIZE][
-            [
-                "step",
-                "type",
-                "amount",
-                "nameOrig",
-                "oldbalanceOrg",
-                "newbalanceOrig",
-                "nameDest",
-                "oldbalanceDest",
-                "newbalanceDest",
-                "isFlaggedFraud",
-            ]
-        ]
-    )
-
-    return train_filename, test_filename, inference_batch
-
-
-def download_census_income(num_inference_samples=5, return_labels=False):
-    CENSUS_INCOME_BASE_DOWNLOAD_URL = "https://www.dropbox.com/scl/fi/xg5jld8rj2h3yciduts6l/census-income.zip?rlkey=xo2zs5mtvbl917kgevok4fk1q&st=ehrcbkzo&dl=1"
-    CENSUS_INCOME_ZIP = "./adult.zip"
-    CENSUS_INCOME_DIR = "./adult"
-    TRAIN_FILE = "./census_income_train.csv"
-    TEST_FILE = "./census_income_test.csv"
-    _download_dataset(
-        url=CENSUS_INCOME_BASE_DOWNLOAD_URL,
-        zip_file=CENSUS_INCOME_ZIP,
-        check_existence=["./adult/adult.data", "./adult/adult.test"],
-        output_dir=CENSUS_INCOME_DIR,
-    )
-    COLUMN_NAMES = [
-        "age",
-        "workclass",
-        "fnlwgt",
-        "education",
-        "education-num",
-        "marital-status",
-        "occupation",
-        "relationship",
-        "race",
-        "sex",
-        "capital-gain",
-        "capital-loss",
-        "hours-per-week",
-        "native-country",
-        "label",
-    ]
-    if not os.path.exists(TRAIN_FILE):
-        # reformat the train file
-        with open("./adult/adult.data", "r") as file:
-            data = file.read().splitlines(True)
-        with open(TRAIN_FILE, "w") as file:
-            # Write header
-            file.write(",".join(COLUMN_NAMES) + "\n")
-            # Convert ", " delimiters to ",".
-            # loop through data[1:] since the first line is bogus
-            lines = [line.replace(", ", ",") for line in data[1:]]
-            # Strip empty lines
-            file.writelines([line for line in lines if len(line.strip()) > 0])
-
-    if not os.path.exists(TEST_FILE):
-        # reformat the test file
-        with open("./adult/adult.test", "r") as file:
-            data = file.read().splitlines(True)
-        with open(TEST_FILE, "w") as file:
-            # Write header
-            file.write(",".join(COLUMN_NAMES) + "\n")
-            # Convert ", " delimiters to ",".
-            # Additionally, for some reason each of the labels end with a "." in the test set
-            # loop through data[1:] since the first line is bogus
-            lines = [line.replace(".", "").replace(", ", ",") for line in data[1:]]
-            # Strip empty lines
-            file.writelines([line for line in lines if len(line.strip()) > 0])
-
-    inference_sample_range_end = (
-        -1 if num_inference_samples == "all" else num_inference_samples + 1
-    )
-
-    inference_samples = []
-    with open(TEST_FILE, "r") as test_file:
-        for line in test_file.readlines()[1:inference_sample_range_end]:
-            column_vals = {
-                col_name: value
-                for col_name, value in zip(COLUMN_NAMES, line.split(","))
-            }
-            label = column_vals["label"].strip()
-            del column_vals["label"]
-
-            if return_labels:
-                inference_samples.append((column_vals, label))
-            else:
-                inference_samples.append(column_vals)
-
-    return TRAIN_FILE, TEST_FILE, inference_samples
-
-
-def download_query_reformulation_dataset(train_file_percentage=0.7):
-    """
-    The dataset is retrieved from HuggingFace:
-    https://huggingface.co/datasets/snips_built_in_intents
-    """
-    import datasets
-
-    dataset = datasets.load_dataset(path="embedding-data/sentence-compression")
-    dataframe = pd.DataFrame(data=dataset)
-
-    extracted_text = []
-
-    for _, row in dataframe.iterrows():
-        extracted_text.append(row.to_dict()["train"]["set"][1])
-
-    return pd.DataFrame(data=extracted_text)
-
-
-def perturb_query_reformulation_data(dataframe, noise_level, seed=42):
-    random.seed(seed)
-
-    transformation_type = ("remove-char", "permute-string")
-    transformed_dataframe = []
-
-    PER_QUERY_COPIES = 5
-
-    for _, row in dataframe.iterrows():
-        correct_query = " ".join(list(row.str.split(" ")[0]))
-        query_length = len(correct_query.split(" "))
-        words_to_transform = math.ceil(noise_level * query_length)
-
-        for _ in range(PER_QUERY_COPIES):
-            incorrect_query_list = correct_query.split(" ")
-            transformed_words = 0
-            visited_indices = set()
-
-            while transformed_words < words_to_transform:
-                random_index = random.randint(0, words_to_transform)
-                if random_index in visited_indices:
-                    continue
-                word_to_transform = incorrect_query_list[random_index]
-
-                if random.choices(transformation_type, k=1) == "remove-char":
-                    # Remove a random character
-                    char_index = random.randint(0, len(word_to_transform) - 1)
-                    transformed_word = (
-                        word_to_transform[0:char_index]
-                        + word_to_transform[char_index + 1 :]
-                    )
-                    incorrect_query_list[random_index] = transformed_word
-
-                else:
-                    # Permute the characters in the string
-                    transformed_word_char_list = list(word_to_transform)
-                    random.shuffle(transformed_word_char_list)
-
-                    incorrect_query_list[random_index] = "".join(
-                        transformed_word_char_list
-                    )
-
-                visited_indices.add(random_index)
-                transformed_words += 1
-
-            transformed_dataframe.append(
-                [correct_query, " ".join(incorrect_query_list)]
-            )
-
-    return pd.DataFrame(
-        transformed_dataframe, columns=["target_queries", "source_queries"]
-    )
-
-
-def prepare_query_reformulation_data(seed=42):
-    TRAIN_FILE_PATH = "train_file.csv"
-    TEST_FILE_PATH = "test_file.csv"
-    TRAIN_FILE_DATASET_PERCENTAGE = 0.7
-    INFERENCE_BATCH_PERCENTAGE = 0.0001
-    TRAIN_NOISE_LEVEL = 0.2
-    TEST_NOISE_LEVEL = 0.4
-
-    def get_inference_batch(dataframe):
-        inference_batch = dataframe.sample(
-            frac=INFERENCE_BATCH_PERCENTAGE, random_state=seed
-        )
-        inference_batch_as_list = []
-        for _, row in inference_batch.iterrows():
-            inference_batch_as_list.append({"phrase": row.to_dict()[0]})
-
-        return inference_batch_as_list
-
-    train_data = download_query_reformulation_dataset(
-        train_file_percentage=TRAIN_FILE_DATASET_PERCENTAGE
-    )
-    inference_batch = get_inference_batch(dataframe=train_data)
-
-    train_data_with_noise = perturb_query_reformulation_data(
-        dataframe=train_data, noise_level=TRAIN_NOISE_LEVEL
-    )
-    sampled_train_data = train_data.sample(
-        frac=1 - TRAIN_FILE_DATASET_PERCENTAGE, random_state=seed
-    )
-
-    test_data_with_noise = perturb_query_reformulation_data(
-        dataframe=pd.DataFrame(sampled_train_data),
-        noise_level=TEST_NOISE_LEVEL,
-    )
-
-    # TODO(Geordie): Fix this when the new CSV parser is in
-    train_data_with_noise = train_data_with_noise.replace(",", "", regex=True)
-    test_data_with_noise = test_data_with_noise.replace(",", "", regex=True)
-
-    # Write dataset to CSV
-    train_data_with_noise.to_csv(TRAIN_FILE_PATH, index=False)
-    test_data_with_noise.to_csv(TEST_FILE_PATH, index=False)
-
-    return (
-        TRAIN_FILE_PATH,
-        TEST_FILE_PATH,
-        inference_batch,
-    )
-
-
-def download_clinc_dataset(
-    num_training_files=1, clinc_small=False, file_prefix="clinc"
-):
-    CLINC_URL = "https://www.dropbox.com/scl/fi/doxyeurqxvgyperfqwk0r/clinc150.zip?rlkey=s4jfwbjzfwdfro2f82vnatldp&st=u0txk4xx&dl=1"
-    CLINC_ZIP = "./clinc150_uci.zip"
-    CLINC_DIR = "./clinc"
-    MAIN_FILE = CLINC_DIR + "/clinc150_uci/data_full.json"
-    SMALL_FILE = CLINC_DIR + "/clinc150_uci/data_small.json"
-    TRAIN_FILE = f"./{file_prefix}_train.csv"
-    TEST_FILE = f"./{file_prefix}_test.csv"
-    TRAIN_FILES = []
-
-    _download_dataset(
-        url=CLINC_URL,
-        zip_file=CLINC_ZIP,
-        check_existence=[MAIN_FILE],
-        output_dir=CLINC_DIR,
-    )
-
-    samples = None
-
-    if clinc_small:
-        samples = json.load(open(SMALL_FILE))
-    else:
-        samples = json.load(open(MAIN_FILE))
-
-    train_samples = samples["train"]
-    test_samples = samples["test"]
-
-    train_text, train_category = zip(*train_samples)
-    test_text, test_category = zip(*test_samples)
-
-    train_df = pd.DataFrame({"text": train_text, "category": train_category})
-    test_df = pd.DataFrame({"text": test_text, "category": test_category})
-
-    train_df["text"] = train_df["text"]
-    train_df["category"] = pd.Categorical(train_df["category"]).codes
-    test_df["text"] = test_df["text"]
-    test_df["category"] = pd.Categorical(test_df["category"]).codes
-
-    test_df.to_csv(TEST_FILE, index=False, columns=["category", "text"])
-
-    inference_samples = []
-    for _, row in test_df.iterrows():
-        inference_samples.append(({"text": row["text"]}, row["category"]))
-
-    # The columns=["category", "text"] is just to force the order of the output
-    # columns which since the model pipeline which uses this function does not
-    # use the header to determine the column ordering.
-    if num_training_files == 1:
-        train_df.to_csv(TRAIN_FILE, index=False, columns=["category", "text"])
-
-        return TRAIN_FILE, TEST_FILE, inference_samples
-    else:
-        training_data_per_file = len(train_df) // num_training_files
-
-        # saving all files with TRAIN_FILE_i(0 indexed)
-        for i in range(num_training_files):
-            l_index, r_index = (
-                i * training_data_per_file,
-                (i + 1) * training_data_per_file,
-            )
-            filename = f"{file_prefix}_train" + f"_{i}.csv"
-            train_df.iloc[l_index:r_index].to_csv(
-                filename, index=False, columns=["category", "text"]
-            )
-            TRAIN_FILES.append(filename)
-        return TRAIN_FILES, TEST_FILE, inference_samples
-
-
-def download_brazilian_houses_dataset():
-    TRAIN_FILE = "./brazilian_houses_train.csv"
-    TEST_FILE = "./brazilian_houses_test.csv"
-
-    if not os.path.exists(TRAIN_FILE) or not os.path.exists(TEST_FILE):
-        import datasets
-
-        dataset = datasets.load_dataset(
-            "inria-soda/tabular-benchmark", data_files="reg_num/Brazilian_houses.csv"
-        )
-
-        df = pd.DataFrame(dataset["train"].shuffle())
-
-        # Split in to train/test, there are about 10,000 rows in entire dataset.
-        train_df = df.iloc[:8000, :]
-        test_df = df.iloc[8000:, :]
-
-        train_df.to_csv(TRAIN_FILE, index=False)
-        test_df.to_csv(TEST_FILE, index=False)
-
-    inference_samples = _create_inference_samples(
-        filename=TEST_FILE, label_col="totalBRL"
-    )
-
-    return TRAIN_FILE, TEST_FILE, inference_samples
-
-
-def download_internet_ads_dataset(seed=42):
-    random.seed(seed)
-
-    INTERNET_ADS_DOWNLOAD_URL = "https://www.dropbox.com/scl/fi/ze6h56r9a2uy8mzpo14yf/internet-advertisements.zip?rlkey=lmgo50xhugjb4wrwblnynye1a&st=2dil84zs&dl=1"
-    INTERNET_ADS_ZIP = "./internet+advertisements.zip"
-    INTERNET_ADS_DIR = "./internet+advertisements"
-    _download_dataset(
-        url=INTERNET_ADS_DOWNLOAD_URL,
-        zip_file=INTERNET_ADS_ZIP,
-        check_existence=["./internet+advertisements/ad.data"],
-        output_dir=INTERNET_ADS_DIR,
-    )
-    INTERNET_ADS_FILE = "./internet+advertisements/ad.data"
-    TRAIN_FILE = "./internet_ads_train.csv"
-    TEST_FILE = "./internet_ads_test.csv"
-
-    column_names = [str(i) for i in range(1558)] + ["label"]
-
-    if not os.path.exists(TRAIN_FILE) or not os.path.exists(TEST_FILE):
-        header = ",".join(column_names) + "\n"
-
-        with open(INTERNET_ADS_FILE, "r") as data_file:
-            lines = data_file.readlines()
-        for i, line in enumerate(lines):
-            cols = line.strip().split(",")
-            for j, col in enumerate(cols[:3]):
-                if "?" in col:
-                    cols[j] = ""
-            lines[i] = ",".join(cols) + "\n"
-
-        random.shuffle(lines)
-
-        train_test_split = int(0.8 * len(lines))
-
-        with open(TRAIN_FILE, "w") as train_file:
-            train_file.write(header)
-            train_file.writelines(lines[:train_test_split])
-
-        with open(TEST_FILE, "w") as test_file:
-            test_file.write(header)
-            test_file.writelines(lines[train_test_split:])
-
-    inference_samples = _create_inference_samples(filename=TEST_FILE, label_col="label")
-
-    return TRAIN_FILE, TEST_FILE, inference_samples
-
-
-def download_mnist_dataset():
-    TRAIN_FILE = "mnist"
-    TEST_FILE = "mnist.t"
-    if not os.path.exists(TRAIN_FILE):
-        os.system(
-            "curl https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/mnist.bz2 --output mnist.bz2"
-        )
-        os.system("bzip2 -d mnist.bz2")
-
-    if not os.path.exists(TEST_FILE):
-        os.system(
-            "curl https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/mnist.t.bz2 --output mnist.t.bz2"
-        )
-        os.system("bzip2 -d mnist.t.bz2")
-
-    return TRAIN_FILE, TEST_FILE
-
-
-def download_yelp_chi_dataset(seed=42):
-    PATH = "yelp_all.csv"
-    URL = "https://www.dropbox.com/s/ge2sr9iab16hc1x/yelp_all.csv"
-    TRAIN_FILE = "yelp_train.csv"
-    TEST_FILE = "yelp_test.csv"
-
-    if not os.path.exists(PATH):
-        # -L will follow the redirects to correctly download the file from dropbox
-        os.system(f"curl -L {URL} --output {PATH}")
-
-    all_data = pd.read_csv("yelp_all.csv")
-    all_data = all_data.sample(frac=1, random_state=seed)
-
-    numerical_col_names = ["col_" + str(i) for i in range(32)]
-    numerical_col_ranges = (
-        all_data[numerical_col_names].agg([min, max]).T.values.tolist()
-    )
-
-    # Create train and test splits
-    train_length = all_data.shape[0] // 2
-    test_length = all_data.shape[0] - train_length
-    train_data, test_data = (
-        all_data.head(train_length).copy(),
-        all_data.tail(test_length).copy(),
-    )
-    train_data.to_csv(TRAIN_FILE, index=False)
-
-    # Save the test data at first with the labels so that we can create inference samples
-    test_data.to_csv(TEST_FILE, index=False)
-    inference_samples = _create_inference_samples(
-        filename=TEST_FILE, label_col="target"
-    )
-
-    # Zero the ground truth so the model doesn't have access to it during evaluation
-    test_data["target"] = np.zeros(test_length)
-    test_data.to_csv(TEST_FILE, index=False)
-
-    udt_data_types = {
-        "node_id": bolt.types.node_id(),
-        **{
-            col_name: bolt.types.numerical(col_range)
-            for col_range, col_name in zip(numerical_col_ranges, numerical_col_names)
-        },
-        "target": bolt.types.categorical(),
-        "neighbors": bolt.types.neighbors(),
-    }
-
-    return TRAIN_FILE, TEST_FILE, inference_samples, udt_data_types
-
-
-def download_amazon_kaggle_product_catalog_sampled():
-    TRAIN_FILE = "amazon-kaggle-product-catalog.csv"
-    if not os.path.exists(TRAIN_FILE):
-        os.system(
-            "curl -L https://www.dropbox.com/s/tf7e5m0cikhcb95/amazon-kaggle-product-catalog-sampled-0.05.csv?dl=0 -o amazon-kaggle-product-catalog.csv"
-        )
-
-    df = pd.read_csv(f"{os.getcwd()}/{TRAIN_FILE}")
-    n_target_classes = df.shape[0]
-
-    return TRAIN_FILE, n_target_classes
-
-
-def download_agnews_dataset(corpus_file):
-    from datasets import load_dataset
-
-    corpus = load_dataset("ag_news")["train"]["text"]
-    with open(corpus_file, "w") as fw:
-        nothing = fw.write("id,text\n")
-        count = 0
-        for line in corpus:
-            nothing = fw.write(str(count) + "," + line.replace(",", " ").lower() + "\n")
-            count += 1
-
-    return len(corpus)
-
-
-def download_beir_dataset(dataset):
-    url = f"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{dataset}.zip"
-    data_path = download_and_unzip(url, ".")
-
-    corpus, queries_test, qrels_test = GenericDataLoader(data_folder=data_path).load(
-        split="test"
-    )
-
-    write_unsupervised_file(corpus, data_path)
-
-    # we remap doc ids from 0 to N-1 so we can specify integer target in UDT
-    # coldstart only works with integer target for now
-    doc_ids_to_integers = remap_doc_ids(corpus)
-    n_target_classes = len(doc_ids_to_integers)
-
-    # Not all of the beir datasets come with a train split, some only have a test
-    # split. In cases without a train split, we won't write a new supervised train file.
-    if os.path.exists(data_path + "/qrels/train.tsv"):
-        _, queries_train, qrels_train = GenericDataLoader(data_folder=data_path).load(
-            split="train"
-        )
-
-        new_qrels_train = remap_query_answers(qrels_train, doc_ids_to_integers)
-
-        write_supervised_file(
-            queries_train, new_qrels_train, data_path, "trn_supervised.csv"
-        )
-    else:
-        print(
-            f"BEIR Dataset {dataset} doesn't come with a train split, returning None for the trn_supervised path."
-        )
-
-    new_qrels_test = remap_query_answers(qrels_test, doc_ids_to_integers)
-
-    write_supervised_file(queries_test, new_qrels_test, data_path, "tst_supervised.csv")
-
-    trn_supervised = (
-        f"{dataset}/trn_supervised.csv"
-        if os.path.exists(data_path + "/qrels/train.tsv")
-        else None
-    )
-
-    return (
-        f"{dataset}/unsupervised.csv",
-        trn_supervised,
-        f"{dataset}/tst_supervised.csv",
-        n_target_classes,
-    )
+import json
+import math
+import os
+import random
+import zipfile
+
+import numpy as np
+import pandas as pd
+import requests
+from thirdai._thirdai import bolt
+
+from .beir_download_utils import (
+    GenericDataLoader,
+    download_and_unzip,
+    remap_doc_ids,
+    remap_query_answers,
+    write_supervised_file,
+    write_unsupervised_file,
+)
+
+
+def download_file(url, output_path):
+    response = requests.get(url, stream=True)
+    response.raise_for_status()
+    with open(output_path, "wb") as file:
+        for chunk in response.iter_content(chunk_size=8192):
+            file.write(chunk)
+
+
+def _download_dataset(url, zip_file, check_existence, output_dir):
+    if not os.path.exists(zip_file):
+        download_file(url, zip_file)
+
+    if any([not os.path.exists(must_exist) for must_exist in check_existence]):
+        with zipfile.ZipFile(zip_file, "r") as zip_ref:
+            zip_ref.extractall(output_dir)
+
+
+def _create_inference_samples(filename, label_col):
+    df = pd.read_csv(filename)
+    inference_samples = []
+    for _, row in df.iterrows():
+        sample = dict(row)
+        label = sample[label_col]
+        del sample[label_col]
+        sample = {x: str(y) for x, y in sample.items()}
+        inference_samples.append((sample, label))
+    return inference_samples
+
+
+def to_udt_input_batch(dataframe):
+    return [
+        {col_name: str(col_value) for col_name, col_value in record.items()}
+        for record in dataframe.to_dict(orient="records")
+    ]
+
+
+def download_movielens():
+    MOVIELENS_1M_URL = "https://files.grouplens.org/datasets/movielens/ml-1m.zip"
+    MOVIELENS_ZIP = "./movielens.zip"
+    MOVIELENS_DIR = "./movielens"
+    RATINGS_FILE = MOVIELENS_DIR + "/ml-1m/ratings.dat"
+    MOVIE_TITLES = MOVIELENS_DIR + "/ml-1m/movies.dat"
+    TRAIN_FILE = "./movielens_train.csv"
+    TEST_FILE = "./movielens_test.csv"
+    SPLIT = 0.9
+    INFERENCE_BATCH_SIZE = 5
+
+    _download_dataset(
+        url=MOVIELENS_1M_URL,
+        zip_file=MOVIELENS_ZIP,
+        check_existence=[RATINGS_FILE, MOVIE_TITLES],
+        output_dir=MOVIELENS_DIR,
+    )
+
+    df = pd.read_csv(RATINGS_FILE, header=None, delimiter="::", engine="python")
+    df.columns = ["userId", "movieId", "rating", "timestamp"]
+    df["timestamp"] = pd.to_datetime(df["timestamp"], unit="s")
+
+    movies_df = pd.read_csv(
+        MOVIE_TITLES,
+        header=None,
+        delimiter="::",
+        engine="python",
+        encoding="ISO-8859-1",
+    )
+    movies_df.columns = ["movieId", "movieTitle", "genre"]
+    movies_df["movieTitle"] = movies_df["movieTitle"].apply(
+        lambda x: x.replace(",", "")
+    )
+
+    df = pd.merge(df, movies_df, on="movieId")
+    df = df[["userId", "movieTitle", "timestamp"]]
+    df = df.sort_values("timestamp")
+
+    n_train_samples = int(SPLIT * len(df))
+    train_df = df.iloc[:n_train_samples]
+    test_df = df.iloc[n_train_samples:]
+    train_df.to_csv(TRAIN_FILE, index=False)
+    test_df.to_csv(TEST_FILE, index=False)
+
+    index_batch = to_udt_input_batch(df.iloc[:INFERENCE_BATCH_SIZE])
+    inference_batch = to_udt_input_batch(
+        df.iloc[:INFERENCE_BATCH_SIZE][["userId", "timestamp"]]
+    )
+
+    return TRAIN_FILE, TEST_FILE, inference_batch, index_batch
+
+
+def download_criteo():
+    CRITEO_URL = "http://go.criteo.net/criteo-research-kaggle-display-advertising-challenge-dataset.tar.gz"
+    CRITEO_ZIP = "./criteo.tar.gz"
+    CRITEO_DIR = "./criteo"
+    MAIN_FILE = CRITEO_DIR + "/train.txt"
+    CREATED_TRAIN_FILE = "./criteo/train_udt.csv"
+    CREATED_TEST_FILE = "./criteo/test_udt.csv"
+
+    os.makedirs("./criteo", exist_ok=True)
+
+    if not os.path.exists(CRITEO_ZIP):
+        print(
+            f"Downloading from {CRITEO_URL}. This can take 20-40 minutes depending on"
+            " the Criteo server."
+        )
+        os.system(f"wget -t inf -c {CRITEO_URL} -O {CRITEO_ZIP}")
+
+    if not os.path.exists(MAIN_FILE):
+        print("Extracting files. This can take up to 10 minutes.")
+        os.system(f"tar -xvzf {CRITEO_ZIP} -C {CRITEO_DIR}")
+
+    df = pd.read_csv(MAIN_FILE, delimiter="\t", header=None)
+    n_train = int(0.8 * df.shape[0])
+    header = (
+        ["label"]
+        + [f"num_{i}" for i in range(1, 14)]
+        + [f"cat_{i}" for i in range(1, 27)]
+    )
+
+    print("Processing the dataset (this will take about 6-7 mins).")
+    min_vals = df.iloc[:, 1:14].min()
+    df.iloc[:, 1:14] = np.round(np.log(df.iloc[:, 1:14] - min_vals + 1), 2)
+    min_vals = np.float32(df.iloc[:, 1:14].min())
+    max_vals = np.float32(df.iloc[:, 1:14].max())
+    y = np.float32(df.iloc[:, 0])
+    n_unique_classes = list(df.iloc[:, 14:].nunique())
+
+    y_train = y[:n_train]
+    y_test = y[n_train:]
+
+    if not os.path.exists(CREATED_TRAIN_FILE) or not os.path.exists(CREATED_TEST_FILE):
+        print("saving the train and test datasets (this will take about 10 mins)")
+        df[:n_train].to_csv(CREATED_TRAIN_FILE, header=header, index=False)
+        df[n_train:].to_csv(CREATED_TEST_FILE, header=header, index=False)
+
+    df_sample = df.iloc[n_train : n_train + 2]
+    df_sample = df_sample.fillna("")
+    sample_batch = [
+        {header[i]: str(df_sample.iloc[0, i]) for i in range(1, 40)}
+    ]  # first sample
+    sample_batch.append(
+        {header[i]: str(df_sample.iloc[1, i]) for i in range(1, 40)}
+    )  # second sample
+
+    return (
+        CREATED_TRAIN_FILE,
+        CREATED_TEST_FILE,
+        y_train,
+        y_test,
+        min_vals,
+        max_vals,
+        n_unique_classes,
+        sample_batch,
+    )
+
+
+def prep_fraud_dataset(dataset_path, seed=42):
+    df = pd.read_csv(dataset_path)
+    df["amount"] = (df["oldbalanceOrg"] - df["newbalanceOrig"]).abs()
+
+    def upsample(df):
+        fraud_samples = df[df["isFraud"] == 1]
+        upsampling_ratio = 5
+        for i in range(upsampling_ratio):
+            df = pd.concat([df, fraud_samples], axis=0)
+        return df
+
+    df = upsample(df)
+
+    df = df.sample(frac=1, random_state=seed)
+
+    SPLIT = 0.8
+    n_train_samples = int(SPLIT * len(df))
+    train_df = df.iloc[:n_train_samples]
+    test_df = df.iloc[n_train_samples:]
+
+    train_filename = "fraud_detection/new_train.csv"
+    test_filename = "fraud_detection/new_test.csv"
+
+    train_df.to_csv(train_filename, index=False)
+    test_df.to_csv(test_filename, index=False)
+
+    INFERENCE_BATCH_SIZE = 5
+    inference_batch = to_udt_input_batch(
+        df.iloc[:INFERENCE_BATCH_SIZE][
+            [
+                "step",
+                "type",
+                "amount",
+                "nameOrig",
+                "oldbalanceOrg",
+                "newbalanceOrig",
+                "nameDest",
+                "oldbalanceDest",
+                "newbalanceDest",
+                "isFlaggedFraud",
+            ]
+        ]
+    )
+
+    return train_filename, test_filename, inference_batch
+
+
+def download_census_income(num_inference_samples=5, return_labels=False):
+    CENSUS_INCOME_BASE_DOWNLOAD_URL = "https://www.dropbox.com/scl/fi/xg5jld8rj2h3yciduts6l/census-income.zip?rlkey=xo2zs5mtvbl917kgevok4fk1q&st=ehrcbkzo&dl=1"
+    CENSUS_INCOME_ZIP = "./adult.zip"
+    CENSUS_INCOME_DIR = "./adult"
+    TRAIN_FILE = "./census_income_train.csv"
+    TEST_FILE = "./census_income_test.csv"
+    _download_dataset(
+        url=CENSUS_INCOME_BASE_DOWNLOAD_URL,
+        zip_file=CENSUS_INCOME_ZIP,
+        check_existence=["./adult/adult.data", "./adult/adult.test"],
+        output_dir=CENSUS_INCOME_DIR,
+    )
+    COLUMN_NAMES = [
+        "age",
+        "workclass",
+        "fnlwgt",
+        "education",
+        "education-num",
+        "marital-status",
+        "occupation",
+        "relationship",
+        "race",
+        "sex",
+        "capital-gain",
+        "capital-loss",
+        "hours-per-week",
+        "native-country",
+        "label",
+    ]
+    if not os.path.exists(TRAIN_FILE):
+        # reformat the train file
+        with open("./adult/adult.data", "r") as file:
+            data = file.read().splitlines(True)
+        with open(TRAIN_FILE, "w") as file:
+            # Write header
+            file.write(",".join(COLUMN_NAMES) + "\n")
+            # Convert ", " delimiters to ",".
+            # loop through data[1:] since the first line is bogus
+            lines = [line.replace(", ", ",") for line in data[1:]]
+            # Strip empty lines
+            file.writelines([line for line in lines if len(line.strip()) > 0])
+
+    if not os.path.exists(TEST_FILE):
+        # reformat the test file
+        with open("./adult/adult.test", "r") as file:
+            data = file.read().splitlines(True)
+        with open(TEST_FILE, "w") as file:
+            # Write header
+            file.write(",".join(COLUMN_NAMES) + "\n")
+            # Convert ", " delimiters to ",".
+            # Additionally, for some reason each of the labels end with a "." in the test set
+            # loop through data[1:] since the first line is bogus
+            lines = [line.replace(".", "").replace(", ", ",") for line in data[1:]]
+            # Strip empty lines
+            file.writelines([line for line in lines if len(line.strip()) > 0])
+
+    inference_sample_range_end = (
+        -1 if num_inference_samples == "all" else num_inference_samples + 1
+    )
+
+    inference_samples = []
+    with open(TEST_FILE, "r") as test_file:
+        for line in test_file.readlines()[1:inference_sample_range_end]:
+            column_vals = {
+                col_name: value
+                for col_name, value in zip(COLUMN_NAMES, line.split(","))
+            }
+            label = column_vals["label"].strip()
+            del column_vals["label"]
+
+            if return_labels:
+                inference_samples.append((column_vals, label))
+            else:
+                inference_samples.append(column_vals)
+
+    return TRAIN_FILE, TEST_FILE, inference_samples
+
+
+def download_query_reformulation_dataset(train_file_percentage=0.7):
+    """
+    The dataset is retrieved from HuggingFace:
+    https://huggingface.co/datasets/snips_built_in_intents
+    """
+    import datasets
+
+    dataset = datasets.load_dataset(path="embedding-data/sentence-compression")
+    dataframe = pd.DataFrame(data=dataset)
+
+    extracted_text = []
+
+    for _, row in dataframe.iterrows():
+        extracted_text.append(row.to_dict()["train"]["set"][1])
+
+    return pd.DataFrame(data=extracted_text)
+
+
+def perturb_query_reformulation_data(dataframe, noise_level, seed=42):
+    random.seed(seed)
+
+    transformation_type = ("remove-char", "permute-string")
+    transformed_dataframe = []
+
+    PER_QUERY_COPIES = 5
+
+    for _, row in dataframe.iterrows():
+        correct_query = " ".join(list(row.str.split(" ")[0]))
+        query_length = len(correct_query.split(" "))
+        words_to_transform = math.ceil(noise_level * query_length)
+
+        for _ in range(PER_QUERY_COPIES):
+            incorrect_query_list = correct_query.split(" ")
+            transformed_words = 0
+            visited_indices = set()
+
+            while transformed_words < words_to_transform:
+                random_index = random.randint(0, words_to_transform)
+                if random_index in visited_indices:
+                    continue
+                word_to_transform = incorrect_query_list[random_index]
+
+                if random.choices(transformation_type, k=1) == "remove-char":
+                    # Remove a random character
+                    char_index = random.randint(0, len(word_to_transform) - 1)
+                    transformed_word = (
+                        word_to_transform[0:char_index]
+                        + word_to_transform[char_index + 1 :]
+                    )
+                    incorrect_query_list[random_index] = transformed_word
+
+                else:
+                    # Permute the characters in the string
+                    transformed_word_char_list = list(word_to_transform)
+                    random.shuffle(transformed_word_char_list)
+
+                    incorrect_query_list[random_index] = "".join(
+                        transformed_word_char_list
+                    )
+
+                visited_indices.add(random_index)
+                transformed_words += 1
+
+            transformed_dataframe.append(
+                [correct_query, " ".join(incorrect_query_list)]
+            )
+
+    return pd.DataFrame(
+        transformed_dataframe, columns=["target_queries", "source_queries"]
+    )
+
+
+def prepare_query_reformulation_data(seed=42):
+    TRAIN_FILE_PATH = "train_file.csv"
+    TEST_FILE_PATH = "test_file.csv"
+    TRAIN_FILE_DATASET_PERCENTAGE = 0.7
+    INFERENCE_BATCH_PERCENTAGE = 0.0001
+    TRAIN_NOISE_LEVEL = 0.2
+    TEST_NOISE_LEVEL = 0.4
+
+    def get_inference_batch(dataframe):
+        inference_batch = dataframe.sample(
+            frac=INFERENCE_BATCH_PERCENTAGE, random_state=seed
+        )
+        inference_batch_as_list = []
+        for _, row in inference_batch.iterrows():
+            inference_batch_as_list.append({"phrase": row.to_dict()[0]})
+
+        return inference_batch_as_list
+
+    train_data = download_query_reformulation_dataset(
+        train_file_percentage=TRAIN_FILE_DATASET_PERCENTAGE
+    )
+    inference_batch = get_inference_batch(dataframe=train_data)
+
+    train_data_with_noise = perturb_query_reformulation_data(
+        dataframe=train_data, noise_level=TRAIN_NOISE_LEVEL
+    )
+    sampled_train_data = train_data.sample(
+        frac=1 - TRAIN_FILE_DATASET_PERCENTAGE, random_state=seed
+    )
+
+    test_data_with_noise = perturb_query_reformulation_data(
+        dataframe=pd.DataFrame(sampled_train_data),
+        noise_level=TEST_NOISE_LEVEL,
+    )
+
+    # TODO(Geordie): Fix this when the new CSV parser is in
+    train_data_with_noise = train_data_with_noise.replace(",", "", regex=True)
+    test_data_with_noise = test_data_with_noise.replace(",", "", regex=True)
+
+    # Write dataset to CSV
+    train_data_with_noise.to_csv(TRAIN_FILE_PATH, index=False)
+    test_data_with_noise.to_csv(TEST_FILE_PATH, index=False)
+
+    return (
+        TRAIN_FILE_PATH,
+        TEST_FILE_PATH,
+        inference_batch,
+    )
+
+
+def download_clinc_dataset(
+    num_training_files=1, clinc_small=False, file_prefix="clinc"
+):
+    CLINC_URL = "https://www.dropbox.com/scl/fi/doxyeurqxvgyperfqwk0r/clinc150.zip?rlkey=s4jfwbjzfwdfro2f82vnatldp&st=u0txk4xx&dl=1"
+    CLINC_ZIP = "./clinc150_uci.zip"
+    CLINC_DIR = "./clinc"
+    MAIN_FILE = CLINC_DIR + "/clinc150_uci/data_full.json"
+    SMALL_FILE = CLINC_DIR + "/clinc150_uci/data_small.json"
+    TRAIN_FILE = f"./{file_prefix}_train.csv"
+    TEST_FILE = f"./{file_prefix}_test.csv"
+    TRAIN_FILES = []
+
+    _download_dataset(
+        url=CLINC_URL,
+        zip_file=CLINC_ZIP,
+        check_existence=[MAIN_FILE],
+        output_dir=CLINC_DIR,
+    )
+
+    samples = None
+
+    if clinc_small:
+        samples = json.load(open(SMALL_FILE))
+    else:
+        samples = json.load(open(MAIN_FILE))
+
+    train_samples = samples["train"]
+    test_samples = samples["test"]
+
+    train_text, train_category = zip(*train_samples)
+    test_text, test_category = zip(*test_samples)
+
+    train_df = pd.DataFrame({"text": train_text, "category": train_category})
+    test_df = pd.DataFrame({"text": test_text, "category": test_category})
+
+    train_df["text"] = train_df["text"]
+    train_df["category"] = pd.Categorical(train_df["category"]).codes
+    test_df["text"] = test_df["text"]
+    test_df["category"] = pd.Categorical(test_df["category"]).codes
+
+    test_df.to_csv(TEST_FILE, index=False, columns=["category", "text"])
+
+    inference_samples = []
+    for _, row in test_df.iterrows():
+        inference_samples.append(({"text": row["text"]}, row["category"]))
+
+    # The columns=["category", "text"] is just to force the order of the output
+    # columns which since the model pipeline which uses this function does not
+    # use the header to determine the column ordering.
+    if num_training_files == 1:
+        train_df.to_csv(TRAIN_FILE, index=False, columns=["category", "text"])
+
+        return TRAIN_FILE, TEST_FILE, inference_samples
+    else:
+        training_data_per_file = len(train_df) // num_training_files
+
+        # saving all files with TRAIN_FILE_i(0 indexed)
+        for i in range(num_training_files):
+            l_index, r_index = (
+                i * training_data_per_file,
+                (i + 1) * training_data_per_file,
+            )
+            filename = f"{file_prefix}_train" + f"_{i}.csv"
+            train_df.iloc[l_index:r_index].to_csv(
+                filename, index=False, columns=["category", "text"]
+            )
+            TRAIN_FILES.append(filename)
+        return TRAIN_FILES, TEST_FILE, inference_samples
+
+
+def download_brazilian_houses_dataset():
+    TRAIN_FILE = "./brazilian_houses_train.csv"
+    TEST_FILE = "./brazilian_houses_test.csv"
+
+    if not os.path.exists(TRAIN_FILE) or not os.path.exists(TEST_FILE):
+        import datasets
+
+        dataset = datasets.load_dataset(
+            "inria-soda/tabular-benchmark", data_files="reg_num/Brazilian_houses.csv"
+        )
+
+        df = pd.DataFrame(dataset["train"].shuffle())
+
+        # Split in to train/test, there are about 10,000 rows in entire dataset.
+        train_df = df.iloc[:8000, :]
+        test_df = df.iloc[8000:, :]
+
+        train_df.to_csv(TRAIN_FILE, index=False)
+        test_df.to_csv(TEST_FILE, index=False)
+
+    inference_samples = _create_inference_samples(
+        filename=TEST_FILE, label_col="totalBRL"
+    )
+
+    return TRAIN_FILE, TEST_FILE, inference_samples
+
+
+def download_internet_ads_dataset(seed=42):
+    random.seed(seed)
+
+    INTERNET_ADS_DOWNLOAD_URL = "https://www.dropbox.com/scl/fi/ze6h56r9a2uy8mzpo14yf/internet-advertisements.zip?rlkey=lmgo50xhugjb4wrwblnynye1a&st=2dil84zs&dl=1"
+    INTERNET_ADS_ZIP = "./internet+advertisements.zip"
+    INTERNET_ADS_DIR = "./internet+advertisements"
+    _download_dataset(
+        url=INTERNET_ADS_DOWNLOAD_URL,
+        zip_file=INTERNET_ADS_ZIP,
+        check_existence=["./internet+advertisements/ad.data"],
+        output_dir=INTERNET_ADS_DIR,
+    )
+    INTERNET_ADS_FILE = "./internet+advertisements/ad.data"
+    TRAIN_FILE = "./internet_ads_train.csv"
+    TEST_FILE = "./internet_ads_test.csv"
+
+    column_names = [str(i) for i in range(1558)] + ["label"]
+
+    if not os.path.exists(TRAIN_FILE) or not os.path.exists(TEST_FILE):
+        header = ",".join(column_names) + "\n"
+
+        with open(INTERNET_ADS_FILE, "r") as data_file:
+            lines = data_file.readlines()
+        for i, line in enumerate(lines):
+            cols = line.strip().split(",")
+            for j, col in enumerate(cols[:3]):
+                if "?" in col:
+                    cols[j] = ""
+            lines[i] = ",".join(cols) + "\n"
+
+        random.shuffle(lines)
+
+        train_test_split = int(0.8 * len(lines))
+
+        with open(TRAIN_FILE, "w") as train_file:
+            train_file.write(header)
+            train_file.writelines(lines[:train_test_split])
+
+        with open(TEST_FILE, "w") as test_file:
+            test_file.write(header)
+            test_file.writelines(lines[train_test_split:])
+
+    inference_samples = _create_inference_samples(filename=TEST_FILE, label_col="label")
+
+    return TRAIN_FILE, TEST_FILE, inference_samples
+
+
+def download_mnist_dataset():
+    TRAIN_FILE = "mnist"
+    TEST_FILE = "mnist.t"
+    if not os.path.exists(TRAIN_FILE):
+        os.system(
+            "curl https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/mnist.bz2 --output mnist.bz2"
+        )
+        os.system("bzip2 -d mnist.bz2")
+
+    if not os.path.exists(TEST_FILE):
+        os.system(
+            "curl https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/mnist.t.bz2 --output mnist.t.bz2"
+        )
+        os.system("bzip2 -d mnist.t.bz2")
+
+    return TRAIN_FILE, TEST_FILE
+
+
+def download_yelp_chi_dataset(seed=42):
+    PATH = "yelp_all.csv"
+    URL = "https://www.dropbox.com/s/ge2sr9iab16hc1x/yelp_all.csv"
+    TRAIN_FILE = "yelp_train.csv"
+    TEST_FILE = "yelp_test.csv"
+
+    if not os.path.exists(PATH):
+        # -L will follow the redirects to correctly download the file from dropbox
+        os.system(f"curl -L {URL} --output {PATH}")
+
+    all_data = pd.read_csv("yelp_all.csv")
+    all_data = all_data.sample(frac=1, random_state=seed)
+
+    numerical_col_names = ["col_" + str(i) for i in range(32)]
+    numerical_col_ranges = (
+        all_data[numerical_col_names].agg([min, max]).T.values.tolist()
+    )
+
+    # Create train and test splits
+    train_length = all_data.shape[0] // 2
+    test_length = all_data.shape[0] - train_length
+    train_data, test_data = (
+        all_data.head(train_length).copy(),
+        all_data.tail(test_length).copy(),
+    )
+    train_data.to_csv(TRAIN_FILE, index=False)
+
+    # Save the test data at first with the labels so that we can create inference samples
+    test_data.to_csv(TEST_FILE, index=False)
+    inference_samples = _create_inference_samples(
+        filename=TEST_FILE, label_col="target"
+    )
+
+    # Zero the ground truth so the model doesn't have access to it during evaluation
+    test_data["target"] = np.zeros(test_length)
+    test_data.to_csv(TEST_FILE, index=False)
+
+    udt_data_types = {
+        "node_id": bolt.types.node_id(),
+        **{
+            col_name: bolt.types.numerical(col_range)
+            for col_range, col_name in zip(numerical_col_ranges, numerical_col_names)
+        },
+        "target": bolt.types.categorical(),
+        "neighbors": bolt.types.neighbors(),
+    }
+
+    return TRAIN_FILE, TEST_FILE, inference_samples, udt_data_types
+
+
+def download_amazon_kaggle_product_catalog_sampled():
+    TRAIN_FILE = "amazon-kaggle-product-catalog.csv"
+    if not os.path.exists(TRAIN_FILE):
+        os.system(
+            "curl -L https://www.dropbox.com/s/tf7e5m0cikhcb95/amazon-kaggle-product-catalog-sampled-0.05.csv?dl=0 -o amazon-kaggle-product-catalog.csv"
+        )
+
+    df = pd.read_csv(f"{os.getcwd()}/{TRAIN_FILE}")
+    n_target_classes = df.shape[0]
+
+    return TRAIN_FILE, n_target_classes
+
+
+def download_agnews_dataset(corpus_file):
+    from datasets import load_dataset
+
+    corpus = load_dataset("ag_news")["train"]["text"]
+    with open(corpus_file, "w") as fw:
+        nothing = fw.write("id,text\n")
+        count = 0
+        for line in corpus:
+            nothing = fw.write(str(count) + "," + line.replace(",", " ").lower() + "\n")
+            count += 1
+
+    return len(corpus)
+
+
+def download_beir_dataset(dataset):
+    url = f"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{dataset}.zip"
+    data_path = download_and_unzip(url, ".")
+
+    corpus, queries_test, qrels_test = GenericDataLoader(data_folder=data_path).load(
+        split="test"
+    )
+
+    write_unsupervised_file(corpus, data_path)
+
+    # we remap doc ids from 0 to N-1 so we can specify integer target in UDT
+    # coldstart only works with integer target for now
+    doc_ids_to_integers = remap_doc_ids(corpus)
+    n_target_classes = len(doc_ids_to_integers)
+
+    # Not all of the beir datasets come with a train split, some only have a test
+    # split. In cases without a train split, we won't write a new supervised train file.
+    if os.path.exists(data_path + "/qrels/train.tsv"):
+        _, queries_train, qrels_train = GenericDataLoader(data_folder=data_path).load(
+            split="train"
+        )
+
+        new_qrels_train = remap_query_answers(qrels_train, doc_ids_to_integers)
+
+        write_supervised_file(
+            queries_train, new_qrels_train, data_path, "trn_supervised.csv"
+        )
+    else:
+        print(
+            f"BEIR Dataset {dataset} doesn't come with a train split, returning None for the trn_supervised path."
+        )
+
+    new_qrels_test = remap_query_answers(qrels_test, doc_ids_to_integers)
+
+    write_supervised_file(queries_test, new_qrels_test, data_path, "tst_supervised.csv")
+
+    trn_supervised = (
+        f"{dataset}/trn_supervised.csv"
+        if os.path.exists(data_path + "/qrels/train.tsv")
+        else None
+    )
+
+    return (
+        f"{dataset}/unsupervised.csv",
+        trn_supervised,
+        f"{dataset}/tst_supervised.csv",
+        n_target_classes,
+    )
```

## thirdai/demos/download_tokenizer_vocabs.py

 * *Ordering differences only*

```diff
@@ -1,16 +1,16 @@
-import os
-
-
-def bert_base_uncased(dirname="."):
-    BERT_TAG = "bert-base-uncased"
-    BERT_VOCAB_PATH = os.path.join(dirname, f"{BERT_TAG}.vocab")
-    BERT_VOCAB_URL = f"https://huggingface.co/{BERT_TAG}/resolve/main/vocab.txt"
-
-    if not os.path.exists(BERT_VOCAB_PATH):
-        import urllib.request
-
-        response = urllib.request.urlopen(BERT_VOCAB_URL)
-        with open(BERT_VOCAB_PATH, "wb+") as bert_vocab_file:
-            bert_vocab_file.write(response.read())
-
-    return BERT_VOCAB_PATH
+import os
+
+
+def bert_base_uncased(dirname="."):
+    BERT_TAG = "bert-base-uncased"
+    BERT_VOCAB_PATH = os.path.join(dirname, f"{BERT_TAG}.vocab")
+    BERT_VOCAB_URL = f"https://huggingface.co/{BERT_TAG}/resolve/main/vocab.txt"
+
+    if not os.path.exists(BERT_VOCAB_PATH):
+        import urllib.request
+
+        response = urllib.request.urlopen(BERT_VOCAB_URL)
+        with open(BERT_VOCAB_PATH, "wb+") as bert_vocab_file:
+            bert_vocab_file.write(response.read())
+
+    return BERT_VOCAB_PATH
```

## thirdai/demos/__init__.py

 * *Ordering differences only*

```diff
@@ -1,2 +1,2 @@
-from .download_datasets import *
-from .download_tokenizer_vocabs import *
+from .download_datasets import *
+from .download_tokenizer_vocabs import *
```

## thirdai/gen/questions.py

 * *Ordering differences only*

```diff
@@ -1,35 +1,35 @@
-from typing import List
-
-import openai
-from tqdm import tqdm
-
-
-class QAGenMethod:
-    def generate(self, texts: List[str]) -> List[List[str]]:
-        pass
-
-
-class OpenAI(QAGenMethod):
-    def __init__(
-        self, api_key: str, model: str, questions_per_paragraph: int, verbose=True
-    ):
-        self.client = openai.OpenAI(api_key=api_key)
-        self.model = model
-        self.prompt = (
-            f"Generate {questions_per_paragraph} questions from the "
-            "following text. Try to ask questions that don't just reference "
-            "keywords in the text. Return your answers as newline separated responses "
-            "without any number or bullet prefixes. Here is the content: \n\n"
-        )
-        self.verbose = verbose
-
-    def generate(self, texts: List[str]) -> List[List[str]]:
-        questions = []
-        for text in tqdm(texts, disable=not self.verbose):
-            chat_completion = self.client.chat.completions.create(
-                messages=[{"role": "user", "content": self.prompt + text}],
-                model=self.model,
-            )
-            response = chat_completion.choices[0].message.content
-            questions.append(response.split("\n"))
-        return questions
+from typing import List
+
+import openai
+from tqdm import tqdm
+
+
+class QAGenMethod:
+    def generate(self, texts: List[str]) -> List[List[str]]:
+        pass
+
+
+class OpenAI(QAGenMethod):
+    def __init__(
+        self, api_key: str, model: str, questions_per_paragraph: int, verbose=True
+    ):
+        self.client = openai.OpenAI(api_key=api_key)
+        self.model = model
+        self.prompt = (
+            f"Generate {questions_per_paragraph} questions from the "
+            "following text. Try to ask questions that don't just reference "
+            "keywords in the text. Return your answers as newline separated responses "
+            "without any number or bullet prefixes. Here is the content: \n\n"
+        )
+        self.verbose = verbose
+
+    def generate(self, texts: List[str]) -> List[List[str]]:
+        questions = []
+        for text in tqdm(texts, disable=not self.verbose):
+            chat_completion = self.client.chat.completions.create(
+                messages=[{"role": "user", "content": self.prompt + text}],
+                model=self.model,
+            )
+            response = chat_completion.choices[0].message.content
+            questions.append(response.split("\n"))
+        return questions
```

## thirdai/gen/__init__.py

 * *Ordering differences only*

```diff
@@ -1 +1 @@
-from .questions import *
+from .questions import *
```

## thirdai/neural_db/connectors.py

 * *Ordering differences only*

```diff
@@ -1,201 +1,201 @@
-import os
-import shutil
-import tempfile
-from typing import List, Optional
-
-import pandas as pd
-from office365.sharepoint.client_context import ClientContext
-from simple_salesforce import Salesforce
-from sqlalchemy import inspect, text
-from sqlalchemy.engine.base import Connection as sqlConn
-
-from .utils import DIRECTORY_CONNECTOR_SUPPORTED_EXT
-
-
-class Connector:
-    def chunk_iterator(self):
-        raise NotImplementedError()
-
-
-class SQLConnector(Connector):
-    def __init__(
-        self,
-        engine: sqlConn,
-        table_name: str,
-        id_col: str,
-        columns: Optional[List[str]] = None,
-        chunk_size: Optional[int] = None,
-    ):
-        self._engine = engine
-        self.id_col = id_col
-        self.columns = columns
-        self.table_name = table_name
-        self.chunk_size = chunk_size
-        self._connection = self._engine.connect()
-
-    def __del__(self):
-        self._connection.close()
-
-    def execute(self, query: str, param={}):
-        result = self._connection.execute(statement=text(query), parameters=param)
-        return result
-
-    def get_engine_url(self):
-        return self._engine.url
-
-    def chunk_iterator(self):
-        return pd.read_sql(
-            sql=f"SELECT {', '.join(self.columns)} FROM {self.table_name} ORDER BY {self.id_col}",
-            con=self._connection,
-            chunksize=self.chunk_size,
-        )
-
-    def total_rows(self):
-        return self.execute(query=f"select count(*) from {self.table_name}").fetchone()[
-            0
-        ]
-
-    def cols_metadata(self):
-        inspector = inspect(self._engine)
-        return inspector.get_columns(self.table_name)
-
-    def get_all_rows(self, cols: List[str] = "*"):
-        if isinstance(cols, list):
-            cols = ", ".join(cols)
-
-        return self.execute(query=f"SELECT {cols} from {self.table_name}")
-
-    def get_primary_keys(self):
-        inspector = inspect(self._engine)
-        pk_constraint = inspector.get_pk_constraint(self.table_name)
-        return pk_constraint["constrained_columns"]
-
-
-class SharePointConnector(Connector):
-    def __init__(
-        self,
-        ctx: ClientContext,
-        library_path: str,
-        chunk_size: int = 10485760,
-    ):
-        self._ctx = ctx
-        self.library_path = library_path
-        self.chunk_size = chunk_size
-        try:
-            # Loading the Sharepoint library's metadata by it's path
-            library = self._ctx.web.get_folder_by_server_relative_path(
-                self.library_path
-            )
-            self._ctx.load(library)
-
-            # Retreiving all the file's metadata from the library
-            self._files = library.files
-            self._ctx.load(self._files)
-            self._ctx.execute_query()
-
-            # filtering to only contain files of supported extensions
-            self._files = list(
-                filter(
-                    lambda file: file.properties["Name"].split(sep=".")[-1]
-                    in DIRECTORY_CONNECTOR_SUPPORTED_EXT,
-                    self._files,
-                )
-            )
-
-            if not len(self._files) > 0:
-                raise FileNotFoundError("No files of supported extension is present")
-
-            # we need to maintain a fixed order of files because local_docs needs to be also equivalent as detailed in the test cases. For more info: ndb_utls.py::build_local_sharepoint_doc & test_connector_document_implementation.py
-            self._files = sorted(self._files, key=lambda file: file.properties["Name"])
-        except Exception as e:
-            print("Unable to retrieve files from SharePoint, Error: " + str(e))
-
-    def chunk_iterator(self):
-        try:
-            files_dict = {}
-            temp_dir = tempfile.mkdtemp()
-            currently_occupied = 0
-
-            for file in self._files:
-                file_size = int(file.properties["Length"])
-                filename = file.properties["Name"]
-                file_server_relative_url = file.properties["ServerRelativeUrl"]
-                if (
-                    len(files_dict) > 0
-                    and file_size + currently_occupied >= self.chunk_size
-                ):
-                    # Return the fetched files
-                    yield files_dict
-                    files_dict.clear()
-                    currently_occupied = 0
-                    shutil.rmtree(temp_dir)
-
-                    temp_dir = tempfile.mkdtemp()
-                else:
-                    filepath = os.path.join(temp_dir, filename)
-                    with open(filepath, "wb") as fp:
-                        file.download(fp).execute_query()
-                        files_dict[file_server_relative_url] = filepath
-                        currently_occupied += file_size
-            if len(files_dict) > 0:
-                yield files_dict
-        except Exception as e:
-            print("Unable to retrieve file(s) from SharePoint, Error: " + str(e))
-        finally:
-            if os.path.exists(temp_dir):
-                shutil.rmtree(temp_dir)
-
-    @property
-    def url(self):
-        web = self._ctx.web.get().execute_query()
-        return web.url
-
-    @property
-    def site_name(self):
-        return self.url.split(sep="/")[-1]
-
-    def num_files(self):
-        return len(self._files)
-
-
-class SalesforceConnector(Connector):
-    def __init__(
-        self,
-        instance: Salesforce,
-        object_name: str,
-        fields: Optional[List[str]] = None,
-    ) -> None:
-        self._instance = instance
-        self._object_name = object_name
-        self._fields = fields
-
-    def execute(self, query: str):
-        # Returns an OrderedDicts with keys ['totalSize', 'done', 'records']
-        return self._instance.query(query)
-
-    def chunk_iterator(self):
-        query = f"SELECT {', '.join(self._fields)} FROM {self._object_name}"
-        results = self._instance.bulk.__getattr__(self._object_name).query(
-            query, lazy_operation=True
-        )
-        for chunk in results:
-            # Number of records in each chunk can atmost 10K (can't be changed with salesforce bulk API).
-            chunk_df = pd.DataFrame(chunk)
-            chunk_df.drop(columns=["attributes"], inplace=True)
-            yield chunk_df
-
-    def total_rows(self):
-        result = self.execute(query=f"SELECT COUNT() from {self._object_name}")
-        return result["totalSize"]
-
-    def field_metadata(self):
-        object_schema = self._instance.__getattr__(self._object_name).describe()
-        return object_schema["fields"]
-
-    @property
-    def sf_instance(self):
-        return self._instance.sf_instance
-
-    @property
-    def base_url(self):
-        return self._instance.base_url
+import os
+import shutil
+import tempfile
+from typing import List, Optional
+
+import pandas as pd
+from office365.sharepoint.client_context import ClientContext
+from simple_salesforce import Salesforce
+from sqlalchemy import inspect, text
+from sqlalchemy.engine.base import Connection as sqlConn
+
+from .utils import DIRECTORY_CONNECTOR_SUPPORTED_EXT
+
+
+class Connector:
+    def chunk_iterator(self):
+        raise NotImplementedError()
+
+
+class SQLConnector(Connector):
+    def __init__(
+        self,
+        engine: sqlConn,
+        table_name: str,
+        id_col: str,
+        columns: Optional[List[str]] = None,
+        chunk_size: Optional[int] = None,
+    ):
+        self._engine = engine
+        self.id_col = id_col
+        self.columns = columns
+        self.table_name = table_name
+        self.chunk_size = chunk_size
+        self._connection = self._engine.connect()
+
+    def __del__(self):
+        self._connection.close()
+
+    def execute(self, query: str, param={}):
+        result = self._connection.execute(statement=text(query), parameters=param)
+        return result
+
+    def get_engine_url(self):
+        return self._engine.url
+
+    def chunk_iterator(self):
+        return pd.read_sql(
+            sql=f"SELECT {', '.join(self.columns)} FROM {self.table_name} ORDER BY {self.id_col}",
+            con=self._connection,
+            chunksize=self.chunk_size,
+        )
+
+    def total_rows(self):
+        return self.execute(query=f"select count(*) from {self.table_name}").fetchone()[
+            0
+        ]
+
+    def cols_metadata(self):
+        inspector = inspect(self._engine)
+        return inspector.get_columns(self.table_name)
+
+    def get_all_rows(self, cols: List[str] = "*"):
+        if isinstance(cols, list):
+            cols = ", ".join(cols)
+
+        return self.execute(query=f"SELECT {cols} from {self.table_name}")
+
+    def get_primary_keys(self):
+        inspector = inspect(self._engine)
+        pk_constraint = inspector.get_pk_constraint(self.table_name)
+        return pk_constraint["constrained_columns"]
+
+
+class SharePointConnector(Connector):
+    def __init__(
+        self,
+        ctx: ClientContext,
+        library_path: str,
+        chunk_size: int = 10485760,
+    ):
+        self._ctx = ctx
+        self.library_path = library_path
+        self.chunk_size = chunk_size
+        try:
+            # Loading the Sharepoint library's metadata by it's path
+            library = self._ctx.web.get_folder_by_server_relative_path(
+                self.library_path
+            )
+            self._ctx.load(library)
+
+            # Retreiving all the file's metadata from the library
+            self._files = library.files
+            self._ctx.load(self._files)
+            self._ctx.execute_query()
+
+            # filtering to only contain files of supported extensions
+            self._files = list(
+                filter(
+                    lambda file: file.properties["Name"].split(sep=".")[-1]
+                    in DIRECTORY_CONNECTOR_SUPPORTED_EXT,
+                    self._files,
+                )
+            )
+
+            if not len(self._files) > 0:
+                raise FileNotFoundError("No files of supported extension is present")
+
+            # we need to maintain a fixed order of files because local_docs needs to be also equivalent as detailed in the test cases. For more info: ndb_utls.py::build_local_sharepoint_doc & test_connector_document_implementation.py
+            self._files = sorted(self._files, key=lambda file: file.properties["Name"])
+        except Exception as e:
+            print("Unable to retrieve files from SharePoint, Error: " + str(e))
+
+    def chunk_iterator(self):
+        try:
+            files_dict = {}
+            temp_dir = tempfile.mkdtemp()
+            currently_occupied = 0
+
+            for file in self._files:
+                file_size = int(file.properties["Length"])
+                filename = file.properties["Name"]
+                file_server_relative_url = file.properties["ServerRelativeUrl"]
+                if (
+                    len(files_dict) > 0
+                    and file_size + currently_occupied >= self.chunk_size
+                ):
+                    # Return the fetched files
+                    yield files_dict
+                    files_dict.clear()
+                    currently_occupied = 0
+                    shutil.rmtree(temp_dir)
+
+                    temp_dir = tempfile.mkdtemp()
+                else:
+                    filepath = os.path.join(temp_dir, filename)
+                    with open(filepath, "wb") as fp:
+                        file.download(fp).execute_query()
+                        files_dict[file_server_relative_url] = filepath
+                        currently_occupied += file_size
+            if len(files_dict) > 0:
+                yield files_dict
+        except Exception as e:
+            print("Unable to retrieve file(s) from SharePoint, Error: " + str(e))
+        finally:
+            if os.path.exists(temp_dir):
+                shutil.rmtree(temp_dir)
+
+    @property
+    def url(self):
+        web = self._ctx.web.get().execute_query()
+        return web.url
+
+    @property
+    def site_name(self):
+        return self.url.split(sep="/")[-1]
+
+    def num_files(self):
+        return len(self._files)
+
+
+class SalesforceConnector(Connector):
+    def __init__(
+        self,
+        instance: Salesforce,
+        object_name: str,
+        fields: Optional[List[str]] = None,
+    ) -> None:
+        self._instance = instance
+        self._object_name = object_name
+        self._fields = fields
+
+    def execute(self, query: str):
+        # Returns an OrderedDicts with keys ['totalSize', 'done', 'records']
+        return self._instance.query(query)
+
+    def chunk_iterator(self):
+        query = f"SELECT {', '.join(self._fields)} FROM {self._object_name}"
+        results = self._instance.bulk.__getattr__(self._object_name).query(
+            query, lazy_operation=True
+        )
+        for chunk in results:
+            # Number of records in each chunk can atmost 10K (can't be changed with salesforce bulk API).
+            chunk_df = pd.DataFrame(chunk)
+            chunk_df.drop(columns=["attributes"], inplace=True)
+            yield chunk_df
+
+    def total_rows(self):
+        result = self.execute(query=f"SELECT COUNT() from {self._object_name}")
+        return result["totalSize"]
+
+    def field_metadata(self):
+        object_schema = self._instance.__getattr__(self._object_name).describe()
+        return object_schema["fields"]
+
+    @property
+    def sf_instance(self):
+        return self._instance.sf_instance
+
+    @property
+    def base_url(self):
+        return self._instance.base_url
```

## thirdai/neural_db/constraint_matcher.py

 * *Ordering differences only*

```diff
@@ -1,254 +1,254 @@
-from __future__ import annotations
-
-import sqlite3
-import warnings
-from collections import defaultdict
-from typing import Any, Dict, Generic, Iterable, List, Optional, Set, TypeVar
-
-import pandas as pd
-from sortedcontainers import SortedDict
-
-ItemT = TypeVar("ItemT")
-
-
-ItemConstraintIndex = SortedDict
-
-
-class Filter(Generic[ItemT]):
-    def filter(self, value_to_items: ItemConstraintIndex) -> Set[ItemT]:
-        raise NotImplementedError()
-
-    def filter_df_column(self, df: pd.DataFrame, column_name: str):
-        raise NotImplementedError()
-
-    def sql_condition(self, column_name):
-        raise NotImplementedError()
-
-
-def format_value_for_sql(val):
-    if not isinstance(val, str):
-        return val
-    return "'" + val.replace("'", "''") + "'"
-
-
-class AnyOf(Filter[ItemT]):
-    def __init__(self, values: Iterable[Any]):
-        self.values = values
-
-    def filter(self, value_to_items: ItemConstraintIndex) -> Set[ItemT]:
-        matches = set()
-        for value in self.values:
-            if value in value_to_items:
-                matches = matches.union(value_to_items[value])
-        return matches
-
-    def filter_df_column(self, df: pd.DataFrame, column_name: str):
-        return df[df[column_name].isin(self.values)]
-
-    def sql_condition(self, column_name: str):
-        formatted_values = [format_value_for_sql(val) for val in self.values]
-        return (
-            "(" + " or ".join(f"{column_name}=={val}" for val in formatted_values) + ")"
-        )
-
-
-class EqualTo(AnyOf[ItemT]):
-    def __init__(self, value: Any):
-        super().__init__([value])
-
-
-class InRange(Filter[ItemT]):
-    def __init__(
-        self, minimum: Any, maximum: Any, inclusive_min=True, inclusive_max=True
-    ):
-        if minimum is None or maximum is None:
-            raise ValueError("InRange cannot accept None for minimum and maximum.")
-
-        self.min = minimum
-        self.max = maximum
-        self.inclusive = (inclusive_min, inclusive_max)
-
-    def filter(self, value_to_items: ItemConstraintIndex) -> Set[ItemT]:
-        values = value_to_items.irange(self.min, self.max, self.inclusive)
-        return AnyOf(values).filter(value_to_items)
-
-    def filter_df_column(self, df: pd.DataFrame, column_name: str):
-        left_inclusive, right_inclusive = self.inclusive
-
-        if left_inclusive and right_inclusive:
-            inclusive = "both"
-        elif left_inclusive:
-            inclusive = "left"
-        elif right_inclusive:
-            inclusive = "right"
-        else:
-            inclusive = "neither"
-
-        return df[df[column_name].between(self.min, self.max, inclusive=inclusive)]
-
-    def sql_condition(self, column_name: str):
-        left_inclusive, right_inclusive = self.inclusive
-        left_comp = ">=" if left_inclusive else ">"
-        right_comp = "<=" if right_inclusive else "<"
-        return (
-            f"{column_name}{left_comp}{format_value_for_sql(self.min)} and "
-            f"{column_name}{right_comp}{format_value_for_sql(self.max)}"
-        )
-
-
-class GreaterThan(InRange[ItemT]):
-    def __init__(self, minimum: Any, include_equal=False):
-        self.minimum = minimum
-        self.include_equal = include_equal
-
-    def filter(self, value_to_items: ItemConstraintIndex) -> Set[ItemT]:
-        values = value_to_items.irange(self.minimum, None, (self.include_equal, False))
-        return AnyOf(values).filter(value_to_items)
-
-    def filter_df_column(self, df: pd.DataFrame, column_name: str):
-        if self.include_equal:
-            return df[df[column_name].ge(self.minimum)]
-        return df[df[column_name].gt(self.minimum)]
-
-    def sql_condition(self, column_name: str):
-        comp = ">=" if self.include_equal else ">"
-        return f"{column_name}{comp}{format_value_for_sql(self.minimum)}"
-
-
-class LessThan(InRange[ItemT]):
-    def __init__(self, maximum: Any, include_equal=False):
-        self.maximum = maximum
-        self.include_equal = include_equal
-
-    def filter(self, value_to_items: ItemConstraintIndex) -> Set[ItemT]:
-        values = value_to_items.irange(None, self.maximum, (False, self.include_equal))
-        return AnyOf(values).filter(value_to_items)
-
-    def filter_df_column(self, df: pd.DataFrame, column_name: str):
-        if self.include_equal:
-            return df[df[column_name].le(self.maximum)]
-        return df[df[column_name].lt(self.maximum)]
-
-    def sql_condition(self, column_name: str):
-        comp = "<=" if self.include_equal else "<"
-        return f"{column_name}{comp}{format_value_for_sql(self.maximum)}"
-
-
-class TableFilter:
-    def __init__(self, filters: Dict[str, Filter]):
-        self.filters = filters
-
-    def filter_df_ids(self, df) -> List[int]:
-        is_pandas_df = isinstance(df, pd.DataFrame)
-        for column_name, filterer in self.filters.items():
-            if column_name not in df.columns:
-                return []
-            df = filterer.filter_df_column(df, column_name)
-        if is_pandas_df:
-            return df.index.to_list()
-        return df.index.compute().tolist()
-
-    def filter_sql_ids(
-        self, con: sqlite3.Connection, table_name: str, id_column: str
-    ) -> List[int]:
-        condition = " and ".join(
-            filterer.sql_condition(column_name)
-            for column_name, filterer in self.filters.items()
-        )
-        return list(
-            pd.read_sql(f"select {id_column} from {table_name} where {condition}", con)
-        )
-
-
-class ConstraintValue:
-    def __init__(self, value: Any = None, is_any: bool = False):
-        if is_any and value is not None:
-            raise RuntimeError(
-                "ConstraintValue cannot accept non-None value and is_any=True at the same time."
-            )
-
-        if value == "__any__":
-            self._value = None
-            self._is_any = True
-            warnings.warn(
-                "Setting the metadata value to '__any__' treats it as a wildcard that matches any value, which differs from the standard behavior where the value is set exactly as provided."
-            )
-        else:
-            self._value = value
-            self._is_any = is_any
-
-    def any(self):
-        return self._is_any
-
-    def value(self):
-        return self._value
-
-
-class ConstraintIndex(Generic[ItemT]):
-    def __init__(self):
-        self._any_value = set()
-        self._match_value = ItemConstraintIndex()
-
-    def match(self, filterer: Filter) -> Set[ItemT]:
-        return self._any_value.union(filterer.filter(self._match_value))
-
-    def index(self, item: ItemT, constraint_value: ConstraintValue) -> None:
-        if constraint_value.any():
-            self._any_value.add(item)
-        else:
-            value = constraint_value.value()
-            if isinstance(value, list) or isinstance(value, set):
-                for sub_value in value:
-                    if not sub_value in self._match_value:
-                        self._match_value[sub_value] = set()
-                    self._match_value[sub_value].add(item)
-            else:
-                if not value in self._match_value:
-                    self._match_value[value] = set()
-                self._match_value[value].add(item)
-
-    def delete(self, item: ItemT, constraint_value: ConstraintValue) -> None:
-        if constraint_value.any():
-            self._any_value.remove(item)
-        else:
-            value = constraint_value.value()
-            if isinstance(value, list) or isinstance(value, set):
-                for sub_value in value:
-                    self._match_value[sub_value].remove(item)
-            else:
-                self._match_value[value].remove(item)
-
-
-class ConstraintMatcher(Generic[ItemT]):
-    def __init__(self):
-        self._all_items = set()
-        self._item_constraints = {}
-
-    def match(self, filters: Dict[str, Filter]) -> Set[ItemT]:
-        matches = self._all_items
-
-        for key, filterer in filters.items():
-            if key not in self._item_constraints:
-                return set()
-            matches = matches.intersection(self._item_constraints[key].match(filterer))
-
-        return matches
-
-    def index(self, item: ItemT, constraints: Dict[str, ConstraintValue]) -> None:
-        self._all_items.add(item)
-        for key, constraint_value in constraints.items():
-            if key not in self._item_constraints:
-                self._item_constraints[key] = ConstraintIndex[ItemT]()
-            self._item_constraints[key].index(item, constraint_value)
-
-    def delete(self, item: ItemT, constraints: Dict[str, ConstraintValue]) -> None:
-        self._all_items.remove(item)
-        for key, constraint_value in constraints.items():
-            self._item_constraints[key].delete(item, constraint_value)
-
-
-def to_filters(constraints: Dict[str, Any]):
-    return {
-        key: value if isinstance(value, Filter) else EqualTo(value)
-        for key, value in constraints.items()
-    }
+from __future__ import annotations
+
+import sqlite3
+import warnings
+from collections import defaultdict
+from typing import Any, Dict, Generic, Iterable, List, Optional, Set, TypeVar
+
+import pandas as pd
+from sortedcontainers import SortedDict
+
+ItemT = TypeVar("ItemT")
+
+
+ItemConstraintIndex = SortedDict
+
+
+class Filter(Generic[ItemT]):
+    def filter(self, value_to_items: ItemConstraintIndex) -> Set[ItemT]:
+        raise NotImplementedError()
+
+    def filter_df_column(self, df: pd.DataFrame, column_name: str):
+        raise NotImplementedError()
+
+    def sql_condition(self, column_name):
+        raise NotImplementedError()
+
+
+def format_value_for_sql(val):
+    if not isinstance(val, str):
+        return val
+    return "'" + val.replace("'", "''") + "'"
+
+
+class AnyOf(Filter[ItemT]):
+    def __init__(self, values: Iterable[Any]):
+        self.values = values
+
+    def filter(self, value_to_items: ItemConstraintIndex) -> Set[ItemT]:
+        matches = set()
+        for value in self.values:
+            if value in value_to_items:
+                matches = matches.union(value_to_items[value])
+        return matches
+
+    def filter_df_column(self, df: pd.DataFrame, column_name: str):
+        return df[df[column_name].isin(self.values)]
+
+    def sql_condition(self, column_name: str):
+        formatted_values = [format_value_for_sql(val) for val in self.values]
+        return (
+            "(" + " or ".join(f"{column_name}=={val}" for val in formatted_values) + ")"
+        )
+
+
+class EqualTo(AnyOf[ItemT]):
+    def __init__(self, value: Any):
+        super().__init__([value])
+
+
+class InRange(Filter[ItemT]):
+    def __init__(
+        self, minimum: Any, maximum: Any, inclusive_min=True, inclusive_max=True
+    ):
+        if minimum is None or maximum is None:
+            raise ValueError("InRange cannot accept None for minimum and maximum.")
+
+        self.min = minimum
+        self.max = maximum
+        self.inclusive = (inclusive_min, inclusive_max)
+
+    def filter(self, value_to_items: ItemConstraintIndex) -> Set[ItemT]:
+        values = value_to_items.irange(self.min, self.max, self.inclusive)
+        return AnyOf(values).filter(value_to_items)
+
+    def filter_df_column(self, df: pd.DataFrame, column_name: str):
+        left_inclusive, right_inclusive = self.inclusive
+
+        if left_inclusive and right_inclusive:
+            inclusive = "both"
+        elif left_inclusive:
+            inclusive = "left"
+        elif right_inclusive:
+            inclusive = "right"
+        else:
+            inclusive = "neither"
+
+        return df[df[column_name].between(self.min, self.max, inclusive=inclusive)]
+
+    def sql_condition(self, column_name: str):
+        left_inclusive, right_inclusive = self.inclusive
+        left_comp = ">=" if left_inclusive else ">"
+        right_comp = "<=" if right_inclusive else "<"
+        return (
+            f"{column_name}{left_comp}{format_value_for_sql(self.min)} and "
+            f"{column_name}{right_comp}{format_value_for_sql(self.max)}"
+        )
+
+
+class GreaterThan(InRange[ItemT]):
+    def __init__(self, minimum: Any, include_equal=False):
+        self.minimum = minimum
+        self.include_equal = include_equal
+
+    def filter(self, value_to_items: ItemConstraintIndex) -> Set[ItemT]:
+        values = value_to_items.irange(self.minimum, None, (self.include_equal, False))
+        return AnyOf(values).filter(value_to_items)
+
+    def filter_df_column(self, df: pd.DataFrame, column_name: str):
+        if self.include_equal:
+            return df[df[column_name].ge(self.minimum)]
+        return df[df[column_name].gt(self.minimum)]
+
+    def sql_condition(self, column_name: str):
+        comp = ">=" if self.include_equal else ">"
+        return f"{column_name}{comp}{format_value_for_sql(self.minimum)}"
+
+
+class LessThan(InRange[ItemT]):
+    def __init__(self, maximum: Any, include_equal=False):
+        self.maximum = maximum
+        self.include_equal = include_equal
+
+    def filter(self, value_to_items: ItemConstraintIndex) -> Set[ItemT]:
+        values = value_to_items.irange(None, self.maximum, (False, self.include_equal))
+        return AnyOf(values).filter(value_to_items)
+
+    def filter_df_column(self, df: pd.DataFrame, column_name: str):
+        if self.include_equal:
+            return df[df[column_name].le(self.maximum)]
+        return df[df[column_name].lt(self.maximum)]
+
+    def sql_condition(self, column_name: str):
+        comp = "<=" if self.include_equal else "<"
+        return f"{column_name}{comp}{format_value_for_sql(self.maximum)}"
+
+
+class TableFilter:
+    def __init__(self, filters: Dict[str, Filter]):
+        self.filters = filters
+
+    def filter_df_ids(self, df) -> List[int]:
+        is_pandas_df = isinstance(df, pd.DataFrame)
+        for column_name, filterer in self.filters.items():
+            if column_name not in df.columns:
+                return []
+            df = filterer.filter_df_column(df, column_name)
+        if is_pandas_df:
+            return df.index.to_list()
+        return df.index.compute().tolist()
+
+    def filter_sql_ids(
+        self, con: sqlite3.Connection, table_name: str, id_column: str
+    ) -> List[int]:
+        condition = " and ".join(
+            filterer.sql_condition(column_name)
+            for column_name, filterer in self.filters.items()
+        )
+        return list(
+            pd.read_sql(f"select {id_column} from {table_name} where {condition}", con)
+        )
+
+
+class ConstraintValue:
+    def __init__(self, value: Any = None, is_any: bool = False):
+        if is_any and value is not None:
+            raise RuntimeError(
+                "ConstraintValue cannot accept non-None value and is_any=True at the same time."
+            )
+
+        if value == "__any__":
+            self._value = None
+            self._is_any = True
+            warnings.warn(
+                "Setting the metadata value to '__any__' treats it as a wildcard that matches any value, which differs from the standard behavior where the value is set exactly as provided."
+            )
+        else:
+            self._value = value
+            self._is_any = is_any
+
+    def any(self):
+        return self._is_any
+
+    def value(self):
+        return self._value
+
+
+class ConstraintIndex(Generic[ItemT]):
+    def __init__(self):
+        self._any_value = set()
+        self._match_value = ItemConstraintIndex()
+
+    def match(self, filterer: Filter) -> Set[ItemT]:
+        return self._any_value.union(filterer.filter(self._match_value))
+
+    def index(self, item: ItemT, constraint_value: ConstraintValue) -> None:
+        if constraint_value.any():
+            self._any_value.add(item)
+        else:
+            value = constraint_value.value()
+            if isinstance(value, list) or isinstance(value, set):
+                for sub_value in value:
+                    if not sub_value in self._match_value:
+                        self._match_value[sub_value] = set()
+                    self._match_value[sub_value].add(item)
+            else:
+                if not value in self._match_value:
+                    self._match_value[value] = set()
+                self._match_value[value].add(item)
+
+    def delete(self, item: ItemT, constraint_value: ConstraintValue) -> None:
+        if constraint_value.any():
+            self._any_value.remove(item)
+        else:
+            value = constraint_value.value()
+            if isinstance(value, list) or isinstance(value, set):
+                for sub_value in value:
+                    self._match_value[sub_value].remove(item)
+            else:
+                self._match_value[value].remove(item)
+
+
+class ConstraintMatcher(Generic[ItemT]):
+    def __init__(self):
+        self._all_items = set()
+        self._item_constraints = {}
+
+    def match(self, filters: Dict[str, Filter]) -> Set[ItemT]:
+        matches = self._all_items
+
+        for key, filterer in filters.items():
+            if key not in self._item_constraints:
+                return set()
+            matches = matches.intersection(self._item_constraints[key].match(filterer))
+
+        return matches
+
+    def index(self, item: ItemT, constraints: Dict[str, ConstraintValue]) -> None:
+        self._all_items.add(item)
+        for key, constraint_value in constraints.items():
+            if key not in self._item_constraints:
+                self._item_constraints[key] = ConstraintIndex[ItemT]()
+            self._item_constraints[key].index(item, constraint_value)
+
+    def delete(self, item: ItemT, constraints: Dict[str, ConstraintValue]) -> None:
+        self._all_items.remove(item)
+        for key, constraint_value in constraints.items():
+            self._item_constraints[key].delete(item, constraint_value)
+
+
+def to_filters(constraints: Dict[str, Any]):
+    return {
+        key: value if isinstance(value, Filter) else EqualTo(value)
+        for key, value in constraints.items()
+    }
```

## thirdai/neural_db/documents.py

```diff
@@ -1,2554 +1,2559 @@
-import hashlib
-import json
-import os
-import pickle
-import shutil
-import string
-from collections import OrderedDict
-from pathlib import Path
-from typing import Any, Dict, List, Optional, Tuple, Union
-
-import dask.dataframe as dd
-import numpy as np
-import pandas as pd
-from nltk.tokenize import sent_tokenize
-from office365.sharepoint.client_context import (
-    ClientContext,
-    ClientCredential,
-    UserCredential,
-)
-from pytrie import StringTrie
-from requests.models import Response
-from simple_salesforce import Salesforce
-from sqlalchemy import Integer, String, create_engine
-from sqlalchemy.engine.base import Connection as sqlConn
-from thirdai import bolt
-from thirdai.data import get_udt_col_types
-from thirdai.dataset.data_source import PyDataSource
-
-from .connectors import SalesforceConnector, SharePointConnector, SQLConnector
-from .constraint_matcher import (
-    ConstraintMatcher,
-    ConstraintValue,
-    Filter,
-    TableFilter,
-    to_filters,
-)
-from .parsing_utils import doc_parse, pdf_parse, sliding_pdf_parse, url_parse
-from .table import DaskDataFrameTable, DataFrameTable, SQLiteTable
-from .utils import hash_file, hash_string, requires_condition
-
-
-class Reference:
-    pass
-
-
-def _raise_unknown_doc_error(element_id: int):
-    raise ValueError(f"Unable to find document that has id {element_id}.")
-
-
-class Document:
-    @property
-    def size(self) -> int:
-        raise NotImplementedError()
-
-    @property
-    def name(self) -> str:
-        raise NotImplementedError()
-
-    @property
-    def source(self) -> str:
-        raise NotImplementedError()
-
-    @property
-    def hash(self) -> str:
-        sha1 = hashlib.sha1()
-        sha1.update(bytes(self.name, "utf-8"))
-        for i in range(self.size):
-            sha1.update(bytes(self.reference(i).text, "utf-8"))
-        return sha1.hexdigest()
-
-    @property
-    def matched_constraints(self) -> Dict[str, ConstraintValue]:
-        raise NotImplementedError()
-
-    def all_entity_ids(self) -> List[int]:
-        raise NotImplementedError()
-
-    def filter_entity_ids(self, filters: Dict[str, Filter]):
-        return self.all_entity_ids()
-
-    def id_map(self) -> Optional[Dict[str, int]]:
-        return None
-
-    # This attribute allows certain things to be saved or not saved during
-    # the pickling of a savable_state object. For example, if we set this
-    # to True for CSV docs, we will save the actual csv file in the pickle.
-    # Utilize this property in save_meta and load_meta of document objs.
-    @property
-    def save_extra_info(self) -> bool:
-        return self._save_extra_info
-
-    @save_extra_info.setter
-    def save_extra_info(self, value: bool):
-        self._save_extra_info = value
-
-    def reference(self, element_id: int) -> Reference:
-        raise NotImplementedError()
-
-    def strong_text(self, element_id: int) -> str:
-        return self.reference(element_id).text
-
-    def weak_text(self, element_id: int) -> str:
-        return self.reference(element_id).text
-
-    def context(self, element_id: int, radius: int) -> str:
-        window_start = max(0, element_id - radius)
-        window_end = min(self.size, element_id + radius + 1)
-        return " \n".join(
-            [self.reference(elid).text for elid in range(window_start, window_end)]
-        )
-
-    def save_meta(self, directory: Path):
-        pass
-
-    def load_meta(self, directory: Path):
-        pass
-
-    def row_iterator(self):
-        for i in range(self.size):
-            yield DocumentRow(
-                element_id=i,
-                strong=self.strong_text(i),
-                weak=self.weak_text(i),
-            )
-
-    def save(self, directory: str):
-        dirpath = Path(directory)
-        if os.path.exists(dirpath):
-            shutil.rmtree(dirpath)
-        os.mkdir(dirpath)
-        with open(dirpath / f"doc.pkl", "wb") as pkl:
-            pickle.dump(self, pkl)
-        os.mkdir(dirpath / "meta")
-        self.save_meta(dirpath / "meta")
-
-    @staticmethod
-    def load(directory: str):
-        dirpath = Path(directory)
-        with open(dirpath / f"doc.pkl", "rb") as pkl:
-            obj = pickle.load(pkl)
-        obj.load_meta(dirpath / "meta")
-        return obj
-
-
-class Reference:
-    def __init__(
-        self,
-        document: Document,
-        element_id: int,
-        text: str,
-        source: str,
-        metadata: dict,
-        upvote_ids: List[int] = None,
-        retriever: str = None,
-    ):
-        self._id = element_id
-        self._upvote_ids = upvote_ids if upvote_ids is not None else [element_id]
-        self._text = text
-        self._source = source
-        self._metadata = metadata
-        self._context_fn = lambda radius: document.context(element_id, radius)
-        self._score = 0
-        self._document = document
-        self._retriever = retriever
-
-    @property
-    def id(self):
-        return self._id
-
-    @property
-    def upvote_ids(self):
-        return self._upvote_ids
-
-    @property
-    def text(self):
-        return self._text
-
-    @property
-    def source(self):
-        return self._source
-
-    @property
-    def metadata(self):
-        return self._metadata
-
-    @property
-    def score(self):
-        return self._score
-
-    @property
-    def document(self):
-        return self._document
-
-    @property
-    def retriever(self):
-        return self._retriever
-
-    def context(self, radius: int):
-        return self._context_fn(radius)
-
-    def __eq__(self, other):
-        if isinstance(other, Reference):
-            return (
-                self.id == other.id
-                and self.text == other.text
-                and self.source == other.source
-            )
-        return False
-
-
-class DocumentRow:
-    def __init__(self, element_id: int, strong: str, weak: str):
-        self.id = element_id
-        self.strong = strong
-        self.weak = weak
-
-
-DocAndOffset = Tuple[Document, int]
-
-
-class DocumentDataSource(PyDataSource):
-    def __init__(self, id_column, strong_column, weak_column):
-        PyDataSource.__init__(self)
-        self.documents: List[DocAndOffset] = []
-        for col in [id_column, strong_column, weak_column]:
-            if '"' in col or "," in col:
-                raise RuntimeError(
-                    "DocumentDataSource columns cannot contain '\"' or ','"
-                )
-        self.id_column = id_column
-        self.strong_column = strong_column
-        self.weak_column = weak_column
-        self._size = 0
-        self.restart()
-
-    def add(self, document: Document, start_id: int):
-        self.documents.append((document, start_id))
-        self._size += document.size
-
-    def row_iterator(self):
-        for doc, start_id in self.documents:
-            for row in doc.row_iterator():
-                row.id = row.id + start_id
-                yield row
-
-    def indices(self):
-        indices = []
-        for doc, start_id in self.documents:
-            for row in doc.row_iterator():
-                indices.append(row.id + start_id)
-
-        return indices
-
-    @property
-    def size(self):
-        return self._size
-
-    def _csv_line(self, element_id: str, strong: str, weak: str):
-        csv_strong = '"' + strong.replace('"', '""') + '"'
-        csv_weak = '"' + weak.replace('"', '""') + '"'
-        return f"{element_id},{csv_strong},{csv_weak}"
-
-    def _get_line_iterator(self):
-        # First yield the header
-        yield f"{self.id_column},{self.strong_column},{self.weak_column}"
-        # Then yield rows
-        for row in self.row_iterator():
-            yield self._csv_line(element_id=row.id, strong=row.strong, weak=row.weak)
-
-    def resource_name(self) -> str:
-        return "Documents:\n" + "\n".join([doc.name for doc, _ in self.documents])
-
-    def save(self, path: Path, save_interval=100_000):
-        """
-        DocumentDataSource is agnostic to the documents that are a part of it as the line_iterator is agnostic to the kind of document and returns data in a specific format. Hence, to serialize DocumentDataSource, we do not need to serialize the documents but rather, dump the lines yielded by the line iterator into a CSV. This makes the saving and loading logic simpler.
-        """
-        path.mkdir(exist_ok=True, parents=True)
-        number_lines_in_buffer = 0
-        with open(path / "source.csv", "w", encoding="utf-8") as f:
-            for line in self._get_line_iterator():
-                f.write(line + "\n")
-                number_lines_in_buffer += 1
-            if number_lines_in_buffer > save_interval:
-                f.flush()
-                number_lines_in_buffer = 0
-
-        with open(path / "arguments.json", "w") as f:
-            json.dump(
-                {
-                    "id_column": self.id_column,
-                    "strong_column": self.strong_column,
-                    "weak_column": self.weak_column,
-                },
-                f,
-                indent=4,
-            )
-        self.restart()
-
-    @staticmethod
-    def load(path: Path):
-        with open(path / "arguments.json", "r") as f:
-            args = json.load(f)
-
-        csv_document = CSV(
-            path=path / "source.csv",
-            id_column=args["id_column"],
-            strong_columns=[args["strong_column"]],
-            weak_columns=[args["weak_column"]],
-            has_offset=True,
-        )
-        data_source = DocumentDataSource(**args)
-        data_source.add(csv_document, start_id=0)
-        return data_source
-
-
-class IntroAndTrainDocuments:
-    def __init__(self, intro: DocumentDataSource, train: DocumentDataSource) -> None:
-        self.intro = intro
-        self.train = train
-
-
-class DocumentManager:
-    def __init__(self, id_column, strong_column, weak_column) -> None:
-        self.id_column = id_column
-        self.strong_column = strong_column
-        self.weak_column = weak_column
-
-        # After python 3.8, we don't need to use OrderedDict as Dict is ordered by default
-        self.registry: OrderedDict[str, DocAndOffset] = OrderedDict()
-        self.source_id_prefix_trie = StringTrie()
-        self.constraint_matcher = ConstraintMatcher[DocAndOffset]()
-
-    def _next_id(self):
-        if len(self.registry) == 0:
-            return 0
-        doc, start_id = next(reversed(self.registry.values()))
-        return start_id + doc.size
-
-    def add(self, documents: List[Document]):
-        intro = DocumentDataSource(self.id_column, self.strong_column, self.weak_column)
-        train = DocumentDataSource(self.id_column, self.strong_column, self.weak_column)
-        for doc in documents:
-            doc_hash = doc.hash
-            if doc_hash not in self.registry:
-                start_id = self._next_id()
-                doc_and_id = (doc, start_id)
-                self.registry[doc_hash] = doc_and_id
-                self.source_id_prefix_trie[doc_hash] = doc_hash
-                intro.add(doc, start_id)
-                self.constraint_matcher.index(
-                    item=(doc, start_id), constraints=doc.matched_constraints
-                )
-            doc, start_id = self.registry[doc_hash]
-            train.add(doc, start_id)
-
-        return IntroAndTrainDocuments(intro=intro, train=train), [
-            doc.hash for doc in documents
-        ]
-
-    def delete(self, source_ids):
-        # TODO(Geordie): Error handling
-        all_sources_exist = all(source_id in self.registry for source_id in source_ids)
-        if not all_sources_exist:
-            raise KeyError("At least one source not found in document manager.")
-
-        deleted_entities = []
-        for source_id in source_ids:
-            doc, offset = self.registry[source_id]
-            deleted_entities += [
-                offset + entity_id for entity_id in doc.all_entity_ids()
-            ]
-            del self.registry[source_id]
-            del self.source_id_prefix_trie[source_id]
-            self.constraint_matcher.delete((doc, offset), doc.matched_constraints)
-
-        return deleted_entities
-
-    def entity_ids_by_constraints(self, constraints: Dict[str, Any]):
-        filters = to_filters(constraints)
-        return [
-            start_id + entity_id
-            for doc, start_id in self.constraint_matcher.match(filters)
-            for entity_id in doc.filter_entity_ids(filters)
-        ]
-
-    def sources(self):
-        return {doc_hash: doc for doc_hash, (doc, _) in self.registry.items()}
-
-    def match_source_id_by_prefix(self, prefix: str) -> Document:
-        if prefix in self.registry:
-            return [prefix]
-        return self.source_id_prefix_trie.values(prefix)
-
-    def source_by_id(self, source_id: str):
-        return self.registry[source_id]
-
-    def clear(self):
-        self.registry = OrderedDict()
-        self.source_id_prefix_trie = StringTrie()
-        self.constraint_matcher = ConstraintMatcher[DocAndOffset]()
-
-    def _get_doc_and_start_id(self, element_id: int):
-        for doc, start_id in reversed(self.registry.values()):
-            if start_id <= element_id:
-                return doc, start_id
-
-        _raise_unknown_doc_error(element_id)
-
-    def reference(self, element_id: int):
-        doc, start_id = self._get_doc_and_start_id(element_id)
-        doc_ref = doc.reference(element_id - start_id)
-        doc_ref._id = element_id
-        doc_ref._upvote_ids = [start_id + uid for uid in doc_ref._upvote_ids]
-        return doc_ref
-
-    def context(self, element_id: int, radius: int):
-        doc, start_id = self._get_doc_and_start_id(element_id)
-        return doc.context(element_id - start_id, radius)
-
-    def get_data_source(self) -> DocumentDataSource:
-        data_source = DocumentDataSource(
-            id_column=self.id_column,
-            strong_column=self.strong_column,
-            weak_column=self.weak_column,
-        )
-
-        for doc, start_id in self.registry.values():
-            data_source.add(document=doc, start_id=start_id)
-
-        return data_source
-
-    def save_meta(self, directory: Path):
-        for i, (doc, _) in enumerate(self.registry.values()):
-            subdir = directory / str(i)
-            os.mkdir(subdir)
-            doc.save_meta(subdir)
-
-    def load_meta(self, directory: Path):
-        for i, (doc, _) in enumerate(self.registry.values()):
-            subdir = directory / str(i)
-            doc.load_meta(subdir)
-
-        if not hasattr(self, "doc_constraints"):
-            self.constraint_matcher = ConstraintMatcher[DocAndOffset]()
-            for item in self.registry.values():
-                self.constraint_matcher.index(item, item[0].matched_constraints)
-
-
-def safe_has_offset(this):
-    """Checks the value of the "has_offset" attribute of a class.
-    Defaults to False when the attribute does not exist.
-    This function is needed for backwards compatibility reasons.
-    """
-    if hasattr(this, "has_offset"):
-        return this.has_offset
-    return False
-
-
-def create_table(df, on_disk):
-    Table = (
-        SQLiteTable
-        if on_disk
-        else DaskDataFrameTable if isinstance(df, dd.DataFrame) else DataFrameTable
-    )
-    return Table(df)
-
-
-def metadata_with_source(metadata, source: str):
-    if "source" in metadata:
-        raise ValueError(
-            "Document metadata cannot contain the key 'source'. 'source' is a reserved key."
-        )
-    return {**metadata, "source": source}
-
-
-class CSV(Document):
-    """
-    A document containing the rows of a csv file.
-
-    Args:
-        path (str): The path to the csv file.
-        id_column (Optional[str]). Optional, defaults to None. If provided then the
-            ids in this column are used to identify the rows in NeuralDB. If not provided
-            then ids are assigned.
-        strong_columns (Optional[List[str]]): Optional, defaults to None. This argument
-            can be used to provide NeuralDB with information about which columns are
-            likely to contain the strongest signal in matching with a given query. For
-            example this could be something like the name of a product.
-        weak_columns (Optional[List[str]]): Optional, defaults to None. This argument
-            can be used to provide NeuralDB with information about which columns are
-            likely to contain weaker signals in matching with a given query. For
-            example this could be something like the description of a product.
-        reference_columns (Optional[List[str]]): Optional, defaults to None. If provided
-            the specified columns are returned by NeuralDB as responses to queries. If
-            not specifed all columns are returned.
-        metadata (Dict[str, Any]): Optional, defaults to {}. Specifies metadata to
-            associate with entities from this file. Queries to NeuralDB can provide
-            constrains to restrict results based on the metadata.
-    """
-
-    def valid_id_column(column):
-        if isinstance(column, dd.Series):
-            unique_count = column.nunique().compute()
-            min_val = column.min().compute()
-            max_val = column.max().compute()
-            length = column.size.compute()
-            condition = (
-                (unique_count == length) and (min_val == 0) and (max_val == length - 1)
-            )
-        else:
-            condition = (
-                (len(column.unique()) == len(column))
-                and (column.min() == 0)
-                and (column.max() == len(column) - 1)
-            )
-
-        return condition
-
-    def remove_spaces(column_name):
-        return column_name.replace(" ", "_")
-
-    def remove_spaces_from_list(column_name_list):
-        return [CSV.remove_spaces(col) for col in column_name_list]
-
-    # blocksize (when using dask) Determines the size of each partition/chunk in bytes.
-    # For example, setting blocksize=25e6 will aim for partitions of approximately 25MB.
-    # If you decrease the block size, Dask will create more partitions, and
-    # increasing it will result in fewer partitions.
-    # Default value is computed based on available physical memory
-    # and the number of cores, up to a maximum of 64MB.
-    def __init__(
-        self,
-        path: str,
-        id_column: Optional[str] = None,
-        strong_columns: Optional[List[str]] = None,
-        weak_columns: Optional[List[str]] = None,
-        reference_columns: Optional[List[str]] = None,
-        save_extra_info=True,
-        metadata=None,
-        has_offset=False,
-        on_disk=False,
-        use_dask=False,
-        blocksize=None,
-    ) -> None:
-        if use_dask:
-            df = (
-                dd.read_csv(path, blocksize=blocksize)
-                if blocksize
-                else dd.read_csv(path)
-            )
-        else:
-            df = pd.read_csv(path)
-
-        # Convert spaces in column names to underscores because df.itertuples
-        # does not work when there are spaces
-        # https://stackoverflow.com/questions/45307376/pandas-df-itertuples-renaming-dataframe-columns-when-printing
-        # While it's possible that saved models contain column names that have
-        # spaces. We don't convert these columns during deserialization because
-        # df.itertuples is only called during CSV construction and during
-        # insertion, which would have completed before serialization.
-        # Additionally, this document's hash takes column names into account.
-        # Consider this scenario:
-        # 1. User inserts CSV with spaced column names into an older version of NDB
-        # 2. User saves the NDB model
-        # 3. User upgrades the ThirdAI package
-        # 4. User loads the saved NDB model
-        # 5. User inserts the same CSV into the loaded model
-        # Here, NeuralDB will actually treat the CSV as a new, unseen document,
-        # so it will not invoke df.itertuples on a dataframe that has spaced
-        # column names.
-        cols_with_spaces = [col for col in df.columns if " " in col]
-        self.with_space_to_no_space = {}
-        if cols_with_spaces:
-            for col in cols_with_spaces:
-                self.with_space_to_no_space[col] = col.replace(" ", "_")
-                while self.with_space_to_no_space[col] in df.columns:
-                    self.with_space_to_no_space[col] += "_"
-
-            def remove_spaces_from_list(cols):
-                return [self.with_space_to_no_space.get(col, col) for col in cols]
-
-            df.columns = remove_spaces_from_list(df.columns)
-            if id_column:
-                id_column = self.with_space_to_no_space.get(id_column, id_column)
-            if strong_columns:
-                strong_columns = remove_spaces_from_list(strong_columns)
-            if weak_columns:
-                weak_columns = remove_spaces_from_list(weak_columns)
-            if reference_columns:
-                reference_columns = remove_spaces_from_list(reference_columns)
-
-        self.no_space_to_with_space = {
-            val: key for key, val in self.with_space_to_no_space.items()
-        }
-
-        # This variable is used to check whether the id's in the CSV are supposed to start with 0 or with some custom offset. We need the latter when we shard the datasource.
-        self.has_offset = has_offset
-
-        if reference_columns is None:
-            reference_columns = list(df.columns)
-
-        self.orig_to_assigned_id = None
-        self.id_column = id_column
-        orig_id_column = id_column
-        if self.id_column and (has_offset or CSV.valid_id_column(df[self.id_column])):
-            df = df.sort_values(self.id_column)
-        else:
-            self.id_column = "thirdai_index"
-            if use_dask:
-                # sets dask df index column to range(len(df))
-                df[self.id_column] = (
-                    df.assign(partition_count=1).partition_count.cumsum() - 1
-                )
-            else:
-                df[self.id_column] = range(df.shape[0])
-
-            if orig_id_column:
-                self.orig_to_assigned_id = {
-                    str(getattr(row, orig_id_column)): getattr(row, self.id_column)
-                    for row in df.itertuples(index=True)
-                }
-
-        if strong_columns is None and weak_columns is None:
-            # autotune column types
-            text_col_names = []
-            try:
-                for col_name, udt_col_type in get_udt_col_types(path).items():
-                    if type(udt_col_type) == type(bolt.types.text()):
-                        text_col_names.append(CSV.remove_spaces(col_name))
-            except:
-                text_col_names = list(df.columns)
-                text_col_names.remove(id_column)
-                if orig_id_column:
-                    text_col_names.remove(orig_id_column)
-                df[text_col_names] = df[text_col_names].astype(str)
-            strong_columns = []
-            weak_columns = text_col_names
-        elif strong_columns is None:
-            strong_columns = []
-        elif weak_columns is None:
-            weak_columns = []
-
-        for col in strong_columns + weak_columns:
-            df[col] = df[col].fillna("")
-
-        if use_dask:
-            # The 'sorted=True' parameter is used to indicate that the column is already sorted.
-            # This optimization helps Dask to avoid expensive data shuffling operations, improving performance.
-            df = df.set_index(self.id_column, sorted=True)
-        else:
-            # Pandas automatically manages the index without needing to explicitly sort it here.
-            df = df.set_index(self.id_column)
-
-        self.table = create_table(df, on_disk)
-
-        self.path = Path(path)
-        self.strong_columns = strong_columns
-        self.weak_columns = weak_columns
-        self.reference_columns = [
-            col for col in reference_columns if col != self.id_column
-        ]
-        self._save_extra_info = save_extra_info
-        self.doc_metadata = metadata_with_source(metadata or {}, Path(path).name)
-        self.doc_metadata_keys = set(self.doc_metadata.keys())
-        # Add column names to hash metadata so that CSVs with different
-        # hyperparameters are treated as different documents. Otherwise, this
-        # may break training.
-        self._hash = hash_file(
-            path,
-            metadata="csv-"
-            + str(self.id_column)
-            + str(sorted(self.strong_columns))
-            + str(sorted(self.weak_columns))
-            + str(sorted(self.reference_columns))
-            + str(sorted(list(self.doc_metadata.items())))
-            + str(sorted(self.table.columns)),
-        )
-
-    @property
-    def hash(self) -> str:
-        return self._hash
-
-    @property
-    def size(self) -> int:
-        return self.table.size
-
-    @property
-    def name(self) -> str:
-        return self.path.name
-
-    @property
-    def source(self) -> str:
-        return str(self.path.absolute())
-
-    @requires_condition(
-        check_func=lambda self: not safe_has_offset(self),
-        method_name="matched_constraints",
-        method_class="CSV(Document)",
-        condition_unmet_string=" when there is an offset in the CSV document",
-    )
-    @property
-    def matched_constraints(self) -> Dict[str, ConstraintValue]:
-        metadata_constraints = {
-            key: ConstraintValue(value) for key, value in self.doc_metadata.items()
-        }
-        indexed_column_constraints = {
-            self.no_space_to_with_space.get(key, key): ConstraintValue(is_any=True)
-            for key in self.table.columns
-        }
-        return {**metadata_constraints, **indexed_column_constraints}
-
-    def all_entity_ids(self) -> List[int]:
-        return self.table.ids
-
-    def filter_entity_ids(self, filters: Dict[str, Filter]):
-        table_filter = TableFilter(
-            {
-                self.with_space_to_no_space.get(k, k): v
-                for k, v in filters.items()
-                if k not in self.doc_metadata_keys
-            }
-        )
-        return self.table.apply_filter(table_filter)
-
-    def id_map(self) -> Optional[Dict[str, int]]:
-        return self.orig_to_assigned_id
-
-    def strong_text_from_row(self, row) -> str:
-        return " ".join(str(row[col]) for col in self.strong_columns)
-
-    def strong_text(self, element_id: int) -> str:
-        row = self.table.row_as_dict(element_id)
-        return self.strong_text_from_row(row)
-
-    def weak_text_from_row(self, row) -> str:
-        return " ".join(str(row[col]) for col in self.weak_columns)
-
-    def weak_text(self, element_id: int) -> str:
-        row = self.table.row_as_dict(element_id)
-        return self.weak_text_from_row(row)
-
-    def row_iterator(self):
-        for row_id, row in self.table.iter_rows_as_dicts():
-            yield DocumentRow(
-                element_id=row_id,
-                strong=self.strong_text_from_row(row),
-                weak=self.weak_text_from_row(row),
-            )
-
-    @requires_condition(
-        check_func=lambda self: not safe_has_offset(self),
-        method_name="reference",
-        method_class="CSV(Document)",
-        condition_unmet_string=" when there is an offset in the CSV document",
-    )
-    def reference(self, element_id: int) -> Reference:
-        if element_id >= self.table.size:
-            _raise_unknown_doc_error(element_id)
-        row = self.table.row_as_dict(element_id)
-        text = "\n\n".join(
-            [
-                f"{self.no_space_to_with_space.get(col, col)}: {row[col]}"
-                for col in self.reference_columns
-            ]
-        )
-        row = {
-            self.no_space_to_with_space.get(col, col): val for col, val in row.items()
-        }
-        return Reference(
-            document=self,
-            element_id=element_id,
-            text=text,
-            source=self.source,
-            metadata={**row, **self.doc_metadata},
-        )
-
-    def context(self, element_id: int, radius) -> str:
-        rows = self.table.range_rows_as_dicts(
-            from_row_id=max(0, element_id - radius),
-            to_row_id=min(self.table.size, element_id + radius + 1),
-        )
-
-        return " ".join(
-            [
-                "\n\n".join(
-                    [
-                        f"{self.no_space_to_with_space.get(col, col)}: {row[col]}"
-                        for col in self.reference_columns
-                    ]
-                )
-                for row in rows
-            ]
-        )
-
-    def __getstate__(self):
-        state = self.__dict__.copy()
-
-        # Save the filename so we can load it with the same name
-        state["doc_name"] = self.name
-
-        state["path"] = str(self.path)
-
-        # End pickling functionality here to support old directory checkpoint save
-        return state
-
-    def __setstate__(self, state):
-        # Add new attributes to state for older document object version backward compatibility
-        if "_save_extra_info" not in state:
-            state["_save_extra_info"] = True
-
-        if "path" in state:
-            state["path"] = Path(state["path"])
-
-        self.__dict__.update(state)
-
-    @requires_condition(
-        check_func=lambda self: not safe_has_offset(self),
-        method_name="save_meta",
-        method_class="CSV(Document)",
-        condition_unmet_string=" when there is an offset in the CSV document",
-    )
-    def save_meta(self, directory: Path):
-        # Let's copy the original CSV file to the provided directory
-        if self.save_extra_info:
-            shutil.copy(self.path, directory)
-        self.table.save_meta(directory)
-
-    @requires_condition(
-        check_func=lambda self: not safe_has_offset(self),
-        method_name="load_meta",
-        method_class="CSV(Document)",
-        condition_unmet_string=" when there is an offset in the CSV document",
-    )
-    def load_meta(self, directory: Path):
-        # Since we've moved the CSV file to the provided directory, let's make
-        # sure that we point to this CSV file.
-        if hasattr(self, "doc_name"):
-            self.path = directory / self.doc_name
-        else:
-            # this else statement handles the deprecated attribute "path" in self, we can remove this soon
-            self.path = directory / self.path.name
-
-        if not hasattr(self, "doc_metadata"):
-            self.doc_metadata = {}
-        if not hasattr(self, "doc_metadata_keys"):
-            self.doc_metadata_keys = set()
-        if not hasattr(self, "orig_to_assigned_id"):
-            self.orig_to_assigned_id = None
-        if not hasattr(self, "has_offset"):
-            self.has_offset = False
-
-        if hasattr(self, "df"):
-            if self.df.index.name != self.id_column:
-                self.reference_columns = [
-                    col for col in self.reference_columns if col != self.id_column
-                ]
-                self.df = self.df.set_index(self.id_column)
-            self.table = DataFrameTable(self.df)
-            del self.df
-        else:
-            self.table.load_meta(directory)
-
-        if hasattr(self, "with_space_to_no_space"):
-            self.no_space_to_with_space = {
-                val: key for key, val in self.with_space_to_no_space.items()
-            }
-        else:
-            self.with_space_to_no_space = {}
-            self.no_space_to_with_space = {}
-
-
-# Base class for PDF, DOCX and Unstructured classes because they share the same logic.
-class Extracted(Document):
-    def __init__(
-        self,
-        path: str,
-        save_extra_info=True,
-        metadata=None,
-        strong_column=None,
-        on_disk=False,
-    ):
-        path = str(path)
-        df = self.process_data(path)
-        self.table = create_table(df, on_disk)
-        self.hash_val = hash_file(path, metadata="extracted-" + str(metadata))
-        self._save_extra_info = save_extra_info
-
-        self.path = Path(path)
-        self.doc_metadata = metadata_with_source(metadata or {}, Path(path).name)
-        self.strong_column = strong_column
-        if self.strong_column and self.strong_column not in self.table.columns:
-            raise RuntimeError(
-                f"Strong column '{self.strong_column}' not found in the dataframe."
-            )
-
-    def process_data(
-        self,
-        path: str,
-    ) -> pd.DataFrame:
-        raise NotImplementedError()
-
-    @property
-    def hash(self) -> str:
-        return self.hash_val
-
-    @property
-    def size(self) -> int:
-        return self.table.size
-
-    @property
-    def name(self) -> str:
-        return self.path.name
-
-    @property
-    def source(self) -> str:
-        return str(self.path.absolute())
-
-    @property
-    def matched_constraints(self) -> Dict[str, ConstraintValue]:
-        return {key: ConstraintValue(value) for key, value in self.doc_metadata.items()}
-
-    def all_entity_ids(self) -> List[int]:
-        return list(range(self.size))
-
-    def strong_text(self, element_id: int) -> str:
-        return (
-            ""
-            if not self.strong_column
-            else self.table.field(element_id, self.strong_column)
-        )
-
-    def weak_text(self, element_id: int) -> str:
-        return self.table.field(element_id, "para")
-
-    def show_fn(text, source, **kwargs):
-        return text
-
-    def reference(self, element_id: int) -> Reference:
-        if element_id >= self.table.size:
-            _raise_unknown_doc_error(element_id)
-        return Reference(
-            document=self,
-            element_id=element_id,
-            text=self.table.field(element_id, "display"),
-            source=self.source,
-            metadata={**self.table.row_as_dict(element_id), **self.doc_metadata},
-        )
-
-    def context(self, element_id, radius) -> str:
-        if not 0 <= element_id or not element_id < self.size:
-            raise ("Element id not in document.")
-
-        rows = self.table.range_rows_as_dicts(
-            from_row_id=max(0, element_id - radius),
-            to_row_id=min(self.table.size, element_id + radius + 1),
-        )
-        return "\n".join(row["para"] for row in rows)
-
-    def __getstate__(self):
-        state = self.__dict__.copy()
-
-        # Remove filename attribute because this is a deprecated attribute for Extracted
-        if "filename" in state:
-            del state["filename"]
-
-        # In older versions of neural_db, we accidentally stored Path objects in the df.
-        # This changes those objects to a string, because PosixPath can't be loaded in Windows
-        def path_to_str(element):
-            if isinstance(element, Path):
-                return element.name
-            return element
-
-        if "df" in state:
-            state["df"] = state["df"].applymap(path_to_str)
-
-        # Save the filename so we can load it with the same name
-        state["doc_name"] = self.name
-
-        state["path"] = str(self.path)
-
-        return state
-
-    def __setstate__(self, state):
-        # Add new attributes to state for older document object version backward compatibility
-        if "_save_extra_info" not in state:
-            state["_save_extra_info"] = True
-        if "filename" in state:
-            state["path"] = state["filename"]
-
-        if "path" in state:
-            state["path"] = Path(state["path"])
-
-        self.__dict__.update(state)
-
-    def save_meta(self, directory: Path):
-        # Let's copy the original file to the provided directory
-        if self.save_extra_info:
-            shutil.copy(self.path, directory)
-        self.table.save_meta(directory)
-
-    def load_meta(self, directory: Path):
-        # Since we've moved the file to the provided directory, let's make
-        # sure that we point to this file.
-        if self.save_extra_info:
-            if hasattr(self, "doc_name"):
-                self.path = directory / self.doc_name
-            else:
-                # this else statement handles the deprecated attribute "path" in self, we can remove this soon
-                self.path = directory / self.path.name
-
-        if not hasattr(self, "doc_metadata"):
-            self.doc_metadata = {}
-
-        if not hasattr(self, "strong_column"):
-            self.strong_column = None
-
-        if hasattr(self, "df"):
-            self.table = DataFrameTable(self.df)
-            del self.df
-        elif hasattr(self, "table"):
-            self.table.load_meta(directory)
-
-
-def process_pdf(path: str) -> pd.DataFrame:
-    elements, success = pdf_parse.process_pdf_file(path)
-
-    if not success:
-        raise ValueError(f"Could not read PDF file: {path}")
-
-    elements_df = pdf_parse.create_train_df(elements)
-
-    return elements_df
-
-
-def process_docx(path: str) -> pd.DataFrame:
-    elements, success = doc_parse.get_elements(path)
-
-    if not success:
-        raise ValueError(f"Could not read DOCX file: {path}")
-
-    elements_df = doc_parse.create_train_df(elements)
-
-    return elements_df
-
-
-class PDF(Extracted):
-    """
-    Parses a PDF document into chunks of text that can be indexed by NeuralDB.
-
-    Args:
-        path (str): path to PDF file
-        chunk_size (int): The number of words in each chunk of text. Defaults to 100
-        stride (int): The number of words between each chunk of text. When stride <
-            chunk_size, the text chunks overlap. When stride = chunk_size, the
-            text chunks do not overlap. Defaults to 40 so adjacent chunks have a
-            60% overlap.
-        emphasize_first_words (int): The number of words at the beginning of the
-            document to be passed into NeuralDB as a strong signal. For example,
-            if your document starts with a descriptive title that is 3 words long,
-            then you can set emphasize_first_words to 3 so that NeuralDB captures
-            this strong signal. Defaults to 0.
-        ignore_header_footer (bool): whether the parser should remove headers and
-            footers. Defaults to True; headers and footers are removed by
-            default.
-        ignore_nonstandard_orientation (bool): whether the parser should remove lines
-            of text that have a nonstandard orientation, such as margins that
-            are oriented vertically. Defaults to True; lines with nonstandard
-            orientation are removed by default.
-        metadata (Dict[str, Any]): Optional, defaults to {}. Specifies metadata to
-            associate with entities from this file. Queries to NeuralDB can provide
-            constrains to restrict results based on the metadata.
-    """
-
-    def __init__(
-        self,
-        path: str,
-        version: str = "v1",
-        chunk_size=100,
-        stride=40,
-        emphasize_first_words=0,
-        ignore_header_footer=True,
-        ignore_nonstandard_orientation=True,
-        metadata=None,
-        on_disk=False,
-        doc_keywords="",
-        emphasize_section_titles=False,
-        table_parsing=False,
-        save_extra_info=True,
-    ):
-        self.version = version
-
-        if version == "v1":
-            super().__init__(
-                path=path,
-                metadata=metadata,
-                on_disk=on_disk,
-                save_extra_info=save_extra_info,
-            )
-            return
-
-        if version != "v2":
-            raise ValueError(
-                f"Received invalid version '{version}'. Choose between 'v1' and 'v2'"
-            )
-
-        self.chunk_size = chunk_size
-        self.stride = stride
-        self.emphasize_first_words = emphasize_first_words
-        self.ignore_header_footer = ignore_header_footer
-        self.ignore_nonstandard_orientation = ignore_nonstandard_orientation
-        self.doc_keywords = doc_keywords
-        self.emphasize_section_titles = emphasize_section_titles
-        self.table_parsing = table_parsing
-        # Add pdf version, chunk size, and stride metadata. The metadata will be
-        # incorporated in the document hash so that the same PDF inserted with
-        # different hyperparameters are treated as different documents.
-        # Otherwise, this may break training.
-        super().__init__(
-            path=path,
-            metadata={
-                **(metadata or {}),
-                "__version__": version,
-                "__chunk_size__": chunk_size,
-                "__stride__": stride,
-            },
-            strong_column="emphasis",
-            on_disk=on_disk,
-            save_extra_info=save_extra_info,
-        )
-
-    def process_data(
-        self,
-        path: str,
-    ) -> pd.DataFrame:
-        if not hasattr(self, "version") or self.version == "v1":
-            return process_pdf(path)
-        return sliding_pdf_parse.make_df(
-            path,
-            self.chunk_size,
-            self.stride,
-            self.emphasize_first_words,
-            self.ignore_header_footer,
-            self.ignore_nonstandard_orientation,
-            self.doc_keywords,
-            self.emphasize_section_titles,
-            self.table_parsing,
-        )
-
-    @staticmethod
-    def highlighted_doc(reference: Reference):
-        old_highlights = pdf_parse.highlighted_doc(reference.source, reference.metadata)
-        if old_highlights:
-            return old_highlights
-        return sliding_pdf_parse.highlighted_doc(reference.source, reference.metadata)
-
-
-class DOCX(Extracted):
-    def __init__(self, path: str, metadata=None, on_disk=False):
-        super().__init__(path=path, metadata=metadata, on_disk=on_disk)
-
-    def process_data(
-        self,
-        path: str,
-    ) -> pd.DataFrame:
-        return process_docx(path)
-
-
-class Unstructured(Extracted):
-    def __init__(
-        self,
-        path: Union[str, Path],
-        save_extra_info: bool = True,
-        metadata=None,
-        on_disk=False,
-    ):
-        super().__init__(
-            path=path,
-            save_extra_info=save_extra_info,
-            metadata=metadata,
-            on_disk=on_disk,
-        )
-
-    def process_data(
-        self,
-        path: str,
-    ) -> pd.DataFrame:
-        if path.endswith(".pdf") or path.endswith(".docx"):
-            raise NotImplementedError(
-                "For PDF and DOCX FileTypes, use neuraldb.PDF and neuraldb.DOCX "
-            )
-        elif path.endswith(".pptx"):
-            from .parsing_utils.unstructured_parse import PptxParse
-
-            self.parser = PptxParse(path)
-
-        elif path.endswith(".txt"):
-            from .parsing_utils.unstructured_parse import TxtParse
-
-            self.parser = TxtParse(path)
-
-        elif path.endswith(".eml"):
-            from .parsing_utils.unstructured_parse import EmlParse
-
-            self.parser = EmlParse(path)
-
-        else:
-            raise Exception(f"File type is not yet supported")
-
-        elements, success = self.parser.process_elements()
-
-        if not success:
-            raise ValueError(f"Could not read file: {path}")
-
-        return self.parser.create_train_df(elements)
-
-
-class URL(Document):
-    """
-    A URL document takes the data found at the provided URL (or in the provided reponse)
-    and creates entities that can be inserted into NeuralDB.
-
-    Args:
-        url (str): The URL where the data is located.
-        url_response (Reponse): Optional, defaults to None. If provided then the
-            data in the response is used to create the entities, otherwise a get request
-            is sent to the url.
-        title_is_strong (bool): Optional, defaults to False. If true then the title is
-            used as a strong signal for NeuralDB.
-        metadata (Dict[str, Any]): Optional, defaults to {}. Specifies metadata to
-            associate with entities from this file. Queries to NeuralDB can provide
-            constrains to restrict results based on the metadata.
-    """
-
-    def __init__(
-        self,
-        url: str,
-        url_response: Response = None,
-        save_extra_info: bool = True,
-        title_is_strong: bool = False,
-        metadata=None,
-        on_disk=False,
-    ):
-        self.url = url
-        df = self.process_data(url, url_response)
-        self.table = create_table(df, on_disk)
-        self.hash_val = hash_string(url + str(metadata))
-        self._save_extra_info = save_extra_info
-        self._strong_column = "title" if title_is_strong else "text"
-        self.doc_metadata = metadata_with_source(metadata or {}, url)
-
-    def process_data(self, url, url_response=None) -> pd.DataFrame:
-        # Extract elements from each file
-        elements, success = url_parse.process_url(url, url_response)
-
-        if not success or not elements:
-            raise ValueError(f"Could not retrieve data from URL: {url}")
-
-        elements_df = url_parse.create_train_df(elements)
-
-        return elements_df
-
-    @property
-    def hash(self) -> str:
-        return self.hash_val
-
-    @property
-    def size(self) -> int:
-        return self.table.size
-
-    @property
-    def name(self) -> str:
-        return self.url
-
-    @property
-    def source(self) -> str:
-        return self.url
-
-    @property
-    def matched_constraints(self) -> Dict[str, ConstraintValue]:
-        return {key: ConstraintValue(value) for key, value in self.doc_metadata.items()}
-
-    def all_entity_ids(self) -> List[int]:
-        return list(range(self.size))
-
-    def strong_text(self, element_id: int) -> str:
-        return self.table.field(
-            row_id=element_id,
-            column=self._strong_column if self._strong_column else "text",
-        )
-
-    def weak_text(self, element_id: int) -> str:
-        return self.table.field(element_id, "text")
-
-    def reference(self, element_id: int) -> Reference:
-        if element_id >= self.table.size:
-            _raise_unknown_doc_error(element_id)
-        return Reference(
-            document=self,
-            element_id=element_id,
-            text=self.table.field(element_id, "display"),
-            source=self.source,
-            metadata=(
-                {"title": self.table.field(element_id, "title"), **self.doc_metadata}
-                if "title" in self.table.columns
-                else self.doc_metadata
-            ),
-        )
-
-    def context(self, element_id, radius) -> str:
-        if not 0 <= element_id or not element_id < self.size:
-            raise ("Element id not in document.")
-        rows = self.table.range_rows_as_dicts(
-            from_row_id=max(0, element_id - radius),
-            to_row_id=min(self.table.size, element_id + radius + 1),
-        )
-        return "\n".join(row["text"] for row in rows)
-
-    def save_meta(self, directory: Path):
-        self.table.save_meta(directory)
-
-    def load_meta(self, directory: Path):
-        if not hasattr(self, "doc_metadata"):
-            self.doc_metadata = {}
-        if hasattr(self, "df"):
-            self.table = DataFrameTable(self.df)
-            del self.df
-        elif hasattr(self, "table"):
-            self.table.load_meta(directory)
-
-
-class DocumentConnector(Document):
-    @property
-    def hash(self) -> str:
-        raise NotImplementedError()
-
-    @property
-    def meta_table(self) -> Optional[pd.DataFrame]:
-        """
-        It stores the mapping from id_in_document to meta_data of the document. It could be used to fetch the minimal document result if the connection is lost.
-        """
-        raise NotImplementedError()
-
-    @property
-    def meta_table_id_col(self) -> str:
-        return "id_in_document"
-
-    def _get_connector_object_name(self):
-        raise NotImplementedError()
-
-    def get_strong_columns(self):
-        raise NotImplementedError()
-
-    def get_weak_columns(self):
-        raise NotImplementedError()
-
-    def row_iterator(self):
-        id_in_document = 0
-        for current_chunk in self.chunk_iterator():
-            for idx in range(len(current_chunk)):
-                yield DocumentRow(
-                    element_id=id_in_document,
-                    strong=self.strong_text_from_chunk(
-                        id_in_chunk=idx, chunk=current_chunk
-                    ),  # Strong text from (idx)th row of the current_chunk
-                    weak=self.weak_text_from_chunk(
-                        id_in_chunk=idx, chunk=current_chunk
-                    ),  # Weak text from (idx)th row of the current_chunk
-                )
-                id_in_document += 1
-
-    def chunk_iterator(self) -> pd.DataFrame:
-        raise NotImplementedError()
-
-    def strong_text_from_chunk(self, id_in_chunk: int, chunk: pd.DataFrame) -> str:
-        try:
-            row = chunk.iloc[id_in_chunk]
-            return " ".join([str(row[col]) for col in self.get_strong_columns()])
-        except Exception as e:
-            return ""
-
-    def weak_text_from_chunk(self, id_in_chunk: int, chunk: pd.DataFrame) -> str:
-        try:
-            row = chunk.iloc[id_in_chunk]
-            return " ".join([str(row[col]) for col in self.get_weak_columns()])
-        except Exception as e:
-            return ""
-
-    def reference(self, element_id: int) -> Reference:
-        raise NotImplementedError()
-
-    def context(self, element_id, radius) -> str:
-        if not 0 <= element_id or not element_id < self.size:
-            raise ("Element id not in document.")
-
-        reference_texts = [
-            self.reference(i).text
-            for i in range(
-                max(0, element_id - radius), min(self.size, element_id + radius + 1)
-            )
-        ]
-        return "\n".join(reference_texts)
-
-    def save_meta(self, directory: Path):
-        # Save the index table
-        if self.save_extra_info and self.meta_table is not None:
-            self.meta_table.to_csv(
-                path_or_buf=directory / (self.name + ".csv"), index=False
-            )
-
-    def __getstate__(self):
-        # Document Connectors are expected to remove their connector(s) object
-        state = self.__dict__.copy()
-
-        del state[self._get_connector_object_name()]
-
-        return state
-
-
-class SQLDatabase(DocumentConnector):
-    """
-    class for handling SQL database connections and data retrieval for training the neural_db model
-
-    This class encapsulates functionality for connecting to an SQL database, executing SQL queries, and retrieving
-    data for use in training the model.
-
-    NOTE: It is being expected that the table will remain static in terms of both rows and columns.
-    """
-
-    def __init__(
-        self,
-        engine: sqlConn,
-        table_name: str,
-        id_col: str,
-        strong_columns: Optional[List[str]] = None,
-        weak_columns: Optional[List[str]] = None,
-        reference_columns: Optional[List[str]] = None,
-        chunk_size: int = 10_000,
-        save_extra_info: bool = False,
-        metadata: dict = {},
-    ) -> None:
-        self.table_name = table_name
-        self.id_col = id_col
-        self.strong_columns = strong_columns
-        self.weak_columns = weak_columns
-        self.reference_columns = reference_columns
-        self.chunk_size = chunk_size
-        self._save_extra_info = save_extra_info
-        self.doc_metadata = metadata
-
-        self._connector = SQLConnector(
-            engine=engine,
-            table_name=self.table_name,
-            id_col=self.id_col,
-            chunk_size=self.chunk_size,
-        )
-        self.total_rows = self._connector.total_rows()
-        if not self.total_rows > 0:
-            raise FileNotFoundError("Empty table")
-
-        self.database_name = engine.url.database
-        self.url = str(engine.url)
-        self.engine_uq = self.url + f"/{self.table_name}"
-        self._hash = hash_string(string=self.engine_uq)
-
-        # Integrity checks
-        self.assert_valid_id()
-        self.assert_valid_columns()
-
-        # setting the columns in the conector object
-        self._connector.columns = list(set(self.strong_columns + self.weak_columns))
-
-    @property
-    def name(self):
-        return self.database_name + "-" + self.table_name
-
-    @property
-    def source(self) -> str:
-        return str(self.engine_uq)
-
-    @property
-    def hash(self):
-        return self._hash
-
-    @property
-    def size(self) -> int:
-        # It is verfied by the uniqueness assertion of the id column.
-        return self.total_rows
-
-    def setup_connection(self, engine: sqlConn):
-        """
-        This is a helper function to re-establish the connection upon loading the
-        saved ndb model containing this SQLDatabase document.
-
-        Args:
-            engine: SQLAlchemy Connection object
-                    NOTE: Provide the same connection object.
-
-        NOTE: Same table would be used to establish connection
-        """
-        try:
-            # The idea is to check for the connector object existence
-            print(
-                "Connector object already exists with url:"
-                f" {self._connector.get_engine_url()}"
-            )
-        except AttributeError as e:
-            assert engine.url.database == self.database_name
-            assert str(engine.url) == self.url
-            self._connector = SQLConnector(
-                engine=engine,
-                table_name=self.table_name,
-                id_col=self.id_col,
-                columns=list(set(self.strong_columns + self.weak_columns)),
-                chunk_size=self.chunk_size,
-            )
-
-    def _get_connector_object_name(self):
-        return "_connector"
-
-    def get_strong_columns(self):
-        return self.strong_columns
-
-    def get_weak_columns(self):
-        return self.weak_columns
-
-    def get_engine(self):
-        try:
-            return self._connector._engine
-        except AttributeError as e:
-            raise AttributeError("engine is not available")
-
-    @property
-    def meta_table(self) -> Optional[pd.DataFrame]:
-        return None
-
-    def strong_text_from_chunk(self, id_in_chunk: int, chunk: pd.DataFrame) -> str:
-        try:
-            row = chunk.iloc[id_in_chunk]
-            return " ".join([str(row[col]) for col in self.get_strong_columns()])
-        except Exception as e:
-            return ""
-
-    def weak_text_from_chunk(self, id_in_chunk: int, chunk: pd.DataFrame) -> str:
-        try:
-            row = chunk.iloc[id_in_chunk]
-            return " ".join([str(row[col]) for col in self.get_weak_columns()])
-        except Exception as e:
-            return ""
-
-    def chunk_iterator(self) -> pd.DataFrame:
-        return self._connector.chunk_iterator()
-
-    def all_entity_ids(self) -> List[int]:
-        return list(range(self.size))
-
-    def reference(self, element_id: int) -> Reference:
-        if element_id >= self.size:
-            _raise_unknown_doc_error(element_id)
-
-        try:
-            reference_texts = self._connector.execute(
-                query=(
-                    f"SELECT {','.join(self.reference_columns)} FROM"
-                    f" {self.table_name} WHERE {self.id_col} = {element_id}"
-                )
-            ).fetchone()
-
-            text = "\n\n".join(
-                [
-                    f"{col_name}: {col_text}"
-                    for col_name, col_text in zip(
-                        self.reference_columns, reference_texts
-                    )
-                ]
-            )
-
-        except Exception as e:
-            text = (
-                f"Unable to connect to database, Referenced row with {self.id_col}:"
-                f" {element_id} "
-            )
-
-        return Reference(
-            document=self,
-            element_id=element_id,
-            text=text,
-            source=self.source,
-            metadata={
-                "Database": self.database_name,
-                "Table": self.table_name,
-                **self.doc_metadata,
-            },
-        )
-
-    @property
-    def matched_constraints(self) -> Dict[str, ConstraintValue]:
-        """
-        This method is called when the document is being added to a DocumentManager in order to build an index for constrained search.
-        """
-        return {key: ConstraintValue(value) for key, value in self.doc_metadata.items()}
-
-    def assert_valid_id(self):
-        all_cols = self._connector.cols_metadata()
-
-        id_col_meta = list(filter(lambda col: col["name"] == self.id_col, all_cols))
-        if len(id_col_meta) == 0:
-            raise AttributeError("id column not present in the table")
-        elif not isinstance(id_col_meta[0]["type"], Integer):
-            raise AttributeError("id column needs to be of type Integer")
-
-        primary_keys = self._connector.get_primary_keys()
-        if len(primary_keys) > 1:
-            raise AttributeError("Composite primary key is not allowed")
-        elif len(primary_keys) == 0 or primary_keys[0] != self.id_col:
-            raise AttributeError(f"{self.id_col} needs to be a primary key")
-
-        min_id = self._connector.execute(
-            query=f"SELECT MIN({self.id_col}) FROM {self.table_name}"
-        ).fetchone()[0]
-
-        max_id = self._connector.execute(
-            query=f"SELECT MAX({self.id_col}) FROM {self.table_name}"
-        ).fetchone()[0]
-
-        if min_id != 0 or max_id != self.size - 1:
-            raise AttributeError(
-                f"id column needs to be unique from 0 to {self.size - 1}"
-            )
-
-    def assert_valid_columns(self):
-        all_cols = self._connector.cols_metadata()
-
-        columns_set = set([col["name"] for col in all_cols])
-
-        # Checking for strong, weak and reference columns (if provided) to be present in column list of the table
-        if (self.strong_columns is not None) and (
-            not set(self.strong_columns).issubset(columns_set)
-        ):
-            raise AttributeError(
-                f"Strong column(s) doesn't exists in the table '{self.table_name}'"
-            )
-        if (self.weak_columns is not None) and (
-            not set(self.weak_columns).issubset(columns_set)
-        ):
-            raise AttributeError(
-                f"Weak column(s) doesn't exists in the table '{self.table_name}'"
-            )
-        if (self.reference_columns is not None) and (
-            not set(self.reference_columns).issubset(columns_set)
-        ):
-            raise AttributeError(
-                f"Reference column(s) doesn't exists in the table '{self.table_name}'"
-            )
-
-        # Checking for strong and weak column to have the correct column type
-        for col in all_cols:
-            if (
-                self.strong_columns is not None
-                and col["name"] in self.strong_columns
-                and not isinstance(col["type"], String)
-            ):
-                raise AttributeError(
-                    f"strong column '{col['name']}' needs to be of type String"
-                )
-            elif (
-                self.weak_columns is not None
-                and col["name"] in self.weak_columns
-                and not isinstance(col["type"], String)
-            ):
-                raise AttributeError(
-                    f"weak column '{col['name']}' needs to be of type String"
-                )
-
-        if self.strong_columns is None and self.weak_columns is None:
-            self.strong_columns = []
-            self.weak_columns = []
-            for col in all_cols:
-                if isinstance(col["type"], String):
-                    self.weak_columns.append(col["name"])
-        elif self.strong_columns is None:
-            self.strong_columns = []
-        elif self.weak_columns is None:
-            self.weak_columns = []
-
-        if self.reference_columns is None:
-            self.reference_columns = list(columns_set)
-
-
-class SharePoint(DocumentConnector):
-    """
-    Class for handling sharepoint connection, retrieving documents, processing and training the neural_db model
-
-    Args:
-        ctx (ClientContext): A ClientContext object for SharePoint connection.
-        library_path (str): The server-relative directory path where documents
-            are stored. Default: 'Shared Documents'
-        chunk_size (int): The maximum amount of data (in bytes) that can be fetched
-            at a time. (This limit may not apply if there are no files within this
-            range.) Default: 10MB
-        metadata (Dict[str, Any]): Optional, defaults to {}. Specifies metadata to
-            associate with entities from this file. Queries to NeuralDB can provide
-            constrains to restrict results based on the metadata.
-    """
-
-    def __init__(
-        self,
-        ctx: ClientContext,
-        library_path: str = "Shared Documents",
-        chunk_size: int = 10485760,
-        save_extra_info: bool = False,
-        metadata: dict = {},
-    ) -> None:
-        # Executing a dummy query to check for the authentication for the ctx object
-        try:
-            SharePoint.dummy_query(ctx=ctx)
-        except Exception as e:
-            raise Exception("Invalid ClientContext Object. Error: " + str(e))
-
-        self._connector = SharePointConnector(
-            ctx=ctx, library_path=library_path, chunk_size=chunk_size
-        )
-        self.library_path = library_path
-        self.chunk_size = chunk_size
-        self._save_extra_info = save_extra_info
-        self.doc_metadata = metadata
-
-        self.strong_column = "strong_text"
-        self.weak_column = "weak_text"
-        self.build_meta_table()
-        self._name = (
-            self._connector.site_name + "-" + (self.library_path).replace(" ", "_")
-        )
-        self.url = self._connector.url
-        self._source = self.url + "/" + library_path
-        self._hash = hash_string(self._source)
-
-    @property
-    def size(self) -> int:
-        return len(self.meta_table)
-
-    @property
-    def name(self) -> str:
-        return self._name
-
-    @property
-    def source(self) -> str:
-        return self._source
-
-    @property
-    def hash(self) -> str:
-        return self._hash
-
-    def setup_connection(self, ctx: ClientContext):
-        """
-        This is a helper function to re-establish the connection upon loading the saved ndb model containing this Sharepoint document.
-
-        Args:
-            engine: SQLAlchemy Connection object. NOTE: Provide the same connection object.
-        NOTE: Same library path would be used
-        """
-        try:
-            # The idea is to check for the connector object existence
-            print(f"Connector object already exists with url: {self._connector.url}")
-        except AttributeError as e:
-            assert self.url == ctx.web.get().execute_query().url
-            self._connector = SharePointConnector(
-                ctx=ctx, library_path=self.library_path, chunk_size=self.chunk_size
-            )
-
-    def get_strong_columns(self):
-        return [self.strong_column]
-
-    def get_weak_columns(self):
-        return [self.weak_column]
-
-    def build_meta_table(self):
-        num_files = self._connector.num_files()
-
-        print(f"Found {num_files} supported files")
-        self._meta_table = pd.DataFrame(
-            columns=[
-                "internal_doc_id",
-                "server_relative_url",
-                "page",
-            ]
-        )
-        self._meta_table = pd.concat(
-            [
-                current_chunk.drop(
-                    columns=self.get_strong_columns() + self.get_weak_columns()
-                )
-                for current_chunk in self.chunk_iterator()
-            ],
-            ignore_index=True,
-        )
-
-        self._meta_table[self.meta_table_id_col] = range(len(self._meta_table))
-        self._meta_table.set_index(keys=self.meta_table_id_col, inplace=True)
-
-    @property
-    def matched_constraints(self) -> Dict[str, ConstraintValue]:
-        """
-        Each constraint will get applied to each supported document on the sharepoint. This method is called when the document is being added to a DocumentManager in order to build an index for constrained search.
-        """
-        return {key: ConstraintValue(value) for key, value in self.doc_metadata.items()}
-
-    def all_entity_ids(self) -> List[int]:
-        return list(range(self.size))
-
-    def reference(self, element_id: int) -> Reference:
-        if element_id >= self.size:
-            _raise_unknown_doc_error(element_id)
-
-        filename = self.meta_table.iloc[element_id]["server_relative_url"].split(
-            sep="/"
-        )[-1]
-        return Reference(
-            document=self,
-            element_id=element_id,
-            text=f"filename: {filename}"
-            + (
-                f", page no: {self.meta_table.iloc[element_id]['page']}"
-                if self.meta_table.iloc[element_id]["page"] is not None
-                else ""
-            ),
-            source=self.source + "/" + filename,
-            metadata={
-                **self.meta_table.loc[element_id].to_dict(),
-                **self.doc_metadata,
-            },
-        )
-
-    @property
-    def meta_table(self) -> Optional[pd.DataFrame]:
-        return self._meta_table
-
-    def chunk_iterator(self) -> pd.DataFrame:
-        chunk_df = pd.DataFrame(
-            columns=[
-                self.strong_column,
-                self.weak_column,
-                "internal_doc_id",
-                "server_relative_url",
-                "page",
-            ]
-        )
-
-        for file_dict in self._connector.chunk_iterator():
-            chunk_df.drop(chunk_df.index, inplace=True)
-            temp_dfs = []
-            for server_relative_url, filepath in file_dict.items():
-                if filepath.endswith(".pdf"):
-                    doc = PDF(path=filepath, metadata=self.doc_metadata)
-                elif filepath.endswith(".docx"):
-                    doc = DOCX(path=filepath, metadata=self.doc_metadata)
-                else:
-                    doc = Unstructured(
-                        path=filepath,
-                        save_extra_info=self._save_extra_info,
-                        metadata=self.doc_metadata,
-                    )
-
-                temp_df = pd.DataFrame(
-                    columns=chunk_df.columns.tolist(), index=range(doc.size)
-                )
-                strong_text, weak_text, internal_doc_id, page = zip(
-                    *[
-                        (
-                            doc.strong_text(i),
-                            doc.weak_text(i),
-                            i,
-                            doc.reference(i).metadata.get("page", None),
-                        )
-                        for i in range(doc.size)
-                    ]
-                )
-                temp_df[self.strong_column] = strong_text
-                temp_df[self.weak_column] = weak_text
-                temp_df["internal_doc_id"] = internal_doc_id
-                temp_df["server_relative_url"] = [server_relative_url] * doc.size
-                temp_df["page"] = page
-
-                temp_dfs.append(temp_df)
-
-            chunk_df = pd.concat(temp_dfs, ignore_index=True)
-            yield chunk_df
-
-    def _get_connector_object_name(self):
-        return "_connector"
-
-    @staticmethod
-    def dummy_query(ctx: ClientContext):
-        # Authenticatiion fails if this dummy query execution fails
-        ctx.web.get().execute_query()
-
-    @staticmethod
-    def setup_clientContext(
-        base_url: str, credentials: Dict[str, str]
-    ) -> ClientContext:
-        """
-        Method to create a ClientContext object given base_url and credentials in the form (username, password) OR (client_id, client_secret)
-        """
-        ctx = None
-        try:
-            if all([cred in credentials.keys() for cred in ("username", "password")]):
-                user_credentials = UserCredential(
-                    user_name=credentials["username"], password=credentials["password"]
-                )
-                ctx = ClientContext(base_url=base_url).with_credentials(
-                    user_credentials
-                )
-            SharePoint.dummy_query(ctx=ctx)
-        except Exception as userCredError:
-            try:
-                if all(
-                    [
-                        cred in credentials.keys()
-                        for cred in ("client_id", "client_secret")
-                    ]
-                ):
-                    client_credentials = ClientCredential(
-                        client_id=credentials["client_id"],
-                        client_secret=credentials["client_secret"],
-                    )
-                    ctx = ClientContext(base_url=base_url).with_credentials(
-                        client_credentials
-                    )
-                    SharePoint.dummy_query(ctx=ctx)
-            except Exception as clientCredError:
-                pass
-
-        if ctx:
-            return ctx
-        raise AttributeError("Incorrect or insufficient credentials")
-
-
-class SalesForce(DocumentConnector):
-    """
-    Class for handling the Salesforce object connections and data retrieval for
-    training the neural_db model
-
-    This class encapsulates functionality for connecting to an object, executing
-    Salesforce Object Query Language (SOQL) queries, and retrieving
-
-    NOTE: Allow the Bulk API access for the provided object. Also, it is being
-    expected that the table will remain static in terms of both rows and columns.
-    """
-
-    def __init__(
-        self,
-        instance: Salesforce,
-        object_name: str,
-        id_col: str,
-        strong_columns: Optional[List[str]] = None,
-        weak_columns: Optional[List[str]] = None,
-        reference_columns: Optional[List[str]] = None,
-        save_extra_info: bool = True,
-        metadata: dict = {},
-    ) -> None:
-        self.object_name = object_name
-        self.id_col = id_col
-        self.strong_columns = strong_columns
-        self.weak_columns = weak_columns
-        self.reference_columns = reference_columns
-        self._save_extra_info = save_extra_info
-        self.doc_metadata = metadata
-        self._connector = SalesforceConnector(
-            instance=instance, object_name=object_name
-        )
-
-        self.total_rows = self._connector.total_rows()
-        if not self.total_rows > 0:
-            raise FileNotFoundError("Empty Object")
-        self._hash = hash_string(self._connector.sf_instance + self._connector.base_url)
-        self._source = self._connector.sf_instance + self.object_name
-
-        # Integrity_checks
-        self.assert_valid_id()
-        self.assert_valid_fields()
-
-        # setting the columns in the connector object
-        self._connector._fields = [self.id_col] + list(
-            set(self.strong_columns + self.weak_columns)
-        )
-
-    @property
-    def name(self) -> str:
-        return self.object_name
-
-    @property
-    def source(self) -> str:
-        return self._source
-
-    @property
-    def hash(self) -> str:
-        return self._hash
-
-    @property
-    def size(self) -> int:
-        return self.total_rows
-
-    def setup_connection(self, instance: Salesforce):
-        """
-        This is a helper function to re-establish the connection upon loading a saved ndb model containing this SalesForce document.
-
-        Args:
-            instance: Salesforce instance. NOTE: Provide the same connection object.
-
-        NOTE: Same object name would be used to establish connection
-        """
-        try:
-            # The idea is to check for the connector object existence
-            print(
-                f"Connector object already exists with url: {self._connector.base_url}"
-            )
-        except AttributeError as e:
-            assert self.hash == hash_string(instance.sf_instance + instance.base_url)
-            self._connector = SalesforceConnector(
-                instance=instance,
-                object_name=self.object_name,
-                fields=[self.id_col]
-                + list(set(self.strong_columns + self.weak_columns)),
-            )
-
-    def _get_connector_object_name(self):
-        return "_connector"
-
-    def row_iterator(self):
-        for current_chunk in self.chunk_iterator():
-            for idx in range(len(current_chunk)):
-                """
-                * Since we are not able to retrieve the rows in sorted order, we have to do this so that (id, strong_text, weak_text) gets mapped correctly.
-                * We cannot sort because the id_col needs to be of type 'autoNumber' which is a string. Neither we can do 'SELECT row FROM object_name ORDER BY LEN(id_col), id_col' because there is no LEN function in SOQL (by default). Owner of the object have to create a formula LEN() to use such query.
-                """
-                yield DocumentRow(
-                    element_id=int(current_chunk.iloc[idx][self.id_col]),
-                    strong=self.strong_text_from_chunk(
-                        id_in_chunk=idx, chunk=current_chunk
-                    ),  # Strong text from (idx)th row of the current_chunk
-                    weak=self.weak_text_from_chunk(
-                        id_in_chunk=idx, chunk=current_chunk
-                    ),  # Weak text from (idx)th row of the current_chunk
-                )
-
-    def get_strong_columns(self):
-        return self.strong_columns
-
-    def get_weak_columns(self):
-        return self.weak_columns
-
-    @property
-    def meta_table(self) -> Optional[pd.DataFrame]:
-        return None
-
-    def strong_text_from_chunk(self, id_in_chunk: int, chunk: pd.DataFrame) -> str:
-        try:
-            row = chunk.iloc[id_in_chunk]
-            return " ".join([str(row[col]) for col in self.get_strong_columns()])
-        except Exception as e:
-            return ""
-
-    def weak_text_from_chunk(self, id_in_chunk: int, chunk: pd.DataFrame) -> str:
-        try:
-            row = chunk.iloc[id_in_chunk]
-            return " ".join([str(row[col]) for col in self.get_weak_columns()])
-        except Exception as e:
-            return ""
-
-    def chunk_iterator(self) -> pd.DataFrame:
-        return self._connector.chunk_iterator()
-
-    def all_entity_ids(self) -> List[int]:
-        return list(range(self.size))
-
-    def reference(self, element_id: int) -> Reference:
-        if element_id >= self.size:
-            _raise_unknown_doc_error(element_id)
-
-        try:
-            result = self._connector.execute(
-                query=(
-                    f"SELECT {','.join(self.reference_columns)} FROM"
-                    f" {self.object_name} WHERE {self.id_col} = '{element_id}'"
-                )
-            )["records"][0]
-            del result["attributes"]
-            text = "\n\n".join(
-                [f"{col_name}: {col_text}" for col_name, col_text in result.items()]
-            )
-
-        except Exception as e:
-            text = (
-                "Unable to connect to the object instance, Referenced row with"
-                f" {self.id_col}: {element_id} "
-            )
-
-        return Reference(
-            document=self,
-            element_id=element_id,
-            text=text,
-            source=self.source,
-            metadata={
-                "object_name": self.object_name,
-                **self.doc_metadata,
-            },
-        )
-
-    @property
-    def matched_constraints(self) -> Dict[str, ConstraintValue]:
-        """
-        This method is called when the document is being added to a DocumentManager in order to build an index for constrained search.
-        """
-        return {key: ConstraintValue(value) for key, value in self.doc_metadata.items()}
-
-    def assert_valid_id(self):
-        all_fields = self._connector.field_metadata()
-
-        all_field_name = [field["name"] for field in all_fields]
-
-        if self.id_col not in all_field_name:
-            raise AttributeError("Id Columns is not present in the object")
-
-        # Uniqueness or primary constraint
-        id_field_meta = list(
-            filter(lambda field: field["name"] == self.id_col, all_fields)
-        )
-        if len(id_field_meta) == 0:
-            raise AttributeError("id col not present in the object")
-        id_field_meta = id_field_meta[0]
-
-        """
-        Reason behinds using AutoNumber as the id column type:
-
-            1. Salesforce doesn't have typical table constraints. Object in a salesforce (or table in conventional sense) uses an alpha-numeric string as a primary key.
-            2. Salesforce doesn't have a pure integer field. It have one in which we can set the decimal field of the double data-type to 0 but it is only for display purpose.
-            3. Only option left is one Auto-number field that can be used but it limits some options.
-        """
-        if not id_field_meta["autoNumber"]:
-            raise AttributeError("id col must be of type Auto-Number")
-        else:
-            # id field is auto-incremented string. Have to check for the form of A-{0}
-
-            result = self._connector.execute(
-                query=f"SELECT {self.id_col} FROM {self.object_name} LIMIT 1"
-            )
-            value: str = result["records"][0][self.id_col]
-            if not value.isdigit():
-                raise AttributeError("id column needs to be of the form \{0\}")
-
-        expected_min_row_id = 0
-        min_id = self._connector.execute(
-            query=(
-                f"SELECT {self.id_col} FROM {self.object_name} WHERE {self.id_col} ="
-                f" '{expected_min_row_id}'"
-            )
-        )
-
-        # This one is not required probably because user can't put the auto-number field mannually.
-        # User just can provide the start of the auto-number so if the min_id is 0, then max_id should be size - 1
-        expected_max_row_id = self.size - 1
-        max_id = self._connector.execute(
-            query=(
-                f"SELECT {self.id_col} FROM {self.object_name} WHERE {self.id_col} ="
-                f" '{expected_max_row_id}'"
-            )
-        )
-
-        if not (min_id["totalSize"] == 1 and max_id["totalSize"] == 1):
-            raise AttributeError(
-                f"id column needs to be unique from 0 to {self.size - 1}"
-            )
-
-    def assert_valid_fields(
-        self, supported_text_types: Tuple[str] = ("string", "textarea")
-    ):
-        all_fields = self._connector.field_metadata()
-        self.assert_field_inclusion(all_fields)
-        self.assert_field_type(all_fields, supported_text_types)
-        self.default_fields(all_fields, supported_text_types)
-
-    def assert_field_inclusion(self, all_fields: List[OrderedDict]):
-        fields_set = set([field["name"] for field in all_fields])
-
-        # Checking for strong, weak and reference columns (if provided) to be present in column list of the table
-        column_name_error = (
-            "Remember if it is a custom column, salesforce requires it to be appended"
-            " with __c."
-        )
-        if (self.strong_columns is not None) and (
-            not set(self.strong_columns).issubset(fields_set)
-        ):
-            raise AttributeError(
-                f"Strong column(s) doesn't exists in the object. {column_name_error}"
-            )
-        if (self.weak_columns is not None) and (
-            not set(self.weak_columns).issubset(fields_set)
-        ):
-            raise AttributeError(
-                f"Weak column(s) doesn't exists in the object. {column_name_error}"
-            )
-        if (self.reference_columns is not None) and (
-            not set(self.reference_columns).issubset(fields_set)
-        ):
-            raise AttributeError(
-                f"Reference column(s) doesn't exists in the object. {column_name_error}"
-            )
-
-    def assert_field_type(
-        self, all_fields: List[OrderedDict], supported_text_types: Tuple[str]
-    ):
-        # Checking for strong and weak column to have the correct column type
-        for field in all_fields:
-            if (
-                self.strong_columns is not None
-                and field["name"] in self.strong_columns
-                and field["type"] not in supported_text_types
-            ):
-                raise AttributeError(
-                    f"Strong column '{field['name']}' needs to be type from"
-                    f" {supported_text_types}"
-                )
-            if (
-                self.weak_columns is not None
-                and field["name"] in self.weak_columns
-                and field["type"] not in supported_text_types
-            ):
-                raise AttributeError(
-                    f"Weak column '{field['name']}' needs to be type"
-                    f" {supported_text_types}"
-                )
-
-    def default_fields(
-        self, all_fields: List[OrderedDict], supported_text_types: Tuple[str]
-    ):
-        if self.strong_columns is None and self.weak_columns is None:
-            self.strong_columns = []
-            self.weak_columns = []
-            for field in all_fields:
-                if field["type"] in supported_text_types:
-                    self.weak_columns.append(field["name"])
-        elif self.strong_columns is None:
-            self.strong_columns = []
-        elif self.weak_columns is None:
-            self.weak_columns = []
-
-        if self.reference_columns is None:
-            self.reference_columns = [self.id_col]
-            for field in all_fields:
-                if field["type"] in supported_text_types:
-                    self.reference_columns.append(field["name"])
-
-
-class SentenceLevelExtracted(Extracted):
-    """Parses a document into sentences and creates a NeuralDB entry for each
-    sentence. The strong column of the entry is the sentence itself while the
-    weak column is the paragraph from which the sentence came. A NeuralDB
-    reference produced by this object displays the paragraph instead of the
-    sentence to increase recall.
-    """
-
-    def __init__(
-        self, path: str, save_extra_info: bool = True, metadata=None, on_disk=False
-    ):
-        self.path = Path(path)
-        self.hash_val = hash_file(
-            path, metadata="sentence-level-extracted-" + str(metadata)
-        )
-        df = self.parse_sentences(self.process_data(path))
-        self.table = create_table(df, on_disk)
-        para_df = pd.DataFrame({"para": df["para"].unique()})
-        self.para_table = create_table(para_df, on_disk)
-        self._save_extra_info = save_extra_info
-        self.doc_metadata = metadata_with_source(metadata or {}, Path(path).name)
-
-    def not_just_punctuation(sentence: str):
-        for character in sentence:
-            if character not in string.punctuation and not character.isspace():
-                return True
-        return False
-
-    def get_sentences(paragraph: str):
-        return [
-            sentence
-            for sentence in sent_tokenize(paragraph)
-            if SentenceLevelExtracted.not_just_punctuation(sentence)
-        ]
-
-    def parse_sentences(
-        self,
-        df: pd.DataFrame,
-    ) -> pd.DataFrame:
-        df["sentences"] = df["para"].apply(SentenceLevelExtracted.get_sentences)
-
-        num_sents_cum_sum = np.cumsum(df["sentences"].apply(lambda sents: len(sents)))
-        df["id_offsets"] = np.zeros(len(df))
-        df["id_offsets"][1:] = num_sents_cum_sum[:-1]
-        df["id_offsets"] = df["id_offsets"].astype(int)
-
-        def get_ids(record):
-            id_offset = record["id_offsets"]
-            n_sents = len(record["sentences"])
-            return list(range(id_offset, id_offset + n_sents))
-
-        df = pd.DataFrame.from_records(
-            [
-                {
-                    "sentence": sentence,
-                    "para_id": para_id,
-                    "sentence_id": i + record["id_offsets"],
-                    "sentence_ids_in_para": str(get_ids(record)),
-                    **record,
-                }
-                for para_id, record in enumerate(df.to_dict(orient="records"))
-                for i, sentence in enumerate(record["sentences"])
-            ]
-        )
-
-        df.drop("sentences", axis=1, inplace=True)
-        df.drop("id_offsets", axis=1, inplace=True)
-        return df
-
-    def process_data(
-        self,
-        path: str,
-    ) -> pd.DataFrame:
-        raise NotImplementedError()
-
-    @property
-    def hash(self) -> str:
-        return self.hash_val
-
-    @property
-    def size(self) -> int:
-        return self.table.size
-
-    def get_strong_columns(self):
-        return ["sentence"]
-
-    @property
-    def name(self) -> str:
-        return self.path.name if self.path else None
-
-    @property
-    def source(self) -> str:
-        return str(self.path.absolute())
-
-    def strong_text(self, element_id: int) -> str:
-        return self.table.field(element_id, "sentence")
-
-    def weak_text(self, element_id: int) -> str:
-        return self.table.field(element_id, "para")
-
-    def show_fn(text, source, **kwargs):
-        return text
-
-    def reference(self, element_id: int) -> Reference:
-        if element_id >= self.table.size:
-            _raise_unknown_doc_error(element_id)
-        return Reference(
-            document=self,
-            element_id=element_id,
-            text=self.table.field(element_id, "display"),
-            source=self.source,
-            metadata={**self.table.row_as_dict(element_id), **self.doc_metadata},
-            upvote_ids=eval(self.table.field(element_id, "sentence_ids_in_para")),
-        )
-
-    def context(self, element_id, radius) -> str:
-        if not 0 <= element_id or not element_id < self.size:
-            raise ("Element id not in document.")
-
-        # Cast to int because the actual return type is numpy.int64, which
-        # causes problems in the self.para_table.range_rows_as_dicts line.
-        para_id = int(self.table.field(element_id, "para_id"))
-
-        rows = self.para_table.range_rows_as_dicts(
-            from_row_id=max(0, para_id - radius),
-            to_row_id=min(self.para_table.size, para_id + radius + 1),
-        )
-        return "\n\n".join(row["para"] for row in rows)
-
-    def save_meta(self, directory: Path):
-        # Let's copy the original file to the provided directory
-        if self.save_extra_info:
-            shutil.copy(self.path, directory)
-        self.table.save_meta(directory)
-
-    def load_meta(self, directory: Path):
-        # Since we've moved the file to the provided directory, let's make
-        # sure that we point to this file.
-        if hasattr(self, "doc_name"):
-            self.path = directory / self.doc_name
-        else:
-            # deprecated, self.path should not be in self
-            self.path = directory / self.path.name
-
-        if not hasattr(self, "doc_metadata"):
-            self.doc_metadata = {}
-
-        if hasattr(self, "df"):
-            self.df["sentence_ids_in_para"] = self.df["sentence_ids_in_para"].apply(str)
-            self.table = DataFrameTable(self.df)
-            self.para_table = DataFrameTable(pd.DataFrame({"para": self.para_df}))
-            del self.df
-            del self.para_df
-        elif hasattr(self, "table"):
-            self.table.load_meta(directory)
-            self.para_table.load_meta(directory)
-
-
-class SentenceLevelPDF(SentenceLevelExtracted):
-    """
-    Parses a document into sentences and creates a NeuralDB entry for each
-    sentence. The strong column of the entry is the sentence itself while the
-    weak column is the paragraph from which the sentence came. A NeuralDB
-    reference produced by this object displays the paragraph instead of the
-    sentence to increase recall.
-
-    Args:
-        path (str): The path to the pdf file.
-        metadata (Dict[str, Any]): Optional, defaults to {}. Specifies metadata to
-            associate with entities from this file. Queries to NeuralDB can provide
-            constrains to restrict results based on the metadata.
-    """
-
-    def __init__(self, path: str, metadata=None, on_disk=False):
-        super().__init__(path=path, metadata=metadata, on_disk=on_disk)
-
-    def process_data(
-        self,
-        path: str,
-    ) -> pd.DataFrame:
-        return process_pdf(path)
-
-
-class SentenceLevelDOCX(SentenceLevelExtracted):
-    """
-    Parses a document into sentences and creates a NeuralDB entry for each
-    sentence. The strong column of the entry is the sentence itself while the
-    weak column is the paragraph from which the sentence came. A NeuralDB
-    reference produced by this object displays the paragraph instead of the
-    sentence to increase recall.
-
-    Args:
-        path (str): The path to the docx file.
-        metadata (Dict[str, Any]): Optional, defaults to {}. Specifies metadata to
-            associate with entities from this file. Queries to NeuralDB can provide
-            constrains to restrict results based on the metadata.
-    """
-
-    def __init__(self, path: str, metadata=None, on_disk=False):
-        super().__init__(path=path, metadata=metadata, on_disk=on_disk)
-
-    def process_data(
-        self,
-        path: str,
-    ) -> pd.DataFrame:
-        return process_docx(path)
-
-
-class InMemoryText(Document):
-    """
-    A wrapper around a batch of texts and their metadata to fit it in the
-    NeuralDB Document framework.
-
-    Args:
-        name (str): A name for the batch of texts.
-        texts (List[str]): A batch of texts.
-        metadatas (List[Dict[str, Any]]): Optional. Metadata for each text.
-        global_metadata (Dict[str, Any]): Optional. Metadata for the whole batch
-        of texts.
-    """
-
-    def __init__(
-        self,
-        name: str,
-        texts: List[str],
-        metadatas: Optional[List[dict]] = None,
-        global_metadata=None,
-        on_disk=False,
-    ):
-        self._name = name
-        df = pd.DataFrame({"texts": texts})
-        self.metadata_columns = []
-        if metadatas:
-            metadata_df = pd.DataFrame.from_records(metadatas)
-            df = pd.concat([df, metadata_df], axis=1)
-            self.metadata_columns = metadata_df.columns
-        self.table = create_table(df, on_disk)
-        self.hash_val = hash_string(str(texts) + str(metadatas))
-        self.global_metadata = global_metadata or {}
-
-    @property
-    def hash(self) -> str:
-        return self.hash_val
-
-    @property
-    def size(self) -> int:
-        return self.table.size
-
-    @property
-    def name(self) -> str:
-        return self._name
-
-    @property
-    def source(self) -> str:
-        return self._name
-
-    @property
-    def matched_constraints(self) -> Dict[str, ConstraintValue]:
-        metadata_constraints = {
-            key: ConstraintValue(value) for key, value in self.global_metadata.items()
-        }
-        indexed_column_constraints = {
-            key: ConstraintValue(is_any=True) for key in self.metadata_columns
-        }
-        return {**metadata_constraints, **indexed_column_constraints}
-
-    def all_entity_ids(self) -> List[int]:
-        return list(range(self.size))
-
-    def filter_entity_ids(self, filters: Dict[str, Filter]):
-        table_filter = TableFilter(
-            {k: v for k, v in filters.items() if k not in self.global_metadata.keys()}
-        )
-        return self.table.apply_filter(table_filter)
-
-    def strong_text(self, element_id: int) -> str:
-        return ""
-
-    def weak_text(self, element_id: int) -> str:
-        return self.table.field(element_id, "texts")
-
-    def reference(self, element_id: int) -> Reference:
-        if element_id >= self.table.size:
-            _raise_unknown_doc_error(element_id)
-        return Reference(
-            document=self,
-            element_id=element_id,
-            text=self.table.field(element_id, "texts"),
-            source=self.source,
-            metadata={**self.table.row_as_dict(element_id), **self.global_metadata},
-        )
-
-    def context(self, element_id, radius) -> str:
-        # We don't return neighboring texts because they are not necessarily
-        # related.
-        return self.table.field(element_id, "texts")
-
-    def save_meta(self, directory: Path):
-        self.table.save_meta(directory)
-
-    def load_meta(self, directory: Path):
-        self.table.load_meta(directory)
+import hashlib
+import json
+import os
+import pickle
+import shutil
+import string
+from collections import OrderedDict
+from pathlib import Path
+from typing import Any, Dict, List, Optional, Tuple, Union
+
+import dask.dataframe as dd
+import numpy as np
+import pandas as pd
+from nltk.tokenize import sent_tokenize
+from office365.sharepoint.client_context import (
+    ClientContext,
+    ClientCredential,
+    UserCredential,
+)
+from pytrie import StringTrie
+from requests.models import Response
+from simple_salesforce import Salesforce
+from sqlalchemy import Integer, String, create_engine
+from sqlalchemy.engine.base import Connection as sqlConn
+from thirdai import bolt
+from thirdai.data import get_udt_col_types
+from thirdai.dataset.data_source import PyDataSource
+
+from .connectors import SalesforceConnector, SharePointConnector, SQLConnector
+from .constraint_matcher import (
+    ConstraintMatcher,
+    ConstraintValue,
+    Filter,
+    TableFilter,
+    to_filters,
+)
+from .parsing_utils import doc_parse, pdf_parse, sliding_pdf_parse, url_parse
+from .table import DaskDataFrameTable, DataFrameTable, SQLiteTable
+from .utils import hash_file, hash_string, requires_condition
+
+
+class Reference:
+    pass
+
+
+def _raise_unknown_doc_error(element_id: int):
+    raise ValueError(f"Unable to find document that has id {element_id}.")
+
+
+class Document:
+    @property
+    def size(self) -> int:
+        raise NotImplementedError()
+
+    @property
+    def name(self) -> str:
+        raise NotImplementedError()
+
+    @property
+    def source(self) -> str:
+        raise NotImplementedError()
+
+    @property
+    def hash(self) -> str:
+        sha1 = hashlib.sha1()
+        sha1.update(bytes(self.name, "utf-8"))
+        for i in range(self.size):
+            sha1.update(bytes(self.reference(i).text, "utf-8"))
+        return sha1.hexdigest()
+
+    @property
+    def matched_constraints(self) -> Dict[str, ConstraintValue]:
+        raise NotImplementedError()
+
+    def all_entity_ids(self) -> List[int]:
+        raise NotImplementedError()
+
+    def filter_entity_ids(self, filters: Dict[str, Filter]):
+        return self.all_entity_ids()
+
+    def id_map(self) -> Optional[Dict[str, int]]:
+        return None
+
+    # This attribute allows certain things to be saved or not saved during
+    # the pickling of a savable_state object. For example, if we set this
+    # to True for CSV docs, we will save the actual csv file in the pickle.
+    # Utilize this property in save_meta and load_meta of document objs.
+    @property
+    def save_extra_info(self) -> bool:
+        return self._save_extra_info
+
+    @save_extra_info.setter
+    def save_extra_info(self, value: bool):
+        self._save_extra_info = value
+
+    def reference(self, element_id: int) -> Reference:
+        raise NotImplementedError()
+
+    def strong_text(self, element_id: int) -> str:
+        return self.reference(element_id).text
+
+    def weak_text(self, element_id: int) -> str:
+        return self.reference(element_id).text
+
+    def context(self, element_id: int, radius: int) -> str:
+        window_start = max(0, element_id - radius)
+        window_end = min(self.size, element_id + radius + 1)
+        return " \n".join(
+            [self.reference(elid).text for elid in range(window_start, window_end)]
+        )
+
+    def save_meta(self, directory: Path):
+        pass
+
+    def load_meta(self, directory: Path):
+        pass
+
+    def row_iterator(self):
+        for i in range(self.size):
+            yield DocumentRow(
+                element_id=i,
+                strong=self.strong_text(i),
+                weak=self.weak_text(i),
+            )
+
+    def save(self, directory: str):
+        dirpath = Path(directory)
+        if os.path.exists(dirpath):
+            shutil.rmtree(dirpath)
+        os.mkdir(dirpath)
+        with open(dirpath / f"doc.pkl", "wb") as pkl:
+            pickle.dump(self, pkl)
+        os.mkdir(dirpath / "meta")
+        self.save_meta(dirpath / "meta")
+
+    @staticmethod
+    def load(directory: str):
+        dirpath = Path(directory)
+        with open(dirpath / f"doc.pkl", "rb") as pkl:
+            obj = pickle.load(pkl)
+        obj.load_meta(dirpath / "meta")
+        return obj
+
+
+class Reference:
+    def __init__(
+        self,
+        document: Document,
+        element_id: int,
+        text: str,
+        source: str,
+        metadata: dict,
+        upvote_ids: List[int] = None,
+        retriever: str = None,
+    ):
+        self._id = element_id
+        self._id_in_document = element_id
+        self._upvote_ids = upvote_ids if upvote_ids is not None else [element_id]
+        self._text = text
+        self._source = source
+        self._metadata = metadata
+        self._context_fn = lambda radius: document.context(element_id, radius)
+        self._score = 0
+        self._document = document
+        self._retriever = retriever
+
+    @property
+    def id(self):
+        return self._id
+
+    @property
+    def id_in_document(self):
+        return self._id_in_document
+
+    @property
+    def upvote_ids(self):
+        return self._upvote_ids
+
+    @property
+    def text(self):
+        return self._text
+
+    @property
+    def source(self):
+        return self._source
+
+    @property
+    def metadata(self):
+        return self._metadata
+
+    @property
+    def score(self):
+        return self._score
+
+    @property
+    def document(self):
+        return self._document
+
+    @property
+    def retriever(self):
+        return self._retriever
+
+    def context(self, radius: int):
+        return self._context_fn(radius)
+
+    def __eq__(self, other):
+        if isinstance(other, Reference):
+            return (
+                self.id == other.id
+                and self.text == other.text
+                and self.source == other.source
+            )
+        return False
+
+
+class DocumentRow:
+    def __init__(self, element_id: int, strong: str, weak: str):
+        self.id = element_id
+        self.strong = strong
+        self.weak = weak
+
+
+DocAndOffset = Tuple[Document, int]
+
+
+class DocumentDataSource(PyDataSource):
+    def __init__(self, id_column, strong_column, weak_column):
+        PyDataSource.__init__(self)
+        self.documents: List[DocAndOffset] = []
+        for col in [id_column, strong_column, weak_column]:
+            if '"' in col or "," in col:
+                raise RuntimeError(
+                    "DocumentDataSource columns cannot contain '\"' or ','"
+                )
+        self.id_column = id_column
+        self.strong_column = strong_column
+        self.weak_column = weak_column
+        self._size = 0
+        self.restart()
+
+    def add(self, document: Document, start_id: int):
+        self.documents.append((document, start_id))
+        self._size += document.size
+
+    def row_iterator(self):
+        for doc, start_id in self.documents:
+            for row in doc.row_iterator():
+                row.id = row.id + start_id
+                yield row
+
+    def indices(self):
+        indices = []
+        for doc, start_id in self.documents:
+            for row in doc.row_iterator():
+                indices.append(row.id + start_id)
+
+        return indices
+
+    @property
+    def size(self):
+        return self._size
+
+    def _csv_line(self, element_id: str, strong: str, weak: str):
+        csv_strong = '"' + strong.replace('"', '""') + '"'
+        csv_weak = '"' + weak.replace('"', '""') + '"'
+        return f"{element_id},{csv_strong},{csv_weak}"
+
+    def _get_line_iterator(self):
+        # First yield the header
+        yield f"{self.id_column},{self.strong_column},{self.weak_column}"
+        # Then yield rows
+        for row in self.row_iterator():
+            yield self._csv_line(element_id=row.id, strong=row.strong, weak=row.weak)
+
+    def resource_name(self) -> str:
+        return "Documents:\n" + "\n".join([doc.name for doc, _ in self.documents])
+
+    def save(self, path: Path, save_interval=100_000):
+        """
+        DocumentDataSource is agnostic to the documents that are a part of it as the line_iterator is agnostic to the kind of document and returns data in a specific format. Hence, to serialize DocumentDataSource, we do not need to serialize the documents but rather, dump the lines yielded by the line iterator into a CSV. This makes the saving and loading logic simpler.
+        """
+        path.mkdir(exist_ok=True, parents=True)
+        number_lines_in_buffer = 0
+        with open(path / "source.csv", "w", encoding="utf-8") as f:
+            for line in self._get_line_iterator():
+                f.write(line + "\n")
+                number_lines_in_buffer += 1
+            if number_lines_in_buffer > save_interval:
+                f.flush()
+                number_lines_in_buffer = 0
+
+        with open(path / "arguments.json", "w") as f:
+            json.dump(
+                {
+                    "id_column": self.id_column,
+                    "strong_column": self.strong_column,
+                    "weak_column": self.weak_column,
+                },
+                f,
+                indent=4,
+            )
+        self.restart()
+
+    @staticmethod
+    def load(path: Path):
+        with open(path / "arguments.json", "r") as f:
+            args = json.load(f)
+
+        csv_document = CSV(
+            path=path / "source.csv",
+            id_column=args["id_column"],
+            strong_columns=[args["strong_column"]],
+            weak_columns=[args["weak_column"]],
+            has_offset=True,
+        )
+        data_source = DocumentDataSource(**args)
+        data_source.add(csv_document, start_id=0)
+        return data_source
+
+
+class IntroAndTrainDocuments:
+    def __init__(self, intro: DocumentDataSource, train: DocumentDataSource) -> None:
+        self.intro = intro
+        self.train = train
+
+
+class DocumentManager:
+    def __init__(self, id_column, strong_column, weak_column) -> None:
+        self.id_column = id_column
+        self.strong_column = strong_column
+        self.weak_column = weak_column
+
+        # After python 3.8, we don't need to use OrderedDict as Dict is ordered by default
+        self.registry: OrderedDict[str, DocAndOffset] = OrderedDict()
+        self.source_id_prefix_trie = StringTrie()
+        self.constraint_matcher = ConstraintMatcher[DocAndOffset]()
+
+    def _next_id(self):
+        if len(self.registry) == 0:
+            return 0
+        doc, start_id = next(reversed(self.registry.values()))
+        return start_id + doc.size
+
+    def add(self, documents: List[Document]):
+        intro = DocumentDataSource(self.id_column, self.strong_column, self.weak_column)
+        train = DocumentDataSource(self.id_column, self.strong_column, self.weak_column)
+        for doc in documents:
+            doc_hash = doc.hash
+            if doc_hash not in self.registry:
+                start_id = self._next_id()
+                doc_and_id = (doc, start_id)
+                self.registry[doc_hash] = doc_and_id
+                self.source_id_prefix_trie[doc_hash] = doc_hash
+                intro.add(doc, start_id)
+                self.constraint_matcher.index(
+                    item=(doc, start_id), constraints=doc.matched_constraints
+                )
+            doc, start_id = self.registry[doc_hash]
+            train.add(doc, start_id)
+
+        return IntroAndTrainDocuments(intro=intro, train=train), [
+            doc.hash for doc in documents
+        ]
+
+    def delete(self, source_ids):
+        # TODO(Geordie): Error handling
+        all_sources_exist = all(source_id in self.registry for source_id in source_ids)
+        if not all_sources_exist:
+            raise KeyError("At least one source not found in document manager.")
+
+        deleted_entities = []
+        for source_id in source_ids:
+            doc, offset = self.registry[source_id]
+            deleted_entities += [
+                offset + entity_id for entity_id in doc.all_entity_ids()
+            ]
+            del self.registry[source_id]
+            del self.source_id_prefix_trie[source_id]
+            self.constraint_matcher.delete((doc, offset), doc.matched_constraints)
+
+        return deleted_entities
+
+    def entity_ids_by_constraints(self, constraints: Dict[str, Any]):
+        filters = to_filters(constraints)
+        return [
+            start_id + entity_id
+            for doc, start_id in self.constraint_matcher.match(filters)
+            for entity_id in doc.filter_entity_ids(filters)
+        ]
+
+    def sources(self):
+        return {doc_hash: doc for doc_hash, (doc, _) in self.registry.items()}
+
+    def match_source_id_by_prefix(self, prefix: str) -> Document:
+        if prefix in self.registry:
+            return [prefix]
+        return self.source_id_prefix_trie.values(prefix)
+
+    def source_by_id(self, source_id: str):
+        return self.registry[source_id]
+
+    def clear(self):
+        self.registry = OrderedDict()
+        self.source_id_prefix_trie = StringTrie()
+        self.constraint_matcher = ConstraintMatcher[DocAndOffset]()
+
+    def _get_doc_and_start_id(self, element_id: int):
+        for doc, start_id in reversed(self.registry.values()):
+            if start_id <= element_id:
+                return doc, start_id
+
+        _raise_unknown_doc_error(element_id)
+
+    def reference(self, element_id: int):
+        doc, start_id = self._get_doc_and_start_id(element_id)
+        doc_ref = doc.reference(element_id - start_id)
+        doc_ref._id = element_id
+        doc_ref._upvote_ids = [start_id + uid for uid in doc_ref._upvote_ids]
+        return doc_ref
+
+    def context(self, element_id: int, radius: int):
+        doc, start_id = self._get_doc_and_start_id(element_id)
+        return doc.context(element_id - start_id, radius)
+
+    def get_data_source(self) -> DocumentDataSource:
+        data_source = DocumentDataSource(
+            id_column=self.id_column,
+            strong_column=self.strong_column,
+            weak_column=self.weak_column,
+        )
+
+        for doc, start_id in self.registry.values():
+            data_source.add(document=doc, start_id=start_id)
+
+        return data_source
+
+    def save_meta(self, directory: Path):
+        for i, (doc, _) in enumerate(self.registry.values()):
+            subdir = directory / str(i)
+            os.mkdir(subdir)
+            doc.save_meta(subdir)
+
+    def load_meta(self, directory: Path):
+        for i, (doc, _) in enumerate(self.registry.values()):
+            subdir = directory / str(i)
+            doc.load_meta(subdir)
+
+        if not hasattr(self, "doc_constraints"):
+            self.constraint_matcher = ConstraintMatcher[DocAndOffset]()
+            for item in self.registry.values():
+                self.constraint_matcher.index(item, item[0].matched_constraints)
+
+
+def safe_has_offset(this):
+    """Checks the value of the "has_offset" attribute of a class.
+    Defaults to False when the attribute does not exist.
+    This function is needed for backwards compatibility reasons.
+    """
+    if hasattr(this, "has_offset"):
+        return this.has_offset
+    return False
+
+
+def create_table(df, on_disk):
+    Table = (
+        SQLiteTable
+        if on_disk
+        else DaskDataFrameTable if isinstance(df, dd.DataFrame) else DataFrameTable
+    )
+    return Table(df)
+
+
+def metadata_with_source(metadata, source: str):
+    if "source" in metadata:
+        raise ValueError(
+            "Document metadata cannot contain the key 'source'. 'source' is a reserved key."
+        )
+    return {**metadata, "source": source}
+
+
+class CSV(Document):
+    """
+    A document containing the rows of a csv file.
+
+    Args:
+        path (str): The path to the csv file.
+        id_column (Optional[str]). Optional, defaults to None. If provided then the
+            ids in this column are used to identify the rows in NeuralDB. If not provided
+            then ids are assigned.
+        strong_columns (Optional[List[str]]): Optional, defaults to None. This argument
+            can be used to provide NeuralDB with information about which columns are
+            likely to contain the strongest signal in matching with a given query. For
+            example this could be something like the name of a product.
+        weak_columns (Optional[List[str]]): Optional, defaults to None. This argument
+            can be used to provide NeuralDB with information about which columns are
+            likely to contain weaker signals in matching with a given query. For
+            example this could be something like the description of a product.
+        reference_columns (Optional[List[str]]): Optional, defaults to None. If provided
+            the specified columns are returned by NeuralDB as responses to queries. If
+            not specifed all columns are returned.
+        metadata (Dict[str, Any]): Optional, defaults to {}. Specifies metadata to
+            associate with entities from this file. Queries to NeuralDB can provide
+            constrains to restrict results based on the metadata.
+    """
+
+    def valid_id_column(column):
+        if isinstance(column, dd.Series):
+            unique_count = column.nunique().compute()
+            min_val = column.min().compute()
+            max_val = column.max().compute()
+            length = column.size.compute()
+            condition = (
+                (unique_count == length) and (min_val == 0) and (max_val == length - 1)
+            )
+        else:
+            condition = (
+                (len(column.unique()) == len(column))
+                and (column.min() == 0)
+                and (column.max() == len(column) - 1)
+            )
+
+        return condition
+
+    def remove_spaces(column_name):
+        return column_name.replace(" ", "_")
+
+    def remove_spaces_from_list(column_name_list):
+        return [CSV.remove_spaces(col) for col in column_name_list]
+
+    # blocksize (when using dask) Determines the size of each partition/chunk in bytes.
+    # For example, setting blocksize=25e6 will aim for partitions of approximately 25MB.
+    # If you decrease the block size, Dask will create more partitions, and
+    # increasing it will result in fewer partitions.
+    # Default value is computed based on available physical memory
+    # and the number of cores, up to a maximum of 64MB.
+    def __init__(
+        self,
+        path: str,
+        id_column: Optional[str] = None,
+        strong_columns: Optional[List[str]] = None,
+        weak_columns: Optional[List[str]] = None,
+        reference_columns: Optional[List[str]] = None,
+        save_extra_info=True,
+        metadata=None,
+        has_offset=False,
+        on_disk=False,
+        use_dask=False,
+        blocksize=None,
+    ) -> None:
+        if use_dask:
+            df = (
+                dd.read_csv(path, blocksize=blocksize)
+                if blocksize
+                else dd.read_csv(path)
+            )
+        else:
+            df = pd.read_csv(path)
+
+        # Convert spaces in column names to underscores because df.itertuples
+        # does not work when there are spaces
+        # https://stackoverflow.com/questions/45307376/pandas-df-itertuples-renaming-dataframe-columns-when-printing
+        # While it's possible that saved models contain column names that have
+        # spaces. We don't convert these columns during deserialization because
+        # df.itertuples is only called during CSV construction and during
+        # insertion, which would have completed before serialization.
+        # Additionally, this document's hash takes column names into account.
+        # Consider this scenario:
+        # 1. User inserts CSV with spaced column names into an older version of NDB
+        # 2. User saves the NDB model
+        # 3. User upgrades the ThirdAI package
+        # 4. User loads the saved NDB model
+        # 5. User inserts the same CSV into the loaded model
+        # Here, NeuralDB will actually treat the CSV as a new, unseen document,
+        # so it will not invoke df.itertuples on a dataframe that has spaced
+        # column names.
+        cols_with_spaces = [col for col in df.columns if " " in col]
+        self.with_space_to_no_space = {}
+        if cols_with_spaces:
+            for col in cols_with_spaces:
+                self.with_space_to_no_space[col] = col.replace(" ", "_")
+                while self.with_space_to_no_space[col] in df.columns:
+                    self.with_space_to_no_space[col] += "_"
+
+            def remove_spaces_from_list(cols):
+                return [self.with_space_to_no_space.get(col, col) for col in cols]
+
+            df.columns = remove_spaces_from_list(df.columns)
+            if id_column:
+                id_column = self.with_space_to_no_space.get(id_column, id_column)
+            if strong_columns:
+                strong_columns = remove_spaces_from_list(strong_columns)
+            if weak_columns:
+                weak_columns = remove_spaces_from_list(weak_columns)
+            if reference_columns:
+                reference_columns = remove_spaces_from_list(reference_columns)
+
+        self.no_space_to_with_space = {
+            val: key for key, val in self.with_space_to_no_space.items()
+        }
+
+        # This variable is used to check whether the id's in the CSV are supposed to start with 0 or with some custom offset. We need the latter when we shard the datasource.
+        self.has_offset = has_offset
+
+        if reference_columns is None:
+            reference_columns = list(df.columns)
+
+        self.orig_to_assigned_id = None
+        self.id_column = id_column
+        orig_id_column = id_column
+        if self.id_column and (has_offset or CSV.valid_id_column(df[self.id_column])):
+            df = df.sort_values(self.id_column)
+        else:
+            self.id_column = "thirdai_index"
+            if use_dask:
+                # sets dask df index column to range(len(df))
+                df[self.id_column] = (
+                    df.assign(partition_count=1).partition_count.cumsum() - 1
+                )
+            else:
+                df[self.id_column] = range(df.shape[0])
+
+            if orig_id_column:
+                self.orig_to_assigned_id = {
+                    str(getattr(row, orig_id_column)): getattr(row, self.id_column)
+                    for row in df.itertuples(index=True)
+                }
+
+        if strong_columns is None and weak_columns is None:
+            # autotune column types
+            text_col_names = []
+            try:
+                for col_name, udt_col_type in get_udt_col_types(path).items():
+                    if type(udt_col_type) == type(bolt.types.text()):
+                        text_col_names.append(CSV.remove_spaces(col_name))
+            except:
+                text_col_names = list(df.columns)
+                text_col_names.remove(id_column)
+                if orig_id_column:
+                    text_col_names.remove(orig_id_column)
+                df[text_col_names] = df[text_col_names].astype(str)
+            strong_columns = []
+            weak_columns = text_col_names
+        elif strong_columns is None:
+            strong_columns = []
+        elif weak_columns is None:
+            weak_columns = []
+
+        for col in strong_columns + weak_columns:
+            df[col] = df[col].fillna("")
+
+        if use_dask:
+            # The 'sorted=True' parameter is used to indicate that the column is already sorted.
+            # This optimization helps Dask to avoid expensive data shuffling operations, improving performance.
+            df = df.set_index(self.id_column, sorted=True)
+        else:
+            # Pandas automatically manages the index without needing to explicitly sort it here.
+            df = df.set_index(self.id_column)
+
+        self.table = create_table(df, on_disk)
+
+        self.path = Path(path)
+        self.strong_columns = strong_columns
+        self.weak_columns = weak_columns
+        self.reference_columns = [
+            col for col in reference_columns if col != self.id_column
+        ]
+        self._save_extra_info = save_extra_info
+        self.doc_metadata = metadata_with_source(metadata or {}, Path(path).name)
+        self.doc_metadata_keys = set(self.doc_metadata.keys())
+        # Add column names to hash metadata so that CSVs with different
+        # hyperparameters are treated as different documents. Otherwise, this
+        # may break training.
+        self._hash = hash_file(
+            path,
+            metadata="csv-"
+            + str(self.id_column)
+            + str(sorted(self.strong_columns))
+            + str(sorted(self.weak_columns))
+            + str(sorted(self.reference_columns))
+            + str(sorted(list(self.doc_metadata.items())))
+            + str(sorted(self.table.columns)),
+        )
+
+    @property
+    def hash(self) -> str:
+        return self._hash
+
+    @property
+    def size(self) -> int:
+        return self.table.size
+
+    @property
+    def name(self) -> str:
+        return self.path.name
+
+    @property
+    def source(self) -> str:
+        return str(self.path.absolute())
+
+    @requires_condition(
+        check_func=lambda self: not safe_has_offset(self),
+        method_name="matched_constraints",
+        method_class="CSV(Document)",
+        condition_unmet_string=" when there is an offset in the CSV document",
+    )
+    @property
+    def matched_constraints(self) -> Dict[str, ConstraintValue]:
+        metadata_constraints = {
+            key: ConstraintValue(value) for key, value in self.doc_metadata.items()
+        }
+        indexed_column_constraints = {
+            self.no_space_to_with_space.get(key, key): ConstraintValue(is_any=True)
+            for key in self.table.columns
+        }
+        return {**metadata_constraints, **indexed_column_constraints}
+
+    def all_entity_ids(self) -> List[int]:
+        return self.table.ids
+
+    def filter_entity_ids(self, filters: Dict[str, Filter]):
+        table_filter = TableFilter(
+            {
+                self.with_space_to_no_space.get(k, k): v
+                for k, v in filters.items()
+                if k not in self.doc_metadata_keys
+            }
+        )
+        return self.table.apply_filter(table_filter)
+
+    def id_map(self) -> Optional[Dict[str, int]]:
+        return self.orig_to_assigned_id
+
+    def strong_text_from_row(self, row) -> str:
+        return " ".join(str(row[col]) for col in self.strong_columns)
+
+    def strong_text(self, element_id: int) -> str:
+        row = self.table.row_as_dict(element_id)
+        return self.strong_text_from_row(row)
+
+    def weak_text_from_row(self, row) -> str:
+        return " ".join(str(row[col]) for col in self.weak_columns)
+
+    def weak_text(self, element_id: int) -> str:
+        row = self.table.row_as_dict(element_id)
+        return self.weak_text_from_row(row)
+
+    def row_iterator(self):
+        for row_id, row in self.table.iter_rows_as_dicts():
+            yield DocumentRow(
+                element_id=row_id,
+                strong=self.strong_text_from_row(row),
+                weak=self.weak_text_from_row(row),
+            )
+
+    @requires_condition(
+        check_func=lambda self: not safe_has_offset(self),
+        method_name="reference",
+        method_class="CSV(Document)",
+        condition_unmet_string=" when there is an offset in the CSV document",
+    )
+    def reference(self, element_id: int) -> Reference:
+        if element_id >= self.table.size:
+            _raise_unknown_doc_error(element_id)
+        row = self.table.row_as_dict(element_id)
+        text = "\n\n".join(
+            [
+                f"{self.no_space_to_with_space.get(col, col)}: {row[col]}"
+                for col in self.reference_columns
+            ]
+        )
+        row = {
+            self.no_space_to_with_space.get(col, col): val for col, val in row.items()
+        }
+        return Reference(
+            document=self,
+            element_id=element_id,
+            text=text,
+            source=self.source,
+            metadata={**row, **self.doc_metadata},
+        )
+
+    def context(self, element_id: int, radius) -> str:
+        rows = self.table.range_rows_as_dicts(
+            from_row_id=max(0, element_id - radius),
+            to_row_id=min(self.table.size, element_id + radius + 1),
+        )
+
+        return " ".join(
+            [
+                "\n\n".join(
+                    [
+                        f"{self.no_space_to_with_space.get(col, col)}: {row[col]}"
+                        for col in self.reference_columns
+                    ]
+                )
+                for row in rows
+            ]
+        )
+
+    def __getstate__(self):
+        state = self.__dict__.copy()
+
+        # Save the filename so we can load it with the same name
+        state["doc_name"] = self.name
+
+        state["path"] = str(self.path)
+
+        # End pickling functionality here to support old directory checkpoint save
+        return state
+
+    def __setstate__(self, state):
+        # Add new attributes to state for older document object version backward compatibility
+        if "_save_extra_info" not in state:
+            state["_save_extra_info"] = True
+
+        if "path" in state:
+            state["path"] = Path(state["path"])
+
+        self.__dict__.update(state)
+
+    @requires_condition(
+        check_func=lambda self: not safe_has_offset(self),
+        method_name="save_meta",
+        method_class="CSV(Document)",
+        condition_unmet_string=" when there is an offset in the CSV document",
+    )
+    def save_meta(self, directory: Path):
+        # Let's copy the original CSV file to the provided directory
+        if self.save_extra_info:
+            shutil.copy(self.path, directory)
+        self.table.save_meta(directory)
+
+    @requires_condition(
+        check_func=lambda self: not safe_has_offset(self),
+        method_name="load_meta",
+        method_class="CSV(Document)",
+        condition_unmet_string=" when there is an offset in the CSV document",
+    )
+    def load_meta(self, directory: Path):
+        # Since we've moved the CSV file to the provided directory, let's make
+        # sure that we point to this CSV file.
+        if hasattr(self, "doc_name"):
+            self.path = directory / self.doc_name
+        else:
+            # this else statement handles the deprecated attribute "path" in self, we can remove this soon
+            self.path = directory / self.path.name
+
+        if not hasattr(self, "doc_metadata"):
+            self.doc_metadata = {}
+        if not hasattr(self, "doc_metadata_keys"):
+            self.doc_metadata_keys = set()
+        if not hasattr(self, "orig_to_assigned_id"):
+            self.orig_to_assigned_id = None
+        if not hasattr(self, "has_offset"):
+            self.has_offset = False
+
+        if hasattr(self, "df"):
+            if self.df.index.name != self.id_column:
+                self.reference_columns = [
+                    col for col in self.reference_columns if col != self.id_column
+                ]
+                self.df = self.df.set_index(self.id_column)
+            self.table = DataFrameTable(self.df)
+            del self.df
+        else:
+            self.table.load_meta(directory)
+
+        if hasattr(self, "with_space_to_no_space"):
+            self.no_space_to_with_space = {
+                val: key for key, val in self.with_space_to_no_space.items()
+            }
+        else:
+            self.with_space_to_no_space = {}
+            self.no_space_to_with_space = {}
+
+
+# Base class for PDF, DOCX and Unstructured classes because they share the same logic.
+class Extracted(Document):
+    def __init__(
+        self,
+        path: str,
+        save_extra_info=True,
+        metadata=None,
+        strong_column=None,
+        on_disk=False,
+    ):
+        path = str(path)
+        df = self.process_data(path)
+        self.table = create_table(df, on_disk)
+        self.hash_val = hash_file(path, metadata="extracted-" + str(metadata))
+        self._save_extra_info = save_extra_info
+
+        self.path = Path(path)
+        self.doc_metadata = metadata_with_source(metadata or {}, Path(path).name)
+        self.strong_column = strong_column
+        if self.strong_column and self.strong_column not in self.table.columns:
+            raise RuntimeError(
+                f"Strong column '{self.strong_column}' not found in the dataframe."
+            )
+
+    def process_data(
+        self,
+        path: str,
+    ) -> pd.DataFrame:
+        raise NotImplementedError()
+
+    @property
+    def hash(self) -> str:
+        return self.hash_val
+
+    @property
+    def size(self) -> int:
+        return self.table.size
+
+    @property
+    def name(self) -> str:
+        return self.path.name
+
+    @property
+    def source(self) -> str:
+        return str(self.path.absolute())
+
+    @property
+    def matched_constraints(self) -> Dict[str, ConstraintValue]:
+        return {key: ConstraintValue(value) for key, value in self.doc_metadata.items()}
+
+    def all_entity_ids(self) -> List[int]:
+        return list(range(self.size))
+
+    def strong_text(self, element_id: int) -> str:
+        return (
+            ""
+            if not self.strong_column
+            else self.table.field(element_id, self.strong_column)
+        )
+
+    def weak_text(self, element_id: int) -> str:
+        return self.table.field(element_id, "para")
+
+    def show_fn(text, source, **kwargs):
+        return text
+
+    def reference(self, element_id: int) -> Reference:
+        if element_id >= self.table.size:
+            _raise_unknown_doc_error(element_id)
+        return Reference(
+            document=self,
+            element_id=element_id,
+            text=self.table.field(element_id, "display"),
+            source=self.source,
+            metadata={**self.table.row_as_dict(element_id), **self.doc_metadata},
+        )
+
+    def context(self, element_id, radius) -> str:
+        if not 0 <= element_id or not element_id < self.size:
+            raise ("Element id not in document.")
+
+        rows = self.table.range_rows_as_dicts(
+            from_row_id=max(0, element_id - radius),
+            to_row_id=min(self.table.size, element_id + radius + 1),
+        )
+        return "\n".join(row["para"] for row in rows)
+
+    def __getstate__(self):
+        state = self.__dict__.copy()
+
+        # Remove filename attribute because this is a deprecated attribute for Extracted
+        if "filename" in state:
+            del state["filename"]
+
+        # In older versions of neural_db, we accidentally stored Path objects in the df.
+        # This changes those objects to a string, because PosixPath can't be loaded in Windows
+        def path_to_str(element):
+            if isinstance(element, Path):
+                return element.name
+            return element
+
+        if "df" in state:
+            state["df"] = state["df"].applymap(path_to_str)
+
+        # Save the filename so we can load it with the same name
+        state["doc_name"] = self.name
+
+        state["path"] = str(self.path)
+
+        return state
+
+    def __setstate__(self, state):
+        # Add new attributes to state for older document object version backward compatibility
+        if "_save_extra_info" not in state:
+            state["_save_extra_info"] = True
+        if "filename" in state:
+            state["path"] = state["filename"]
+
+        if "path" in state:
+            state["path"] = Path(state["path"])
+
+        self.__dict__.update(state)
+
+    def save_meta(self, directory: Path):
+        # Let's copy the original file to the provided directory
+        if self.save_extra_info:
+            shutil.copy(self.path, directory)
+        self.table.save_meta(directory)
+
+    def load_meta(self, directory: Path):
+        # Since we've moved the file to the provided directory, let's make
+        # sure that we point to this file.
+        if self.save_extra_info:
+            if hasattr(self, "doc_name"):
+                self.path = directory / self.doc_name
+            else:
+                # this else statement handles the deprecated attribute "path" in self, we can remove this soon
+                self.path = directory / self.path.name
+
+        if not hasattr(self, "doc_metadata"):
+            self.doc_metadata = {}
+
+        if not hasattr(self, "strong_column"):
+            self.strong_column = None
+
+        if hasattr(self, "df"):
+            self.table = DataFrameTable(self.df)
+            del self.df
+        elif hasattr(self, "table"):
+            self.table.load_meta(directory)
+
+
+def process_pdf(path: str) -> pd.DataFrame:
+    elements, success = pdf_parse.process_pdf_file(path)
+
+    if not success:
+        raise ValueError(f"Could not read PDF file: {path}")
+
+    elements_df = pdf_parse.create_train_df(elements)
+
+    return elements_df
+
+
+def process_docx(path: str) -> pd.DataFrame:
+    elements, success = doc_parse.get_elements(path)
+
+    if not success:
+        raise ValueError(f"Could not read DOCX file: {path}")
+
+    elements_df = doc_parse.create_train_df(elements)
+
+    return elements_df
+
+
+class PDF(Extracted):
+    """
+    Parses a PDF document into chunks of text that can be indexed by NeuralDB.
+
+    Args:
+        path (str): path to PDF file
+        chunk_size (int): The number of words in each chunk of text. Defaults to 100
+        stride (int): The number of words between each chunk of text. When stride <
+            chunk_size, the text chunks overlap. When stride = chunk_size, the
+            text chunks do not overlap. Defaults to 40 so adjacent chunks have a
+            60% overlap.
+        emphasize_first_words (int): The number of words at the beginning of the
+            document to be passed into NeuralDB as a strong signal. For example,
+            if your document starts with a descriptive title that is 3 words long,
+            then you can set emphasize_first_words to 3 so that NeuralDB captures
+            this strong signal. Defaults to 0.
+        ignore_header_footer (bool): whether the parser should remove headers and
+            footers. Defaults to True; headers and footers are removed by
+            default.
+        ignore_nonstandard_orientation (bool): whether the parser should remove lines
+            of text that have a nonstandard orientation, such as margins that
+            are oriented vertically. Defaults to True; lines with nonstandard
+            orientation are removed by default.
+        metadata (Dict[str, Any]): Optional, defaults to {}. Specifies metadata to
+            associate with entities from this file. Queries to NeuralDB can provide
+            constrains to restrict results based on the metadata.
+    """
+
+    def __init__(
+        self,
+        path: str,
+        version: str = "v1",
+        chunk_size=100,
+        stride=40,
+        emphasize_first_words=0,
+        ignore_header_footer=True,
+        ignore_nonstandard_orientation=True,
+        metadata=None,
+        on_disk=False,
+        doc_keywords="",
+        emphasize_section_titles=False,
+        table_parsing=False,
+        save_extra_info=True,
+    ):
+        self.version = version
+
+        if version == "v1":
+            super().__init__(
+                path=path,
+                metadata=metadata,
+                on_disk=on_disk,
+                save_extra_info=save_extra_info,
+            )
+            return
+
+        if version != "v2":
+            raise ValueError(
+                f"Received invalid version '{version}'. Choose between 'v1' and 'v2'"
+            )
+
+        self.chunk_size = chunk_size
+        self.stride = stride
+        self.emphasize_first_words = emphasize_first_words
+        self.ignore_header_footer = ignore_header_footer
+        self.ignore_nonstandard_orientation = ignore_nonstandard_orientation
+        self.doc_keywords = doc_keywords
+        self.emphasize_section_titles = emphasize_section_titles
+        self.table_parsing = table_parsing
+        # Add pdf version, chunk size, and stride metadata. The metadata will be
+        # incorporated in the document hash so that the same PDF inserted with
+        # different hyperparameters are treated as different documents.
+        # Otherwise, this may break training.
+        super().__init__(
+            path=path,
+            metadata={
+                **(metadata or {}),
+                "__version__": version,
+                "__chunk_size__": chunk_size,
+                "__stride__": stride,
+            },
+            strong_column="emphasis",
+            on_disk=on_disk,
+            save_extra_info=save_extra_info,
+        )
+
+    def process_data(
+        self,
+        path: str,
+    ) -> pd.DataFrame:
+        if not hasattr(self, "version") or self.version == "v1":
+            return process_pdf(path)
+        return sliding_pdf_parse.make_df(
+            path,
+            self.chunk_size,
+            self.stride,
+            self.emphasize_first_words,
+            self.ignore_header_footer,
+            self.ignore_nonstandard_orientation,
+            self.doc_keywords,
+            self.emphasize_section_titles,
+            self.table_parsing,
+        )
+
+    @staticmethod
+    def highlighted_doc(reference: Reference):
+        old_highlights = pdf_parse.highlighted_doc(reference.source, reference.metadata)
+        if old_highlights:
+            return old_highlights
+        return sliding_pdf_parse.highlighted_doc(reference.source, reference.metadata)
+
+
+class DOCX(Extracted):
+    def __init__(self, path: str, metadata=None, on_disk=False):
+        super().__init__(path=path, metadata=metadata, on_disk=on_disk)
+
+    def process_data(
+        self,
+        path: str,
+    ) -> pd.DataFrame:
+        return process_docx(path)
+
+
+class Unstructured(Extracted):
+    def __init__(
+        self,
+        path: Union[str, Path],
+        save_extra_info: bool = True,
+        metadata=None,
+        on_disk=False,
+    ):
+        super().__init__(
+            path=path,
+            save_extra_info=save_extra_info,
+            metadata=metadata,
+            on_disk=on_disk,
+        )
+
+    def process_data(
+        self,
+        path: str,
+    ) -> pd.DataFrame:
+        if path.endswith(".pdf") or path.endswith(".docx"):
+            raise NotImplementedError(
+                "For PDF and DOCX FileTypes, use neuraldb.PDF and neuraldb.DOCX "
+            )
+        elif path.endswith(".pptx"):
+            from .parsing_utils.unstructured_parse import PptxParse
+
+            self.parser = PptxParse(path)
+
+        elif path.endswith(".txt"):
+            from .parsing_utils.unstructured_parse import TxtParse
+
+            self.parser = TxtParse(path)
+
+        elif path.endswith(".eml"):
+            from .parsing_utils.unstructured_parse import EmlParse
+
+            self.parser = EmlParse(path)
+
+        else:
+            raise Exception(f"File type is not yet supported")
+
+        elements, success = self.parser.process_elements()
+
+        if not success:
+            raise ValueError(f"Could not read file: {path}")
+
+        return self.parser.create_train_df(elements)
+
+
+class URL(Document):
+    """
+    A URL document takes the data found at the provided URL (or in the provided reponse)
+    and creates entities that can be inserted into NeuralDB.
+
+    Args:
+        url (str): The URL where the data is located.
+        url_response (Reponse): Optional, defaults to None. If provided then the
+            data in the response is used to create the entities, otherwise a get request
+            is sent to the url.
+        title_is_strong (bool): Optional, defaults to False. If true then the title is
+            used as a strong signal for NeuralDB.
+        metadata (Dict[str, Any]): Optional, defaults to {}. Specifies metadata to
+            associate with entities from this file. Queries to NeuralDB can provide
+            constrains to restrict results based on the metadata.
+    """
+
+    def __init__(
+        self,
+        url: str,
+        url_response: Response = None,
+        save_extra_info: bool = True,
+        title_is_strong: bool = False,
+        metadata=None,
+        on_disk=False,
+    ):
+        self.url = url
+        df = self.process_data(url, url_response)
+        self.table = create_table(df, on_disk)
+        self.hash_val = hash_string(url + str(metadata))
+        self._save_extra_info = save_extra_info
+        self._strong_column = "title" if title_is_strong else "text"
+        self.doc_metadata = metadata_with_source(metadata or {}, url)
+
+    def process_data(self, url, url_response=None) -> pd.DataFrame:
+        # Extract elements from each file
+        elements, success = url_parse.process_url(url, url_response)
+
+        if not success or not elements:
+            raise ValueError(f"Could not retrieve data from URL: {url}")
+
+        elements_df = url_parse.create_train_df(elements)
+
+        return elements_df
+
+    @property
+    def hash(self) -> str:
+        return self.hash_val
+
+    @property
+    def size(self) -> int:
+        return self.table.size
+
+    @property
+    def name(self) -> str:
+        return self.url
+
+    @property
+    def source(self) -> str:
+        return self.url
+
+    @property
+    def matched_constraints(self) -> Dict[str, ConstraintValue]:
+        return {key: ConstraintValue(value) for key, value in self.doc_metadata.items()}
+
+    def all_entity_ids(self) -> List[int]:
+        return list(range(self.size))
+
+    def strong_text(self, element_id: int) -> str:
+        return self.table.field(
+            row_id=element_id,
+            column=self._strong_column if self._strong_column else "text",
+        )
+
+    def weak_text(self, element_id: int) -> str:
+        return self.table.field(element_id, "text")
+
+    def reference(self, element_id: int) -> Reference:
+        if element_id >= self.table.size:
+            _raise_unknown_doc_error(element_id)
+        return Reference(
+            document=self,
+            element_id=element_id,
+            text=self.table.field(element_id, "display"),
+            source=self.source,
+            metadata=(
+                {"title": self.table.field(element_id, "title"), **self.doc_metadata}
+                if "title" in self.table.columns
+                else self.doc_metadata
+            ),
+        )
+
+    def context(self, element_id, radius) -> str:
+        if not 0 <= element_id or not element_id < self.size:
+            raise ("Element id not in document.")
+        rows = self.table.range_rows_as_dicts(
+            from_row_id=max(0, element_id - radius),
+            to_row_id=min(self.table.size, element_id + radius + 1),
+        )
+        return "\n".join(row["text"] for row in rows)
+
+    def save_meta(self, directory: Path):
+        self.table.save_meta(directory)
+
+    def load_meta(self, directory: Path):
+        if not hasattr(self, "doc_metadata"):
+            self.doc_metadata = {}
+        if hasattr(self, "df"):
+            self.table = DataFrameTable(self.df)
+            del self.df
+        elif hasattr(self, "table"):
+            self.table.load_meta(directory)
+
+
+class DocumentConnector(Document):
+    @property
+    def hash(self) -> str:
+        raise NotImplementedError()
+
+    @property
+    def meta_table(self) -> Optional[pd.DataFrame]:
+        """
+        It stores the mapping from id_in_document to meta_data of the document. It could be used to fetch the minimal document result if the connection is lost.
+        """
+        raise NotImplementedError()
+
+    @property
+    def meta_table_id_col(self) -> str:
+        return "id_in_document"
+
+    def _get_connector_object_name(self):
+        raise NotImplementedError()
+
+    def get_strong_columns(self):
+        raise NotImplementedError()
+
+    def get_weak_columns(self):
+        raise NotImplementedError()
+
+    def row_iterator(self):
+        id_in_document = 0
+        for current_chunk in self.chunk_iterator():
+            for idx in range(len(current_chunk)):
+                yield DocumentRow(
+                    element_id=id_in_document,
+                    strong=self.strong_text_from_chunk(
+                        id_in_chunk=idx, chunk=current_chunk
+                    ),  # Strong text from (idx)th row of the current_chunk
+                    weak=self.weak_text_from_chunk(
+                        id_in_chunk=idx, chunk=current_chunk
+                    ),  # Weak text from (idx)th row of the current_chunk
+                )
+                id_in_document += 1
+
+    def chunk_iterator(self) -> pd.DataFrame:
+        raise NotImplementedError()
+
+    def strong_text_from_chunk(self, id_in_chunk: int, chunk: pd.DataFrame) -> str:
+        try:
+            row = chunk.iloc[id_in_chunk]
+            return " ".join([str(row[col]) for col in self.get_strong_columns()])
+        except Exception as e:
+            return ""
+
+    def weak_text_from_chunk(self, id_in_chunk: int, chunk: pd.DataFrame) -> str:
+        try:
+            row = chunk.iloc[id_in_chunk]
+            return " ".join([str(row[col]) for col in self.get_weak_columns()])
+        except Exception as e:
+            return ""
+
+    def reference(self, element_id: int) -> Reference:
+        raise NotImplementedError()
+
+    def context(self, element_id, radius) -> str:
+        if not 0 <= element_id or not element_id < self.size:
+            raise ("Element id not in document.")
+
+        reference_texts = [
+            self.reference(i).text
+            for i in range(
+                max(0, element_id - radius), min(self.size, element_id + radius + 1)
+            )
+        ]
+        return "\n".join(reference_texts)
+
+    def save_meta(self, directory: Path):
+        # Save the index table
+        if self.save_extra_info and self.meta_table is not None:
+            self.meta_table.to_csv(
+                path_or_buf=directory / (self.name + ".csv"), index=False
+            )
+
+    def __getstate__(self):
+        # Document Connectors are expected to remove their connector(s) object
+        state = self.__dict__.copy()
+
+        del state[self._get_connector_object_name()]
+
+        return state
+
+
+class SQLDatabase(DocumentConnector):
+    """
+    class for handling SQL database connections and data retrieval for training the neural_db model
+
+    This class encapsulates functionality for connecting to an SQL database, executing SQL queries, and retrieving
+    data for use in training the model.
+
+    NOTE: It is being expected that the table will remain static in terms of both rows and columns.
+    """
+
+    def __init__(
+        self,
+        engine: sqlConn,
+        table_name: str,
+        id_col: str,
+        strong_columns: Optional[List[str]] = None,
+        weak_columns: Optional[List[str]] = None,
+        reference_columns: Optional[List[str]] = None,
+        chunk_size: int = 10_000,
+        save_extra_info: bool = False,
+        metadata: dict = {},
+    ) -> None:
+        self.table_name = table_name
+        self.id_col = id_col
+        self.strong_columns = strong_columns
+        self.weak_columns = weak_columns
+        self.reference_columns = reference_columns
+        self.chunk_size = chunk_size
+        self._save_extra_info = save_extra_info
+        self.doc_metadata = metadata
+
+        self._connector = SQLConnector(
+            engine=engine,
+            table_name=self.table_name,
+            id_col=self.id_col,
+            chunk_size=self.chunk_size,
+        )
+        self.total_rows = self._connector.total_rows()
+        if not self.total_rows > 0:
+            raise FileNotFoundError("Empty table")
+
+        self.database_name = engine.url.database
+        self.url = str(engine.url)
+        self.engine_uq = self.url + f"/{self.table_name}"
+        self._hash = hash_string(string=self.engine_uq)
+
+        # Integrity checks
+        self.assert_valid_id()
+        self.assert_valid_columns()
+
+        # setting the columns in the conector object
+        self._connector.columns = list(set(self.strong_columns + self.weak_columns))
+
+    @property
+    def name(self):
+        return self.database_name + "-" + self.table_name
+
+    @property
+    def source(self) -> str:
+        return str(self.engine_uq)
+
+    @property
+    def hash(self):
+        return self._hash
+
+    @property
+    def size(self) -> int:
+        # It is verfied by the uniqueness assertion of the id column.
+        return self.total_rows
+
+    def setup_connection(self, engine: sqlConn):
+        """
+        This is a helper function to re-establish the connection upon loading the
+        saved ndb model containing this SQLDatabase document.
+
+        Args:
+            engine: SQLAlchemy Connection object
+                    NOTE: Provide the same connection object.
+
+        NOTE: Same table would be used to establish connection
+        """
+        try:
+            # The idea is to check for the connector object existence
+            print(
+                "Connector object already exists with url:"
+                f" {self._connector.get_engine_url()}"
+            )
+        except AttributeError as e:
+            assert engine.url.database == self.database_name
+            assert str(engine.url) == self.url
+            self._connector = SQLConnector(
+                engine=engine,
+                table_name=self.table_name,
+                id_col=self.id_col,
+                columns=list(set(self.strong_columns + self.weak_columns)),
+                chunk_size=self.chunk_size,
+            )
+
+    def _get_connector_object_name(self):
+        return "_connector"
+
+    def get_strong_columns(self):
+        return self.strong_columns
+
+    def get_weak_columns(self):
+        return self.weak_columns
+
+    def get_engine(self):
+        try:
+            return self._connector._engine
+        except AttributeError as e:
+            raise AttributeError("engine is not available")
+
+    @property
+    def meta_table(self) -> Optional[pd.DataFrame]:
+        return None
+
+    def strong_text_from_chunk(self, id_in_chunk: int, chunk: pd.DataFrame) -> str:
+        try:
+            row = chunk.iloc[id_in_chunk]
+            return " ".join([str(row[col]) for col in self.get_strong_columns()])
+        except Exception as e:
+            return ""
+
+    def weak_text_from_chunk(self, id_in_chunk: int, chunk: pd.DataFrame) -> str:
+        try:
+            row = chunk.iloc[id_in_chunk]
+            return " ".join([str(row[col]) for col in self.get_weak_columns()])
+        except Exception as e:
+            return ""
+
+    def chunk_iterator(self) -> pd.DataFrame:
+        return self._connector.chunk_iterator()
+
+    def all_entity_ids(self) -> List[int]:
+        return list(range(self.size))
+
+    def reference(self, element_id: int) -> Reference:
+        if element_id >= self.size:
+            _raise_unknown_doc_error(element_id)
+
+        try:
+            reference_texts = self._connector.execute(
+                query=(
+                    f"SELECT {','.join(self.reference_columns)} FROM"
+                    f" {self.table_name} WHERE {self.id_col} = {element_id}"
+                )
+            ).fetchone()
+
+            text = "\n\n".join(
+                [
+                    f"{col_name}: {col_text}"
+                    for col_name, col_text in zip(
+                        self.reference_columns, reference_texts
+                    )
+                ]
+            )
+
+        except Exception as e:
+            text = (
+                f"Unable to connect to database, Referenced row with {self.id_col}:"
+                f" {element_id} "
+            )
+
+        return Reference(
+            document=self,
+            element_id=element_id,
+            text=text,
+            source=self.source,
+            metadata={
+                "Database": self.database_name,
+                "Table": self.table_name,
+                **self.doc_metadata,
+            },
+        )
+
+    @property
+    def matched_constraints(self) -> Dict[str, ConstraintValue]:
+        """
+        This method is called when the document is being added to a DocumentManager in order to build an index for constrained search.
+        """
+        return {key: ConstraintValue(value) for key, value in self.doc_metadata.items()}
+
+    def assert_valid_id(self):
+        all_cols = self._connector.cols_metadata()
+
+        id_col_meta = list(filter(lambda col: col["name"] == self.id_col, all_cols))
+        if len(id_col_meta) == 0:
+            raise AttributeError("id column not present in the table")
+        elif not isinstance(id_col_meta[0]["type"], Integer):
+            raise AttributeError("id column needs to be of type Integer")
+
+        primary_keys = self._connector.get_primary_keys()
+        if len(primary_keys) > 1:
+            raise AttributeError("Composite primary key is not allowed")
+        elif len(primary_keys) == 0 or primary_keys[0] != self.id_col:
+            raise AttributeError(f"{self.id_col} needs to be a primary key")
+
+        min_id = self._connector.execute(
+            query=f"SELECT MIN({self.id_col}) FROM {self.table_name}"
+        ).fetchone()[0]
+
+        max_id = self._connector.execute(
+            query=f"SELECT MAX({self.id_col}) FROM {self.table_name}"
+        ).fetchone()[0]
+
+        if min_id != 0 or max_id != self.size - 1:
+            raise AttributeError(
+                f"id column needs to be unique from 0 to {self.size - 1}"
+            )
+
+    def assert_valid_columns(self):
+        all_cols = self._connector.cols_metadata()
+
+        columns_set = set([col["name"] for col in all_cols])
+
+        # Checking for strong, weak and reference columns (if provided) to be present in column list of the table
+        if (self.strong_columns is not None) and (
+            not set(self.strong_columns).issubset(columns_set)
+        ):
+            raise AttributeError(
+                f"Strong column(s) doesn't exists in the table '{self.table_name}'"
+            )
+        if (self.weak_columns is not None) and (
+            not set(self.weak_columns).issubset(columns_set)
+        ):
+            raise AttributeError(
+                f"Weak column(s) doesn't exists in the table '{self.table_name}'"
+            )
+        if (self.reference_columns is not None) and (
+            not set(self.reference_columns).issubset(columns_set)
+        ):
+            raise AttributeError(
+                f"Reference column(s) doesn't exists in the table '{self.table_name}'"
+            )
+
+        # Checking for strong and weak column to have the correct column type
+        for col in all_cols:
+            if (
+                self.strong_columns is not None
+                and col["name"] in self.strong_columns
+                and not isinstance(col["type"], String)
+            ):
+                raise AttributeError(
+                    f"strong column '{col['name']}' needs to be of type String"
+                )
+            elif (
+                self.weak_columns is not None
+                and col["name"] in self.weak_columns
+                and not isinstance(col["type"], String)
+            ):
+                raise AttributeError(
+                    f"weak column '{col['name']}' needs to be of type String"
+                )
+
+        if self.strong_columns is None and self.weak_columns is None:
+            self.strong_columns = []
+            self.weak_columns = []
+            for col in all_cols:
+                if isinstance(col["type"], String):
+                    self.weak_columns.append(col["name"])
+        elif self.strong_columns is None:
+            self.strong_columns = []
+        elif self.weak_columns is None:
+            self.weak_columns = []
+
+        if self.reference_columns is None:
+            self.reference_columns = list(columns_set)
+
+
+class SharePoint(DocumentConnector):
+    """
+    Class for handling sharepoint connection, retrieving documents, processing and training the neural_db model
+
+    Args:
+        ctx (ClientContext): A ClientContext object for SharePoint connection.
+        library_path (str): The server-relative directory path where documents
+            are stored. Default: 'Shared Documents'
+        chunk_size (int): The maximum amount of data (in bytes) that can be fetched
+            at a time. (This limit may not apply if there are no files within this
+            range.) Default: 10MB
+        metadata (Dict[str, Any]): Optional, defaults to {}. Specifies metadata to
+            associate with entities from this file. Queries to NeuralDB can provide
+            constrains to restrict results based on the metadata.
+    """
+
+    def __init__(
+        self,
+        ctx: ClientContext,
+        library_path: str = "Shared Documents",
+        chunk_size: int = 10485760,
+        save_extra_info: bool = False,
+        metadata: dict = {},
+    ) -> None:
+        # Executing a dummy query to check for the authentication for the ctx object
+        try:
+            SharePoint.dummy_query(ctx=ctx)
+        except Exception as e:
+            raise Exception("Invalid ClientContext Object. Error: " + str(e))
+
+        self._connector = SharePointConnector(
+            ctx=ctx, library_path=library_path, chunk_size=chunk_size
+        )
+        self.library_path = library_path
+        self.chunk_size = chunk_size
+        self._save_extra_info = save_extra_info
+        self.doc_metadata = metadata
+
+        self.strong_column = "strong_text"
+        self.weak_column = "weak_text"
+        self.build_meta_table()
+        self._name = (
+            self._connector.site_name + "-" + (self.library_path).replace(" ", "_")
+        )
+        self.url = self._connector.url
+        self._source = self.url + "/" + library_path
+        self._hash = hash_string(self._source)
+
+    @property
+    def size(self) -> int:
+        return len(self.meta_table)
+
+    @property
+    def name(self) -> str:
+        return self._name
+
+    @property
+    def source(self) -> str:
+        return self._source
+
+    @property
+    def hash(self) -> str:
+        return self._hash
+
+    def setup_connection(self, ctx: ClientContext):
+        """
+        This is a helper function to re-establish the connection upon loading the saved ndb model containing this Sharepoint document.
+
+        Args:
+            engine: SQLAlchemy Connection object. NOTE: Provide the same connection object.
+        NOTE: Same library path would be used
+        """
+        try:
+            # The idea is to check for the connector object existence
+            print(f"Connector object already exists with url: {self._connector.url}")
+        except AttributeError as e:
+            assert self.url == ctx.web.get().execute_query().url
+            self._connector = SharePointConnector(
+                ctx=ctx, library_path=self.library_path, chunk_size=self.chunk_size
+            )
+
+    def get_strong_columns(self):
+        return [self.strong_column]
+
+    def get_weak_columns(self):
+        return [self.weak_column]
+
+    def build_meta_table(self):
+        num_files = self._connector.num_files()
+
+        print(f"Found {num_files} supported files")
+        self._meta_table = pd.DataFrame(
+            columns=[
+                "internal_doc_id",
+                "server_relative_url",
+                "page",
+            ]
+        )
+        self._meta_table = pd.concat(
+            [
+                current_chunk.drop(
+                    columns=self.get_strong_columns() + self.get_weak_columns()
+                )
+                for current_chunk in self.chunk_iterator()
+            ],
+            ignore_index=True,
+        )
+
+        self._meta_table[self.meta_table_id_col] = range(len(self._meta_table))
+        self._meta_table.set_index(keys=self.meta_table_id_col, inplace=True)
+
+    @property
+    def matched_constraints(self) -> Dict[str, ConstraintValue]:
+        """
+        Each constraint will get applied to each supported document on the sharepoint. This method is called when the document is being added to a DocumentManager in order to build an index for constrained search.
+        """
+        return {key: ConstraintValue(value) for key, value in self.doc_metadata.items()}
+
+    def all_entity_ids(self) -> List[int]:
+        return list(range(self.size))
+
+    def reference(self, element_id: int) -> Reference:
+        if element_id >= self.size:
+            _raise_unknown_doc_error(element_id)
+
+        filename = self.meta_table.iloc[element_id]["server_relative_url"].split(
+            sep="/"
+        )[-1]
+        return Reference(
+            document=self,
+            element_id=element_id,
+            text=f"filename: {filename}"
+            + (
+                f", page no: {self.meta_table.iloc[element_id]['page']}"
+                if self.meta_table.iloc[element_id]["page"] is not None
+                else ""
+            ),
+            source=self.source + "/" + filename,
+            metadata={
+                **self.meta_table.loc[element_id].to_dict(),
+                **self.doc_metadata,
+            },
+        )
+
+    @property
+    def meta_table(self) -> Optional[pd.DataFrame]:
+        return self._meta_table
+
+    def chunk_iterator(self) -> pd.DataFrame:
+        chunk_df = pd.DataFrame(
+            columns=[
+                self.strong_column,
+                self.weak_column,
+                "internal_doc_id",
+                "server_relative_url",
+                "page",
+            ]
+        )
+
+        for file_dict in self._connector.chunk_iterator():
+            chunk_df.drop(chunk_df.index, inplace=True)
+            temp_dfs = []
+            for server_relative_url, filepath in file_dict.items():
+                if filepath.endswith(".pdf"):
+                    doc = PDF(path=filepath, metadata=self.doc_metadata)
+                elif filepath.endswith(".docx"):
+                    doc = DOCX(path=filepath, metadata=self.doc_metadata)
+                else:
+                    doc = Unstructured(
+                        path=filepath,
+                        save_extra_info=self._save_extra_info,
+                        metadata=self.doc_metadata,
+                    )
+
+                temp_df = pd.DataFrame(
+                    columns=chunk_df.columns.tolist(), index=range(doc.size)
+                )
+                strong_text, weak_text, internal_doc_id, page = zip(
+                    *[
+                        (
+                            doc.strong_text(i),
+                            doc.weak_text(i),
+                            i,
+                            doc.reference(i).metadata.get("page", None),
+                        )
+                        for i in range(doc.size)
+                    ]
+                )
+                temp_df[self.strong_column] = strong_text
+                temp_df[self.weak_column] = weak_text
+                temp_df["internal_doc_id"] = internal_doc_id
+                temp_df["server_relative_url"] = [server_relative_url] * doc.size
+                temp_df["page"] = page
+
+                temp_dfs.append(temp_df)
+
+            chunk_df = pd.concat(temp_dfs, ignore_index=True)
+            yield chunk_df
+
+    def _get_connector_object_name(self):
+        return "_connector"
+
+    @staticmethod
+    def dummy_query(ctx: ClientContext):
+        # Authenticatiion fails if this dummy query execution fails
+        ctx.web.get().execute_query()
+
+    @staticmethod
+    def setup_clientContext(
+        base_url: str, credentials: Dict[str, str]
+    ) -> ClientContext:
+        """
+        Method to create a ClientContext object given base_url and credentials in the form (username, password) OR (client_id, client_secret)
+        """
+        ctx = None
+        try:
+            if all([cred in credentials.keys() for cred in ("username", "password")]):
+                user_credentials = UserCredential(
+                    user_name=credentials["username"], password=credentials["password"]
+                )
+                ctx = ClientContext(base_url=base_url).with_credentials(
+                    user_credentials
+                )
+            SharePoint.dummy_query(ctx=ctx)
+        except Exception as userCredError:
+            try:
+                if all(
+                    [
+                        cred in credentials.keys()
+                        for cred in ("client_id", "client_secret")
+                    ]
+                ):
+                    client_credentials = ClientCredential(
+                        client_id=credentials["client_id"],
+                        client_secret=credentials["client_secret"],
+                    )
+                    ctx = ClientContext(base_url=base_url).with_credentials(
+                        client_credentials
+                    )
+                    SharePoint.dummy_query(ctx=ctx)
+            except Exception as clientCredError:
+                pass
+
+        if ctx:
+            return ctx
+        raise AttributeError("Incorrect or insufficient credentials")
+
+
+class SalesForce(DocumentConnector):
+    """
+    Class for handling the Salesforce object connections and data retrieval for
+    training the neural_db model
+
+    This class encapsulates functionality for connecting to an object, executing
+    Salesforce Object Query Language (SOQL) queries, and retrieving
+
+    NOTE: Allow the Bulk API access for the provided object. Also, it is being
+    expected that the table will remain static in terms of both rows and columns.
+    """
+
+    def __init__(
+        self,
+        instance: Salesforce,
+        object_name: str,
+        id_col: str,
+        strong_columns: Optional[List[str]] = None,
+        weak_columns: Optional[List[str]] = None,
+        reference_columns: Optional[List[str]] = None,
+        save_extra_info: bool = True,
+        metadata: dict = {},
+    ) -> None:
+        self.object_name = object_name
+        self.id_col = id_col
+        self.strong_columns = strong_columns
+        self.weak_columns = weak_columns
+        self.reference_columns = reference_columns
+        self._save_extra_info = save_extra_info
+        self.doc_metadata = metadata
+        self._connector = SalesforceConnector(
+            instance=instance, object_name=object_name
+        )
+
+        self.total_rows = self._connector.total_rows()
+        if not self.total_rows > 0:
+            raise FileNotFoundError("Empty Object")
+        self._hash = hash_string(self._connector.sf_instance + self._connector.base_url)
+        self._source = self._connector.sf_instance + self.object_name
+
+        # Integrity_checks
+        self.assert_valid_id()
+        self.assert_valid_fields()
+
+        # setting the columns in the connector object
+        self._connector._fields = [self.id_col] + list(
+            set(self.strong_columns + self.weak_columns)
+        )
+
+    @property
+    def name(self) -> str:
+        return self.object_name
+
+    @property
+    def source(self) -> str:
+        return self._source
+
+    @property
+    def hash(self) -> str:
+        return self._hash
+
+    @property
+    def size(self) -> int:
+        return self.total_rows
+
+    def setup_connection(self, instance: Salesforce):
+        """
+        This is a helper function to re-establish the connection upon loading a saved ndb model containing this SalesForce document.
+
+        Args:
+            instance: Salesforce instance. NOTE: Provide the same connection object.
+
+        NOTE: Same object name would be used to establish connection
+        """
+        try:
+            # The idea is to check for the connector object existence
+            print(
+                f"Connector object already exists with url: {self._connector.base_url}"
+            )
+        except AttributeError as e:
+            assert self.hash == hash_string(instance.sf_instance + instance.base_url)
+            self._connector = SalesforceConnector(
+                instance=instance,
+                object_name=self.object_name,
+                fields=[self.id_col]
+                + list(set(self.strong_columns + self.weak_columns)),
+            )
+
+    def _get_connector_object_name(self):
+        return "_connector"
+
+    def row_iterator(self):
+        for current_chunk in self.chunk_iterator():
+            for idx in range(len(current_chunk)):
+                """
+                * Since we are not able to retrieve the rows in sorted order, we have to do this so that (id, strong_text, weak_text) gets mapped correctly.
+                * We cannot sort because the id_col needs to be of type 'autoNumber' which is a string. Neither we can do 'SELECT row FROM object_name ORDER BY LEN(id_col), id_col' because there is no LEN function in SOQL (by default). Owner of the object have to create a formula LEN() to use such query.
+                """
+                yield DocumentRow(
+                    element_id=int(current_chunk.iloc[idx][self.id_col]),
+                    strong=self.strong_text_from_chunk(
+                        id_in_chunk=idx, chunk=current_chunk
+                    ),  # Strong text from (idx)th row of the current_chunk
+                    weak=self.weak_text_from_chunk(
+                        id_in_chunk=idx, chunk=current_chunk
+                    ),  # Weak text from (idx)th row of the current_chunk
+                )
+
+    def get_strong_columns(self):
+        return self.strong_columns
+
+    def get_weak_columns(self):
+        return self.weak_columns
+
+    @property
+    def meta_table(self) -> Optional[pd.DataFrame]:
+        return None
+
+    def strong_text_from_chunk(self, id_in_chunk: int, chunk: pd.DataFrame) -> str:
+        try:
+            row = chunk.iloc[id_in_chunk]
+            return " ".join([str(row[col]) for col in self.get_strong_columns()])
+        except Exception as e:
+            return ""
+
+    def weak_text_from_chunk(self, id_in_chunk: int, chunk: pd.DataFrame) -> str:
+        try:
+            row = chunk.iloc[id_in_chunk]
+            return " ".join([str(row[col]) for col in self.get_weak_columns()])
+        except Exception as e:
+            return ""
+
+    def chunk_iterator(self) -> pd.DataFrame:
+        return self._connector.chunk_iterator()
+
+    def all_entity_ids(self) -> List[int]:
+        return list(range(self.size))
+
+    def reference(self, element_id: int) -> Reference:
+        if element_id >= self.size:
+            _raise_unknown_doc_error(element_id)
+
+        try:
+            result = self._connector.execute(
+                query=(
+                    f"SELECT {','.join(self.reference_columns)} FROM"
+                    f" {self.object_name} WHERE {self.id_col} = '{element_id}'"
+                )
+            )["records"][0]
+            del result["attributes"]
+            text = "\n\n".join(
+                [f"{col_name}: {col_text}" for col_name, col_text in result.items()]
+            )
+
+        except Exception as e:
+            text = (
+                "Unable to connect to the object instance, Referenced row with"
+                f" {self.id_col}: {element_id} "
+            )
+
+        return Reference(
+            document=self,
+            element_id=element_id,
+            text=text,
+            source=self.source,
+            metadata={
+                "object_name": self.object_name,
+                **self.doc_metadata,
+            },
+        )
+
+    @property
+    def matched_constraints(self) -> Dict[str, ConstraintValue]:
+        """
+        This method is called when the document is being added to a DocumentManager in order to build an index for constrained search.
+        """
+        return {key: ConstraintValue(value) for key, value in self.doc_metadata.items()}
+
+    def assert_valid_id(self):
+        all_fields = self._connector.field_metadata()
+
+        all_field_name = [field["name"] for field in all_fields]
+
+        if self.id_col not in all_field_name:
+            raise AttributeError("Id Columns is not present in the object")
+
+        # Uniqueness or primary constraint
+        id_field_meta = list(
+            filter(lambda field: field["name"] == self.id_col, all_fields)
+        )
+        if len(id_field_meta) == 0:
+            raise AttributeError("id col not present in the object")
+        id_field_meta = id_field_meta[0]
+
+        """
+        Reason behinds using AutoNumber as the id column type:
+
+            1. Salesforce doesn't have typical table constraints. Object in a salesforce (or table in conventional sense) uses an alpha-numeric string as a primary key.
+            2. Salesforce doesn't have a pure integer field. It have one in which we can set the decimal field of the double data-type to 0 but it is only for display purpose.
+            3. Only option left is one Auto-number field that can be used but it limits some options.
+        """
+        if not id_field_meta["autoNumber"]:
+            raise AttributeError("id col must be of type Auto-Number")
+        else:
+            # id field is auto-incremented string. Have to check for the form of A-{0}
+
+            result = self._connector.execute(
+                query=f"SELECT {self.id_col} FROM {self.object_name} LIMIT 1"
+            )
+            value: str = result["records"][0][self.id_col]
+            if not value.isdigit():
+                raise AttributeError("id column needs to be of the form \{0\}")
+
+        expected_min_row_id = 0
+        min_id = self._connector.execute(
+            query=(
+                f"SELECT {self.id_col} FROM {self.object_name} WHERE {self.id_col} ="
+                f" '{expected_min_row_id}'"
+            )
+        )
+
+        # This one is not required probably because user can't put the auto-number field mannually.
+        # User just can provide the start of the auto-number so if the min_id is 0, then max_id should be size - 1
+        expected_max_row_id = self.size - 1
+        max_id = self._connector.execute(
+            query=(
+                f"SELECT {self.id_col} FROM {self.object_name} WHERE {self.id_col} ="
+                f" '{expected_max_row_id}'"
+            )
+        )
+
+        if not (min_id["totalSize"] == 1 and max_id["totalSize"] == 1):
+            raise AttributeError(
+                f"id column needs to be unique from 0 to {self.size - 1}"
+            )
+
+    def assert_valid_fields(
+        self, supported_text_types: Tuple[str] = ("string", "textarea")
+    ):
+        all_fields = self._connector.field_metadata()
+        self.assert_field_inclusion(all_fields)
+        self.assert_field_type(all_fields, supported_text_types)
+        self.default_fields(all_fields, supported_text_types)
+
+    def assert_field_inclusion(self, all_fields: List[OrderedDict]):
+        fields_set = set([field["name"] for field in all_fields])
+
+        # Checking for strong, weak and reference columns (if provided) to be present in column list of the table
+        column_name_error = (
+            "Remember if it is a custom column, salesforce requires it to be appended"
+            " with __c."
+        )
+        if (self.strong_columns is not None) and (
+            not set(self.strong_columns).issubset(fields_set)
+        ):
+            raise AttributeError(
+                f"Strong column(s) doesn't exists in the object. {column_name_error}"
+            )
+        if (self.weak_columns is not None) and (
+            not set(self.weak_columns).issubset(fields_set)
+        ):
+            raise AttributeError(
+                f"Weak column(s) doesn't exists in the object. {column_name_error}"
+            )
+        if (self.reference_columns is not None) and (
+            not set(self.reference_columns).issubset(fields_set)
+        ):
+            raise AttributeError(
+                f"Reference column(s) doesn't exists in the object. {column_name_error}"
+            )
+
+    def assert_field_type(
+        self, all_fields: List[OrderedDict], supported_text_types: Tuple[str]
+    ):
+        # Checking for strong and weak column to have the correct column type
+        for field in all_fields:
+            if (
+                self.strong_columns is not None
+                and field["name"] in self.strong_columns
+                and field["type"] not in supported_text_types
+            ):
+                raise AttributeError(
+                    f"Strong column '{field['name']}' needs to be type from"
+                    f" {supported_text_types}"
+                )
+            if (
+                self.weak_columns is not None
+                and field["name"] in self.weak_columns
+                and field["type"] not in supported_text_types
+            ):
+                raise AttributeError(
+                    f"Weak column '{field['name']}' needs to be type"
+                    f" {supported_text_types}"
+                )
+
+    def default_fields(
+        self, all_fields: List[OrderedDict], supported_text_types: Tuple[str]
+    ):
+        if self.strong_columns is None and self.weak_columns is None:
+            self.strong_columns = []
+            self.weak_columns = []
+            for field in all_fields:
+                if field["type"] in supported_text_types:
+                    self.weak_columns.append(field["name"])
+        elif self.strong_columns is None:
+            self.strong_columns = []
+        elif self.weak_columns is None:
+            self.weak_columns = []
+
+        if self.reference_columns is None:
+            self.reference_columns = [self.id_col]
+            for field in all_fields:
+                if field["type"] in supported_text_types:
+                    self.reference_columns.append(field["name"])
+
+
+class SentenceLevelExtracted(Extracted):
+    """Parses a document into sentences and creates a NeuralDB entry for each
+    sentence. The strong column of the entry is the sentence itself while the
+    weak column is the paragraph from which the sentence came. A NeuralDB
+    reference produced by this object displays the paragraph instead of the
+    sentence to increase recall.
+    """
+
+    def __init__(
+        self, path: str, save_extra_info: bool = True, metadata=None, on_disk=False
+    ):
+        self.path = Path(path)
+        self.hash_val = hash_file(
+            path, metadata="sentence-level-extracted-" + str(metadata)
+        )
+        df = self.parse_sentences(self.process_data(path))
+        self.table = create_table(df, on_disk)
+        para_df = pd.DataFrame({"para": df["para"].unique()})
+        self.para_table = create_table(para_df, on_disk)
+        self._save_extra_info = save_extra_info
+        self.doc_metadata = metadata_with_source(metadata or {}, Path(path).name)
+
+    def not_just_punctuation(sentence: str):
+        for character in sentence:
+            if character not in string.punctuation and not character.isspace():
+                return True
+        return False
+
+    def get_sentences(paragraph: str):
+        return [
+            sentence
+            for sentence in sent_tokenize(paragraph)
+            if SentenceLevelExtracted.not_just_punctuation(sentence)
+        ]
+
+    def parse_sentences(
+        self,
+        df: pd.DataFrame,
+    ) -> pd.DataFrame:
+        df["sentences"] = df["para"].apply(SentenceLevelExtracted.get_sentences)
+
+        num_sents_cum_sum = np.cumsum(df["sentences"].apply(lambda sents: len(sents)))
+        df["id_offsets"] = np.zeros(len(df))
+        df["id_offsets"][1:] = num_sents_cum_sum[:-1]
+        df["id_offsets"] = df["id_offsets"].astype(int)
+
+        def get_ids(record):
+            id_offset = record["id_offsets"]
+            n_sents = len(record["sentences"])
+            return list(range(id_offset, id_offset + n_sents))
+
+        df = pd.DataFrame.from_records(
+            [
+                {
+                    "sentence": sentence,
+                    "para_id": para_id,
+                    "sentence_id": i + record["id_offsets"],
+                    "sentence_ids_in_para": str(get_ids(record)),
+                    **record,
+                }
+                for para_id, record in enumerate(df.to_dict(orient="records"))
+                for i, sentence in enumerate(record["sentences"])
+            ]
+        )
+
+        df.drop("sentences", axis=1, inplace=True)
+        df.drop("id_offsets", axis=1, inplace=True)
+        return df
+
+    def process_data(
+        self,
+        path: str,
+    ) -> pd.DataFrame:
+        raise NotImplementedError()
+
+    @property
+    def hash(self) -> str:
+        return self.hash_val
+
+    @property
+    def size(self) -> int:
+        return self.table.size
+
+    def get_strong_columns(self):
+        return ["sentence"]
+
+    @property
+    def name(self) -> str:
+        return self.path.name if self.path else None
+
+    @property
+    def source(self) -> str:
+        return str(self.path.absolute())
+
+    def strong_text(self, element_id: int) -> str:
+        return self.table.field(element_id, "sentence")
+
+    def weak_text(self, element_id: int) -> str:
+        return self.table.field(element_id, "para")
+
+    def show_fn(text, source, **kwargs):
+        return text
+
+    def reference(self, element_id: int) -> Reference:
+        if element_id >= self.table.size:
+            _raise_unknown_doc_error(element_id)
+        return Reference(
+            document=self,
+            element_id=element_id,
+            text=self.table.field(element_id, "display"),
+            source=self.source,
+            metadata={**self.table.row_as_dict(element_id), **self.doc_metadata},
+            upvote_ids=eval(self.table.field(element_id, "sentence_ids_in_para")),
+        )
+
+    def context(self, element_id, radius) -> str:
+        if not 0 <= element_id or not element_id < self.size:
+            raise ("Element id not in document.")
+
+        # Cast to int because the actual return type is numpy.int64, which
+        # causes problems in the self.para_table.range_rows_as_dicts line.
+        para_id = int(self.table.field(element_id, "para_id"))
+
+        rows = self.para_table.range_rows_as_dicts(
+            from_row_id=max(0, para_id - radius),
+            to_row_id=min(self.para_table.size, para_id + radius + 1),
+        )
+        return "\n\n".join(row["para"] for row in rows)
+
+    def save_meta(self, directory: Path):
+        # Let's copy the original file to the provided directory
+        if self.save_extra_info:
+            shutil.copy(self.path, directory)
+        self.table.save_meta(directory)
+
+    def load_meta(self, directory: Path):
+        # Since we've moved the file to the provided directory, let's make
+        # sure that we point to this file.
+        if hasattr(self, "doc_name"):
+            self.path = directory / self.doc_name
+        else:
+            # deprecated, self.path should not be in self
+            self.path = directory / self.path.name
+
+        if not hasattr(self, "doc_metadata"):
+            self.doc_metadata = {}
+
+        if hasattr(self, "df"):
+            self.df["sentence_ids_in_para"] = self.df["sentence_ids_in_para"].apply(str)
+            self.table = DataFrameTable(self.df)
+            self.para_table = DataFrameTable(pd.DataFrame({"para": self.para_df}))
+            del self.df
+            del self.para_df
+        elif hasattr(self, "table"):
+            self.table.load_meta(directory)
+            self.para_table.load_meta(directory)
+
+
+class SentenceLevelPDF(SentenceLevelExtracted):
+    """
+    Parses a document into sentences and creates a NeuralDB entry for each
+    sentence. The strong column of the entry is the sentence itself while the
+    weak column is the paragraph from which the sentence came. A NeuralDB
+    reference produced by this object displays the paragraph instead of the
+    sentence to increase recall.
+
+    Args:
+        path (str): The path to the pdf file.
+        metadata (Dict[str, Any]): Optional, defaults to {}. Specifies metadata to
+            associate with entities from this file. Queries to NeuralDB can provide
+            constrains to restrict results based on the metadata.
+    """
+
+    def __init__(self, path: str, metadata=None, on_disk=False):
+        super().__init__(path=path, metadata=metadata, on_disk=on_disk)
+
+    def process_data(
+        self,
+        path: str,
+    ) -> pd.DataFrame:
+        return process_pdf(path)
+
+
+class SentenceLevelDOCX(SentenceLevelExtracted):
+    """
+    Parses a document into sentences and creates a NeuralDB entry for each
+    sentence. The strong column of the entry is the sentence itself while the
+    weak column is the paragraph from which the sentence came. A NeuralDB
+    reference produced by this object displays the paragraph instead of the
+    sentence to increase recall.
+
+    Args:
+        path (str): The path to the docx file.
+        metadata (Dict[str, Any]): Optional, defaults to {}. Specifies metadata to
+            associate with entities from this file. Queries to NeuralDB can provide
+            constrains to restrict results based on the metadata.
+    """
+
+    def __init__(self, path: str, metadata=None, on_disk=False):
+        super().__init__(path=path, metadata=metadata, on_disk=on_disk)
+
+    def process_data(
+        self,
+        path: str,
+    ) -> pd.DataFrame:
+        return process_docx(path)
+
+
+class InMemoryText(Document):
+    """
+    A wrapper around a batch of texts and their metadata to fit it in the
+    NeuralDB Document framework.
+
+    Args:
+        name (str): A name for the batch of texts.
+        texts (List[str]): A batch of texts.
+        metadatas (List[Dict[str, Any]]): Optional. Metadata for each text.
+        global_metadata (Dict[str, Any]): Optional. Metadata for the whole batch
+        of texts.
+    """
+
+    def __init__(
+        self,
+        name: str,
+        texts: List[str],
+        metadatas: Optional[List[dict]] = None,
+        global_metadata=None,
+        on_disk=False,
+    ):
+        self._name = name
+        df = pd.DataFrame({"texts": texts})
+        self.metadata_columns = []
+        if metadatas:
+            metadata_df = pd.DataFrame.from_records(metadatas)
+            df = pd.concat([df, metadata_df], axis=1)
+            self.metadata_columns = metadata_df.columns
+        self.table = create_table(df, on_disk)
+        self.hash_val = hash_string(str(texts) + str(metadatas))
+        self.global_metadata = global_metadata or {}
+
+    @property
+    def hash(self) -> str:
+        return self.hash_val
+
+    @property
+    def size(self) -> int:
+        return self.table.size
+
+    @property
+    def name(self) -> str:
+        return self._name
+
+    @property
+    def source(self) -> str:
+        return self._name
+
+    @property
+    def matched_constraints(self) -> Dict[str, ConstraintValue]:
+        metadata_constraints = {
+            key: ConstraintValue(value) for key, value in self.global_metadata.items()
+        }
+        indexed_column_constraints = {
+            key: ConstraintValue(is_any=True) for key in self.metadata_columns
+        }
+        return {**metadata_constraints, **indexed_column_constraints}
+
+    def all_entity_ids(self) -> List[int]:
+        return list(range(self.size))
+
+    def filter_entity_ids(self, filters: Dict[str, Filter]):
+        table_filter = TableFilter(
+            {k: v for k, v in filters.items() if k not in self.global_metadata.keys()}
+        )
+        return self.table.apply_filter(table_filter)
+
+    def strong_text(self, element_id: int) -> str:
+        return ""
+
+    def weak_text(self, element_id: int) -> str:
+        return self.table.field(element_id, "texts")
+
+    def reference(self, element_id: int) -> Reference:
+        if element_id >= self.table.size:
+            _raise_unknown_doc_error(element_id)
+        return Reference(
+            document=self,
+            element_id=element_id,
+            text=self.table.field(element_id, "texts"),
+            source=self.source,
+            metadata={**self.table.row_as_dict(element_id), **self.global_metadata},
+        )
+
+    def context(self, element_id, radius) -> str:
+        # We don't return neighboring texts because they are not necessarily
+        # related.
+        return self.table.field(element_id, "texts")
+
+    def save_meta(self, directory: Path):
+        self.table.save_meta(directory)
+
+    def load_meta(self, directory: Path):
+        self.table.load_meta(directory)
```

## thirdai/neural_db/inverted_index.py

 * *Ordering differences only*

```diff
@@ -1,90 +1,90 @@
-from typing import List, Tuple
-
-from thirdai import search
-
-from .documents import DocumentDataSource
-from .supervised_datasource import SupDataSource
-
-
-class ChunkedRowIterator:
-    def __init__(self, iterator):
-        self.iterator = iterator
-
-    def next(self, n):
-        ids = []
-        docs = []
-
-        for row in self.iterator:
-            ids.append(row.id)
-            docs.append(row.strong + " " + row.weak)
-
-            if len(ids) == n:
-                return ids, docs
-
-        if len(ids) == 0:
-            return None
-
-        return ids, docs
-
-
-class InvertedIndex:
-    def __init__(self, max_shard_size: int = 8_000_000):
-        self.indexes = []
-        self.max_shard_size = max_shard_size
-
-    def insert(self, doc_data_source: DocumentDataSource):
-        if len(self.indexes) > 0 and self.indexes[-1].size() < self.max_shard_size:
-            curr_index = self.indexes[-1]
-        else:
-            curr_index = search.InvertedIndex()
-
-        chunked_iterator = ChunkedRowIterator(doc_data_source.row_iterator())
-
-        while chunk := chunked_iterator.next(self.max_shard_size - curr_index.size()):
-            curr_index.index(ids=chunk[0], docs=chunk[1])
-            if curr_index.size() == self.max_shard_size:
-                self.indexes.append(curr_index)
-                curr_index = search.InvertedIndex()
-
-        if curr_index.size() > 0:
-            self.indexes.append(curr_index)
-
-    def export(self):
-        if len(self.indexes) != 1:
-            raise ValueError(
-                "Checkpoint is not compatible with this version of thirdai."
-            )
-        return self.indexes[0]
-
-    def supervised_train(self, data_source: SupDataSource):
-        pass
-
-    def upvote(self, pairs: List[Tuple[str, int]]) -> None:
-        pass
-
-    def associate(self, pairs: List[Tuple[str, str]]) -> None:
-        pass
-
-    def query(self, queries: str, k: int):
-        if len(self.indexes) == 0:
-            raise ValueError("Cannot query before inserting documents.")
-
-        if len(self.indexes) == 1:
-            return self.indexes[0].query(queries=[q for q in queries], k=k)
-        return [
-            search.InvertedIndex.parallel_query(self.indexes, q, k=k) for q in queries
-        ]
-
-    def forget(self, ids):
-        for index in self.indexes:
-            index.remove(ids)
-
-    def clear(self):
-        self.indexes = []
-
-    def __setstate__(self, state):
-        if "indexes" not in state:
-            state["indexes"] = [state["index"]]
-            state["max_shard_size"] = 8_000_000
-            del state["index"]
-        self.__dict__.update(state)
+from typing import List, Tuple
+
+from thirdai import search
+
+from .documents import DocumentDataSource
+from .supervised_datasource import SupDataSource
+
+
+class ChunkedRowIterator:
+    def __init__(self, iterator):
+        self.iterator = iterator
+
+    def next(self, n):
+        ids = []
+        docs = []
+
+        for row in self.iterator:
+            ids.append(row.id)
+            docs.append(row.strong + " " + row.weak)
+
+            if len(ids) == n:
+                return ids, docs
+
+        if len(ids) == 0:
+            return None
+
+        return ids, docs
+
+
+class InvertedIndex:
+    def __init__(self, max_shard_size: int = 8_000_000):
+        self.indexes = []
+        self.max_shard_size = max_shard_size
+
+    def insert(self, doc_data_source: DocumentDataSource):
+        if len(self.indexes) > 0 and self.indexes[-1].size() < self.max_shard_size:
+            curr_index = self.indexes[-1]
+        else:
+            curr_index = search.InvertedIndex()
+
+        chunked_iterator = ChunkedRowIterator(doc_data_source.row_iterator())
+
+        while chunk := chunked_iterator.next(self.max_shard_size - curr_index.size()):
+            curr_index.index(ids=chunk[0], docs=chunk[1])
+            if curr_index.size() == self.max_shard_size:
+                self.indexes.append(curr_index)
+                curr_index = search.InvertedIndex()
+
+        if curr_index.size() > 0:
+            self.indexes.append(curr_index)
+
+    def export(self):
+        if len(self.indexes) != 1:
+            raise ValueError(
+                "Checkpoint is not compatible with this version of thirdai."
+            )
+        return self.indexes[0]
+
+    def supervised_train(self, data_source: SupDataSource):
+        pass
+
+    def upvote(self, pairs: List[Tuple[str, int]]) -> None:
+        pass
+
+    def associate(self, pairs: List[Tuple[str, str]]) -> None:
+        pass
+
+    def query(self, queries: str, k: int):
+        if len(self.indexes) == 0:
+            raise ValueError("Cannot query before inserting documents.")
+
+        if len(self.indexes) == 1:
+            return self.indexes[0].query(queries=[q for q in queries], k=k)
+        return [
+            search.InvertedIndex.parallel_query(self.indexes, q, k=k) for q in queries
+        ]
+
+    def forget(self, ids):
+        for index in self.indexes:
+            index.remove(ids)
+
+    def clear(self):
+        self.indexes = []
+
+    def __setstate__(self, state):
+        if "indexes" not in state:
+            state["indexes"] = [state["index"]]
+            state["max_shard_size"] = 8_000_000
+            del state["index"]
+        self.__dict__.update(state)
```

## thirdai/neural_db/loggers.py

 * *Ordering differences only*

```diff
@@ -1,145 +1,145 @@
-import os
-from pathlib import Path
-from typing import List, Optional
-
-import pandas as pd
-
-
-class Logger:
-    def name(self):
-        raise NotImplementedError()
-
-    def log(
-        self,
-        session_id: str,
-        action: str,
-        args: dict,
-        train_samples: Optional[any] = None,
-    ):
-        raise NotImplementedError()
-
-    def get_logs(self) -> pd.DataFrame:
-        raise NotImplementedError()
-
-    def save_meta(self, directory: Path):
-        raise NotImplementedError()
-
-    def load_meta(self, directory: Path):
-        raise NotImplementedError()
-
-
-class InMemoryLogger(Logger):
-    def make_log(session_id=[], action=[], args=[], train_samples=[]):
-        return pd.DataFrame(
-            {
-                "session_id": session_id,
-                "action": action,
-                "args": args,
-                "train_samples": train_samples,
-            }
-        )
-
-    def __init__(self, logs=make_log()):
-        self.logs = logs
-
-    def name(self):
-        return "in_memory"
-
-    def log(self, session_id: str, action: str, args: dict, train_samples=None):
-        self.logs = pd.concat(
-            [
-                self.logs,
-                InMemoryLogger.make_log(
-                    session_id=[session_id],
-                    action=[action],
-                    args=[args],
-                    train_samples=[train_samples],
-                ),
-            ]
-        )
-
-    def get_logs(self) -> pd.DataFrame:
-        return self.logs
-
-    def save_meta(self, directory: Path):
-        pass
-
-    def load_meta(self, directory: Path):
-        pass
-
-
-class LoggerList(Logger):
-    def __init__(self, loggers: List[Logger]):
-        self.loggers = list(
-            filter(
-                lambda logger: not isinstance(logger, (NoOpLogger, LoggerList)), loggers
-            )
-        )
-
-    def name(self):
-        return "list"
-
-    def log(
-        self,
-        session_id: str,
-        action: str,
-        args: dict,
-        train_samples: Optional[any] = None,
-    ):
-        [
-            logger.log(
-                session_id=session_id,
-                action=action,
-                args=args,
-                train_samples=train_samples,
-            )
-            for logger in self.loggers
-        ]
-
-    def get_logs(self):
-        if len(self.loggers) == 0:
-            return pd.DataFrame(
-                {
-                    "session_id": [],
-                    "action": [],
-                    "args": [],
-                    "train_samples": [],
-                }
-            )
-        return self.loggers[0].get_logs()
-
-    def save_meta(self, directory: Path):
-        for logger in self.loggers:
-            os.mkdir(directory / logger.name())
-            logger.save_meta(directory / logger.name())
-
-    def load_meta(self, directory: Path):
-        for logger in self.loggers:
-            logger.load_meta(directory / logger.name())
-
-
-class NoOpLogger(Logger):
-    def __init__(self) -> None:
-        pass
-
-    def name(self):
-        return "no_op"
-
-    def log(self, session_id: str, action: str, args: dict, train_samples=None):
-        pass
-
-    def get_logs(self) -> pd.DataFrame:
-        return pd.DataFrame(
-            {
-                "session_id": [],
-                "action": [],
-                "args": [],
-                "train_samples": [],
-            }
-        )
-
-    def save_meta(self, directory: Path):
-        pass
-
-    def load_meta(self, directory: Path):
-        pass
+import os
+from pathlib import Path
+from typing import List, Optional
+
+import pandas as pd
+
+
+class Logger:
+    def name(self):
+        raise NotImplementedError()
+
+    def log(
+        self,
+        session_id: str,
+        action: str,
+        args: dict,
+        train_samples: Optional[any] = None,
+    ):
+        raise NotImplementedError()
+
+    def get_logs(self) -> pd.DataFrame:
+        raise NotImplementedError()
+
+    def save_meta(self, directory: Path):
+        raise NotImplementedError()
+
+    def load_meta(self, directory: Path):
+        raise NotImplementedError()
+
+
+class InMemoryLogger(Logger):
+    def make_log(session_id=[], action=[], args=[], train_samples=[]):
+        return pd.DataFrame(
+            {
+                "session_id": session_id,
+                "action": action,
+                "args": args,
+                "train_samples": train_samples,
+            }
+        )
+
+    def __init__(self, logs=make_log()):
+        self.logs = logs
+
+    def name(self):
+        return "in_memory"
+
+    def log(self, session_id: str, action: str, args: dict, train_samples=None):
+        self.logs = pd.concat(
+            [
+                self.logs,
+                InMemoryLogger.make_log(
+                    session_id=[session_id],
+                    action=[action],
+                    args=[args],
+                    train_samples=[train_samples],
+                ),
+            ]
+        )
+
+    def get_logs(self) -> pd.DataFrame:
+        return self.logs
+
+    def save_meta(self, directory: Path):
+        pass
+
+    def load_meta(self, directory: Path):
+        pass
+
+
+class LoggerList(Logger):
+    def __init__(self, loggers: List[Logger]):
+        self.loggers = list(
+            filter(
+                lambda logger: not isinstance(logger, (NoOpLogger, LoggerList)), loggers
+            )
+        )
+
+    def name(self):
+        return "list"
+
+    def log(
+        self,
+        session_id: str,
+        action: str,
+        args: dict,
+        train_samples: Optional[any] = None,
+    ):
+        [
+            logger.log(
+                session_id=session_id,
+                action=action,
+                args=args,
+                train_samples=train_samples,
+            )
+            for logger in self.loggers
+        ]
+
+    def get_logs(self):
+        if len(self.loggers) == 0:
+            return pd.DataFrame(
+                {
+                    "session_id": [],
+                    "action": [],
+                    "args": [],
+                    "train_samples": [],
+                }
+            )
+        return self.loggers[0].get_logs()
+
+    def save_meta(self, directory: Path):
+        for logger in self.loggers:
+            os.mkdir(directory / logger.name())
+            logger.save_meta(directory / logger.name())
+
+    def load_meta(self, directory: Path):
+        for logger in self.loggers:
+            logger.load_meta(directory / logger.name())
+
+
+class NoOpLogger(Logger):
+    def __init__(self) -> None:
+        pass
+
+    def name(self):
+        return "no_op"
+
+    def log(self, session_id: str, action: str, args: dict, train_samples=None):
+        pass
+
+    def get_logs(self) -> pd.DataFrame:
+        return pd.DataFrame(
+            {
+                "session_id": [],
+                "action": [],
+                "args": [],
+                "train_samples": [],
+            }
+        )
+
+    def save_meta(self, directory: Path):
+        pass
+
+    def load_meta(self, directory: Path):
+        pass
```

## thirdai/neural_db/neural_db.py

 * *Ordering differences only*

```diff
@@ -1,1063 +1,1063 @@
-import copy
-import shutil
-from enum import Enum
-from pathlib import Path
-from typing import Callable, Dict, List, Optional, Sequence, Tuple, Union
-
-import numpy as np
-import pandas as pd
-import thirdai
-from thirdai._thirdai import bolt, data
-
-from . import loggers, teachers
-from .documents import CSV, Document, DocumentManager, Reference
-from .models.finetunable_retriever import FinetunableRetriever
-from .models.mach import Mach
-from .models.mach_mixture_model import MachMixture
-from .models.model_interface import CancelState
-from .savable_state import (
-    State,
-    load_checkpoint,
-    make_preinsertion_checkpoint,
-    make_training_checkpoint,
-)
-from .supervised_datasource import Sup, SupDataSource
-from .trainer.checkpoint_config import CheckpointConfig
-
-Strength = Enum("Strength", ["Weak", "Medium", "Strong"])
-
-
-def no_op(*args, **kwargs):
-    pass
-
-
-class NeuralDB:
-    """
-    NeuralDB is a search and retrieval system that can be used to search over
-    knowledge bases and documents. It can also be used in RAG pipelines for the
-    search retrieval phase.
-
-    Examples:
-        >>> ndb = NeuralDB()
-        >>> ndb.insert([CSV(...), PDF(...), DOCX(...)])
-        >>> results = ndb.search("how to make chocolate chip cookies")
-    """
-
-    def __init__(
-        self,
-        user_id: str = "user",
-        num_shards: int = 1,
-        num_models_per_shard: int = 1,
-        retriever="finetunable_retriever",
-        low_memory=None,
-        **kwargs,
-    ) -> None:
-        """
-        Constructs an empty NeuralDB.
-
-        Args:
-            user_id (str): Optional, used to identify user/session in logging.
-            retriever (str): One of 'finetunable_retriever', 'mach', or 'hybrid'.
-                Identifies which retriever to use as the backend. Defaults to
-                'finetunable_retriever'.
-
-        Returns:
-            A NeuralDB.
-        """
-        if low_memory is not None:
-            print(
-                "Warning: 'low_memory' flag will be deprecated soon in the NeuralDB constructor. Please pass 'retriever=' instead."
-            )
-            if low_memory == True:
-                retriever = "finetunable_retriever"
-            elif low_memory == False:
-                retriever = "hybrid"
-        self._user_id: str = user_id
-
-        # The savable_state kwarg is only used in static constructor methods
-        # and should not be used by an external user.
-        # We read savable_state from kwargs so that it doesn't appear in the
-        # arguments list and confuse users.
-        if "savable_state" not in kwargs:
-            if num_shards <= 0:
-                raise Exception(
-                    f"Invalid Value Passed for num_shards : {num_shards}."
-                    " NeuralDB can only be initialized with a positive number of"
-                    " shards."
-                )
-            if num_models_per_shard <= 0:
-                raise Exception(
-                    f"Invalid Value Passed for num_models_per_shard : {num_models_per_shard}."
-                    " NeuralDB can only be initialized with a positive number of"
-                    " models per shard."
-                )
-            if retriever == "finetunable_retriever":
-                model = FinetunableRetriever()
-            elif retriever == "mach" or retriever == "hybrid":
-                if num_shards > 1 or num_models_per_shard > 1:
-                    model = MachMixture(
-                        num_shards=num_shards,
-                        num_models_per_shard=num_models_per_shard,
-                        id_col="id",
-                        query_col="query",
-                        hybrid=(retriever == "hybrid"),
-                        **kwargs,
-                    )
-                else:
-                    model = Mach(
-                        id_col="id",
-                        query_col="query",
-                        hybrid=(retriever == "hybrid"),
-                        **kwargs,
-                    )
-            else:
-                raise ValueError(
-                    f"Invalid retriever '{retriever}'. Please use 'finetunable_retriever', 'mach', or 'hybrid'."
-                )
-
-            self._savable_state = State(
-                model, logger=loggers.LoggerList([loggers.InMemoryLogger()])
-            )
-        else:
-            self._savable_state = kwargs["savable_state"]
-
-    @staticmethod
-    def from_checkpoint(
-        checkpoint_path: str,
-        user_id: str = "user",
-        on_progress: Callable = no_op,
-    ):
-        """
-        Constructs a NeuralDB from a checkpoint. This can be used save and reload
-        NeuralDBs, it is also used for loading pretrained NeuralDB models.
-
-        Args:
-            checkpoint_path (str): The path to the checkpoint directory.
-            user_id (str): Optional, used to identify user/session in logging.
-            on_progress (Callable): Optional, callback that can be called as loading the checkpoint progresses.
-
-        Returns:
-            A NeuralDB.
-        """
-        checkpoint_path = Path(checkpoint_path)
-        savable_state = State.load(checkpoint_path, on_progress)
-        if savable_state.model and savable_state.model.get_model():
-            savable_state.model.set_mach_sampling_threshold(0.01)
-        if not isinstance(savable_state.logger, loggers.LoggerList):
-            # TODO(Geordie / Yash): Add DBLogger to LoggerList once ready.
-            savable_state.logger = loggers.LoggerList([savable_state.logger])
-
-        return NeuralDB(user_id, savable_state=savable_state)
-
-    @staticmethod
-    def from_udt(
-        udt: bolt.UniversalDeepTransformer,
-        user_id: str = "user",
-        csv: Optional[str] = None,
-        csv_id_column: Optional[str] = None,
-        csv_strong_columns: Optional[List[str]] = None,
-        csv_weak_columns: Optional[List[str]] = None,
-        csv_reference_columns: Optional[List[str]] = None,
-    ):
-        """
-        Instantiate a NeuralDB, using the given UDT as the underlying model.
-        Usually for porting a pretrained model into the NeuralDB format.
-        Use the optional csv-related arguments to insert the pretraining dataset
-        into the NeuralDB instance.
-
-        Args:
-            udt (bolt.UniversalDeepTransformer): The udt model to use in the NeuralDB.
-            user_id (str): Optional, used to identify user/session in logging.
-            csv (Optional[str]): Optional, default None. The path to the CSV file
-                used to train the udt model. If supplied, the CSV file will be
-                inserted into NeuralDB.
-            csv_id_column (Optional[str]): Optional, default None. The id column
-                of the training dataset. Required only if the data is being inserted via
-                the `csv` arg.
-            csv_strong_columns (Optional[str]): Optional, default None. The strong
-                signal columns from the training data. Required only if the data is
-                being inserted via the `csv` arg.
-            csv_weak_columns (Optional[str]): Optional, default None. The weak signal
-                columns from the training data. Required only if the data is being
-                inserted via the `csv` arg.
-            csv_reference_columns (Optional[str]): Optional, default None. The
-                columns whose data should be returned as search results to queries.
-                Required only if the data is being inserted via the `csv` arg.
-
-        Returns:
-            A NeuralDB.
-        """
-        if csv is None:
-            udt.clear_index()
-
-        udt.enable_rlhf()
-        udt.set_mach_sampling_threshold(0.01)
-        fhr, emb_dim, out_dim = udt.model_dims()
-
-        text_dataset_config = udt.text_dataset_config()
-
-        model = Mach(
-            id_col=text_dataset_config.label_column,
-            id_delimiter=text_dataset_config.label_delimiter,
-            query_col=text_dataset_config.text_column,
-            fhr=fhr,
-            embedding_dimension=emb_dim,
-            extreme_output_dim=out_dim,
-        )
-        model.model = udt
-        logger = loggers.LoggerList([loggers.InMemoryLogger()])
-        savable_state = State(model=model, logger=logger)
-
-        if csv is not None:
-            if (
-                csv_id_column is None
-                or csv_strong_columns is None
-                or csv_weak_columns is None
-                or csv_reference_columns is None
-            ):
-                error_msg = (
-                    "If the `csv` arg is provided, then the following args must also be"
-                    " provided:\n"
-                )
-                error_msg += " - `csv_id_column`\n"
-                error_msg += " - `csv_strong_columns`\n"
-                error_msg += " - `csv_weak_columns`\n"
-                error_msg += " - `csv_reference_columns`\n"
-                raise ValueError(error_msg)
-            csv_doc = CSV(
-                path=csv,
-                id_column=csv_id_column,
-                strong_columns=csv_strong_columns,
-                weak_columns=csv_weak_columns,
-                reference_columns=csv_reference_columns,
-            )
-            savable_state.documents.add([csv_doc])
-            savable_state.model.set_n_ids(csv_doc.size)
-
-        return NeuralDB(user_id, savable_state=savable_state)
-
-    def pretrain_distributed(
-        self,
-        documents,
-        scaling_config,
-        run_config,
-        learning_rate: float = 0.001,
-        epochs: int = 5,
-        batch_size: int = None,
-        metrics: List[str] = [],
-        max_in_memory_batches: Optional[int] = None,
-        communication_backend="gloo",
-        log_folder=None,
-    ):
-        """
-        Pretrains a model in a distributed manner using the provided documents.
-
-        Args:
-            documents: List of documents for pretraining. All the documents must have the same id column.
-            scaling_config: Configuration related to the scaling aspects for Ray trainer. Read
-                https://docs.ray.io/en/latest/train/api/doc/ray.train.ScalingConfig.html
-            run_config: Configuration related to the runtime aspects for Ray trainer. Read
-                https://docs.ray.io/en/latest/train/api/doc/ray.train.RunConfig.html
-                ** Note: We need to specify `storage_path` in `RunConfig` which must be a networked **
-                ** file system or cloud storage path accessible by all workers. (Ray 2.7.0 onwards) **
-            learning_rate (float, optional): Learning rate for the optimizer. Default is 0.001.
-            epochs (int, optional): Number of epochs to train. Default is 5.
-            batch_size (int, optional): Size of each batch for training. If not provided, will be determined automatically.
-            metrics (List[str], optional): List of metrics to evaluate during training. Default is an empty list.
-            max_in_memory_batches (Optional[int], optional): Number of batches to load in memory at once. Useful for
-                streaming support when dataset is too large to fit in memory. If None, all batches will be loaded.
-            communication_backend (str, optional): Bolt Distributed Training uses Torch Communication Backend. This
-                refers to backend for inter-worker communication. Default is "gloo".
-
-        Notes:
-            - Make sure to pass id_column to neural_db.CSV() making sure the ids are in ascending order starting from 0.
-            - The `scaling_config`, `run_config`, and `resume_from_checkpoint` arguments are related to the Ray trainer configuration. Read
-                https://docs.ray.io/en/latest/ray-air/trainers.html#trainer-basics
-            - Ensure that the communication backend specified is compatible with the hardware and network setup for MPI/Gloo backend.
-        """
-        if isinstance(self._savable_state.model, MachMixture):
-            raise NotImplementedError(
-                "Distributed Training is not supported for NeuralDB initialized with a"
-                " mixture of experts."
-            )
-        import warnings
-        from distutils.version import LooseVersion
-
-        import ray
-        import thirdai.distributed_bolt as dist
-        from ray.train.torch import TorchConfig
-
-        ray_version = ray.__version__
-        if LooseVersion(ray_version) >= LooseVersion("2.7"):
-            warnings.warn(
-                """
-                Using ray version 2.7 or higher requires specifying a remote or NFS storage path. 
-                Support for local checkpoints has been discontinued in these versions. 
-                Refer to https://github.com/ray-project/ray/issues/37177 for details.
-                """.strip()
-            )
-
-        if not isinstance(documents, list) or not all(
-            isinstance(doc, CSV) for doc in documents
-        ):
-            raise ValueError(
-                "The pretrain_distributed function currently only supports CSV"
-                " documents."
-            )
-
-        def training_loop_per_worker(config):
-            import os
-
-            import thirdai.distributed_bolt as dist
-            from ray import train
-            from thirdai.dataset import RayCsvDataSource
-
-            if config["licensing_lambda"]:
-                config["licensing_lambda"]()
-
-            strong_column_names = config["strong_column_names"]
-            weak_column_names = config["weak_column_names"]
-            learning_rate = config["learning_rate"]
-            epochs = config["epochs"]
-            batch_size = config["batch_size"]
-            metrics = config["metrics"]
-            max_in_memory_batches = config["max_in_memory_batches"]
-            model_ref = config["model_ref"]
-            model_target_column = config["model_target_col"]
-            document_target_col = config["document_target_col"]
-            log_folder = train_loop_config["log_folder"]
-
-            # ray data will automatically split the data if the dataset is passed with key "train"
-            # to training loop. Read https://docs.ray.io/en/latest/ray-air/check-ingest.html#splitting-data-across-workers
-            stream_split_data_iterator = train.get_dataset_shard("train")
-
-            model = ray.get(model_ref)
-
-            if log_folder:
-                if not os.path.exists(log_folder):
-                    print(f"Folder '{log_folder}' does not exist. Creating it...")
-                    os.makedirs(log_folder)
-                    print(f"Folder '{log_folder}' created successfully!")
-                thirdai.logging.setup(
-                    log_to_stderr=False,
-                    path=os.path.join(
-                        log_folder, f"worker-{train.get_context().get_world_rank()}.log"
-                    ),
-                    level="info",
-                )
-
-            metrics = model.coldstart_distributed_on_data_source(
-                data_source=RayCsvDataSource(
-                    stream_split_data_iterator, model_target_column, document_target_col
-                ),
-                strong_column_names=strong_column_names,
-                weak_column_names=weak_column_names,
-                learning_rate=learning_rate,
-                epochs=epochs,
-                batch_size=batch_size,
-                metrics=metrics,
-                max_in_memory_batches=max_in_memory_batches,
-            )
-
-            rank = train.get_context().get_world_rank()
-            checkpoint = None
-            if rank == 0:
-                # Use `with_optimizers=False` to save model without optimizer states
-                checkpoint = dist.UDTCheckPoint.from_model(model, with_optimizers=False)
-
-            train.report(metrics=metrics, checkpoint=checkpoint)
-
-        csv_paths = [str(document.path.resolve()) for document in documents]
-
-        train_ray_ds = ray.data.read_csv(csv_paths)
-
-        train_loop_config = {}
-
-        # we cannot pass the model directly to config given config results in OOM very frequently with bigger model.
-        model_ref = ray.put(self._savable_state.model.get_model())
-
-        # If this is a file based license, it will assume the license to available at the same location on each of the
-        # machine
-        licensing_lambda = None
-        if hasattr(thirdai._thirdai, "licensing"):
-            license_state = thirdai._thirdai.licensing._get_license_state()
-            licensing_lambda = lambda: thirdai._thirdai.licensing._set_license_state(
-                license_state
-            )
-
-        train_loop_config["licensing_lambda"] = licensing_lambda
-        train_loop_config["strong_column_names"] = documents[0].strong_columns
-        train_loop_config["weak_column_names"] = documents[0].weak_columns
-        train_loop_config["learning_rate"] = learning_rate
-        train_loop_config["epochs"] = epochs
-        train_loop_config["batch_size"] = batch_size
-        train_loop_config["metrics"] = metrics
-        train_loop_config["max_in_memory_batches"] = max_in_memory_batches
-        train_loop_config["model_ref"] = model_ref
-        train_loop_config["model_target_col"] = self._savable_state.model.get_id_col()
-        # Note(pratik): We are having an assumption here, that each of the document must have the
-        # same target column
-        train_loop_config["document_target_col"] = documents[0].id_column
-        train_loop_config["log_folder"] = log_folder
-
-        trainer = dist.BoltTrainer(
-            train_loop_per_worker=training_loop_per_worker,
-            train_loop_config=train_loop_config,
-            scaling_config=scaling_config,
-            backend_config=TorchConfig(backend=communication_backend),
-            datasets={"train": train_ray_ds},
-            run_config=run_config,
-        )
-
-        result_and_checkpoint = trainer.fit()
-
-        # TODO(pratik/mritunjay): This will stop working with ray==2.7 if runconfig doesnt specify s3 storage path.
-        # Update: https://github.com/ThirdAILabs/Universe/pull/1784
-        # `run_config` is made required argument in `pretrained_distributed` function
-        model = dist.UDTCheckPoint.get_model(result_and_checkpoint.checkpoint)
-
-        self._savable_state.model.set_model(model)
-
-    def ready_to_search(self) -> bool:
-        """Returns True if documents have been inserted and the model is
-        prepared to serve queries, False otherwise.
-        """
-        return self._savable_state.ready()
-
-    def sources(self) -> Dict[str, Document]:
-        """Returns a mapping from source IDs to their corresponding document
-        objects. This is useful when you need to know the source ID of a
-        document you inserted, e.g. for creating a Sup object for
-        supervised_train().
-        """
-        return self._savable_state.documents.sources()
-
-    def save(self, save_to: Union[str, Path], on_progress: Callable = no_op) -> str:
-        return self._savable_state.save(Path(save_to), on_progress)
-
-    def _resume(
-        self,
-        on_progress: Callable,
-        cancel_state: CancelState,
-        checkpoint_config: CheckpointConfig,
-        callbacks: List[bolt.train.callbacks.Callback] = None,
-    ):
-        documents, ids, resource_name = load_checkpoint(
-            checkpoint_config=checkpoint_config
-        )
-        self._savable_state.documents = documents
-        self._savable_state.model.resume(
-            on_progress=on_progress,
-            cancel_state=cancel_state,
-            checkpoint_config=checkpoint_config.get_mach_config(),
-            callbacks=callbacks,
-        )
-
-        return ids, resource_name
-
-    def _insert_from_start(
-        self,
-        sources: List[Document],
-        train: bool,
-        fast_approximation: bool,
-        num_buckets_to_sample: Optional[int],
-        on_progress: Callable,
-        on_error: Callable,
-        cancel_state: CancelState,
-        max_in_memory_batches: int,
-        variable_length: Optional[data.transformations.VariableLengthConfig],
-        checkpoint_config: CheckpointConfig,
-        callbacks: List[bolt.train.callbacks.Callback] = None,
-        **kwargs,
-    ):
-        documents_copy = copy.deepcopy(self._savable_state.documents)
-        try:
-            intro_and_train, ids = self._savable_state.documents.add(sources)
-        except Exception as e:
-            self._savable_state.documents = documents_copy
-            if on_error is not None:
-                on_error(error_msg=f"Failed to add files. {e.__str__()}")
-                return []
-            raise e
-
-        if checkpoint_config:
-            """
-            We need to store the document manager state so that our label_id -> reference mapping remains consistent on resuming.
-            """
-            make_preinsertion_checkpoint(
-                savable_state=self._savable_state,
-                ids=ids,
-                resource_name=intro_and_train.intro.resource_name(),
-                checkpoint_config=checkpoint_config,
-            )
-
-        self._savable_state.model.index_from_start(
-            intro_documents=intro_and_train.intro,
-            train_documents=intro_and_train.train,
-            num_buckets_to_sample=num_buckets_to_sample,
-            fast_approximation=fast_approximation,
-            should_train=train,
-            on_progress=on_progress,
-            cancel_state=cancel_state,
-            max_in_memory_batches=max_in_memory_batches,
-            variable_length=variable_length,
-            checkpoint_config=(
-                checkpoint_config.get_mach_config() if checkpoint_config else None
-            ),
-            callbacks=callbacks,
-            **kwargs,
-        )
-
-        return ids, intro_and_train.intro.resource_name()
-
-    def insert(
-        self,
-        sources: List[Document],
-        train: bool = True,
-        fast_approximation: bool = True,
-        num_buckets_to_sample: Optional[int] = None,
-        on_progress: Callable = no_op,
-        on_success: Callable = no_op,
-        on_error: Callable = None,
-        cancel_state: CancelState = None,
-        max_in_memory_batches: int = None,
-        variable_length: Optional[
-            data.transformations.VariableLengthConfig
-        ] = data.transformations.VariableLengthConfig(),
-        checkpoint_config: Optional[CheckpointConfig] = None,
-        callbacks: List[bolt.train.callbacks.Callback] = None,
-        **kwargs,
-    ) -> List[str]:
-        """
-        Inserts documents/resources into the database.
-
-        Args:
-            sources (List[Doc]): List of NeuralDB documents to be inserted.
-            train (bool): Optional, defaults True. When True this means that the
-                underlying model in the NeuralDB will undergo unsupervised pretraining
-                on the inserted documents.
-            fast_approximation (bool): Optional, default True. Much faster insertion
-                with a slight drop in performance.
-            num_buckets_to_sample (Optional[int]): Used to control load balacing when
-                inserting entities into the NeuralDB.
-            on_progress (Callable): Optional, a callback that is called at intervals
-                as documents are inserted.
-            on_success (Callable): Optional, a callback that is invoked when document
-                insertion is finished successfully.
-            on_error (Callable): Optional, a callback taht is invoked if an error occurs
-                during insertion.
-            cancel_state (CancelState): An object that can be used to stop an ongoing
-                insertion. Primarily used for PocketLLM.
-            max_in_memory_batches (int): Optional, default None. When supplied this limits
-                the maximum amount of data that is loaded into memory at once during training.
-                Useful for lower memory paradigms or with large datasets.
-            checkpoint_config (CheckpointConfig): Optional, default None. Configuration for checkpointing during insertion. No checkpoints are created if checkpoint_config is unspecified.
-
-        Returns:
-            A list of the ids assigned to the inserted documents.
-        """
-        if checkpoint_config and checkpoint_config.resume_from_checkpoint:
-            ids, resource_name = self._resume(
-                on_progress=on_progress,
-                cancel_state=cancel_state,
-                checkpoint_config=checkpoint_config,
-                callbacks=callbacks,
-            )
-        else:
-            ids, resource_name = self._insert_from_start(
-                sources=sources,
-                train=train,
-                fast_approximation=fast_approximation,
-                num_buckets_to_sample=num_buckets_to_sample,
-                on_progress=on_progress,
-                on_error=on_error,
-                cancel_state=cancel_state,
-                max_in_memory_batches=max_in_memory_batches,
-                variable_length=variable_length,
-                checkpoint_config=checkpoint_config,
-                callbacks=callbacks,
-                **kwargs,
-            )
-
-        self._savable_state.logger.log(
-            session_id=self._user_id,
-            action="Train",
-            args={"files": resource_name},
-        )
-
-        if checkpoint_config:
-            # Once we have saved the model, we will delete the ndb checkpoint and save updated neural db with trained models.
-            make_training_checkpoint(
-                savable_state=self._savable_state, checkpoint_config=checkpoint_config
-            )
-
-        on_success()
-
-        return ids
-
-    def delete(self, source_ids: List[str]):
-        """Deletes documents from the NeuralDB."""
-        deleted_entities = self._savable_state.documents.delete(source_ids)
-        self._savable_state.model.delete_entities(deleted_entities)
-        self._savable_state.logger.log(
-            session_id=self._user_id, action="delete", args={"source_ids": source_ids}
-        )
-
-    def clear_sources(self) -> None:
-        """Removes all documents stored in the NeuralDB."""
-        self._savable_state.documents.clear()
-        self._savable_state.model.forget_documents()
-
-    def _get_query_references(
-        self,
-        query: str,
-        result_ids: List[Tuple[int, float, str]],
-        top_k: int,
-        rerank: bool,
-        rerank_threshold,
-        top_k_threshold,
-    ):
-        references = []
-        for rid, score, retriever in result_ids:
-            ref = self._savable_state.documents.reference(rid)
-            ref._score = score
-            ref._retriever = retriever
-            references.append(ref)
-
-        if rerank:
-            keep, to_rerank = NeuralDB._split_references_for_reranking(
-                references,
-                rerank_threshold,
-                average_top_k_scores=top_k_threshold if top_k_threshold else top_k,
-            )
-
-            ranker = thirdai.dataset.KeywordOverlapRanker()
-            reranked_indices, reranked_scores = ranker.rank(
-                query, [ref.text for ref in to_rerank]
-            )
-            reranked_scores = NeuralDB._scale_reranked_scores(
-                original=[ref.score for ref in to_rerank],
-                reranked=reranked_scores,
-                leq=keep[-1].score if len(keep) > 0 else 1.0,
-            )
-
-            reranked = [to_rerank[i] for i in reranked_indices]
-            for i, ref in enumerate(reranked):
-                ref._score = reranked_scores[i]
-            references = (keep + reranked)[:top_k]
-
-        return references
-
-    @staticmethod
-    def _split_references_for_reranking(
-        references,
-        rerank_threshold,
-        average_top_k_scores,
-    ):
-        if rerank_threshold is None:
-            rerank_start = 0
-        else:
-            scores = np.array([ref.score for ref in references])
-            mean_score = np.mean(scores[:average_top_k_scores])
-            rerank_start = np.searchsorted(
-                -scores, -rerank_threshold * mean_score, side="right"
-            )
-        return references[:rerank_start], references[rerank_start:]
-
-    @staticmethod
-    def _scale_reranked_scores(
-        original: List[float], reranked: List[float], leq: float
-    ):
-        """The scores returned by the reranker are not in the same scale as
-        the original score. To fix this, transform the reranked scores such that
-        they are in the same range as the original scores.
-        """
-        if len(original) == 0:
-            return []
-        reranked_delta = reranked[0] - reranked[-1]
-        if reranked_delta == 0:
-            return [original[0] for _ in reranked]
-        original_delta = original[0] - original[-1]
-        delta_scaler = original_delta / reranked_delta
-        return [
-            original[-1] + (score - reranked[-1]) * delta_scaler for score in reranked
-        ]
-
-    def search(
-        self,
-        query: str,
-        top_k: int,
-        constraints=None,
-        rerank=False,
-        top_k_rerank=100,
-        rerank_threshold=1.5,
-        top_k_threshold=None,
-        retriever=None,
-        label_probing=False,
-        mach_first=False,
-    ) -> List[Reference]:
-        """
-        Searches the contents of the NeuralDB for documents relevant to the given query.
-
-        Args:
-            query (str): The query to search with.
-            top_k (int): The number of results to return.
-            constraints (Dict[str, Any]): A dictionary containing constraints to
-                apply to the metadata field of each document in the NeuralDB. This
-                allows for queries that will only return results with a certain property.
-                The constrains are in the form {"metadata_key": <constraint>} where
-                <constraint> is either an explicit value for the key in the metadata,
-                or a Filter object.
-            rerank (bool): Optional, default False. When True an additional reranking
-                step is applied to results.
-            top_k_rerank (int): Optional, default 100. If rerank=True then this argument
-                determines how many candidates are retrieved, before reranking and
-                returning the top_k.
-            rerank_threshold (float): Optional, default 1.5. In reranking all candidates
-                with a score under a certain threshold are reranked. This threshold
-                is computed as this argument (`rerank_threshold`) times the average score
-                over the first top_k_threshold candidates. Candidates with scores lower
-                than this threshold will be reranked. Thus, increasing this value
-                causes more candidates to be reranked.
-            top_k_threshold (Optional[float]): Optional, default None, which means
-                the arg `top_k` will be used. If specified this argument controls
-                how many of the top candidates' scores are averaged to obtain the
-                mean that is used to determine which candidates are reranked. For
-                example passing rerank_threshold=2 and top_k_threshold=4 means that
-                the scores of the top 4 elements are averaged, and all elements below
-                2x this average are reranked.
-            retriever (Optional[str]): Optional, default None. This arg controls which
-                retriever to use for search when a hybrid retrieval model is used. Passing
-                None means that NeuralDB will automatically decide which retrievers (or
-                combination of retrievers) to use.
-
-        Returns:
-            List[Reference]: A list of Reference objects. Each reference object contains text data matching
-            the query, along with information about which document contained that text.
-
-        Examples:
-            >>> ndb.search("what is ...", top_k=5)
-            >>> ndb.search("what is ...", top_k=5, constraints={"file_type": "pdf", "file_created", GreaterThan(10)})
-        """
-        return self.search_batch(
-            queries=[query],
-            top_k=top_k,
-            constraints=constraints,
-            rerank=rerank,
-            top_k_rerank=top_k_rerank,
-            rerank_threshold=rerank_threshold,
-            top_k_threshold=top_k_threshold,
-            retriever=retriever,
-            label_probing=label_probing,
-            mach_first=mach_first,
-        )[0]
-
-    def search_batch(
-        self,
-        queries: List[str],
-        top_k: int,
-        constraints=None,
-        rerank=False,
-        top_k_rerank=100,
-        rerank_threshold=1.5,
-        top_k_threshold=None,
-        retriever=None,
-        label_probing=False,
-        mach_first=False,
-    ):
-        """
-        Runs search on a batch of queries for much faster throughput.
-
-        Args:
-            queries (List[str]): The queries to search.
-
-        Returns:
-            List[List[Reference]]: Combines each result of db.search into a list.
-        """
-        matching_entities = None
-        top_k_to_search = top_k_rerank if rerank else top_k
-        if constraints:
-            matching_entities = self._savable_state.documents.entity_ids_by_constraints(
-                constraints
-            )
-            queries_result_ids = self._savable_state.model.score(
-                samples=queries,
-                entities=[matching_entities] * len(queries),
-                n_results=top_k_to_search,
-            )
-        else:
-            queries_result_ids = self._savable_state.model.infer_labels(
-                samples=queries,
-                n_results=top_k_to_search,
-                retriever="mach" if rerank else retriever,
-                label_probing=label_probing,
-                mach_first=mach_first,
-            )
-
-        return [
-            self._get_query_references(
-                query, result_ids, top_k, rerank, rerank_threshold, top_k_threshold
-            )
-            for query, result_ids in zip(queries, queries_result_ids)
-        ]
-
-    def reference(self, element_id: int):
-        """Returns a reference containing the text and other information for a given entity id."""
-        return self._savable_state.documents.reference(element_id)
-
-    def _get_text(self, result_id) -> str:
-        return self._savable_state.documents.reference(result_id).text
-
-    def text_to_result(self, text: str, result_id: int, **kwargs) -> None:
-        """Trains NeuralDB to map the given text to the given entity ID.
-        Also known as "upvoting".
-
-        Example:
-            >>> ndb.text_to_result("a new query", result_id=4)
-        """
-        teachers.upvote(
-            model=self._savable_state.model,
-            logger=self._savable_state.logger,
-            user_id=self._user_id,
-            query_id_para=[
-                (text, upvote_id, self._get_text(result_id))
-                for upvote_id in self._savable_state.documents.reference(
-                    result_id
-                ).upvote_ids
-            ],
-            **kwargs,
-        )
-
-    def text_to_result_batch(
-        self, text_id_pairs: List[Tuple[str, int]], **kwargs
-    ) -> None:
-        """Trains NeuralDB to map the given texts to the given entity IDs.
-        Also known as "batch upvoting".
-        """
-        query_id_para = [
-            (query, upvote_id, self._get_text(result_id))
-            for query, result_id in text_id_pairs
-            for upvote_id in self._savable_state.documents.reference(
-                result_id
-            ).upvote_ids
-        ]
-        teachers.upvote(
-            model=self._savable_state.model,
-            logger=self._savable_state.logger,
-            user_id=self._user_id,
-            query_id_para=query_id_para,
-            **kwargs,
-        )
-
-    def associate(
-        self, source: str, target: str, strength: Strength = Strength.Strong, **kwargs
-    ):
-        """
-        Teaches the underlying model in the NeuralDB that two different texts
-        correspond to similar concepts or queries.
-
-        Args:
-            source (str): The source is the new text you want to teach the model about.
-            target (str): The target is the known text that is provided to the model
-                as an example of the type of information or query the source resembles.
-
-        Examples:
-            >>> ndb.associate("asap", "as soon as possible")
-            >>> ndb.associate("what is a 401k", "explain different types of retirement savings")
-        """
-        top_k = self._get_associate_top_k(strength)
-        teachers.associate(
-            model=self._savable_state.model,
-            logger=self._savable_state.logger,
-            user_id=self._user_id,
-            text_pairs=[(source, target)],
-            top_k=top_k,
-            **kwargs,
-        )
-
-    def associate_batch(
-        self,
-        text_pairs: List[Tuple[str, str]],
-        strength: Strength = Strength.Strong,
-        **kwargs,
-    ):
-        """Same as associate, but the process is applied to a batch of (source, target) pairs at once."""
-        top_k = self._get_associate_top_k(strength)
-        teachers.associate(
-            model=self._savable_state.model,
-            logger=self._savable_state.logger,
-            user_id=self._user_id,
-            text_pairs=text_pairs,
-            top_k=top_k,
-            **kwargs,
-        )
-
-    def _get_associate_top_k(self, strength):
-        if strength == Strength.Weak:
-            return 3
-        elif strength == Strength.Medium:
-            return 5
-        elif strength == Strength.Strong:
-            return 7
-        else:
-            return 7
-
-    def supervised_train(
-        self,
-        data: List[Sup],
-        learning_rate=0.0001,
-        epochs=3,
-        batch_size: Optional[int] = None,
-        max_in_memory_batches: Optional[int] = None,
-        metrics: List[str] = [],
-        callbacks: List[bolt.train.callbacks.Callback] = [],
-        checkpoint_config: Optional[CheckpointConfig] = None,
-        **kwargs,
-    ):
-        """
-        Train on supervised datasets that correspond to specific sources.
-        Suppose you inserted a "sports" product catalog and a "furniture"
-        product catalog. You also have supervised datasets - pairs of queries
-        and correct products - for both categories. You can use this method to
-        train NeuralDB on these supervised datasets.
-
-        Args:
-            data (List[Sup]): Supervised training samples.
-            learning_rate (float): Optional. The learning rate to use for training.
-            epochs (int): Optional. The number of epochs to train for.
-        """
-        doc_manager = self._savable_state.documents
-        query_col = self._savable_state.model.get_query_col()
-        self._savable_state.model.train_on_supervised_data_source(
-            supervised_data_source=SupDataSource(
-                doc_manager=doc_manager,
-                query_col=query_col,
-                data=data,
-                id_delimiter=self._savable_state.model.get_id_delimiter(),
-            ),
-            learning_rate=learning_rate,
-            epochs=epochs,
-            batch_size=batch_size,
-            max_in_memory_batches=max_in_memory_batches,
-            metrics=metrics,
-            callbacks=callbacks,
-            disable_finetunable_retriever=kwargs.get(
-                "disable_finetunable_retriever", True
-            ),
-            checkpoint_config=checkpoint_config,
-        )
-
-        if checkpoint_config:
-            make_training_checkpoint(self._savable_state, checkpoint_config)
-
-    def supervised_train_with_ref_ids(
-        self,
-        csv: str = None,
-        query_column: str = None,
-        id_column: str = None,
-        id_delimiter: str = None,
-        queries: Sequence[str] = None,
-        labels: Sequence[Sequence[int]] = None,
-        learning_rate=0.0001,
-        epochs=3,
-        batch_size: Optional[int] = None,
-        max_in_memory_batches: Optional[int] = None,
-        metrics: List[str] = [],
-        callbacks: List[bolt.train.callbacks.Callback] = [],
-        checkpoint_config: Optional[CheckpointConfig] = None,
-        **kwargs,
-    ):
-        """Train on supervised datasets that correspond to specific sources.
-        Suppose you inserted a "sports" product catalog and a "furniture"
-        product catalog. You also have supervised datasets - pairs of queries
-        and correct products - for both categories. You can use this method to
-        train NeuralDB on these supervised datasets. This method must be invoked
-        with either A) a csv file with the query and id columns within it, or B) an
-        explicit list of queries and expected labels.
-        """
-        doc_manager = self._savable_state.documents
-        model_query_col = self._savable_state.model.get_query_col()
-        self._savable_state.model.train_on_supervised_data_source(
-            supervised_data_source=SupDataSource(
-                doc_manager=doc_manager,
-                query_col=model_query_col,
-                data=[
-                    Sup(
-                        csv=csv,
-                        query_column=query_column,
-                        id_column=id_column,
-                        id_delimiter=id_delimiter,
-                        queries=queries,
-                        labels=labels,
-                        uses_db_id=True,
-                    )
-                ],
-                id_delimiter=self._savable_state.model.get_id_delimiter(),
-            ),
-            learning_rate=learning_rate,
-            epochs=epochs,
-            batch_size=batch_size,
-            max_in_memory_batches=max_in_memory_batches,
-            metrics=metrics,
-            callbacks=callbacks,
-            disable_finetunable_retriever=kwargs.get(
-                "disable_finetunable_retriever", True
-            ),
-            checkpoint_config=checkpoint_config,
-        )
-        if checkpoint_config:
-            make_training_checkpoint(self._savable_state, checkpoint_config)
-
-    def get_associate_samples(self):
-        """Get past associate() and associate_batch() samples from NeuralDB logs."""
-        logs = self._savable_state.logger.get_logs()
-
-        associate_logs = logs[logs["action"] == "associate"]
-        associate_samples = []
-        for _, row in associate_logs.iterrows():
-            for source, target in row["args"]["pairs"]:
-                associate_samples.append((source, target))
-
-        return associate_samples
-
-    def get_upvote_samples(self):
-        """Get past text_to_result() and text_to_result_batch() samples from
-        NeuralDB logs.
-        """
-        logs = self._savable_state.logger.get_logs()
-
-        upvote_associate_samples = []
-        upvote_logs = logs[logs["action"] == "upvote"]
-        for _, row in upvote_logs.iterrows():
-            if "query_id_para" in row["args"]:
-                for source, _, target in row["args"]["query_id_para"]:
-                    upvote_associate_samples.append((source, target))
-
-        return upvote_associate_samples
-
-    def get_rlhf_samples(self):
-        """Get past associate(), associate_batch(), text_to_result(), and
-        text_to_result_batch() samples from NeuralDB logs.
-        """
-        return self.get_associate_samples() + self.get_upvote_samples()
-
-    def retrain(
-        self,
-        text_pairs: List[Tuple[str, str]] = [],
-        learning_rate: float = 0.0001,
-        epochs: int = 3,
-        strength: Strength = Strength.Strong,
-    ):
-        """Train NeuralDB on all inserted documents and logged RLHF samples."""
-        doc_manager = self._savable_state.documents
-
-        if not text_pairs:
-            text_pairs = self.get_rlhf_samples()
-
-        self._savable_state.model.retrain(
-            balancing_data=doc_manager.get_data_source(),
-            source_target_pairs=text_pairs,
-            n_buckets=self._get_associate_top_k(strength),
-            learning_rate=learning_rate,
-            epochs=epochs,
-        )
+import copy
+import shutil
+from enum import Enum
+from pathlib import Path
+from typing import Callable, Dict, List, Optional, Sequence, Tuple, Union
+
+import numpy as np
+import pandas as pd
+import thirdai
+from thirdai._thirdai import bolt, data
+
+from . import loggers, teachers
+from .documents import CSV, Document, DocumentManager, Reference
+from .models.finetunable_retriever import FinetunableRetriever
+from .models.mach import Mach
+from .models.mach_mixture_model import MachMixture
+from .models.model_interface import CancelState
+from .savable_state import (
+    State,
+    load_checkpoint,
+    make_preinsertion_checkpoint,
+    make_training_checkpoint,
+)
+from .supervised_datasource import Sup, SupDataSource
+from .trainer.checkpoint_config import CheckpointConfig
+
+Strength = Enum("Strength", ["Weak", "Medium", "Strong"])
+
+
+def no_op(*args, **kwargs):
+    pass
+
+
+class NeuralDB:
+    """
+    NeuralDB is a search and retrieval system that can be used to search over
+    knowledge bases and documents. It can also be used in RAG pipelines for the
+    search retrieval phase.
+
+    Examples:
+        >>> ndb = NeuralDB()
+        >>> ndb.insert([CSV(...), PDF(...), DOCX(...)])
+        >>> results = ndb.search("how to make chocolate chip cookies")
+    """
+
+    def __init__(
+        self,
+        user_id: str = "user",
+        num_shards: int = 1,
+        num_models_per_shard: int = 1,
+        retriever="finetunable_retriever",
+        low_memory=None,
+        **kwargs,
+    ) -> None:
+        """
+        Constructs an empty NeuralDB.
+
+        Args:
+            user_id (str): Optional, used to identify user/session in logging.
+            retriever (str): One of 'finetunable_retriever', 'mach', or 'hybrid'.
+                Identifies which retriever to use as the backend. Defaults to
+                'finetunable_retriever'.
+
+        Returns:
+            A NeuralDB.
+        """
+        if low_memory is not None:
+            print(
+                "Warning: 'low_memory' flag will be deprecated soon in the NeuralDB constructor. Please pass 'retriever=' instead."
+            )
+            if low_memory == True:
+                retriever = "finetunable_retriever"
+            elif low_memory == False:
+                retriever = "hybrid"
+        self._user_id: str = user_id
+
+        # The savable_state kwarg is only used in static constructor methods
+        # and should not be used by an external user.
+        # We read savable_state from kwargs so that it doesn't appear in the
+        # arguments list and confuse users.
+        if "savable_state" not in kwargs:
+            if num_shards <= 0:
+                raise Exception(
+                    f"Invalid Value Passed for num_shards : {num_shards}."
+                    " NeuralDB can only be initialized with a positive number of"
+                    " shards."
+                )
+            if num_models_per_shard <= 0:
+                raise Exception(
+                    f"Invalid Value Passed for num_models_per_shard : {num_models_per_shard}."
+                    " NeuralDB can only be initialized with a positive number of"
+                    " models per shard."
+                )
+            if retriever == "finetunable_retriever":
+                model = FinetunableRetriever()
+            elif retriever == "mach" or retriever == "hybrid":
+                if num_shards > 1 or num_models_per_shard > 1:
+                    model = MachMixture(
+                        num_shards=num_shards,
+                        num_models_per_shard=num_models_per_shard,
+                        id_col="id",
+                        query_col="query",
+                        hybrid=(retriever == "hybrid"),
+                        **kwargs,
+                    )
+                else:
+                    model = Mach(
+                        id_col="id",
+                        query_col="query",
+                        hybrid=(retriever == "hybrid"),
+                        **kwargs,
+                    )
+            else:
+                raise ValueError(
+                    f"Invalid retriever '{retriever}'. Please use 'finetunable_retriever', 'mach', or 'hybrid'."
+                )
+
+            self._savable_state = State(
+                model, logger=loggers.LoggerList([loggers.InMemoryLogger()])
+            )
+        else:
+            self._savable_state = kwargs["savable_state"]
+
+    @staticmethod
+    def from_checkpoint(
+        checkpoint_path: str,
+        user_id: str = "user",
+        on_progress: Callable = no_op,
+    ):
+        """
+        Constructs a NeuralDB from a checkpoint. This can be used save and reload
+        NeuralDBs, it is also used for loading pretrained NeuralDB models.
+
+        Args:
+            checkpoint_path (str): The path to the checkpoint directory.
+            user_id (str): Optional, used to identify user/session in logging.
+            on_progress (Callable): Optional, callback that can be called as loading the checkpoint progresses.
+
+        Returns:
+            A NeuralDB.
+        """
+        checkpoint_path = Path(checkpoint_path)
+        savable_state = State.load(checkpoint_path, on_progress)
+        if savable_state.model and savable_state.model.get_model():
+            savable_state.model.set_mach_sampling_threshold(0.01)
+        if not isinstance(savable_state.logger, loggers.LoggerList):
+            # TODO(Geordie / Yash): Add DBLogger to LoggerList once ready.
+            savable_state.logger = loggers.LoggerList([savable_state.logger])
+
+        return NeuralDB(user_id, savable_state=savable_state)
+
+    @staticmethod
+    def from_udt(
+        udt: bolt.UniversalDeepTransformer,
+        user_id: str = "user",
+        csv: Optional[str] = None,
+        csv_id_column: Optional[str] = None,
+        csv_strong_columns: Optional[List[str]] = None,
+        csv_weak_columns: Optional[List[str]] = None,
+        csv_reference_columns: Optional[List[str]] = None,
+    ):
+        """
+        Instantiate a NeuralDB, using the given UDT as the underlying model.
+        Usually for porting a pretrained model into the NeuralDB format.
+        Use the optional csv-related arguments to insert the pretraining dataset
+        into the NeuralDB instance.
+
+        Args:
+            udt (bolt.UniversalDeepTransformer): The udt model to use in the NeuralDB.
+            user_id (str): Optional, used to identify user/session in logging.
+            csv (Optional[str]): Optional, default None. The path to the CSV file
+                used to train the udt model. If supplied, the CSV file will be
+                inserted into NeuralDB.
+            csv_id_column (Optional[str]): Optional, default None. The id column
+                of the training dataset. Required only if the data is being inserted via
+                the `csv` arg.
+            csv_strong_columns (Optional[str]): Optional, default None. The strong
+                signal columns from the training data. Required only if the data is
+                being inserted via the `csv` arg.
+            csv_weak_columns (Optional[str]): Optional, default None. The weak signal
+                columns from the training data. Required only if the data is being
+                inserted via the `csv` arg.
+            csv_reference_columns (Optional[str]): Optional, default None. The
+                columns whose data should be returned as search results to queries.
+                Required only if the data is being inserted via the `csv` arg.
+
+        Returns:
+            A NeuralDB.
+        """
+        if csv is None:
+            udt.clear_index()
+
+        udt.enable_rlhf()
+        udt.set_mach_sampling_threshold(0.01)
+        fhr, emb_dim, out_dim = udt.model_dims()
+
+        text_dataset_config = udt.text_dataset_config()
+
+        model = Mach(
+            id_col=text_dataset_config.label_column,
+            id_delimiter=text_dataset_config.label_delimiter,
+            query_col=text_dataset_config.text_column,
+            fhr=fhr,
+            embedding_dimension=emb_dim,
+            extreme_output_dim=out_dim,
+        )
+        model.model = udt
+        logger = loggers.LoggerList([loggers.InMemoryLogger()])
+        savable_state = State(model=model, logger=logger)
+
+        if csv is not None:
+            if (
+                csv_id_column is None
+                or csv_strong_columns is None
+                or csv_weak_columns is None
+                or csv_reference_columns is None
+            ):
+                error_msg = (
+                    "If the `csv` arg is provided, then the following args must also be"
+                    " provided:\n"
+                )
+                error_msg += " - `csv_id_column`\n"
+                error_msg += " - `csv_strong_columns`\n"
+                error_msg += " - `csv_weak_columns`\n"
+                error_msg += " - `csv_reference_columns`\n"
+                raise ValueError(error_msg)
+            csv_doc = CSV(
+                path=csv,
+                id_column=csv_id_column,
+                strong_columns=csv_strong_columns,
+                weak_columns=csv_weak_columns,
+                reference_columns=csv_reference_columns,
+            )
+            savable_state.documents.add([csv_doc])
+            savable_state.model.set_n_ids(csv_doc.size)
+
+        return NeuralDB(user_id, savable_state=savable_state)
+
+    def pretrain_distributed(
+        self,
+        documents,
+        scaling_config,
+        run_config,
+        learning_rate: float = 0.001,
+        epochs: int = 5,
+        batch_size: int = None,
+        metrics: List[str] = [],
+        max_in_memory_batches: Optional[int] = None,
+        communication_backend="gloo",
+        log_folder=None,
+    ):
+        """
+        Pretrains a model in a distributed manner using the provided documents.
+
+        Args:
+            documents: List of documents for pretraining. All the documents must have the same id column.
+            scaling_config: Configuration related to the scaling aspects for Ray trainer. Read
+                https://docs.ray.io/en/latest/train/api/doc/ray.train.ScalingConfig.html
+            run_config: Configuration related to the runtime aspects for Ray trainer. Read
+                https://docs.ray.io/en/latest/train/api/doc/ray.train.RunConfig.html
+                ** Note: We need to specify `storage_path` in `RunConfig` which must be a networked **
+                ** file system or cloud storage path accessible by all workers. (Ray 2.7.0 onwards) **
+            learning_rate (float, optional): Learning rate for the optimizer. Default is 0.001.
+            epochs (int, optional): Number of epochs to train. Default is 5.
+            batch_size (int, optional): Size of each batch for training. If not provided, will be determined automatically.
+            metrics (List[str], optional): List of metrics to evaluate during training. Default is an empty list.
+            max_in_memory_batches (Optional[int], optional): Number of batches to load in memory at once. Useful for
+                streaming support when dataset is too large to fit in memory. If None, all batches will be loaded.
+            communication_backend (str, optional): Bolt Distributed Training uses Torch Communication Backend. This
+                refers to backend for inter-worker communication. Default is "gloo".
+
+        Notes:
+            - Make sure to pass id_column to neural_db.CSV() making sure the ids are in ascending order starting from 0.
+            - The `scaling_config`, `run_config`, and `resume_from_checkpoint` arguments are related to the Ray trainer configuration. Read
+                https://docs.ray.io/en/latest/ray-air/trainers.html#trainer-basics
+            - Ensure that the communication backend specified is compatible with the hardware and network setup for MPI/Gloo backend.
+        """
+        if isinstance(self._savable_state.model, MachMixture):
+            raise NotImplementedError(
+                "Distributed Training is not supported for NeuralDB initialized with a"
+                " mixture of experts."
+            )
+        import warnings
+        from distutils.version import LooseVersion
+
+        import ray
+        import thirdai.distributed_bolt as dist
+        from ray.train.torch import TorchConfig
+
+        ray_version = ray.__version__
+        if LooseVersion(ray_version) >= LooseVersion("2.7"):
+            warnings.warn(
+                """
+                Using ray version 2.7 or higher requires specifying a remote or NFS storage path. 
+                Support for local checkpoints has been discontinued in these versions. 
+                Refer to https://github.com/ray-project/ray/issues/37177 for details.
+                """.strip()
+            )
+
+        if not isinstance(documents, list) or not all(
+            isinstance(doc, CSV) for doc in documents
+        ):
+            raise ValueError(
+                "The pretrain_distributed function currently only supports CSV"
+                " documents."
+            )
+
+        def training_loop_per_worker(config):
+            import os
+
+            import thirdai.distributed_bolt as dist
+            from ray import train
+            from thirdai.dataset import RayCsvDataSource
+
+            if config["licensing_lambda"]:
+                config["licensing_lambda"]()
+
+            strong_column_names = config["strong_column_names"]
+            weak_column_names = config["weak_column_names"]
+            learning_rate = config["learning_rate"]
+            epochs = config["epochs"]
+            batch_size = config["batch_size"]
+            metrics = config["metrics"]
+            max_in_memory_batches = config["max_in_memory_batches"]
+            model_ref = config["model_ref"]
+            model_target_column = config["model_target_col"]
+            document_target_col = config["document_target_col"]
+            log_folder = train_loop_config["log_folder"]
+
+            # ray data will automatically split the data if the dataset is passed with key "train"
+            # to training loop. Read https://docs.ray.io/en/latest/ray-air/check-ingest.html#splitting-data-across-workers
+            stream_split_data_iterator = train.get_dataset_shard("train")
+
+            model = ray.get(model_ref)
+
+            if log_folder:
+                if not os.path.exists(log_folder):
+                    print(f"Folder '{log_folder}' does not exist. Creating it...")
+                    os.makedirs(log_folder)
+                    print(f"Folder '{log_folder}' created successfully!")
+                thirdai.logging.setup(
+                    log_to_stderr=False,
+                    path=os.path.join(
+                        log_folder, f"worker-{train.get_context().get_world_rank()}.log"
+                    ),
+                    level="info",
+                )
+
+            metrics = model.coldstart_distributed_on_data_source(
+                data_source=RayCsvDataSource(
+                    stream_split_data_iterator, model_target_column, document_target_col
+                ),
+                strong_column_names=strong_column_names,
+                weak_column_names=weak_column_names,
+                learning_rate=learning_rate,
+                epochs=epochs,
+                batch_size=batch_size,
+                metrics=metrics,
+                max_in_memory_batches=max_in_memory_batches,
+            )
+
+            rank = train.get_context().get_world_rank()
+            checkpoint = None
+            if rank == 0:
+                # Use `with_optimizers=False` to save model without optimizer states
+                checkpoint = dist.UDTCheckPoint.from_model(model, with_optimizers=False)
+
+            train.report(metrics=metrics, checkpoint=checkpoint)
+
+        csv_paths = [str(document.path.resolve()) for document in documents]
+
+        train_ray_ds = ray.data.read_csv(csv_paths)
+
+        train_loop_config = {}
+
+        # we cannot pass the model directly to config given config results in OOM very frequently with bigger model.
+        model_ref = ray.put(self._savable_state.model.get_model())
+
+        # If this is a file based license, it will assume the license to available at the same location on each of the
+        # machine
+        licensing_lambda = None
+        if hasattr(thirdai._thirdai, "licensing"):
+            license_state = thirdai._thirdai.licensing._get_license_state()
+            licensing_lambda = lambda: thirdai._thirdai.licensing._set_license_state(
+                license_state
+            )
+
+        train_loop_config["licensing_lambda"] = licensing_lambda
+        train_loop_config["strong_column_names"] = documents[0].strong_columns
+        train_loop_config["weak_column_names"] = documents[0].weak_columns
+        train_loop_config["learning_rate"] = learning_rate
+        train_loop_config["epochs"] = epochs
+        train_loop_config["batch_size"] = batch_size
+        train_loop_config["metrics"] = metrics
+        train_loop_config["max_in_memory_batches"] = max_in_memory_batches
+        train_loop_config["model_ref"] = model_ref
+        train_loop_config["model_target_col"] = self._savable_state.model.get_id_col()
+        # Note(pratik): We are having an assumption here, that each of the document must have the
+        # same target column
+        train_loop_config["document_target_col"] = documents[0].id_column
+        train_loop_config["log_folder"] = log_folder
+
+        trainer = dist.BoltTrainer(
+            train_loop_per_worker=training_loop_per_worker,
+            train_loop_config=train_loop_config,
+            scaling_config=scaling_config,
+            backend_config=TorchConfig(backend=communication_backend),
+            datasets={"train": train_ray_ds},
+            run_config=run_config,
+        )
+
+        result_and_checkpoint = trainer.fit()
+
+        # TODO(pratik/mritunjay): This will stop working with ray==2.7 if runconfig doesnt specify s3 storage path.
+        # Update: https://github.com/ThirdAILabs/Universe/pull/1784
+        # `run_config` is made required argument in `pretrained_distributed` function
+        model = dist.UDTCheckPoint.get_model(result_and_checkpoint.checkpoint)
+
+        self._savable_state.model.set_model(model)
+
+    def ready_to_search(self) -> bool:
+        """Returns True if documents have been inserted and the model is
+        prepared to serve queries, False otherwise.
+        """
+        return self._savable_state.ready()
+
+    def sources(self) -> Dict[str, Document]:
+        """Returns a mapping from source IDs to their corresponding document
+        objects. This is useful when you need to know the source ID of a
+        document you inserted, e.g. for creating a Sup object for
+        supervised_train().
+        """
+        return self._savable_state.documents.sources()
+
+    def save(self, save_to: Union[str, Path], on_progress: Callable = no_op) -> str:
+        return self._savable_state.save(Path(save_to), on_progress)
+
+    def _resume(
+        self,
+        on_progress: Callable,
+        cancel_state: CancelState,
+        checkpoint_config: CheckpointConfig,
+        callbacks: List[bolt.train.callbacks.Callback] = None,
+    ):
+        documents, ids, resource_name = load_checkpoint(
+            checkpoint_config=checkpoint_config
+        )
+        self._savable_state.documents = documents
+        self._savable_state.model.resume(
+            on_progress=on_progress,
+            cancel_state=cancel_state,
+            checkpoint_config=checkpoint_config.get_mach_config(),
+            callbacks=callbacks,
+        )
+
+        return ids, resource_name
+
+    def _insert_from_start(
+        self,
+        sources: List[Document],
+        train: bool,
+        fast_approximation: bool,
+        num_buckets_to_sample: Optional[int],
+        on_progress: Callable,
+        on_error: Callable,
+        cancel_state: CancelState,
+        max_in_memory_batches: int,
+        variable_length: Optional[data.transformations.VariableLengthConfig],
+        checkpoint_config: CheckpointConfig,
+        callbacks: List[bolt.train.callbacks.Callback] = None,
+        **kwargs,
+    ):
+        documents_copy = copy.deepcopy(self._savable_state.documents)
+        try:
+            intro_and_train, ids = self._savable_state.documents.add(sources)
+        except Exception as e:
+            self._savable_state.documents = documents_copy
+            if on_error is not None:
+                on_error(error_msg=f"Failed to add files. {e.__str__()}")
+                return []
+            raise e
+
+        if checkpoint_config:
+            """
+            We need to store the document manager state so that our label_id -> reference mapping remains consistent on resuming.
+            """
+            make_preinsertion_checkpoint(
+                savable_state=self._savable_state,
+                ids=ids,
+                resource_name=intro_and_train.intro.resource_name(),
+                checkpoint_config=checkpoint_config,
+            )
+
+        self._savable_state.model.index_from_start(
+            intro_documents=intro_and_train.intro,
+            train_documents=intro_and_train.train,
+            num_buckets_to_sample=num_buckets_to_sample,
+            fast_approximation=fast_approximation,
+            should_train=train,
+            on_progress=on_progress,
+            cancel_state=cancel_state,
+            max_in_memory_batches=max_in_memory_batches,
+            variable_length=variable_length,
+            checkpoint_config=(
+                checkpoint_config.get_mach_config() if checkpoint_config else None
+            ),
+            callbacks=callbacks,
+            **kwargs,
+        )
+
+        return ids, intro_and_train.intro.resource_name()
+
+    def insert(
+        self,
+        sources: List[Document],
+        train: bool = True,
+        fast_approximation: bool = True,
+        num_buckets_to_sample: Optional[int] = None,
+        on_progress: Callable = no_op,
+        on_success: Callable = no_op,
+        on_error: Callable = None,
+        cancel_state: CancelState = None,
+        max_in_memory_batches: int = None,
+        variable_length: Optional[
+            data.transformations.VariableLengthConfig
+        ] = data.transformations.VariableLengthConfig(),
+        checkpoint_config: Optional[CheckpointConfig] = None,
+        callbacks: List[bolt.train.callbacks.Callback] = None,
+        **kwargs,
+    ) -> List[str]:
+        """
+        Inserts documents/resources into the database.
+
+        Args:
+            sources (List[Doc]): List of NeuralDB documents to be inserted.
+            train (bool): Optional, defaults True. When True this means that the
+                underlying model in the NeuralDB will undergo unsupervised pretraining
+                on the inserted documents.
+            fast_approximation (bool): Optional, default True. Much faster insertion
+                with a slight drop in performance.
+            num_buckets_to_sample (Optional[int]): Used to control load balacing when
+                inserting entities into the NeuralDB.
+            on_progress (Callable): Optional, a callback that is called at intervals
+                as documents are inserted.
+            on_success (Callable): Optional, a callback that is invoked when document
+                insertion is finished successfully.
+            on_error (Callable): Optional, a callback taht is invoked if an error occurs
+                during insertion.
+            cancel_state (CancelState): An object that can be used to stop an ongoing
+                insertion. Primarily used for PocketLLM.
+            max_in_memory_batches (int): Optional, default None. When supplied this limits
+                the maximum amount of data that is loaded into memory at once during training.
+                Useful for lower memory paradigms or with large datasets.
+            checkpoint_config (CheckpointConfig): Optional, default None. Configuration for checkpointing during insertion. No checkpoints are created if checkpoint_config is unspecified.
+
+        Returns:
+            A list of the ids assigned to the inserted documents.
+        """
+        if checkpoint_config and checkpoint_config.resume_from_checkpoint:
+            ids, resource_name = self._resume(
+                on_progress=on_progress,
+                cancel_state=cancel_state,
+                checkpoint_config=checkpoint_config,
+                callbacks=callbacks,
+            )
+        else:
+            ids, resource_name = self._insert_from_start(
+                sources=sources,
+                train=train,
+                fast_approximation=fast_approximation,
+                num_buckets_to_sample=num_buckets_to_sample,
+                on_progress=on_progress,
+                on_error=on_error,
+                cancel_state=cancel_state,
+                max_in_memory_batches=max_in_memory_batches,
+                variable_length=variable_length,
+                checkpoint_config=checkpoint_config,
+                callbacks=callbacks,
+                **kwargs,
+            )
+
+        self._savable_state.logger.log(
+            session_id=self._user_id,
+            action="Train",
+            args={"files": resource_name},
+        )
+
+        if checkpoint_config:
+            # Once we have saved the model, we will delete the ndb checkpoint and save updated neural db with trained models.
+            make_training_checkpoint(
+                savable_state=self._savable_state, checkpoint_config=checkpoint_config
+            )
+
+        on_success()
+
+        return ids
+
+    def delete(self, source_ids: List[str]):
+        """Deletes documents from the NeuralDB."""
+        deleted_entities = self._savable_state.documents.delete(source_ids)
+        self._savable_state.model.delete_entities(deleted_entities)
+        self._savable_state.logger.log(
+            session_id=self._user_id, action="delete", args={"source_ids": source_ids}
+        )
+
+    def clear_sources(self) -> None:
+        """Removes all documents stored in the NeuralDB."""
+        self._savable_state.documents.clear()
+        self._savable_state.model.forget_documents()
+
+    def _get_query_references(
+        self,
+        query: str,
+        result_ids: List[Tuple[int, float, str]],
+        top_k: int,
+        rerank: bool,
+        rerank_threshold,
+        top_k_threshold,
+    ):
+        references = []
+        for rid, score, retriever in result_ids:
+            ref = self._savable_state.documents.reference(rid)
+            ref._score = score
+            ref._retriever = retriever
+            references.append(ref)
+
+        if rerank:
+            keep, to_rerank = NeuralDB._split_references_for_reranking(
+                references,
+                rerank_threshold,
+                average_top_k_scores=top_k_threshold if top_k_threshold else top_k,
+            )
+
+            ranker = thirdai.dataset.KeywordOverlapRanker()
+            reranked_indices, reranked_scores = ranker.rank(
+                query, [ref.text for ref in to_rerank]
+            )
+            reranked_scores = NeuralDB._scale_reranked_scores(
+                original=[ref.score for ref in to_rerank],
+                reranked=reranked_scores,
+                leq=keep[-1].score if len(keep) > 0 else 1.0,
+            )
+
+            reranked = [to_rerank[i] for i in reranked_indices]
+            for i, ref in enumerate(reranked):
+                ref._score = reranked_scores[i]
+            references = (keep + reranked)[:top_k]
+
+        return references
+
+    @staticmethod
+    def _split_references_for_reranking(
+        references,
+        rerank_threshold,
+        average_top_k_scores,
+    ):
+        if rerank_threshold is None:
+            rerank_start = 0
+        else:
+            scores = np.array([ref.score for ref in references])
+            mean_score = np.mean(scores[:average_top_k_scores])
+            rerank_start = np.searchsorted(
+                -scores, -rerank_threshold * mean_score, side="right"
+            )
+        return references[:rerank_start], references[rerank_start:]
+
+    @staticmethod
+    def _scale_reranked_scores(
+        original: List[float], reranked: List[float], leq: float
+    ):
+        """The scores returned by the reranker are not in the same scale as
+        the original score. To fix this, transform the reranked scores such that
+        they are in the same range as the original scores.
+        """
+        if len(original) == 0:
+            return []
+        reranked_delta = reranked[0] - reranked[-1]
+        if reranked_delta == 0:
+            return [original[0] for _ in reranked]
+        original_delta = original[0] - original[-1]
+        delta_scaler = original_delta / reranked_delta
+        return [
+            original[-1] + (score - reranked[-1]) * delta_scaler for score in reranked
+        ]
+
+    def search(
+        self,
+        query: str,
+        top_k: int,
+        constraints=None,
+        rerank=False,
+        top_k_rerank=100,
+        rerank_threshold=1.5,
+        top_k_threshold=None,
+        retriever=None,
+        label_probing=False,
+        mach_first=False,
+    ) -> List[Reference]:
+        """
+        Searches the contents of the NeuralDB for documents relevant to the given query.
+
+        Args:
+            query (str): The query to search with.
+            top_k (int): The number of results to return.
+            constraints (Dict[str, Any]): A dictionary containing constraints to
+                apply to the metadata field of each document in the NeuralDB. This
+                allows for queries that will only return results with a certain property.
+                The constrains are in the form {"metadata_key": <constraint>} where
+                <constraint> is either an explicit value for the key in the metadata,
+                or a Filter object.
+            rerank (bool): Optional, default False. When True an additional reranking
+                step is applied to results.
+            top_k_rerank (int): Optional, default 100. If rerank=True then this argument
+                determines how many candidates are retrieved, before reranking and
+                returning the top_k.
+            rerank_threshold (float): Optional, default 1.5. In reranking all candidates
+                with a score under a certain threshold are reranked. This threshold
+                is computed as this argument (`rerank_threshold`) times the average score
+                over the first top_k_threshold candidates. Candidates with scores lower
+                than this threshold will be reranked. Thus, increasing this value
+                causes more candidates to be reranked.
+            top_k_threshold (Optional[float]): Optional, default None, which means
+                the arg `top_k` will be used. If specified this argument controls
+                how many of the top candidates' scores are averaged to obtain the
+                mean that is used to determine which candidates are reranked. For
+                example passing rerank_threshold=2 and top_k_threshold=4 means that
+                the scores of the top 4 elements are averaged, and all elements below
+                2x this average are reranked.
+            retriever (Optional[str]): Optional, default None. This arg controls which
+                retriever to use for search when a hybrid retrieval model is used. Passing
+                None means that NeuralDB will automatically decide which retrievers (or
+                combination of retrievers) to use.
+
+        Returns:
+            List[Reference]: A list of Reference objects. Each reference object contains text data matching
+            the query, along with information about which document contained that text.
+
+        Examples:
+            >>> ndb.search("what is ...", top_k=5)
+            >>> ndb.search("what is ...", top_k=5, constraints={"file_type": "pdf", "file_created", GreaterThan(10)})
+        """
+        return self.search_batch(
+            queries=[query],
+            top_k=top_k,
+            constraints=constraints,
+            rerank=rerank,
+            top_k_rerank=top_k_rerank,
+            rerank_threshold=rerank_threshold,
+            top_k_threshold=top_k_threshold,
+            retriever=retriever,
+            label_probing=label_probing,
+            mach_first=mach_first,
+        )[0]
+
+    def search_batch(
+        self,
+        queries: List[str],
+        top_k: int,
+        constraints=None,
+        rerank=False,
+        top_k_rerank=100,
+        rerank_threshold=1.5,
+        top_k_threshold=None,
+        retriever=None,
+        label_probing=False,
+        mach_first=False,
+    ):
+        """
+        Runs search on a batch of queries for much faster throughput.
+
+        Args:
+            queries (List[str]): The queries to search.
+
+        Returns:
+            List[List[Reference]]: Combines each result of db.search into a list.
+        """
+        matching_entities = None
+        top_k_to_search = top_k_rerank if rerank else top_k
+        if constraints:
+            matching_entities = self._savable_state.documents.entity_ids_by_constraints(
+                constraints
+            )
+            queries_result_ids = self._savable_state.model.score(
+                samples=queries,
+                entities=[matching_entities] * len(queries),
+                n_results=top_k_to_search,
+            )
+        else:
+            queries_result_ids = self._savable_state.model.infer_labels(
+                samples=queries,
+                n_results=top_k_to_search,
+                retriever="mach" if rerank else retriever,
+                label_probing=label_probing,
+                mach_first=mach_first,
+            )
+
+        return [
+            self._get_query_references(
+                query, result_ids, top_k, rerank, rerank_threshold, top_k_threshold
+            )
+            for query, result_ids in zip(queries, queries_result_ids)
+        ]
+
+    def reference(self, element_id: int):
+        """Returns a reference containing the text and other information for a given entity id."""
+        return self._savable_state.documents.reference(element_id)
+
+    def _get_text(self, result_id) -> str:
+        return self._savable_state.documents.reference(result_id).text
+
+    def text_to_result(self, text: str, result_id: int, **kwargs) -> None:
+        """Trains NeuralDB to map the given text to the given entity ID.
+        Also known as "upvoting".
+
+        Example:
+            >>> ndb.text_to_result("a new query", result_id=4)
+        """
+        teachers.upvote(
+            model=self._savable_state.model,
+            logger=self._savable_state.logger,
+            user_id=self._user_id,
+            query_id_para=[
+                (text, upvote_id, self._get_text(result_id))
+                for upvote_id in self._savable_state.documents.reference(
+                    result_id
+                ).upvote_ids
+            ],
+            **kwargs,
+        )
+
+    def text_to_result_batch(
+        self, text_id_pairs: List[Tuple[str, int]], **kwargs
+    ) -> None:
+        """Trains NeuralDB to map the given texts to the given entity IDs.
+        Also known as "batch upvoting".
+        """
+        query_id_para = [
+            (query, upvote_id, self._get_text(result_id))
+            for query, result_id in text_id_pairs
+            for upvote_id in self._savable_state.documents.reference(
+                result_id
+            ).upvote_ids
+        ]
+        teachers.upvote(
+            model=self._savable_state.model,
+            logger=self._savable_state.logger,
+            user_id=self._user_id,
+            query_id_para=query_id_para,
+            **kwargs,
+        )
+
+    def associate(
+        self, source: str, target: str, strength: Strength = Strength.Strong, **kwargs
+    ):
+        """
+        Teaches the underlying model in the NeuralDB that two different texts
+        correspond to similar concepts or queries.
+
+        Args:
+            source (str): The source is the new text you want to teach the model about.
+            target (str): The target is the known text that is provided to the model
+                as an example of the type of information or query the source resembles.
+
+        Examples:
+            >>> ndb.associate("asap", "as soon as possible")
+            >>> ndb.associate("what is a 401k", "explain different types of retirement savings")
+        """
+        top_k = self._get_associate_top_k(strength)
+        teachers.associate(
+            model=self._savable_state.model,
+            logger=self._savable_state.logger,
+            user_id=self._user_id,
+            text_pairs=[(source, target)],
+            top_k=top_k,
+            **kwargs,
+        )
+
+    def associate_batch(
+        self,
+        text_pairs: List[Tuple[str, str]],
+        strength: Strength = Strength.Strong,
+        **kwargs,
+    ):
+        """Same as associate, but the process is applied to a batch of (source, target) pairs at once."""
+        top_k = self._get_associate_top_k(strength)
+        teachers.associate(
+            model=self._savable_state.model,
+            logger=self._savable_state.logger,
+            user_id=self._user_id,
+            text_pairs=text_pairs,
+            top_k=top_k,
+            **kwargs,
+        )
+
+    def _get_associate_top_k(self, strength):
+        if strength == Strength.Weak:
+            return 3
+        elif strength == Strength.Medium:
+            return 5
+        elif strength == Strength.Strong:
+            return 7
+        else:
+            return 7
+
+    def supervised_train(
+        self,
+        data: List[Sup],
+        learning_rate=0.0001,
+        epochs=3,
+        batch_size: Optional[int] = None,
+        max_in_memory_batches: Optional[int] = None,
+        metrics: List[str] = [],
+        callbacks: List[bolt.train.callbacks.Callback] = [],
+        checkpoint_config: Optional[CheckpointConfig] = None,
+        **kwargs,
+    ):
+        """
+        Train on supervised datasets that correspond to specific sources.
+        Suppose you inserted a "sports" product catalog and a "furniture"
+        product catalog. You also have supervised datasets - pairs of queries
+        and correct products - for both categories. You can use this method to
+        train NeuralDB on these supervised datasets.
+
+        Args:
+            data (List[Sup]): Supervised training samples.
+            learning_rate (float): Optional. The learning rate to use for training.
+            epochs (int): Optional. The number of epochs to train for.
+        """
+        doc_manager = self._savable_state.documents
+        query_col = self._savable_state.model.get_query_col()
+        self._savable_state.model.train_on_supervised_data_source(
+            supervised_data_source=SupDataSource(
+                doc_manager=doc_manager,
+                query_col=query_col,
+                data=data,
+                id_delimiter=self._savable_state.model.get_id_delimiter(),
+            ),
+            learning_rate=learning_rate,
+            epochs=epochs,
+            batch_size=batch_size,
+            max_in_memory_batches=max_in_memory_batches,
+            metrics=metrics,
+            callbacks=callbacks,
+            disable_finetunable_retriever=kwargs.get(
+                "disable_finetunable_retriever", True
+            ),
+            checkpoint_config=checkpoint_config,
+        )
+
+        if checkpoint_config:
+            make_training_checkpoint(self._savable_state, checkpoint_config)
+
+    def supervised_train_with_ref_ids(
+        self,
+        csv: str = None,
+        query_column: str = None,
+        id_column: str = None,
+        id_delimiter: str = None,
+        queries: Sequence[str] = None,
+        labels: Sequence[Sequence[int]] = None,
+        learning_rate=0.0001,
+        epochs=3,
+        batch_size: Optional[int] = None,
+        max_in_memory_batches: Optional[int] = None,
+        metrics: List[str] = [],
+        callbacks: List[bolt.train.callbacks.Callback] = [],
+        checkpoint_config: Optional[CheckpointConfig] = None,
+        **kwargs,
+    ):
+        """Train on supervised datasets that correspond to specific sources.
+        Suppose you inserted a "sports" product catalog and a "furniture"
+        product catalog. You also have supervised datasets - pairs of queries
+        and correct products - for both categories. You can use this method to
+        train NeuralDB on these supervised datasets. This method must be invoked
+        with either A) a csv file with the query and id columns within it, or B) an
+        explicit list of queries and expected labels.
+        """
+        doc_manager = self._savable_state.documents
+        model_query_col = self._savable_state.model.get_query_col()
+        self._savable_state.model.train_on_supervised_data_source(
+            supervised_data_source=SupDataSource(
+                doc_manager=doc_manager,
+                query_col=model_query_col,
+                data=[
+                    Sup(
+                        csv=csv,
+                        query_column=query_column,
+                        id_column=id_column,
+                        id_delimiter=id_delimiter,
+                        queries=queries,
+                        labels=labels,
+                        uses_db_id=True,
+                    )
+                ],
+                id_delimiter=self._savable_state.model.get_id_delimiter(),
+            ),
+            learning_rate=learning_rate,
+            epochs=epochs,
+            batch_size=batch_size,
+            max_in_memory_batches=max_in_memory_batches,
+            metrics=metrics,
+            callbacks=callbacks,
+            disable_finetunable_retriever=kwargs.get(
+                "disable_finetunable_retriever", True
+            ),
+            checkpoint_config=checkpoint_config,
+        )
+        if checkpoint_config:
+            make_training_checkpoint(self._savable_state, checkpoint_config)
+
+    def get_associate_samples(self):
+        """Get past associate() and associate_batch() samples from NeuralDB logs."""
+        logs = self._savable_state.logger.get_logs()
+
+        associate_logs = logs[logs["action"] == "associate"]
+        associate_samples = []
+        for _, row in associate_logs.iterrows():
+            for source, target in row["args"]["pairs"]:
+                associate_samples.append((source, target))
+
+        return associate_samples
+
+    def get_upvote_samples(self):
+        """Get past text_to_result() and text_to_result_batch() samples from
+        NeuralDB logs.
+        """
+        logs = self._savable_state.logger.get_logs()
+
+        upvote_associate_samples = []
+        upvote_logs = logs[logs["action"] == "upvote"]
+        for _, row in upvote_logs.iterrows():
+            if "query_id_para" in row["args"]:
+                for source, _, target in row["args"]["query_id_para"]:
+                    upvote_associate_samples.append((source, target))
+
+        return upvote_associate_samples
+
+    def get_rlhf_samples(self):
+        """Get past associate(), associate_batch(), text_to_result(), and
+        text_to_result_batch() samples from NeuralDB logs.
+        """
+        return self.get_associate_samples() + self.get_upvote_samples()
+
+    def retrain(
+        self,
+        text_pairs: List[Tuple[str, str]] = [],
+        learning_rate: float = 0.0001,
+        epochs: int = 3,
+        strength: Strength = Strength.Strong,
+    ):
+        """Train NeuralDB on all inserted documents and logged RLHF samples."""
+        doc_manager = self._savable_state.documents
+
+        if not text_pairs:
+            text_pairs = self.get_rlhf_samples()
+
+        self._savable_state.model.retrain(
+            balancing_data=doc_manager.get_data_source(),
+            source_target_pairs=text_pairs,
+            n_buckets=self._get_associate_top_k(strength),
+            learning_rate=learning_rate,
+            epochs=epochs,
+        )
```

## thirdai/neural_db/question_generation.py

 * *Ordering differences only*

```diff
@@ -1,19 +1,19 @@
-from typing import List
-
-from thirdai.gen.questions import QAGenMethod
-
-from .documents import Document
-from .supervised_datasource import Sup
-
-
-def gen_questions(documents: List[Document], generator: QAGenMethod):
-    sups = []
-    for doc in documents:
-        texts = [doc.reference(i).text for i in range(doc.size)]
-        generated_questions = []
-        labels = []
-        for i, questions in enumerate(generator.generate(texts)):
-            generated_questions.extend(questions)
-            labels.extend([[i] for _ in range(len(questions))])
-        sups.append(Sup(queries=generated_questions, labels=labels, source_id=doc.hash))
-    return sups
+from typing import List
+
+from thirdai.gen.questions import QAGenMethod
+
+from .documents import Document
+from .supervised_datasource import Sup
+
+
+def gen_questions(documents: List[Document], generator: QAGenMethod):
+    sups = []
+    for doc in documents:
+        texts = [doc.reference(i).text for i in range(doc.size)]
+        generated_questions = []
+        labels = []
+        for i, questions in enumerate(generator.generate(texts)):
+            generated_questions.extend(questions)
+            labels.extend([[i] for _ in range(len(questions))])
+        sups.append(Sup(queries=generated_questions, labels=labels, source_id=doc.hash))
+    return sups
```

## thirdai/neural_db/savable_state.py

 * *Ordering differences only*

```diff
@@ -1,157 +1,157 @@
-import datetime
-import os
-from pathlib import Path
-from typing import Callable, List
-
-from .documents import DocumentManager
-from .loggers import Logger
-from .models.model_interface import Model
-from .trainer.checkpoint_config import CheckpointConfig
-from .utils import delete_file, delete_folder, pickle_to, unpickle_from
-
-
-def default_checkpoint_name():
-    return Path(f"checkpoint_{datetime.datetime.now()}.ndb")
-
-
-class State:
-    def __init__(self, model: Model, logger: Logger) -> None:
-        self.model = model
-        self.logger = logger
-        self.documents = DocumentManager(
-            id_column=model.get_id_col(),
-            strong_column="strong",
-            weak_column="weak",
-        )
-
-    def ready(self) -> bool:
-        return (
-            self.model is not None
-            and self.logger is not None
-            and self.documents is not None
-            and self.model.searchable
-        )
-
-    @staticmethod
-    def model_pkl_path(directory: Path) -> Path:
-        return directory / "model.pkl"
-
-    @staticmethod
-    def model_meta_path(directory: Path) -> Path:
-        return directory / "model"
-
-    @staticmethod
-    def logger_pkl_path(directory: Path) -> Path:
-        return directory / "logger.pkl"
-
-    @staticmethod
-    def logger_meta_path(directory: Path) -> Path:
-        return directory / "logger"
-
-    @staticmethod
-    def documents_pkl_path(directory: Path) -> Path:
-        return directory / "documents.pkl"
-
-    @staticmethod
-    def documents_meta_path(directory: Path) -> Path:
-        return directory / "documents"
-
-    def save(
-        self,
-        location=default_checkpoint_name(),
-        on_progress: Callable = lambda *args, **kwargs: None,
-    ) -> str:
-        total_steps = 7
-
-        # make directory
-        directory = Path(location)
-        os.makedirs(directory)
-        on_progress(1 / total_steps)
-
-        # pickle model
-        pickle_to(self.model, State.model_pkl_path(directory))
-        on_progress(2 / total_steps)
-        # save model meta
-        os.mkdir(State.model_meta_path(directory))
-        self.model.save_meta(State.model_meta_path(directory))
-        on_progress(3 / total_steps)
-
-        # pickle logger
-        pickle_to(self.logger, State.logger_pkl_path(directory))
-        on_progress(4 / total_steps)
-        # save logger meta
-        os.mkdir(State.logger_meta_path(directory))
-        self.logger.save_meta(State.logger_meta_path(directory))
-        on_progress(5 / total_steps)
-
-        # pickle documents
-        pickle_to(self.documents, State.documents_pkl_path(directory))
-        on_progress(6 / total_steps)
-        # save documents meta
-        os.mkdir(State.documents_meta_path(directory))
-        self.documents.save_meta(State.documents_meta_path(directory))
-        on_progress(7 / total_steps)
-
-        return str(directory)
-
-    @staticmethod
-    def load(location: Path, on_progress: Callable = lambda *args, **kwargs: None):
-        total_steps = 6
-
-        # load model
-        model = unpickle_from(State.model_pkl_path(location))
-        on_progress(1 / total_steps)
-        model.load_meta(State.model_meta_path(location))
-        on_progress(2 / total_steps)
-
-        # load logger
-        logger = unpickle_from(State.logger_pkl_path(location))
-        on_progress(3 / total_steps)
-        logger.load_meta(State.logger_meta_path(location))
-        on_progress(4 / total_steps)
-
-        state = State(model=model, logger=logger)
-
-        # load documents
-        state.documents = unpickle_from(State.documents_pkl_path(location))
-        on_progress(5 / total_steps)
-        state.documents.load_meta(State.documents_meta_path(location))
-        on_progress(6 / total_steps)
-
-        return state
-
-
-def load_checkpoint(checkpoint_config: CheckpointConfig):
-    try:
-        documents, ids, resource_name = unpickle_from(
-            checkpoint_config.pickled_documents_ids_resource_name_path
-        )
-        return documents, ids, resource_name
-    except:
-        raise Exception(
-            "Failed to load"
-            f" '{checkpoint_config.pickled_documents_ids_resource_name_path}'."
-            " Please verify it's a valid document manager checkpoint and the training is"
-            " incomplete."
-        )
-
-
-def make_preinsertion_checkpoint(
-    savable_state: State,
-    ids: List[str],
-    resource_name: str,
-    checkpoint_config: CheckpointConfig,
-):
-    checkpoint_config.checkpoint_dir.mkdir(exist_ok=True, parents=True)
-    # saving the state of the document manager
-    pickle_to(
-        (savable_state.documents, ids, resource_name),
-        checkpoint_config.pickled_documents_ids_resource_name_path,
-    )
-
-
-def make_training_checkpoint(savable_state: State, checkpoint_config: CheckpointConfig):
-    # removing last trained ndb
-    delete_folder(checkpoint_config.ndb_trained_path)
-    savable_state.save(location=checkpoint_config.ndb_trained_path)
-    delete_file(checkpoint_config.pickled_documents_ids_resource_name_path)
+import datetime
+import os
+from pathlib import Path
+from typing import Callable, List
+
+from .documents import DocumentManager
+from .loggers import Logger
+from .models.model_interface import Model
+from .trainer.checkpoint_config import CheckpointConfig
+from .utils import delete_file, delete_folder, pickle_to, unpickle_from
+
+
+def default_checkpoint_name():
+    return Path(f"checkpoint_{datetime.datetime.now()}.ndb")
+
+
+class State:
+    def __init__(self, model: Model, logger: Logger) -> None:
+        self.model = model
+        self.logger = logger
+        self.documents = DocumentManager(
+            id_column=model.get_id_col(),
+            strong_column="strong",
+            weak_column="weak",
+        )
+
+    def ready(self) -> bool:
+        return (
+            self.model is not None
+            and self.logger is not None
+            and self.documents is not None
+            and self.model.searchable
+        )
+
+    @staticmethod
+    def model_pkl_path(directory: Path) -> Path:
+        return directory / "model.pkl"
+
+    @staticmethod
+    def model_meta_path(directory: Path) -> Path:
+        return directory / "model"
+
+    @staticmethod
+    def logger_pkl_path(directory: Path) -> Path:
+        return directory / "logger.pkl"
+
+    @staticmethod
+    def logger_meta_path(directory: Path) -> Path:
+        return directory / "logger"
+
+    @staticmethod
+    def documents_pkl_path(directory: Path) -> Path:
+        return directory / "documents.pkl"
+
+    @staticmethod
+    def documents_meta_path(directory: Path) -> Path:
+        return directory / "documents"
+
+    def save(
+        self,
+        location=default_checkpoint_name(),
+        on_progress: Callable = lambda *args, **kwargs: None,
+    ) -> str:
+        total_steps = 7
+
+        # make directory
+        directory = Path(location)
+        os.makedirs(directory)
+        on_progress(1 / total_steps)
+
+        # pickle model
+        pickle_to(self.model, State.model_pkl_path(directory))
+        on_progress(2 / total_steps)
+        # save model meta
+        os.mkdir(State.model_meta_path(directory))
+        self.model.save_meta(State.model_meta_path(directory))
+        on_progress(3 / total_steps)
+
+        # pickle logger
+        pickle_to(self.logger, State.logger_pkl_path(directory))
+        on_progress(4 / total_steps)
+        # save logger meta
+        os.mkdir(State.logger_meta_path(directory))
+        self.logger.save_meta(State.logger_meta_path(directory))
+        on_progress(5 / total_steps)
+
+        # pickle documents
+        pickle_to(self.documents, State.documents_pkl_path(directory))
+        on_progress(6 / total_steps)
+        # save documents meta
+        os.mkdir(State.documents_meta_path(directory))
+        self.documents.save_meta(State.documents_meta_path(directory))
+        on_progress(7 / total_steps)
+
+        return str(directory)
+
+    @staticmethod
+    def load(location: Path, on_progress: Callable = lambda *args, **kwargs: None):
+        total_steps = 6
+
+        # load model
+        model = unpickle_from(State.model_pkl_path(location))
+        on_progress(1 / total_steps)
+        model.load_meta(State.model_meta_path(location))
+        on_progress(2 / total_steps)
+
+        # load logger
+        logger = unpickle_from(State.logger_pkl_path(location))
+        on_progress(3 / total_steps)
+        logger.load_meta(State.logger_meta_path(location))
+        on_progress(4 / total_steps)
+
+        state = State(model=model, logger=logger)
+
+        # load documents
+        state.documents = unpickle_from(State.documents_pkl_path(location))
+        on_progress(5 / total_steps)
+        state.documents.load_meta(State.documents_meta_path(location))
+        on_progress(6 / total_steps)
+
+        return state
+
+
+def load_checkpoint(checkpoint_config: CheckpointConfig):
+    try:
+        documents, ids, resource_name = unpickle_from(
+            checkpoint_config.pickled_documents_ids_resource_name_path
+        )
+        return documents, ids, resource_name
+    except:
+        raise Exception(
+            "Failed to load"
+            f" '{checkpoint_config.pickled_documents_ids_resource_name_path}'."
+            " Please verify it's a valid document manager checkpoint and the training is"
+            " incomplete."
+        )
+
+
+def make_preinsertion_checkpoint(
+    savable_state: State,
+    ids: List[str],
+    resource_name: str,
+    checkpoint_config: CheckpointConfig,
+):
+    checkpoint_config.checkpoint_dir.mkdir(exist_ok=True, parents=True)
+    # saving the state of the document manager
+    pickle_to(
+        (savable_state.documents, ids, resource_name),
+        checkpoint_config.pickled_documents_ids_resource_name_path,
+    )
+
+
+def make_training_checkpoint(savable_state: State, checkpoint_config: CheckpointConfig):
+    # removing last trained ndb
+    delete_folder(checkpoint_config.ndb_trained_path)
+    savable_state.save(location=checkpoint_config.ndb_trained_path)
+    delete_file(checkpoint_config.pickled_documents_ids_resource_name_path)
```

## thirdai/neural_db/sharded_documents.py

 * *Ordering differences only*

```diff
@@ -1,246 +1,246 @@
-import random
-import tempfile
-from collections import defaultdict
-from typing import List, Union
-
-from .documents import CSV, DocumentDataSource
-from .supervised_datasource import Sup, SupDataSource
-
-
-class DataLoadMultiplexer:
-    """
-    A data loader that efficiently handles large datasets by segmenting them into smaller,
-    manageable temporary CSV files. This approach is particularly useful for processing
-    large datasets that do not fit entirely into memory.
-
-    The class optimizes memory usage by reading and writing data line by line,
-    rather than loading entire datasets into DataFrames. This method reduces the
-    memory footprint, especially when dealing with large datasets, and avoids
-    the overhead of storing extra DataFrame objects in memory.
-
-    Attributes:
-        num_segments (int): Number of segments to divide the data into.
-        flush_frequency (int): Frequency at which to flush data to the temporary files,
-                               reducing memory usage during processing.
-
-    Methods:
-        create_segments_with_data_source(data_source, label_to_segment_map, update_index):
-            Creates data segments based on the provided data source and label mapping,
-            optionally sharding the data using an index.
-
-    The class utilizes temporary files to store individual data segments, which are then
-    processed independently. This design allows for efficient data handling and scalability,
-    making it suitable for large-scale data processing tasks where memory optimization is crucial.
-
-    NOTE: This multiplexer assumes that the id column is the first element of the csv lines yielded by the line iterator of the data source. And hence, this multiplexer is only supposed to be used with DataSources with the above behaviour.
-    """
-
-    def __init__(self, num_segments, flush_frequency=1_000_000):
-        self.num_segments = num_segments
-        self.flush_frequency = flush_frequency
-        self.seed = 42
-
-    def _generate_temp_csvs(self):
-        """
-        Stores a list of dataframes in temporary files so that they can be read as CSV files later.
-        """
-        segment_prefix = f"{random.randint(100000, 999999)}"
-        segment_filenames = []
-        # We need to store the segment objects so that we can delete the files once we are done with sharding and creating a new dataframe
-        segment_objects = []
-        for index in range(self.num_segments):
-            temp_file = tempfile.NamedTemporaryFile(
-                mode="w",
-                delete=True,
-                suffix=".csv",
-                prefix=f"{segment_prefix}_{index}_",
-            )
-
-            segment_filenames.append(temp_file.name)
-            segment_objects.append(temp_file)
-        return segment_filenames, segment_objects
-
-    def _create_segments_with_segment_map(
-        self,
-        data_source: Union[DocumentDataSource, SupDataSource],
-        label_to_segment_map,
-    ):
-        segment_filenames, segment_objects = self._generate_temp_csvs()
-
-        current_index = 0
-
-        if isinstance(data_source, DocumentDataSource):
-            line_iterator = data_source._get_line_iterator()
-        if isinstance(data_source, SupDataSource):
-            # SupDataSource's line iterator can return multiple labels in the same line concatened by its id_delimiter and hence, we pass concat_labels = False so that each line has just one label.
-            line_iterator = data_source._get_line_iterator(concat_labels=False)  # type: ignore
-
-        for data in line_iterator:  # type: ignore
-            # header
-            if current_index == 0:
-                for segments in segment_objects:
-                    segments.write(data)
-            else:
-                current_label = int(data.split(",", 1)[0])
-                # TODO(pratik/shubh): Having list as map values is for experiments,
-                # we would be just having one elment in list for each index. We should
-                # remove this going forward.
-                if current_label not in label_to_segment_map:
-                    raise ValueError(
-                        "Label '{}' is not in 'label_to_segment_map'. Ensure it is"
-                        " included if sharding by index.".format(current_label)
-                    )
-                current_segment = label_to_segment_map[current_label][-1]
-                segment_objects[current_segment].write("\n" + data)
-
-            current_index += 1
-            if current_index % self.flush_frequency == 0:
-                for segment in segment_objects:
-                    segment.flush()
-
-        for segment in segment_objects:
-            segment.flush()
-
-        data_source.restart()
-        return (
-            segment_filenames,
-            segment_objects,
-            label_to_segment_map,
-        )
-
-    def update_label_segment_map(self, data_source, label_to_segment_map):
-        indices = data_source.indices()
-        random.seed(self.seed)
-        random.shuffle(indices)
-        for randomised_index in indices:
-            label_to_segment_map[randomised_index].append(
-                randomised_index % self.num_segments
-            )
-
-    def create_segments_with_data_source(
-        self, data_source, label_to_segment_map, update_index
-    ):
-        if update_index:
-            self.update_label_segment_map(data_source, label_to_segment_map)
-        else:
-            if len(label_to_segment_map) == 0:
-                raise Exception("label_to_segment_map is empty")
-
-        return self._create_segments_with_segment_map(data_source, label_to_segment_map)
-
-
-def verify_data_source_type(data_source, valid_types: List, operation_name: str):
-    if not any(isinstance(data_source, data_type) for data_type in valid_types):
-        valid_types_str = ", ".join([data_type.__name__ for data_type in valid_types])
-        raise TypeError(
-            f"{operation_name} not supported for data source of the type"
-            f" {type(data_source)}. Expected types are: {valid_types_str}"
-        )
-
-
-def transform_shard_to_datasource(
-    original_data_source: Union[DocumentDataSource, SupDataSource],
-    shard_path,
-    shard_object,
-):
-    """
-    We assume that the shard is stored as a CSV on the machine. Depending on the underlying original data source, we create a new data source from the shard with the same attributes except that the documents are loaded from the shard.
-    """
-    verify_data_source_type(
-        data_source=original_data_source,
-        valid_types=[DocumentDataSource, SupDataSource],
-        operation_name="transform_shard_to_datasource",
-    )
-    if isinstance(original_data_source, DocumentDataSource):
-        data_source = DocumentDataSource(
-            id_column=original_data_source.id_column,
-            strong_column=original_data_source.strong_column,
-            weak_column=original_data_source.weak_column,
-        )
-        csv_doc = CSV(
-            path=shard_path,
-            id_column=original_data_source.id_column,
-            strong_columns=[original_data_source.strong_column],
-            weak_columns=[original_data_source.weak_column],
-            has_offset=True,
-        )
-
-        shard_object.close()
-        data_source.add(document=csv_doc, start_id=0)
-        return data_source
-
-    if isinstance(original_data_source, SupDataSource):
-        sup = Sup(
-            csv=shard_path,
-            query_column=original_data_source.query_col,
-            id_column=original_data_source.id_column,
-            id_delimiter=original_data_source.id_delimiter,
-            uses_db_id=True,
-        )
-        shard_object.close()
-        return SupDataSource(
-            doc_manager=original_data_source.doc_manager,
-            query_col=original_data_source.query_col,
-            data=[sup],
-            id_delimiter=original_data_source.id_delimiter,
-            id_column=original_data_source.id_column,
-        )
-
-
-def shard_data_source(
-    data_source: Union[DocumentDataSource, SupDataSource],
-    label_to_segment_map: defaultdict,
-    number_shards: int,
-    update_segment_map: bool,
-    flush_frequency: int = 1_000_000,
-):
-    """
-    NOTE:
-    1. Sharding only works for data sources that have id column as the first element of the lines yielded by their line iterator and that each line contains only one id.
-    2. Sharding is supported for DocumentDataSource and SupDataSource currently.
-    Args:
-        data_source : DocumentDataSource or SupDataSource
-            The data source to be sharded
-        label_to_segment_map : defaultdict
-            map of label id to shard id
-        number_shards : int
-            number of shards to shard the dataset into
-            If number_shards is 1, then returns the datasource as it is.
-        update_segment_map : bool
-            If set to True, then we first randomly shard the data_source, and update the label_to_segment_map.
-            If set to False, then the data_source is sharded using the label_to_segment_map provided by the user.
-    Returns:
-        sharded_data_sources : List[DocumentDataSource] or List[SupDataSource]
-            Each element in the list corresponds to a shard of the original data source
-    Note:
-        Updates the label_to_segment_map with label_id -> shard index map if update_segment_map is True.
-    """
-    verify_data_source_type(
-        data_source=data_source,
-        valid_types=[DocumentDataSource, SupDataSource],
-        operation_name="shard_data_source",
-    )
-
-    data_load_multiplexer = DataLoadMultiplexer(
-        number_shards, flush_frequency=flush_frequency
-    )
-
-    if number_shards == 1:
-        if update_segment_map:
-            data_load_multiplexer.update_label_segment_map(
-                data_source, label_to_segment_map
-            )
-        return [data_source]
-
-    (
-        shard_names,
-        shard_objects,
-        _,
-    ) = data_load_multiplexer.create_segments_with_data_source(
-        data_source, label_to_segment_map, update_index=update_segment_map
-    )
-
-    return [
-        transform_shard_to_datasource(data_source, shard_path, shard_object)
-        for shard_path, shard_object in zip(shard_names, shard_objects)
-    ]
+import random
+import tempfile
+from collections import defaultdict
+from typing import List, Union
+
+from .documents import CSV, DocumentDataSource
+from .supervised_datasource import Sup, SupDataSource
+
+
+class DataLoadMultiplexer:
+    """
+    A data loader that efficiently handles large datasets by segmenting them into smaller,
+    manageable temporary CSV files. This approach is particularly useful for processing
+    large datasets that do not fit entirely into memory.
+
+    The class optimizes memory usage by reading and writing data line by line,
+    rather than loading entire datasets into DataFrames. This method reduces the
+    memory footprint, especially when dealing with large datasets, and avoids
+    the overhead of storing extra DataFrame objects in memory.
+
+    Attributes:
+        num_segments (int): Number of segments to divide the data into.
+        flush_frequency (int): Frequency at which to flush data to the temporary files,
+                               reducing memory usage during processing.
+
+    Methods:
+        create_segments_with_data_source(data_source, label_to_segment_map, update_index):
+            Creates data segments based on the provided data source and label mapping,
+            optionally sharding the data using an index.
+
+    The class utilizes temporary files to store individual data segments, which are then
+    processed independently. This design allows for efficient data handling and scalability,
+    making it suitable for large-scale data processing tasks where memory optimization is crucial.
+
+    NOTE: This multiplexer assumes that the id column is the first element of the csv lines yielded by the line iterator of the data source. And hence, this multiplexer is only supposed to be used with DataSources with the above behaviour.
+    """
+
+    def __init__(self, num_segments, flush_frequency=1_000_000):
+        self.num_segments = num_segments
+        self.flush_frequency = flush_frequency
+        self.seed = 42
+
+    def _generate_temp_csvs(self):
+        """
+        Stores a list of dataframes in temporary files so that they can be read as CSV files later.
+        """
+        segment_prefix = f"{random.randint(100000, 999999)}"
+        segment_filenames = []
+        # We need to store the segment objects so that we can delete the files once we are done with sharding and creating a new dataframe
+        segment_objects = []
+        for index in range(self.num_segments):
+            temp_file = tempfile.NamedTemporaryFile(
+                mode="w",
+                delete=True,
+                suffix=".csv",
+                prefix=f"{segment_prefix}_{index}_",
+            )
+
+            segment_filenames.append(temp_file.name)
+            segment_objects.append(temp_file)
+        return segment_filenames, segment_objects
+
+    def _create_segments_with_segment_map(
+        self,
+        data_source: Union[DocumentDataSource, SupDataSource],
+        label_to_segment_map,
+    ):
+        segment_filenames, segment_objects = self._generate_temp_csvs()
+
+        current_index = 0
+
+        if isinstance(data_source, DocumentDataSource):
+            line_iterator = data_source._get_line_iterator()
+        if isinstance(data_source, SupDataSource):
+            # SupDataSource's line iterator can return multiple labels in the same line concatened by its id_delimiter and hence, we pass concat_labels = False so that each line has just one label.
+            line_iterator = data_source._get_line_iterator(concat_labels=False)  # type: ignore
+
+        for data in line_iterator:  # type: ignore
+            # header
+            if current_index == 0:
+                for segments in segment_objects:
+                    segments.write(data)
+            else:
+                current_label = int(data.split(",", 1)[0])
+                # TODO(pratik/shubh): Having list as map values is for experiments,
+                # we would be just having one elment in list for each index. We should
+                # remove this going forward.
+                if current_label not in label_to_segment_map:
+                    raise ValueError(
+                        "Label '{}' is not in 'label_to_segment_map'. Ensure it is"
+                        " included if sharding by index.".format(current_label)
+                    )
+                current_segment = label_to_segment_map[current_label][-1]
+                segment_objects[current_segment].write("\n" + data)
+
+            current_index += 1
+            if current_index % self.flush_frequency == 0:
+                for segment in segment_objects:
+                    segment.flush()
+
+        for segment in segment_objects:
+            segment.flush()
+
+        data_source.restart()
+        return (
+            segment_filenames,
+            segment_objects,
+            label_to_segment_map,
+        )
+
+    def update_label_segment_map(self, data_source, label_to_segment_map):
+        indices = data_source.indices()
+        random.seed(self.seed)
+        random.shuffle(indices)
+        for randomised_index in indices:
+            label_to_segment_map[randomised_index].append(
+                randomised_index % self.num_segments
+            )
+
+    def create_segments_with_data_source(
+        self, data_source, label_to_segment_map, update_index
+    ):
+        if update_index:
+            self.update_label_segment_map(data_source, label_to_segment_map)
+        else:
+            if len(label_to_segment_map) == 0:
+                raise Exception("label_to_segment_map is empty")
+
+        return self._create_segments_with_segment_map(data_source, label_to_segment_map)
+
+
+def verify_data_source_type(data_source, valid_types: List, operation_name: str):
+    if not any(isinstance(data_source, data_type) for data_type in valid_types):
+        valid_types_str = ", ".join([data_type.__name__ for data_type in valid_types])
+        raise TypeError(
+            f"{operation_name} not supported for data source of the type"
+            f" {type(data_source)}. Expected types are: {valid_types_str}"
+        )
+
+
+def transform_shard_to_datasource(
+    original_data_source: Union[DocumentDataSource, SupDataSource],
+    shard_path,
+    shard_object,
+):
+    """
+    We assume that the shard is stored as a CSV on the machine. Depending on the underlying original data source, we create a new data source from the shard with the same attributes except that the documents are loaded from the shard.
+    """
+    verify_data_source_type(
+        data_source=original_data_source,
+        valid_types=[DocumentDataSource, SupDataSource],
+        operation_name="transform_shard_to_datasource",
+    )
+    if isinstance(original_data_source, DocumentDataSource):
+        data_source = DocumentDataSource(
+            id_column=original_data_source.id_column,
+            strong_column=original_data_source.strong_column,
+            weak_column=original_data_source.weak_column,
+        )
+        csv_doc = CSV(
+            path=shard_path,
+            id_column=original_data_source.id_column,
+            strong_columns=[original_data_source.strong_column],
+            weak_columns=[original_data_source.weak_column],
+            has_offset=True,
+        )
+
+        shard_object.close()
+        data_source.add(document=csv_doc, start_id=0)
+        return data_source
+
+    if isinstance(original_data_source, SupDataSource):
+        sup = Sup(
+            csv=shard_path,
+            query_column=original_data_source.query_col,
+            id_column=original_data_source.id_column,
+            id_delimiter=original_data_source.id_delimiter,
+            uses_db_id=True,
+        )
+        shard_object.close()
+        return SupDataSource(
+            doc_manager=original_data_source.doc_manager,
+            query_col=original_data_source.query_col,
+            data=[sup],
+            id_delimiter=original_data_source.id_delimiter,
+            id_column=original_data_source.id_column,
+        )
+
+
+def shard_data_source(
+    data_source: Union[DocumentDataSource, SupDataSource],
+    label_to_segment_map: defaultdict,
+    number_shards: int,
+    update_segment_map: bool,
+    flush_frequency: int = 1_000_000,
+):
+    """
+    NOTE:
+    1. Sharding only works for data sources that have id column as the first element of the lines yielded by their line iterator and that each line contains only one id.
+    2. Sharding is supported for DocumentDataSource and SupDataSource currently.
+    Args:
+        data_source : DocumentDataSource or SupDataSource
+            The data source to be sharded
+        label_to_segment_map : defaultdict
+            map of label id to shard id
+        number_shards : int
+            number of shards to shard the dataset into
+            If number_shards is 1, then returns the datasource as it is.
+        update_segment_map : bool
+            If set to True, then we first randomly shard the data_source, and update the label_to_segment_map.
+            If set to False, then the data_source is sharded using the label_to_segment_map provided by the user.
+    Returns:
+        sharded_data_sources : List[DocumentDataSource] or List[SupDataSource]
+            Each element in the list corresponds to a shard of the original data source
+    Note:
+        Updates the label_to_segment_map with label_id -> shard index map if update_segment_map is True.
+    """
+    verify_data_source_type(
+        data_source=data_source,
+        valid_types=[DocumentDataSource, SupDataSource],
+        operation_name="shard_data_source",
+    )
+
+    data_load_multiplexer = DataLoadMultiplexer(
+        number_shards, flush_frequency=flush_frequency
+    )
+
+    if number_shards == 1:
+        if update_segment_map:
+            data_load_multiplexer.update_label_segment_map(
+                data_source, label_to_segment_map
+            )
+        return [data_source]
+
+    (
+        shard_names,
+        shard_objects,
+        _,
+    ) = data_load_multiplexer.create_segments_with_data_source(
+        data_source, label_to_segment_map, update_index=update_segment_map
+    )
+
+    return [
+        transform_shard_to_datasource(data_source, shard_path, shard_object)
+        for shard_path, shard_object in zip(shard_names, shard_objects)
+    ]
```

## thirdai/neural_db/sql_helpers.py

 * *Ordering differences only*

```diff
@@ -1,71 +1,71 @@
-from typing import Any, List
-
-import pandas as pd
-from pandas.api import types as pd_types
-from sqlalchemy import (
-    Column,
-    Engine,
-    Float,
-    Integer,
-    MetaData,
-    String,
-    Table,
-    create_engine,
-    select,
-)
-
-
-def get_engine(db_path: str):
-    return create_engine(f"sqlite:///{db_path}")
-
-
-def infer_type(series: pd.Series):
-    if pd_types.is_integer_dtype(series):
-        return Integer()
-    if pd_types.is_float_dtype(series):
-        return Float()
-    return String()
-
-
-def infer_types(df: pd.DataFrame):
-    types = {col: infer_type(df[col]) for col in df.columns}
-    types[df.index.name] = Integer()
-    return types
-
-
-def create_table(engine: Engine, df: pd.DataFrame, name: str, types: dict):
-    metadata_obj = MetaData()
-    columns = [
-        Column(col, dtype, primary_key=col == df.index.name)
-        for col, dtype in types.items()
-    ]
-    table = Table(
-        name,
-        metadata_obj,
-        *columns,
-    )
-    metadata_obj.create_all(engine)
-    return table
-
-
-def df_to_sql(db_path: str, df: pd.DataFrame, table_name: str):
-    engine = get_engine(db_path)
-    types = infer_types(df)
-    table = create_table(engine, df, table_name, types)
-    df.to_sql(
-        name=table_name,
-        con=engine,
-        index=True,
-        dtype=types,
-        if_exists="append",
-    )
-    return table
-
-
-def select_as_df(db_path: str, table: Table, constraints: List[Any] = None):
-    engine = get_engine(db_path)
-    selection = select(table)
-    if constraints:
-        for constraint in constraints:
-            selection = selection.where(constraint)
-    return pd.read_sql(selection, engine)
+from typing import Any, List
+
+import pandas as pd
+from pandas.api import types as pd_types
+from sqlalchemy import (
+    Column,
+    Engine,
+    Float,
+    Integer,
+    MetaData,
+    String,
+    Table,
+    create_engine,
+    select,
+)
+
+
+def get_engine(db_path: str):
+    return create_engine(f"sqlite:///{db_path}")
+
+
+def infer_type(series: pd.Series):
+    if pd_types.is_integer_dtype(series):
+        return Integer()
+    if pd_types.is_float_dtype(series):
+        return Float()
+    return String()
+
+
+def infer_types(df: pd.DataFrame):
+    types = {col: infer_type(df[col]) for col in df.columns}
+    types[df.index.name] = Integer()
+    return types
+
+
+def create_table(engine: Engine, df: pd.DataFrame, name: str, types: dict):
+    metadata_obj = MetaData()
+    columns = [
+        Column(col, dtype, primary_key=col == df.index.name)
+        for col, dtype in types.items()
+    ]
+    table = Table(
+        name,
+        metadata_obj,
+        *columns,
+    )
+    metadata_obj.create_all(engine)
+    return table
+
+
+def df_to_sql(db_path: str, df: pd.DataFrame, table_name: str):
+    engine = get_engine(db_path)
+    types = infer_types(df)
+    table = create_table(engine, df, table_name, types)
+    df.to_sql(
+        name=table_name,
+        con=engine,
+        index=True,
+        dtype=types,
+        if_exists="append",
+    )
+    return table
+
+
+def select_as_df(db_path: str, table: Table, constraints: List[Any] = None):
+    engine = get_engine(db_path)
+    selection = select(table)
+    if constraints:
+        for constraint in constraints:
+            selection = selection.where(constraint)
+    return pd.read_sql(selection, engine)
```

## thirdai/neural_db/supervised_datasource.py

 * *Ordering differences only*

```diff
@@ -1,253 +1,253 @@
-import json
-from pathlib import Path
-from typing import List, Optional, Sequence
-
-import pandas as pd
-from thirdai.dataset.data_source import PyDataSource
-
-from .documents import DocumentManager
-
-"""Sup and SupDataSource are classes that manage entity IDs for supervised
-training.
-
-Entity = an item that can be retrieved by NeuralDB. If we insert an ndb.CSV
-object into NeuralDB, then each row of the CSV file is an entity. If we insert 
-an ndb.PDF object, then each paragraph is an entity. If we insert an 
-ndb.SentenceLevelDOCX object, then each sentence is an entity.
-
-If this still doesn't make sense, consider a scenario where you insert a CSV 
-file into NeuralDB and want to improve the performance of the database by
-training it on supervised training samples. That is, you want the model to 
-learn from (query, ID) pairs.
-
-Since you only inserted one file, the ID of each entity in NeuralDB's index
-is the same as the ID given in the file. Thus, the model can directly ingest
-the (query, ID) pairs from your supervised dataset. However, this is not the 
-case if you inserted multiple CSV files. For example, suppose you insert file A 
-containing entities with IDs 0 through 100 and also insert file B containing 
-its own set of entities with IDs 0 through 100. To disambiguate between entities
-from different files, NeuralDB automatically offsets the IDs of the second file.
-Consequently, you also have to adjust the labels of supervised training samples
-corresponding to entities in file B. 
-
-Instead of leaking the abstraction by making the user manually change the labels
-of their dataset, we created Sup and SupDataSource to handle this.
-
-If the user would rather use the database-assigned IDs instead of IDs from the 
-original document, this can be done by passing uses_db_id = True to Sup(). This
-is useful for cases where the user does not know the IDs of the entities in the
-original documents. For example, if the original document is a PDF, then it is
-NeuralDB that parses it into paragraphs; the user does not know the ID of each
-paragraph beforehand. In this scenario, it is much easier for the user to just
-use the database-assigned IDs.
-"""
-
-
-class Sup:
-    """An object that contains supervised samples. This object is to be passed
-    into NeuralDB.supervised_train().
-
-    It can be initialized either with a CSV file, in which case it needs query
-    and ID column names, or with sequences of queries and labels. It also needs
-    to know which source object (i.e. which inserted CSV or PDF object) contains
-    the relevant entities to the supervised samples.
-
-    If uses_db_id is True, then the labels are assumed to use database-assigned
-    IDs and will not be converted.
-    """
-
-    def __init__(
-        self,
-        csv: str = None,
-        query_column: str = None,
-        id_column: str = None,
-        id_delimiter: str = None,
-        queries: Sequence[str] = None,
-        labels: Sequence[Sequence[int]] = None,
-        source_id: str = "",
-        uses_db_id: bool = False,
-    ):
-        if csv is not None and query_column is not None and id_column is not None:
-            df = pd.read_csv(csv)
-            self.queries = df[query_column].fillna("")
-            self.labels = df[id_column]
-            for i, label in enumerate(self.labels):
-                if label == None or label == "":
-                    raise ValueError(
-                        "Got a supervised sample with an empty label, query:"
-                        f" '{self.queries[i]}'"
-                    )
-            if id_delimiter:
-                self.labels = self.labels.apply(
-                    lambda label: list(
-                        str(label).strip(id_delimiter).split(id_delimiter)
-                    )
-                )
-            else:
-                self.labels = self.labels.apply(lambda label: [str(label)])
-
-        elif queries is not None and labels is not None:
-            if len(queries) != len(labels):
-                raise ValueError(
-                    "Queries and labels sequences must be the same length."
-                )
-            self.queries = queries
-            self.labels = labels
-        else:
-            raise ValueError(
-                "Sup must be initialized with csv, query_column and id_column, or"
-                " queries and labels."
-            )
-        self.source_id = source_id
-        self.uses_db_id = uses_db_id
-
-    @property
-    def size(self):
-        return len(self.queries)
-
-
-class SupDataSource(PyDataSource):
-    """Combines supervised samples from multiple Sup objects into a single data
-    source. This allows NeuralDB's underlying model to train on all provided
-    supervised datasets simultaneously.
-    """
-
-    def __init__(
-        self,
-        query_col: str,
-        data: List[Sup],
-        id_delimiter: Optional[str],
-        doc_manager: Optional[DocumentManager] = None,
-        id_column: Optional[str] = None,
-    ):
-        PyDataSource.__init__(self)
-        self.query_col = query_col
-        self.data = data
-        self.id_delimiter = id_delimiter
-        if not self.id_delimiter:
-            print("WARNING: this model does not fully support multi-label datasets.")
-
-        self.doc_manager = doc_manager
-
-        if not self.doc_manager and not id_column:
-            raise Exception(
-                "Cannot initialize a SupDataSource with None values for both doc_manager and id_column"
-            )
-
-        self.id_column = id_column if id_column else doc_manager.id_column
-
-        self.restart()
-
-    def _csv_line(self, label: str, query: str):
-        query = '"' + query.replace('"', '""') + '"'
-        return f"{label},{query}"
-
-    def _source_for_sup(self, sup: Sup):
-        if not self.doc_manager:
-            raise Exception(
-                "Cannot get document ids for a SupDataSource with no document manager"
-            )
-
-        source_ids = self.doc_manager.match_source_id_by_prefix(sup.source_id)
-        if len(source_ids) == 0:
-            raise ValueError(f"Cannot find source with id {sup.source_id}")
-        if len(source_ids) > 1:
-            raise ValueError(f"Multiple sources match the prefix {sup.source_id}")
-        return self.doc_manager.source_by_id(source_ids[0])
-
-    def _labels(self, sup: Sup):
-        if sup.uses_db_id:
-            return [map(str, labels) for labels in sup.labels]
-
-        doc, start_id = self._source_for_sup(sup)
-        doc_id_map = doc.id_map()
-        if doc_id_map:
-            mapper = lambda label: str(doc_id_map[label] + start_id)
-        else:
-            mapper = lambda label: str(int(label) + start_id)
-
-        return [map(mapper, labels) for labels in sup.labels]
-
-    def _get_line_iterator(self, concat_labels=True):
-        """
-        If concat_labels is True and id_delimiter is not None, then the labels are joined using id_delimiter and yielded in a single row. Return one label per row in all other cases.
-
-        This is done to enable data sharding for SupDataSource as currently we can only shard data sources that have a label per line without any delimiters.
-        """
-        # First yield the header
-        yield self._csv_line(self.id_column, self.query_col)
-        # Then yield rows
-        for sup in self.data:
-            for query, labels in zip(sup.queries, self._labels(sup)):
-                if query == "":
-                    continue
-                if self.id_delimiter and concat_labels:
-                    yield self._csv_line(
-                        self.id_delimiter.join(labels),
-                        query,
-                    )
-                else:
-                    for label in labels:
-                        yield self._csv_line(
-                            label,
-                            query,
-                        )
-
-    def indices(self):
-        indices = set()
-        for sup in self.data:
-            for _, labels in zip(sup.queries, self._labels(sup)):
-                for label in labels:
-                    indices.add(label)
-
-        return list(indices)
-
-    def resource_name(self) -> str:
-        return "Supervised training samples"
-
-    @property
-    def size(self):
-        sizes_sup = [sup.size for sup in self.data]
-        return sum(sizes_sup)
-
-    def save(self, path: Path, save_interval=100_000):
-        path.mkdir(exist_ok=True, parents=True)
-        number_lines_in_buffer = 0
-        with open(path / "source.csv", "w", encoding="utf-8") as f:
-            for line in self._get_line_iterator():
-                f.write(line + "\n")
-                number_lines_in_buffer += 1
-            if number_lines_in_buffer > save_interval:
-                f.flush()
-                number_lines_in_buffer = 0
-
-        with open(path / "arguments.json", "w") as f:
-            json.dump(
-                {
-                    "query_column": self.query_col,
-                    "id_column": self.id_column,
-                    "id_delimiter": self.id_delimiter,
-                },
-                f,
-                indent=4,
-            )
-
-    @staticmethod
-    def load(path: Path):
-        with open(path / "arguments.json", "r") as f:
-            args = json.load(f)
-
-        sup_data = Sup(
-            csv=path / "source.csv",
-            query_column=args["query_column"],
-            id_column=args["id_column"],
-            id_delimiter=args["id_delimiter"],
-            uses_db_id=True,
-        )
-        data_source = SupDataSource(
-            query_col=args["query_column"],
-            data=[sup_data],
-            id_delimiter=args["id_delimiter"],
-            id_column=args["id_column"],
-        )
-        return data_source
+import json
+from pathlib import Path
+from typing import List, Optional, Sequence
+
+import pandas as pd
+from thirdai.dataset.data_source import PyDataSource
+
+from .documents import DocumentManager
+
+"""Sup and SupDataSource are classes that manage entity IDs for supervised
+training.
+
+Entity = an item that can be retrieved by NeuralDB. If we insert an ndb.CSV
+object into NeuralDB, then each row of the CSV file is an entity. If we insert 
+an ndb.PDF object, then each paragraph is an entity. If we insert an 
+ndb.SentenceLevelDOCX object, then each sentence is an entity.
+
+If this still doesn't make sense, consider a scenario where you insert a CSV 
+file into NeuralDB and want to improve the performance of the database by
+training it on supervised training samples. That is, you want the model to 
+learn from (query, ID) pairs.
+
+Since you only inserted one file, the ID of each entity in NeuralDB's index
+is the same as the ID given in the file. Thus, the model can directly ingest
+the (query, ID) pairs from your supervised dataset. However, this is not the 
+case if you inserted multiple CSV files. For example, suppose you insert file A 
+containing entities with IDs 0 through 100 and also insert file B containing 
+its own set of entities with IDs 0 through 100. To disambiguate between entities
+from different files, NeuralDB automatically offsets the IDs of the second file.
+Consequently, you also have to adjust the labels of supervised training samples
+corresponding to entities in file B. 
+
+Instead of leaking the abstraction by making the user manually change the labels
+of their dataset, we created Sup and SupDataSource to handle this.
+
+If the user would rather use the database-assigned IDs instead of IDs from the 
+original document, this can be done by passing uses_db_id = True to Sup(). This
+is useful for cases where the user does not know the IDs of the entities in the
+original documents. For example, if the original document is a PDF, then it is
+NeuralDB that parses it into paragraphs; the user does not know the ID of each
+paragraph beforehand. In this scenario, it is much easier for the user to just
+use the database-assigned IDs.
+"""
+
+
+class Sup:
+    """An object that contains supervised samples. This object is to be passed
+    into NeuralDB.supervised_train().
+
+    It can be initialized either with a CSV file, in which case it needs query
+    and ID column names, or with sequences of queries and labels. It also needs
+    to know which source object (i.e. which inserted CSV or PDF object) contains
+    the relevant entities to the supervised samples.
+
+    If uses_db_id is True, then the labels are assumed to use database-assigned
+    IDs and will not be converted.
+    """
+
+    def __init__(
+        self,
+        csv: str = None,
+        query_column: str = None,
+        id_column: str = None,
+        id_delimiter: str = None,
+        queries: Sequence[str] = None,
+        labels: Sequence[Sequence[int]] = None,
+        source_id: str = "",
+        uses_db_id: bool = False,
+    ):
+        if csv is not None and query_column is not None and id_column is not None:
+            df = pd.read_csv(csv)
+            self.queries = df[query_column].fillna("")
+            self.labels = df[id_column]
+            for i, label in enumerate(self.labels):
+                if label == None or label == "":
+                    raise ValueError(
+                        "Got a supervised sample with an empty label, query:"
+                        f" '{self.queries[i]}'"
+                    )
+            if id_delimiter:
+                self.labels = self.labels.apply(
+                    lambda label: list(
+                        str(label).strip(id_delimiter).split(id_delimiter)
+                    )
+                )
+            else:
+                self.labels = self.labels.apply(lambda label: [str(label)])
+
+        elif queries is not None and labels is not None:
+            if len(queries) != len(labels):
+                raise ValueError(
+                    "Queries and labels sequences must be the same length."
+                )
+            self.queries = queries
+            self.labels = labels
+        else:
+            raise ValueError(
+                "Sup must be initialized with csv, query_column and id_column, or"
+                " queries and labels."
+            )
+        self.source_id = source_id
+        self.uses_db_id = uses_db_id
+
+    @property
+    def size(self):
+        return len(self.queries)
+
+
+class SupDataSource(PyDataSource):
+    """Combines supervised samples from multiple Sup objects into a single data
+    source. This allows NeuralDB's underlying model to train on all provided
+    supervised datasets simultaneously.
+    """
+
+    def __init__(
+        self,
+        query_col: str,
+        data: List[Sup],
+        id_delimiter: Optional[str],
+        doc_manager: Optional[DocumentManager] = None,
+        id_column: Optional[str] = None,
+    ):
+        PyDataSource.__init__(self)
+        self.query_col = query_col
+        self.data = data
+        self.id_delimiter = id_delimiter
+        if not self.id_delimiter:
+            print("WARNING: this model does not fully support multi-label datasets.")
+
+        self.doc_manager = doc_manager
+
+        if not self.doc_manager and not id_column:
+            raise Exception(
+                "Cannot initialize a SupDataSource with None values for both doc_manager and id_column"
+            )
+
+        self.id_column = id_column if id_column else doc_manager.id_column
+
+        self.restart()
+
+    def _csv_line(self, label: str, query: str):
+        query = '"' + query.replace('"', '""') + '"'
+        return f"{label},{query}"
+
+    def _source_for_sup(self, sup: Sup):
+        if not self.doc_manager:
+            raise Exception(
+                "Cannot get document ids for a SupDataSource with no document manager"
+            )
+
+        source_ids = self.doc_manager.match_source_id_by_prefix(sup.source_id)
+        if len(source_ids) == 0:
+            raise ValueError(f"Cannot find source with id {sup.source_id}")
+        if len(source_ids) > 1:
+            raise ValueError(f"Multiple sources match the prefix {sup.source_id}")
+        return self.doc_manager.source_by_id(source_ids[0])
+
+    def _labels(self, sup: Sup):
+        if sup.uses_db_id:
+            return [map(str, labels) for labels in sup.labels]
+
+        doc, start_id = self._source_for_sup(sup)
+        doc_id_map = doc.id_map()
+        if doc_id_map:
+            mapper = lambda label: str(doc_id_map[label] + start_id)
+        else:
+            mapper = lambda label: str(int(label) + start_id)
+
+        return [map(mapper, labels) for labels in sup.labels]
+
+    def _get_line_iterator(self, concat_labels=True):
+        """
+        If concat_labels is True and id_delimiter is not None, then the labels are joined using id_delimiter and yielded in a single row. Return one label per row in all other cases.
+
+        This is done to enable data sharding for SupDataSource as currently we can only shard data sources that have a label per line without any delimiters.
+        """
+        # First yield the header
+        yield self._csv_line(self.id_column, self.query_col)
+        # Then yield rows
+        for sup in self.data:
+            for query, labels in zip(sup.queries, self._labels(sup)):
+                if query == "":
+                    continue
+                if self.id_delimiter and concat_labels:
+                    yield self._csv_line(
+                        self.id_delimiter.join(labels),
+                        query,
+                    )
+                else:
+                    for label in labels:
+                        yield self._csv_line(
+                            label,
+                            query,
+                        )
+
+    def indices(self):
+        indices = set()
+        for sup in self.data:
+            for _, labels in zip(sup.queries, self._labels(sup)):
+                for label in labels:
+                    indices.add(label)
+
+        return list(indices)
+
+    def resource_name(self) -> str:
+        return "Supervised training samples"
+
+    @property
+    def size(self):
+        sizes_sup = [sup.size for sup in self.data]
+        return sum(sizes_sup)
+
+    def save(self, path: Path, save_interval=100_000):
+        path.mkdir(exist_ok=True, parents=True)
+        number_lines_in_buffer = 0
+        with open(path / "source.csv", "w", encoding="utf-8") as f:
+            for line in self._get_line_iterator():
+                f.write(line + "\n")
+                number_lines_in_buffer += 1
+            if number_lines_in_buffer > save_interval:
+                f.flush()
+                number_lines_in_buffer = 0
+
+        with open(path / "arguments.json", "w") as f:
+            json.dump(
+                {
+                    "query_column": self.query_col,
+                    "id_column": self.id_column,
+                    "id_delimiter": self.id_delimiter,
+                },
+                f,
+                indent=4,
+            )
+
+    @staticmethod
+    def load(path: Path):
+        with open(path / "arguments.json", "r") as f:
+            args = json.load(f)
+
+        sup_data = Sup(
+            csv=path / "source.csv",
+            query_column=args["query_column"],
+            id_column=args["id_column"],
+            id_delimiter=args["id_delimiter"],
+            uses_db_id=True,
+        )
+        data_source = SupDataSource(
+            query_col=args["query_column"],
+            data=[sup_data],
+            id_delimiter=args["id_delimiter"],
+            id_column=args["id_column"],
+        )
+        return data_source
```

## thirdai/neural_db/table.py

 * *Ordering differences only*

```diff
@@ -1,257 +1,257 @@
-# Python
-import shutil
-import sqlite3
-import uuid
-from abc import ABC, abstractmethod
-from pathlib import Path
-from typing import Generator, List, Tuple
-
-import dask.dataframe as dd
-
-# Libraries
-import pandas as pd
-
-# Local
-from .constraint_matcher import TableFilter
-from .sql_helpers import df_to_sql, select_as_df
-
-
-def df_with_index_name(df):
-    index_name = df.index.name
-    if not index_name:
-        index_name = "__id__"
-        while index_name in df.columns:
-            index_name += "_"
-        df.index.name = index_name
-    return df
-
-
-class Table(ABC):
-    @property
-    @abstractmethod
-    def columns(self) -> List[str]:
-        pass
-
-    @property
-    @abstractmethod
-    def size(self) -> int:
-        pass
-
-    @property
-    @abstractmethod
-    def ids(self) -> List[int]:
-        pass
-
-    @abstractmethod
-    def field(self, row_id: int, column: str):
-        pass
-
-    @abstractmethod
-    def row_as_dict(self, row_id: int) -> dict:
-        pass
-
-    @abstractmethod
-    def range_rows_as_dicts(self, from_row_id: int, to_row_id: int) -> List[dict]:
-        pass
-
-    @abstractmethod
-    def iter_rows_as_dicts(self) -> Generator[Tuple[int, dict], None, None]:
-        pass
-
-    @abstractmethod
-    def apply_filter(self, table_filter: TableFilter, column_name: str):
-        pass
-
-    def save_meta(self, directory: Path):
-        pass
-
-    def load_meta(self, directory: Path):
-        pass
-
-
-class DaskDataFrameTable(Table):
-    def __init__(self, df: dd.DataFrame):
-        self.df = df_with_index_name(df)
-        self.row_id_to_dict = (
-            {}
-        )  # store row_id_to_dict before hand for quick retrieval in future
-        index_name = self.df.index.name
-        meta = pd.DataFrame({"data": pd.Series(dtype="object")})
-        results = self.df.map_partitions(self._partition_to_dicts, meta=meta)
-        row_id = 0  # We are maintaining a global row_id because each partition will have it's local index
-        for batch in results.compute():
-            for row_dict in batch:
-                row_dict[index_name] = row_id
-                self.row_id_to_dict[row_id] = row_dict
-                row_id += 1
-
-    @property
-    def columns(self) -> List[str]:
-        return [col for col in self.df.columns if col != self.df.index.name]
-
-    @property
-    def size(self) -> int:
-        # For Dask, compute() is required to get the actual size
-        return int(self.df.shape[0].compute())
-
-    @property
-    def ids(self) -> List[int]:
-        # Dask requires computation to convert index to a list
-        return self.df.index.compute().to_list()
-
-    def field(self, row_id: int, column: str):
-        # For Dask, use .compute() to get actual values
-        return self.df.loc[row_id][column].compute()
-
-    def row_as_dict(self, row_id: int) -> dict:
-        return self.row_id_to_dict[row_id]
-
-    def range_rows_as_dicts(self, from_row_id: int, to_row_id: int) -> List[dict]:
-        return self.row_id_to_dict[from_row_id:to_row_id]
-
-    def _partition_to_dicts(self, df_partition):
-        dicts = []
-        for row in df_partition.itertuples(index=True):
-            row_dict = row._asdict()
-            dicts.append(row_dict)
-        return dicts
-
-    def iter_rows_as_dicts(self) -> Generator[Tuple[int, dict], None, None]:
-        for row_id, row_dict in self.row_id_to_dict.items():
-            yield (row_id, row_dict)
-
-    def apply_filter(self, table_filter: TableFilter):
-        return table_filter.filter_df_ids(self.df)
-
-
-class DataFrameTable(Table):
-    def __init__(self, df: pd.DataFrame):
-        """The index of the dataframe is assumed to be the ID column.
-        In other words, the ID column of a data frame must be set as its index
-        before being passed into this constructor.
-        """
-        self.df = df_with_index_name(df)
-
-    @property
-    def columns(self) -> List[str]:
-        # Excludes ID column
-        return self.df.columns
-
-    @property
-    def size(self) -> int:
-        return len(self.df)
-
-    @property
-    def ids(self) -> List[int]:
-        return self.df.index.to_list()
-
-    def field(self, row_id: int, column: str):
-        if column == self.df.index.name:
-            return row_id
-        return self.df[column].loc[row_id]
-
-    def row_as_dict(self, row_id: int) -> dict:
-        row = self.df.loc[row_id].to_dict()
-        row[self.df.index.name] = row_id
-        return row
-
-    def range_rows_as_dicts(self, from_row_id: int, to_row_id: int) -> List[dict]:
-        return (
-            self.df.loc[from_row_id:to_row_id].reset_index().to_dict(orient="records")
-        )
-
-    def iter_rows_as_dicts(self) -> Generator[Tuple[int, dict], None, None]:
-        for row in self.df.itertuples(index=True):
-            row_id = row.Index
-            row_dict = row._asdict()
-            row_dict[self.df.index.name] = row_id
-            yield (row_id, row_dict)
-
-    def apply_filter(self, table_filter: TableFilter):
-        return table_filter.filter_df_ids(self.df)
-
-
-class SQLiteTable(Table):
-    EVAL_PREFIX = "__eval__"
-    TABLE_NAME = "sqlitetable"
-
-    def __init__(self, df: pd.DataFrame):
-        # TODO: Reset index first?
-        self.db_path = f"{uuid.uuid4()}.db"
-        self.db_columns = df.columns
-        self.db_size = len(df)
-
-        # We don't save the db connection and instead create a new connection
-        # each time to simplify serialization.
-        df = df_with_index_name(df)
-        self.id_column = df.index.name
-        self.sql_table = df_to_sql(self.db_path, df, SQLiteTable.TABLE_NAME)
-
-    @property
-    def columns(self) -> List[str]:
-        # Excludes ID column
-        return self.db_columns
-
-    @property
-    def size(self) -> int:
-        return self.db_size
-
-    @property
-    def ids(self) -> List[int]:
-        return select_as_df(
-            db_path=self.db_path,
-            table=self.sql_table.c[self.id_column],
-        )[self.id_column]
-
-    def field(self, row_id: int, column: str):
-        return select_as_df(
-            db_path=self.db_path,
-            table=self.sql_table.c[column],
-            constraints=[self.sql_table.c[self.id_column] == row_id],
-        )[column][0]
-
-    def row_as_dict(self, row_id: int) -> dict:
-        return select_as_df(
-            db_path=self.db_path,
-            table=self.sql_table,
-            constraints=[self.sql_table.c[self.id_column] == row_id],
-        ).to_dict("records")[0]
-
-    def range_rows_as_dicts(self, from_row_id: int, to_row_id: int) -> List[dict]:
-        return select_as_df(
-            db_path=self.db_path,
-            table=self.sql_table,
-            constraints=[
-                self.sql_table.c[self.id_column] >= from_row_id,
-                self.sql_table.c[self.id_column] < to_row_id,
-            ],
-        ).to_dict("records")
-
-    def select_with_constraint(self, column, value) -> List[dict]:
-        return select_as_df(
-            db_path=self.db_path,
-            table=self.sql_table,
-            constraints=[
-                self.sql_table.c[column] == value,
-            ],
-        ).to_dict("records")
-
-    def iter_rows_as_dicts(self) -> Generator[Tuple[int, dict], None, None]:
-        size = self.size
-        chunk_size = 1000  # Hardcoded for now
-        # Load in chunks
-        for chunk_start in range(0, size, chunk_size):
-            chunk_end = min(chunk_start + chunk_size, size)
-            for row in self.range_rows_as_dicts(chunk_start, chunk_end):
-                yield row[self.id_column], row
-
-    def save_meta(self, directory: Path):
-        shutil.copy(self.db_path, directory / Path(self.db_path).name)
-
-    def load_meta(self, directory: Path):
-        self.db_path = str(directory / Path(self.db_path).name)
-
-    def apply_filter(self, table_filter: TableFilter):
-        return table_filter.filter_sql_ids(
-            sqlite3.connect(self.db_path), SQLiteTable.TABLE_NAME
-        )
+# Python
+import shutil
+import sqlite3
+import uuid
+from abc import ABC, abstractmethod
+from pathlib import Path
+from typing import Generator, List, Tuple
+
+import dask.dataframe as dd
+
+# Libraries
+import pandas as pd
+
+# Local
+from .constraint_matcher import TableFilter
+from .sql_helpers import df_to_sql, select_as_df
+
+
+def df_with_index_name(df):
+    index_name = df.index.name
+    if not index_name:
+        index_name = "__id__"
+        while index_name in df.columns:
+            index_name += "_"
+        df.index.name = index_name
+    return df
+
+
+class Table(ABC):
+    @property
+    @abstractmethod
+    def columns(self) -> List[str]:
+        pass
+
+    @property
+    @abstractmethod
+    def size(self) -> int:
+        pass
+
+    @property
+    @abstractmethod
+    def ids(self) -> List[int]:
+        pass
+
+    @abstractmethod
+    def field(self, row_id: int, column: str):
+        pass
+
+    @abstractmethod
+    def row_as_dict(self, row_id: int) -> dict:
+        pass
+
+    @abstractmethod
+    def range_rows_as_dicts(self, from_row_id: int, to_row_id: int) -> List[dict]:
+        pass
+
+    @abstractmethod
+    def iter_rows_as_dicts(self) -> Generator[Tuple[int, dict], None, None]:
+        pass
+
+    @abstractmethod
+    def apply_filter(self, table_filter: TableFilter, column_name: str):
+        pass
+
+    def save_meta(self, directory: Path):
+        pass
+
+    def load_meta(self, directory: Path):
+        pass
+
+
+class DaskDataFrameTable(Table):
+    def __init__(self, df: dd.DataFrame):
+        self.df = df_with_index_name(df)
+        self.row_id_to_dict = (
+            {}
+        )  # store row_id_to_dict before hand for quick retrieval in future
+        index_name = self.df.index.name
+        meta = pd.DataFrame({"data": pd.Series(dtype="object")})
+        results = self.df.map_partitions(self._partition_to_dicts, meta=meta)
+        row_id = 0  # We are maintaining a global row_id because each partition will have it's local index
+        for batch in results.compute():
+            for row_dict in batch:
+                row_dict[index_name] = row_id
+                self.row_id_to_dict[row_id] = row_dict
+                row_id += 1
+
+    @property
+    def columns(self) -> List[str]:
+        return [col for col in self.df.columns if col != self.df.index.name]
+
+    @property
+    def size(self) -> int:
+        # For Dask, compute() is required to get the actual size
+        return int(self.df.shape[0].compute())
+
+    @property
+    def ids(self) -> List[int]:
+        # Dask requires computation to convert index to a list
+        return self.df.index.compute().to_list()
+
+    def field(self, row_id: int, column: str):
+        # For Dask, use .compute() to get actual values
+        return self.df.loc[row_id][column].compute()
+
+    def row_as_dict(self, row_id: int) -> dict:
+        return self.row_id_to_dict[row_id]
+
+    def range_rows_as_dicts(self, from_row_id: int, to_row_id: int) -> List[dict]:
+        return self.row_id_to_dict[from_row_id:to_row_id]
+
+    def _partition_to_dicts(self, df_partition):
+        dicts = []
+        for row in df_partition.itertuples(index=True):
+            row_dict = row._asdict()
+            dicts.append(row_dict)
+        return dicts
+
+    def iter_rows_as_dicts(self) -> Generator[Tuple[int, dict], None, None]:
+        for row_id, row_dict in self.row_id_to_dict.items():
+            yield (row_id, row_dict)
+
+    def apply_filter(self, table_filter: TableFilter):
+        return table_filter.filter_df_ids(self.df)
+
+
+class DataFrameTable(Table):
+    def __init__(self, df: pd.DataFrame):
+        """The index of the dataframe is assumed to be the ID column.
+        In other words, the ID column of a data frame must be set as its index
+        before being passed into this constructor.
+        """
+        self.df = df_with_index_name(df)
+
+    @property
+    def columns(self) -> List[str]:
+        # Excludes ID column
+        return self.df.columns
+
+    @property
+    def size(self) -> int:
+        return len(self.df)
+
+    @property
+    def ids(self) -> List[int]:
+        return self.df.index.to_list()
+
+    def field(self, row_id: int, column: str):
+        if column == self.df.index.name:
+            return row_id
+        return self.df[column].loc[row_id]
+
+    def row_as_dict(self, row_id: int) -> dict:
+        row = self.df.loc[row_id].to_dict()
+        row[self.df.index.name] = row_id
+        return row
+
+    def range_rows_as_dicts(self, from_row_id: int, to_row_id: int) -> List[dict]:
+        return (
+            self.df.loc[from_row_id:to_row_id].reset_index().to_dict(orient="records")
+        )
+
+    def iter_rows_as_dicts(self) -> Generator[Tuple[int, dict], None, None]:
+        for row in self.df.itertuples(index=True):
+            row_id = row.Index
+            row_dict = row._asdict()
+            row_dict[self.df.index.name] = row_id
+            yield (row_id, row_dict)
+
+    def apply_filter(self, table_filter: TableFilter):
+        return table_filter.filter_df_ids(self.df)
+
+
+class SQLiteTable(Table):
+    EVAL_PREFIX = "__eval__"
+    TABLE_NAME = "sqlitetable"
+
+    def __init__(self, df: pd.DataFrame):
+        # TODO: Reset index first?
+        self.db_path = f"{uuid.uuid4()}.db"
+        self.db_columns = df.columns
+        self.db_size = len(df)
+
+        # We don't save the db connection and instead create a new connection
+        # each time to simplify serialization.
+        df = df_with_index_name(df)
+        self.id_column = df.index.name
+        self.sql_table = df_to_sql(self.db_path, df, SQLiteTable.TABLE_NAME)
+
+    @property
+    def columns(self) -> List[str]:
+        # Excludes ID column
+        return self.db_columns
+
+    @property
+    def size(self) -> int:
+        return self.db_size
+
+    @property
+    def ids(self) -> List[int]:
+        return select_as_df(
+            db_path=self.db_path,
+            table=self.sql_table.c[self.id_column],
+        )[self.id_column]
+
+    def field(self, row_id: int, column: str):
+        return select_as_df(
+            db_path=self.db_path,
+            table=self.sql_table.c[column],
+            constraints=[self.sql_table.c[self.id_column] == row_id],
+        )[column][0]
+
+    def row_as_dict(self, row_id: int) -> dict:
+        return select_as_df(
+            db_path=self.db_path,
+            table=self.sql_table,
+            constraints=[self.sql_table.c[self.id_column] == row_id],
+        ).to_dict("records")[0]
+
+    def range_rows_as_dicts(self, from_row_id: int, to_row_id: int) -> List[dict]:
+        return select_as_df(
+            db_path=self.db_path,
+            table=self.sql_table,
+            constraints=[
+                self.sql_table.c[self.id_column] >= from_row_id,
+                self.sql_table.c[self.id_column] < to_row_id,
+            ],
+        ).to_dict("records")
+
+    def select_with_constraint(self, column, value) -> List[dict]:
+        return select_as_df(
+            db_path=self.db_path,
+            table=self.sql_table,
+            constraints=[
+                self.sql_table.c[column] == value,
+            ],
+        ).to_dict("records")
+
+    def iter_rows_as_dicts(self) -> Generator[Tuple[int, dict], None, None]:
+        size = self.size
+        chunk_size = 1000  # Hardcoded for now
+        # Load in chunks
+        for chunk_start in range(0, size, chunk_size):
+            chunk_end = min(chunk_start + chunk_size, size)
+            for row in self.range_rows_as_dicts(chunk_start, chunk_end):
+                yield row[self.id_column], row
+
+    def save_meta(self, directory: Path):
+        shutil.copy(self.db_path, directory / Path(self.db_path).name)
+
+    def load_meta(self, directory: Path):
+        self.db_path = str(directory / Path(self.db_path).name)
+
+    def apply_filter(self, table_filter: TableFilter):
+        return table_filter.filter_sql_ids(
+            sqlite3.connect(self.db_path), SQLiteTable.TABLE_NAME
+        )
```

## thirdai/neural_db/teachers.py

 * *Ordering differences only*

```diff
@@ -1,44 +1,44 @@
-import math
-import random
-from typing import List, Tuple
-
-import pandas as pd
-from nltk.tokenize import sent_tokenize
-
-from . import utils
-from .loggers import Logger
-from .models.model_interface import Model
-
-
-def associate(
-    model: Model,
-    logger: Logger,
-    user_id: str,
-    text_pairs: List[Tuple[str, str]],
-    top_k: int,
-    **kwargs,
-):
-    model.associate(text_pairs, n_buckets=top_k, **kwargs)
-    logger.log(
-        session_id=user_id,
-        action="associate",
-        args={
-            "pairs": text_pairs,
-            "top_k": top_k,
-        },
-    )
-
-
-def upvote(
-    model: Model,
-    logger: Logger,
-    user_id: str,
-    query_id_para: List[Tuple[str, int, str]],
-    **kwargs,
-):
-    model.upvote([(query, _id) for query, _id, para in query_id_para], **kwargs)
-    logger.log(
-        session_id=user_id,
-        action="upvote",
-        args={"query_id_para": query_id_para},
-    )
+import math
+import random
+from typing import List, Tuple
+
+import pandas as pd
+from nltk.tokenize import sent_tokenize
+
+from . import utils
+from .loggers import Logger
+from .models.model_interface import Model
+
+
+def associate(
+    model: Model,
+    logger: Logger,
+    user_id: str,
+    text_pairs: List[Tuple[str, str]],
+    top_k: int,
+    **kwargs,
+):
+    model.associate(text_pairs, n_buckets=top_k, **kwargs)
+    logger.log(
+        session_id=user_id,
+        action="associate",
+        args={
+            "pairs": text_pairs,
+            "top_k": top_k,
+        },
+    )
+
+
+def upvote(
+    model: Model,
+    logger: Logger,
+    user_id: str,
+    query_id_para: List[Tuple[str, int, str]],
+    **kwargs,
+):
+    model.upvote([(query, _id) for query, _id, para in query_id_para], **kwargs)
+    logger.log(
+        session_id=user_id,
+        action="upvote",
+        args={"query_id_para": query_id_para},
+    )
```

## thirdai/neural_db/utils.py

 * *Ordering differences only*

```diff
@@ -1,133 +1,133 @@
-import hashlib
-import math
-import os
-import pickle
-import random
-import shutil
-from functools import wraps
-from pathlib import Path
-
-DIRECTORY_CONNECTOR_SUPPORTED_EXT = ["pdf", "docx", "pptx", "txt", "eml"]
-SUPPORTED_EXT = ["csv"] + DIRECTORY_CONNECTOR_SUPPORTED_EXT
-
-
-def convert_str_to_path(str_path):
-    if isinstance(str_path, str):
-        return Path(str_path)
-    elif isinstance(str_path, Path):
-        return str_path
-    else:
-        raise TypeError(
-            "Error converting to Path. Expected the type a 'str' or 'pathlib.Path', but"
-            f" received: {type(str_path)}"
-        )
-
-
-def pickle_to(obj: object, filepath: Path):
-    with open(filepath, "wb") as pkl:
-        pickle.dump(obj, pkl)
-
-
-def unpickle_from(filepath: Path):
-    with open(filepath, "rb") as pkl:
-        obj = pickle.load(pkl)
-    return obj
-
-
-def assert_file_exists(path: Path):
-    if not path:
-        raise ValueError("Path cannot be none")
-    if not path.exists():
-        raise FileNotFoundError(f"File not found: {str(path)}")
-
-
-def clean_text(text):
-    return text.encode("utf-8", "replace").decode("utf-8").lower()
-
-
-def hash_file(path: str, metadata=None):
-    """https://stackoverflow.com/questions/22058048/hashing-a-file-in-python"""
-    BUF_SIZE = 65536  # lets read stuff in 64kb chunks!
-
-    sha1 = hashlib.sha1()
-
-    with open(path, "rb") as f:
-        while True:
-            data = f.read(BUF_SIZE)
-            if not data:
-                break
-            sha1.update(data)
-
-    if metadata:
-        sha1.update(str(metadata).encode())
-
-    return sha1.hexdigest()
-
-
-def hash_string(string: str):
-    sha1 = hashlib.sha1(bytes(string, "utf-8"))
-    return sha1.hexdigest()
-
-
-def random_sample(sequence, k):
-    if len(sequence) > k:
-        return random.sample(sequence, k)
-    mult_factor = math.ceil(k / len(sequence))
-    return (sequence * mult_factor)[:k]
-
-
-def move_between_directories(src, dest):
-    import os
-    import shutil
-
-    # gather all files
-    allfiles = os.listdir(src)
-
-    # iterate on all files to move them to destination folder
-    for f in allfiles:
-        src_path = os.path.join(src, f)
-        dst_path = os.path.join(dest, f)
-        shutil.move(src_path, dst_path)
-
-
-def delete_folder(path: Path, ignore_errors: bool = True):
-    shutil.rmtree(path, ignore_errors=ignore_errors)
-
-
-def delete_file(path: Path, ignore_errors: bool = True):
-    try:
-        os.remove(path)
-    except:
-        if not ignore_errors:
-            raise
-
-
-# This decorator is used to raise a NotImplemented error if the check_func returns false. This is used for scenarios when a Funciton is not implemented for a particular class depending upon a condition
-def requires_condition(
-    check_func, method_name: str, method_class: str, condition_unmet_string: str = None
-):
-    def decorator(func):
-        error_message = (
-            f"The property {method_name} is not implemented for the class"
-            f" {method_class}{condition_unmet_string if condition_unmet_string else ''}"
-        )
-        if isinstance(func, property):  # If the decorator is applied to a property.
-
-            def wrapped_fget(self):
-                if check_func(self):
-                    return func.fget(self)
-                else:
-                    raise NotImplementedError(error_message)
-
-            return property(wrapped_fget, func.fset, func.fdel, func.__doc__)
-
-        @wraps(func)
-        def wrapper(self, *args, **kwargs):  # If the decorator is applied to a method.
-            if check_func(self):
-                return func(self, *args, **kwargs)
-            else:
-                raise NotImplementedError(error_message)
-
-        return wrapper
-
-    return decorator
+import hashlib
+import math
+import os
+import pickle
+import random
+import shutil
+from functools import wraps
+from pathlib import Path
+
+DIRECTORY_CONNECTOR_SUPPORTED_EXT = ["pdf", "docx", "pptx", "txt", "eml"]
+SUPPORTED_EXT = ["csv"] + DIRECTORY_CONNECTOR_SUPPORTED_EXT
+
+
+def convert_str_to_path(str_path):
+    if isinstance(str_path, str):
+        return Path(str_path)
+    elif isinstance(str_path, Path):
+        return str_path
+    else:
+        raise TypeError(
+            "Error converting to Path. Expected the type a 'str' or 'pathlib.Path', but"
+            f" received: {type(str_path)}"
+        )
+
+
+def pickle_to(obj: object, filepath: Path):
+    with open(filepath, "wb") as pkl:
+        pickle.dump(obj, pkl)
+
+
+def unpickle_from(filepath: Path):
+    with open(filepath, "rb") as pkl:
+        obj = pickle.load(pkl)
+    return obj
+
+
+def assert_file_exists(path: Path):
+    if not path:
+        raise ValueError("Path cannot be none")
+    if not path.exists():
+        raise FileNotFoundError(f"File not found: {str(path)}")
+
+
+def clean_text(text):
+    return text.encode("utf-8", "replace").decode("utf-8").lower()
+
+
+def hash_file(path: str, metadata=None):
+    """https://stackoverflow.com/questions/22058048/hashing-a-file-in-python"""
+    BUF_SIZE = 65536  # lets read stuff in 64kb chunks!
+
+    sha1 = hashlib.sha1()
+
+    with open(path, "rb") as f:
+        while True:
+            data = f.read(BUF_SIZE)
+            if not data:
+                break
+            sha1.update(data)
+
+    if metadata:
+        sha1.update(str(metadata).encode())
+
+    return sha1.hexdigest()
+
+
+def hash_string(string: str):
+    sha1 = hashlib.sha1(bytes(string, "utf-8"))
+    return sha1.hexdigest()
+
+
+def random_sample(sequence, k):
+    if len(sequence) > k:
+        return random.sample(sequence, k)
+    mult_factor = math.ceil(k / len(sequence))
+    return (sequence * mult_factor)[:k]
+
+
+def move_between_directories(src, dest):
+    import os
+    import shutil
+
+    # gather all files
+    allfiles = os.listdir(src)
+
+    # iterate on all files to move them to destination folder
+    for f in allfiles:
+        src_path = os.path.join(src, f)
+        dst_path = os.path.join(dest, f)
+        shutil.move(src_path, dst_path)
+
+
+def delete_folder(path: Path, ignore_errors: bool = True):
+    shutil.rmtree(path, ignore_errors=ignore_errors)
+
+
+def delete_file(path: Path, ignore_errors: bool = True):
+    try:
+        os.remove(path)
+    except:
+        if not ignore_errors:
+            raise
+
+
+# This decorator is used to raise a NotImplemented error if the check_func returns false. This is used for scenarios when a Funciton is not implemented for a particular class depending upon a condition
+def requires_condition(
+    check_func, method_name: str, method_class: str, condition_unmet_string: str = None
+):
+    def decorator(func):
+        error_message = (
+            f"The property {method_name} is not implemented for the class"
+            f" {method_class}{condition_unmet_string if condition_unmet_string else ''}"
+        )
+        if isinstance(func, property):  # If the decorator is applied to a property.
+
+            def wrapped_fget(self):
+                if check_func(self):
+                    return func.fget(self)
+                else:
+                    raise NotImplementedError(error_message)
+
+            return property(wrapped_fget, func.fset, func.fdel, func.__doc__)
+
+        @wraps(func)
+        def wrapper(self, *args, **kwargs):  # If the decorator is applied to a method.
+            if check_func(self):
+                return func(self, *args, **kwargs)
+            else:
+                raise NotImplementedError(error_message)
+
+        return wrapper
+
+    return decorator
```

## thirdai/neural_db/__init__.py

 * *Ordering differences only*

```diff
@@ -1,29 +1,29 @@
-import nltk
-from nltk.data import find
-
-try:
-    find("tokenizers/punkt")
-except LookupError:
-    nltk.download("punkt")
-
-from . import parsing_utils
-from .constraint_matcher import AnyOf, EqualTo, GreaterThan, InRange, LessThan
-from .documents import (
-    CSV,
-    DOCX,
-    PDF,
-    URL,
-    Document,
-    InMemoryText,
-    Reference,
-    SalesForce,
-    SentenceLevelDOCX,
-    SentenceLevelPDF,
-    SharePoint,
-    SQLDatabase,
-    Unstructured,
-)
-from .model_bazaar import Login, ModelBazaar, NeuralDBClient
-from .neural_db import CancelState, CheckpointConfig, NeuralDB, Strength, Sup
-from .question_generation import gen_questions
-from .trainer import training_data_manager, training_progress_manager
+import nltk
+from nltk.data import find
+
+try:
+    find("tokenizers/punkt")
+except LookupError:
+    nltk.download("punkt")
+
+from . import parsing_utils
+from .constraint_matcher import AnyOf, EqualTo, GreaterThan, InRange, LessThan
+from .documents import (
+    CSV,
+    DOCX,
+    PDF,
+    URL,
+    Document,
+    InMemoryText,
+    Reference,
+    SalesForce,
+    SentenceLevelDOCX,
+    SentenceLevelPDF,
+    SharePoint,
+    SQLDatabase,
+    Unstructured,
+)
+from .model_bazaar import Login, ModelBazaar, NeuralDBClient
+from .neural_db import CancelState, CheckpointConfig, NeuralDB, Strength, Sup
+from .question_generation import gen_questions
+from .trainer import training_data_manager, training_progress_manager
```

## thirdai/neural_db/models/finetunable_retriever.py

 * *Ordering differences only*

```diff
@@ -1,105 +1,105 @@
-from pathlib import Path
-from typing import Callable, List, Optional, Tuple
-
-from thirdai import search
-
-from ..documents import DocumentDataSource
-from ..supervised_datasource import SupDataSource
-from .model_interface import InferSamples, Model, Predictions, add_retriever_tag
-
-
-class FinetunableRetriever(Model):
-    def __init__(self, retriever: Optional[search.FinetunableRetriever] = None):
-        self.retriever = retriever or search.FinetunableRetriever()
-
-    def index_from_start(
-        self,
-        intro_documents: DocumentDataSource,
-        on_progress: Callable = lambda *args, **kwargs: None,
-        batch_size=100000,
-        **kwargs
-    ):
-        docs = []
-        ids = []
-
-        for row in intro_documents.row_iterator():
-            docs.append(row.strong + " " + row.weak)
-            ids.append(row.id)
-
-            if len(docs) == batch_size:
-                self.retriever.index(ids=ids, docs=docs)
-                docs = []
-                ids = []
-
-                on_progress(self.retriever.size() / intro_documents.size)
-
-        if len(docs):
-            self.retriever.index(ids=ids, docs=docs)
-            on_progress(self.retriever.size() / intro_documents.size)
-
-    def forget_documents(self) -> None:
-        self.retriever = search.FinetunableRetriever()
-
-    def delete_entities(self, entities) -> None:
-        self.retriever.remove(entities)
-
-    @property
-    def searchable(self) -> bool:
-        return self.retriever.size() > 0
-
-    def get_query_col(self) -> str:
-        return "QUERY"
-
-    def get_id_col(self) -> str:
-        return "DOC_ID"
-
-    def get_id_delimiter(self) -> str:
-        return ":"
-
-    def infer_labels(
-        self, samples: InferSamples, n_results: int, **kwargs
-    ) -> Predictions:
-        results = self.retriever.query(queries=samples, k=n_results)
-        return add_retriever_tag(results, "finetunable_retriever")
-
-    def score(
-        self, samples: InferSamples, entities: List[List[int]], n_results: int = None
-    ) -> Predictions:
-
-        # retriever.rank() expects candidates to be a list of sets
-        candidates = [set(ids) for ids in entities]
-        results = self.retriever.rank(
-            queries=samples, candidates=candidates, k=n_results
-        )
-        return add_retriever_tag(results, "finetunable_retriever")
-
-    def save_meta(self, directory: Path) -> None:
-        pass
-
-    def load_meta(self, directory: Path):
-        pass
-
-    def associate(self, pairs: List[Tuple[str, str]], retriever_strength=4, **kwargs):
-        sources, targets = list(zip(*pairs))
-        self.retriever.associate(
-            sources=sources, targets=targets, strength=retriever_strength
-        )
-
-    def upvote(self, pairs: List[Tuple[str, int]], **kwargs):
-        queries, ids = list(zip(*pairs))
-        ids = [[y] for y in ids]
-        self.retriever.finetune(doc_ids=ids, queries=queries)
-
-    def train_on_supervised_data_source(
-        self, supervised_data_source: SupDataSource, **kwargs
-    ):
-        for sup in supervised_data_source.data:
-            labels = [list(map(int, y)) for y in supervised_data_source._labels(sup)]
-
-            self.retriever.finetune(doc_ids=labels, queries=list(sup.queries))
-
-    def get_model(self):
-        return None
-
-    def retrain(self, **kwargs):
-        pass
+from pathlib import Path
+from typing import Callable, List, Optional, Tuple
+
+from thirdai import search
+
+from ..documents import DocumentDataSource
+from ..supervised_datasource import SupDataSource
+from .model_interface import InferSamples, Model, Predictions, add_retriever_tag
+
+
+class FinetunableRetriever(Model):
+    def __init__(self, retriever: Optional[search.FinetunableRetriever] = None):
+        self.retriever = retriever or search.FinetunableRetriever()
+
+    def index_from_start(
+        self,
+        intro_documents: DocumentDataSource,
+        on_progress: Callable = lambda *args, **kwargs: None,
+        batch_size=100000,
+        **kwargs
+    ):
+        docs = []
+        ids = []
+
+        for row in intro_documents.row_iterator():
+            docs.append(row.strong + " " + row.weak)
+            ids.append(row.id)
+
+            if len(docs) == batch_size:
+                self.retriever.index(ids=ids, docs=docs)
+                docs = []
+                ids = []
+
+                on_progress(self.retriever.size() / intro_documents.size)
+
+        if len(docs):
+            self.retriever.index(ids=ids, docs=docs)
+            on_progress(self.retriever.size() / intro_documents.size)
+
+    def forget_documents(self) -> None:
+        self.retriever = search.FinetunableRetriever()
+
+    def delete_entities(self, entities) -> None:
+        self.retriever.remove(entities)
+
+    @property
+    def searchable(self) -> bool:
+        return self.retriever.size() > 0
+
+    def get_query_col(self) -> str:
+        return "QUERY"
+
+    def get_id_col(self) -> str:
+        return "DOC_ID"
+
+    def get_id_delimiter(self) -> str:
+        return ":"
+
+    def infer_labels(
+        self, samples: InferSamples, n_results: int, **kwargs
+    ) -> Predictions:
+        results = self.retriever.query(queries=samples, k=n_results)
+        return add_retriever_tag(results, "finetunable_retriever")
+
+    def score(
+        self, samples: InferSamples, entities: List[List[int]], n_results: int = None
+    ) -> Predictions:
+
+        # retriever.rank() expects candidates to be a list of sets
+        candidates = [set(ids) for ids in entities]
+        results = self.retriever.rank(
+            queries=samples, candidates=candidates, k=n_results
+        )
+        return add_retriever_tag(results, "finetunable_retriever")
+
+    def save_meta(self, directory: Path) -> None:
+        pass
+
+    def load_meta(self, directory: Path):
+        pass
+
+    def associate(self, pairs: List[Tuple[str, str]], retriever_strength=4, **kwargs):
+        sources, targets = list(zip(*pairs))
+        self.retriever.associate(
+            sources=sources, targets=targets, strength=retriever_strength
+        )
+
+    def upvote(self, pairs: List[Tuple[str, int]], **kwargs):
+        queries, ids = list(zip(*pairs))
+        ids = [[y] for y in ids]
+        self.retriever.finetune(doc_ids=ids, queries=queries)
+
+    def train_on_supervised_data_source(
+        self, supervised_data_source: SupDataSource, **kwargs
+    ):
+        for sup in supervised_data_source.data:
+            labels = [list(map(int, y)) for y in supervised_data_source._labels(sup)]
+
+            self.retriever.finetune(doc_ids=labels, queries=list(sup.queries))
+
+    def get_model(self):
+        return None
+
+    def retrain(self, **kwargs):
+        pass
```

## thirdai/neural_db/models/mach.py

 * *Ordering differences only*

```diff
@@ -1,743 +1,743 @@
-from __future__ import annotations
-
-import math
-import os
-import random
-from pathlib import Path
-from typing import Callable, List, Optional, Tuple
-
-import requests
-import tqdm
-from thirdai import bolt, data, demos, search
-
-from ..documents import DocumentDataSource
-from ..supervised_datasource import SupDataSource
-from ..trainer.checkpoint_config import CheckpointConfig
-from ..trainer.training_progress_manager import (
-    TrainingProgressCallback,
-    TrainingProgressManager,
-)
-from ..utils import clean_text, pickle_to
-from .finetunable_retriever import FinetunableRetriever
-from .mach_defaults import acc_to_stop, metric_to_track
-from .model_interface import (
-    CancelState,
-    InferSamples,
-    Model,
-    Predictions,
-    add_retriever_tag,
-    merge_results,
-)
-
-
-class EarlyStopWithMinEpochs(bolt.train.callbacks.Callback):
-    def __init__(self, min_epochs, tracked_metric, metric_threshold):
-        super().__init__()
-
-        self.epoch_count = 0
-        self.min_epochs = min_epochs
-        self.tracked_metric = tracked_metric
-        self.metric_threshold = metric_threshold
-
-    def on_epoch_end(self):
-        self.epoch_count += 1
-
-        if (
-            self.epoch_count > self.min_epochs
-            and self.history[f"train_{self.tracked_metric}"][-1] > self.metric_threshold
-        ):
-            self.train_state.stop_training()
-
-
-class ProgressUpdate(bolt.train.callbacks.Callback):
-    def __init__(
-        self,
-        max_epochs,
-        progress_callback_fn,
-        total_num_batches,
-    ):
-        super().__init__()
-
-        self.batch_count = 0
-        self.max_epochs = max_epochs
-        self.progress_callback_fn = progress_callback_fn
-        self.total_num_batches = total_num_batches
-
-    def on_batch_end(self):
-        self.batch_count += 1
-
-        # We update progress every other batch because otherwise the updates are
-        # too fast for frontend components to display these changes.
-        if self.batch_count % 2:
-            batch_progress = self.batch_count / self.total_num_batches
-            progress = batch_progress / self.max_epochs
-
-            # TODO revisit this progress bar update
-            # This function (sqrt) increases faster at the beginning
-            progress = progress ** (1.0 / 2)
-            self.progress_callback_fn(progress)
-
-
-class FreezeHashTable(bolt.train.callbacks.Callback):
-    def __init__(
-        self, freeze_before_train, freeze_after_epoch, tracked_metric, metric_threshold
-    ):
-        super().__init__()
-
-        self.epoch_count = 0
-        self.freeze_after_epoch = freeze_after_epoch
-        self.tracked_metric = tracked_metric
-        self.metric_threshold = metric_threshold
-        self.freeze_before_train = freeze_before_train
-
-    def on_train_start(self):
-        if self.freeze_before_train:
-            self.model.freeze_hash_tables()
-
-    def on_epoch_end(self):
-        self.epoch_count += 1
-        if self.freeze_before_train:
-            return
-        if (self.epoch_count == self.freeze_after_epoch) or (
-            self.history[f"train_{self.tracked_metric}"][-1] > self.metric_threshold
-        ):
-            self.model.freeze_hash_tables()
-
-
-class CancelTraining(bolt.train.callbacks.Callback):
-    def __init__(self, cancel_state):
-        super().__init__()
-        self.cancel_state = cancel_state
-
-    def on_batch_end(self):
-        if self.cancel_state is not None and self.cancel_state.is_canceled():
-            self.train_state.stop_training()
-
-
-def download_semantic_enhancement_model(cache_dir, model_name="bolt-splade-medium"):
-    if not os.path.exists(cache_dir):
-        os.makedirs(cache_dir)
-
-    semantic_model_path = os.path.join(cache_dir, model_name)
-    if not os.path.exists(semantic_model_path):
-        response = requests.get(
-            "https://modelzoo-cdn.azureedge.net/test-models/bolt-splade-medium",
-            stream=True,
-        )
-        total_size_in_bytes = int(response.headers.get("content-length", 0))
-        block_size = 4096  # 4 Kibibyte
-
-        progress_bar = tqdm.tqdm(total=total_size_in_bytes, unit="iB", unit_scale=True)
-        with open(semantic_model_path, "wb") as f:
-            for data_chunk in response.iter_content(block_size):
-                progress_bar.update(len(data_chunk))
-                f.write(data_chunk)
-        progress_bar.close()
-
-    vocab_path = os.path.join(cache_dir, "bert-base-uncased.vocab")
-    if not os.path.exists(vocab_path):
-        demos.bert_base_uncased(dirname=cache_dir)
-
-    return data.transformations.SpladeConfig(
-        model_checkpoint=semantic_model_path, tokenizer_vocab=vocab_path
-    )
-
-
-def unsupervised_train_on_docs(
-    model,
-    documents: DocumentDataSource,
-    min_epochs: int,
-    max_epochs: int,
-    metric: str,
-    learning_rate: float,
-    batch_size: int,
-    acc_to_stop: float,
-    on_progress: Callable,
-    freeze_before_train: bool,
-    freeze_after_epoch: int,
-    freeze_after_acc: float,
-    cancel_state: CancelState,
-    max_in_memory_batches: int,
-    variable_length: Optional[data.transformations.VariableLengthConfig],
-    training_progress_callback: Optional[TrainingProgressCallback],
-    balancing_samples=False,
-    semantic_enhancement=False,
-    semantic_model_cache_dir=".cache/neural_db_semantic_model",
-    coldstart_callbacks: List[bolt.train.callbacks.Callback] = None,
-    **kwargs,
-):
-    documents.restart()
-
-    early_stop_callback = EarlyStopWithMinEpochs(
-        min_epochs=min_epochs, tracked_metric=metric, metric_threshold=acc_to_stop
-    )
-
-    progress_callback = ProgressUpdate(
-        max_epochs=max_epochs,
-        progress_callback_fn=on_progress,
-        total_num_batches=(
-            math.ceil(documents.size / batch_size)
-            if batch_size
-            else math.ceil(documents.size / 2048)  # default batch size we use in UDT.
-        ),
-    )
-
-    cancel_training_callback = CancelTraining(cancel_state=cancel_state)
-
-    freeze_hashtable_callback = FreezeHashTable(
-        freeze_before_train=freeze_before_train,
-        freeze_after_epoch=freeze_after_epoch,
-        tracked_metric=metric,
-        metric_threshold=freeze_after_acc,
-    )
-
-    callbacks = [
-        early_stop_callback,
-        progress_callback,
-        cancel_training_callback,
-        freeze_hashtable_callback,
-    ]
-
-    if coldstart_callbacks:
-        callbacks.extend(coldstart_callbacks)
-
-    if training_progress_callback:
-        callbacks.append(training_progress_callback)
-
-    splade_config = None
-    if semantic_enhancement:
-        splade_config = download_semantic_enhancement_model(semantic_model_cache_dir)
-
-    if balancing_samples:
-        model.cold_start_with_balancing_samples(
-            data=documents,
-            strong_column_names=[documents.strong_column],
-            weak_column_names=[documents.weak_column],
-            batch_size=batch_size,
-            learning_rate=learning_rate,
-            epochs=max_epochs,
-            train_metrics=[metric],
-            callbacks=callbacks,
-            variable_length=variable_length,
-        )
-    else:
-        model.cold_start_on_data_source(
-            data_source=documents,
-            strong_column_names=[documents.strong_column],
-            weak_column_names=[documents.weak_column],
-            batch_size=batch_size,
-            learning_rate=learning_rate,
-            epochs=max_epochs,
-            metrics=[metric],
-            callbacks=callbacks,
-            max_in_memory_batches=max_in_memory_batches,
-            variable_length=variable_length,
-            splade_config=splade_config,
-        )
-
-
-def make_balancing_samples(documents: DocumentDataSource):
-    samples = [
-        (". ".join([row.strong, row.weak]), [row.id])
-        for row in documents.row_iterator()
-    ]
-    if len(samples) > 25000:
-        samples = random.sample(samples, k=25000)
-    return samples
-
-
-class Mach(Model):
-    def __init__(
-        self,
-        id_col="DOC_ID",
-        id_delimiter=" ",
-        query_col="QUERY",
-        fhr=50_000,
-        embedding_dimension=2048,
-        extreme_output_dim=50_000,
-        extreme_num_hashes=8,
-        tokenizer="char-4",
-        hidden_bias=False,
-        model_config=None,
-        hybrid=True,
-        mach_index_seed: int = 341,
-        index_max_shard_size=8_000_000,
-        **kwargs,
-    ):
-        self.id_col = id_col
-        self.id_delimiter = id_delimiter
-        self.tokenizer = tokenizer
-        self.query_col = query_col
-        self.fhr = fhr
-        self.embedding_dimension = embedding_dimension
-        self.extreme_output_dim = extreme_output_dim
-        self.extreme_num_hashes = extreme_num_hashes
-        self.hidden_bias = hidden_bias
-        self.n_ids = 0
-        self.model = None
-        self.balancing_samples = []
-        self.model_config = model_config
-        self.mach_index_seed = mach_index_seed
-
-        if hybrid:
-            self.finetunable_retriever = FinetunableRetriever()
-        else:
-            self.finetunable_retriever = None
-
-    def set_mach_sampling_threshold(self, threshold: float):
-        if self.model is None:
-            raise Exception(
-                "Cannot set Sampling Threshold for a model that has not been"
-                " initialized"
-            )
-        self.model.set_mach_sampling_threshold(threshold)
-
-    def reset_model(self, new_model: Mach):
-        self.id_col = new_model.id_col
-        self.id_delimiter = new_model.id_delimiter
-        self.tokenizer = new_model.tokenizer
-        self.query_col = new_model.query_col
-        self.fhr = new_model.fhr
-        self.embedding_dimension = new_model.embedding_dimension
-        self.extreme_output_dim = new_model.extreme_output_dim
-        self.extreme_num_hashes = new_model.extreme_num_hashes
-        self.hidden_bias = new_model.hidden_bias
-        self.n_ids = new_model.n_ids
-        self.model = new_model.model
-        self.balancing_samples = new_model.balancing_samples
-        self.model_config = new_model.model_config
-        self.finetunable_retriever = new_model.finetunable_retriever
-
-    def save(self, path: Path):
-        pickle_to(self, filepath=path)
-
-    def get_model(self) -> bolt.UniversalDeepTransformer:
-        return self.model
-
-    def set_model(self, model):
-        self.model = model
-
-    def save_meta(self, directory: Path):
-        pass
-
-    def load_meta(self, directory: Path):
-        pass
-
-    def set_n_ids(self, n_ids: int):
-        self.n_ids = n_ids
-
-    def get_query_col(self) -> str:
-        return self.query_col
-
-    def get_id_col(self) -> str:
-        return self.id_col
-
-    def get_id_delimiter(self) -> str:
-        return self.id_delimiter
-
-    def introduce_documents(
-        self,
-        intro_documents: DocumentDataSource,
-        fast_approximation: bool,
-        num_buckets_to_sample: Optional[int],
-        override_number_classes: int,
-    ):
-        if intro_documents.id_column != self.id_col:
-            raise ValueError(
-                f"Model configured to use id_col={self.id_col}, received document with"
-                f" id_col={intro_documents.id_column}"
-            )
-
-        if self.model is None:
-            self.id_col = intro_documents.id_column
-            self.model = self.model_from_scratch(
-                intro_documents, number_classes=override_number_classes
-            )
-        else:
-            if intro_documents.size > 0:
-                doc_id = intro_documents.id_column
-                if doc_id != self.id_col:
-                    raise ValueError(
-                        f"Document has a different id column ({doc_id}) than the model"
-                        f" configuration ({self.id_col})."
-                    )
-
-                num_buckets_to_sample = num_buckets_to_sample or int(
-                    self.model.get_index().num_hashes() * 2.0
-                )
-
-                self.model.introduce_documents_on_data_source(
-                    data_source=intro_documents,
-                    strong_column_names=[intro_documents.strong_column],
-                    weak_column_names=[intro_documents.weak_column],
-                    fast_approximation=fast_approximation,
-                    num_buckets_to_sample=num_buckets_to_sample,
-                )
-
-        if self.finetunable_retriever:
-            intro_documents.restart()
-            self.finetunable_retriever.index_from_start(intro_documents)
-
-        self.n_ids += intro_documents.size
-
-    def index_documents_impl(
-        self,
-        training_progress_manager: TrainingProgressManager,
-        on_progress: Callable = lambda **kwargs: None,
-        cancel_state: CancelState = None,
-        callbacks: List[bolt.train.callbacks.Callback] = None,
-    ):
-        intro_documents = training_progress_manager.intro_source
-        train_documents = training_progress_manager.train_source
-
-        if not training_progress_manager.is_insert_completed:
-            self.introduce_documents(
-                intro_documents=intro_documents,
-                **training_progress_manager.introduce_arguments(),
-            )
-            training_progress_manager.insert_complete()
-
-        if not training_progress_manager.is_training_completed:
-            train_arguments = training_progress_manager.training_arguments()
-            unsupervised_train_on_docs(
-                model=self.model,
-                documents=train_documents,
-                metric=metric_to_track,
-                acc_to_stop=acc_to_stop,
-                on_progress=on_progress,
-                cancel_state=cancel_state,
-                training_progress_callback=TrainingProgressCallback(
-                    training_progress_manager=training_progress_manager
-                ),
-                coldstart_callbacks=callbacks,
-                **train_arguments,
-            )
-            training_progress_manager.training_complete()
-
-    def resume(
-        self,
-        on_progress: Callable,
-        cancel_state: CancelState,
-        checkpoint_config: CheckpointConfig,
-        callbacks: List[bolt.train.callbacks.Callback] = None,
-    ):
-        # This will load the datasources, model, training config and upload the current model with the loaded one. This updates the underlying UDT MACH of the current model with the one from the checkpoint along with other class attributes.
-        training_progress_manager = TrainingProgressManager.from_checkpoint(
-            self, checkpoint_config=checkpoint_config, for_supervised=False
-        )
-
-        self.index_documents_impl(
-            training_progress_manager=training_progress_manager,
-            on_progress=on_progress,
-            cancel_state=cancel_state,
-            callbacks=callbacks,
-        )
-
-    def index_from_start(
-        self,
-        intro_documents: DocumentDataSource,
-        train_documents: DocumentDataSource,
-        should_train: bool,
-        fast_approximation: bool = True,
-        num_buckets_to_sample: Optional[int] = None,
-        on_progress: Callable = lambda **kwargs: None,
-        cancel_state: CancelState = None,
-        max_in_memory_batches: int = None,
-        override_number_classes: int = None,
-        variable_length: Optional[
-            data.transformations.VariableLengthConfig
-        ] = data.transformations.VariableLengthConfig(),
-        checkpoint_config: CheckpointConfig = None,
-        callbacks: List[bolt.train.callbacks.Callback] = None,
-        **kwargs,
-    ):
-        """
-        override_number_classes : The number of classes for the Mach model
-
-        Note: Given the datasources for introduction and training, we initialize a Mach model that has number_classes set to the size of introduce documents. But if we want to use this Mach model in our mixture of Models, this will not work because each Mach will be initialized with number of classes equal to the size of the datasource shard. Hence, we add override_number_classes parameters which if set, will initialize Mach Model with number of classes passed by the Mach Mixture.
-        """
-
-        training_progress_manager = (
-            TrainingProgressManager.from_scratch_for_unsupervised(
-                model=self,
-                intro_documents=intro_documents,
-                train_documents=train_documents,
-                should_train=should_train,
-                fast_approximation=fast_approximation,
-                num_buckets_to_sample=num_buckets_to_sample,
-                max_in_memory_batches=max_in_memory_batches,
-                override_number_classes=override_number_classes,
-                variable_length=variable_length,
-                checkpoint_config=checkpoint_config,
-                **kwargs,
-            )
-        )
-
-        training_progress_manager.make_preindexing_checkpoint()
-        self.index_documents_impl(
-            training_progress_manager=training_progress_manager,
-            on_progress=on_progress,
-            cancel_state=cancel_state,
-            callbacks=callbacks,
-        )
-
-    def add_balancing_samples(self, documents: DocumentDataSource):
-        samples = make_balancing_samples(documents)
-        self.balancing_samples += samples
-        if len(self.balancing_samples) > 25000:
-            self.balancing_samples = random.sample(self.balancing_samples, k=25000)
-
-    def delete_entities(self, entities) -> None:
-        for entity in entities:
-            self.get_model().forget(entity)
-
-        if self.finetunable_retriever:
-            self.finetunable_retriever.delete_entities(entities)
-
-    def model_from_scratch(
-        self, documents: DocumentDataSource, number_classes: int = None
-    ):
-        model = bolt.UniversalDeepTransformer(
-            data_types={
-                self.query_col: bolt.types.text(tokenizer=self.tokenizer),
-                self.id_col: bolt.types.categorical(delimiter=self.id_delimiter),
-            },
-            target=self.id_col,
-            n_target_classes=(
-                documents.size if number_classes is None else number_classes
-            ),
-            integer_target=True,
-            options={
-                "extreme_classification": True,
-                "extreme_output_dim": self.extreme_output_dim,
-                "fhr": self.fhr,
-                "embedding_dimension": self.embedding_dimension,
-                "extreme_num_hashes": self.extreme_num_hashes,
-                "hidden_bias": self.hidden_bias,
-                "rlhf": True,
-                "mach_index_seed": self.mach_index_seed,
-            },
-            model_config=self.model_config,
-        )
-        model.insert_new_doc_ids(documents)
-        return model
-
-    def forget_documents(self) -> None:
-        if self.model is not None:
-            self.model.clear_index()
-        self.n_ids = 0
-        self.balancing_samples = []
-
-        if self.finetunable_retriever:
-            self.finetunable_retriever.forget_documents()
-
-    @property
-    def searchable(self) -> bool:
-        return self.n_ids != 0
-
-    def query_mach(self, samples, n_results):
-        self.model.set_decode_params(min(self.n_ids, n_results), min(self.n_ids, 100))
-        infer_batch = self.infer_samples_to_infer_batch(samples)
-        return add_retriever_tag(
-            results=self.model.predict_batch(infer_batch), tag="mach"
-        )
-
-    def query_finetunable_retriever(self, samples, n_results):
-        return self.finetunable_retriever.infer_labels(
-            samples=samples, n_results=min(self.n_ids, n_results)
-        )
-
-    def infer_labels(
-        self,
-        samples: InferSamples,
-        n_results: int,
-        retriever=None,
-        mach_first=False,
-        **kwargs,
-    ) -> Predictions:
-        if not retriever:
-            if not self.finetunable_retriever:
-                retriever = "mach"
-            else:
-                mach_results = self.query_mach(samples=samples, n_results=n_results)
-                index_results = self.query_finetunable_retriever(
-                    samples=samples, n_results=n_results
-                )
-                return [
-                    (
-                        merge_results(mach_res, index_res, n_results)
-                        if mach_first
-                        # Prioritize retriver results.
-                        else merge_results(index_res, mach_res, n_results)
-                    )
-                    for mach_res, index_res in zip(mach_results, index_results)
-                ]
-
-        if retriever == "mach":
-            return self.query_mach(samples=samples, n_results=n_results)
-
-        if retriever == "finetunable_retriever":
-            if not self.finetunable_retriever:
-                raise ValueError(
-                    "Cannot use retriever 'finetunable_retriever' since the retriever is None."
-                )
-            return self.query_finetunable_retriever(
-                samples=samples, n_results=n_results
-            )
-
-        raise ValueError(
-            f"Invalid retriever '{retriever}'. Please use 'mach', 'finetunable_retriever', "
-            "or pass None to allow the model to autotune which is used."
-        )
-
-    def score(
-        self, samples: InferSamples, entities: List[List[int]], n_results: int = None
-    ) -> Predictions:
-        infer_batch = self.infer_samples_to_infer_batch(samples)
-        results = self.model.score_batch(infer_batch, classes=entities, top_k=n_results)
-        return add_retriever_tag(results=results, tag="mach")
-
-    def _format_associate_samples(self, pairs: List[Tuple[str, str]]):
-        return [(clean_text(source), clean_text(target)) for source, target in pairs]
-
-    def associate(
-        self,
-        pairs: List[Tuple[str, str]],
-        n_buckets: int,
-        n_association_samples: int = 16,
-        n_balancing_samples: int = 50,
-        learning_rate: float = 0.001,
-        epochs: int = 3,
-        **kwargs,
-    ):
-        self.model.associate(
-            source_target_samples=self._format_associate_samples(pairs),
-            n_buckets=n_buckets,
-            n_association_samples=n_association_samples,
-            n_balancing_samples=n_balancing_samples,
-            learning_rate=learning_rate,
-            epochs=epochs,
-            force_non_empty=kwargs.get("force_non_empty", True),
-        )
-
-        if self.finetunable_retriever:
-            self.finetunable_retriever.associate(pairs)
-
-    def upvote(
-        self,
-        pairs: List[Tuple[str, int]],
-        n_upvote_samples: int = 16,
-        n_balancing_samples: int = 50,
-        learning_rate: float = 0.001,
-        epochs: int = 3,
-    ):
-        samples = [(clean_text(text), label) for text, label in pairs]
-
-        self.model.upvote(
-            source_target_samples=samples,
-            n_upvote_samples=n_upvote_samples,
-            n_balancing_samples=n_balancing_samples,
-            learning_rate=learning_rate,
-            epochs=epochs,
-        )
-
-        if self.finetunable_retriever:
-            self.finetunable_retriever.upvote(pairs)
-
-    def retrain(
-        self,
-        balancing_data: DocumentDataSource,
-        source_target_pairs: List[Tuple[str, str]],
-        n_buckets: int,
-        learning_rate: float,
-        epochs: int,
-    ):
-        self.model.associate_cold_start_data_source(
-            balancing_data=balancing_data,
-            strong_column_names=[balancing_data.strong_column],
-            weak_column_names=[balancing_data.weak_column],
-            source_target_samples=self._format_associate_samples(source_target_pairs),
-            n_buckets=n_buckets,
-            n_association_samples=1,
-            learning_rate=learning_rate,
-            epochs=epochs,
-            metrics=["hash_precision@5"],
-            options=bolt.TrainOptions(),
-        )
-
-    def __setstate__(self, state):
-        if "model_config" not in state:
-            # Add model_config field if an older model is being loaded.
-            state["model_config"] = None
-        if "finetunable_retriever" not in state:
-            state["finetunable_retriever"] = None
-        if "inverted_index" in state:
-            state["finetunable_retriever"] = FinetunableRetriever(
-                search.FinetunableRetriever.train_from(state["inverted_index"].export())
-            )
-        self.__dict__.update(state)
-
-    def supervised_training_impl(
-        self,
-        supervised_progress_manager: TrainingProgressManager,
-        callbacks: List[bolt.train.callbacks.Callback],
-    ):
-        if not supervised_progress_manager.is_training_completed:
-            train_args = supervised_progress_manager.training_arguments()
-            self.model.train_on_data_source(
-                data_source=supervised_progress_manager.train_source,
-                callbacks=callbacks
-                + [
-                    TrainingProgressCallback(
-                        training_progress_manager=supervised_progress_manager
-                    )
-                ],
-                **train_args,
-            )
-
-            if (
-                supervised_progress_manager.tracker._train_state.disable_finetunable_retriever
-            ):
-                self.finetunable_retriever = None
-            elif self.finetunable_retriever:
-                supervised_progress_manager.train_source.restart()
-                self.finetunable_retriever.train_on_supervised_data_source(
-                    supervised_progress_manager.train_source
-                )
-
-            supervised_progress_manager.training_complete()
-
-    def train_on_supervised_data_source(
-        self,
-        supervised_data_source: SupDataSource,
-        learning_rate: float,
-        epochs: int,
-        batch_size: Optional[int],
-        max_in_memory_batches: Optional[int],
-        metrics: List[str],
-        callbacks: List[bolt.train.callbacks.Callback],
-        disable_finetunable_retriever: bool,
-        checkpoint_config: Optional[CheckpointConfig] = None,
-    ):
-        if (
-            checkpoint_config is None
-            or checkpoint_config.resume_from_checkpoint is False
-        ):
-            training_manager = TrainingProgressManager.from_scratch_for_supervised(
-                model=self,
-                supervised_datasource=supervised_data_source,
-                learning_rate=learning_rate,
-                epochs=epochs,
-                batch_size=batch_size,
-                max_in_memory_batches=max_in_memory_batches,
-                metrics=metrics,
-                disable_finetunable_retriever=disable_finetunable_retriever,
-                checkpoint_config=checkpoint_config,
-            )
-            training_manager.make_preindexing_checkpoint(save_datasource=True)
-        else:
-            training_manager = TrainingProgressManager.from_checkpoint(
-                self, checkpoint_config, for_supervised=True
-            )
-
-        self.supervised_training_impl(training_manager, callbacks=callbacks)
+from __future__ import annotations
+
+import math
+import os
+import random
+from pathlib import Path
+from typing import Callable, List, Optional, Tuple
+
+import requests
+import tqdm
+from thirdai import bolt, data, demos, search
+
+from ..documents import DocumentDataSource
+from ..supervised_datasource import SupDataSource
+from ..trainer.checkpoint_config import CheckpointConfig
+from ..trainer.training_progress_manager import (
+    TrainingProgressCallback,
+    TrainingProgressManager,
+)
+from ..utils import clean_text, pickle_to
+from .finetunable_retriever import FinetunableRetriever
+from .mach_defaults import acc_to_stop, metric_to_track
+from .model_interface import (
+    CancelState,
+    InferSamples,
+    Model,
+    Predictions,
+    add_retriever_tag,
+    merge_results,
+)
+
+
+class EarlyStopWithMinEpochs(bolt.train.callbacks.Callback):
+    def __init__(self, min_epochs, tracked_metric, metric_threshold):
+        super().__init__()
+
+        self.epoch_count = 0
+        self.min_epochs = min_epochs
+        self.tracked_metric = tracked_metric
+        self.metric_threshold = metric_threshold
+
+    def on_epoch_end(self):
+        self.epoch_count += 1
+
+        if (
+            self.epoch_count > self.min_epochs
+            and self.history[f"train_{self.tracked_metric}"][-1] > self.metric_threshold
+        ):
+            self.train_state.stop_training()
+
+
+class ProgressUpdate(bolt.train.callbacks.Callback):
+    def __init__(
+        self,
+        max_epochs,
+        progress_callback_fn,
+        total_num_batches,
+    ):
+        super().__init__()
+
+        self.batch_count = 0
+        self.max_epochs = max_epochs
+        self.progress_callback_fn = progress_callback_fn
+        self.total_num_batches = total_num_batches
+
+    def on_batch_end(self):
+        self.batch_count += 1
+
+        # We update progress every other batch because otherwise the updates are
+        # too fast for frontend components to display these changes.
+        if self.batch_count % 2:
+            batch_progress = self.batch_count / self.total_num_batches
+            progress = batch_progress / self.max_epochs
+
+            # TODO revisit this progress bar update
+            # This function (sqrt) increases faster at the beginning
+            progress = progress ** (1.0 / 2)
+            self.progress_callback_fn(progress)
+
+
+class FreezeHashTable(bolt.train.callbacks.Callback):
+    def __init__(
+        self, freeze_before_train, freeze_after_epoch, tracked_metric, metric_threshold
+    ):
+        super().__init__()
+
+        self.epoch_count = 0
+        self.freeze_after_epoch = freeze_after_epoch
+        self.tracked_metric = tracked_metric
+        self.metric_threshold = metric_threshold
+        self.freeze_before_train = freeze_before_train
+
+    def on_train_start(self):
+        if self.freeze_before_train:
+            self.model.freeze_hash_tables()
+
+    def on_epoch_end(self):
+        self.epoch_count += 1
+        if self.freeze_before_train:
+            return
+        if (self.epoch_count == self.freeze_after_epoch) or (
+            self.history[f"train_{self.tracked_metric}"][-1] > self.metric_threshold
+        ):
+            self.model.freeze_hash_tables()
+
+
+class CancelTraining(bolt.train.callbacks.Callback):
+    def __init__(self, cancel_state):
+        super().__init__()
+        self.cancel_state = cancel_state
+
+    def on_batch_end(self):
+        if self.cancel_state is not None and self.cancel_state.is_canceled():
+            self.train_state.stop_training()
+
+
+def download_semantic_enhancement_model(cache_dir, model_name="bolt-splade-medium"):
+    if not os.path.exists(cache_dir):
+        os.makedirs(cache_dir)
+
+    semantic_model_path = os.path.join(cache_dir, model_name)
+    if not os.path.exists(semantic_model_path):
+        response = requests.get(
+            "https://modelzoo-cdn.azureedge.net/test-models/bolt-splade-medium",
+            stream=True,
+        )
+        total_size_in_bytes = int(response.headers.get("content-length", 0))
+        block_size = 4096  # 4 Kibibyte
+
+        progress_bar = tqdm.tqdm(total=total_size_in_bytes, unit="iB", unit_scale=True)
+        with open(semantic_model_path, "wb") as f:
+            for data_chunk in response.iter_content(block_size):
+                progress_bar.update(len(data_chunk))
+                f.write(data_chunk)
+        progress_bar.close()
+
+    vocab_path = os.path.join(cache_dir, "bert-base-uncased.vocab")
+    if not os.path.exists(vocab_path):
+        demos.bert_base_uncased(dirname=cache_dir)
+
+    return data.transformations.SpladeConfig(
+        model_checkpoint=semantic_model_path, tokenizer_vocab=vocab_path
+    )
+
+
+def unsupervised_train_on_docs(
+    model,
+    documents: DocumentDataSource,
+    min_epochs: int,
+    max_epochs: int,
+    metric: str,
+    learning_rate: float,
+    batch_size: int,
+    acc_to_stop: float,
+    on_progress: Callable,
+    freeze_before_train: bool,
+    freeze_after_epoch: int,
+    freeze_after_acc: float,
+    cancel_state: CancelState,
+    max_in_memory_batches: int,
+    variable_length: Optional[data.transformations.VariableLengthConfig],
+    training_progress_callback: Optional[TrainingProgressCallback],
+    balancing_samples=False,
+    semantic_enhancement=False,
+    semantic_model_cache_dir=".cache/neural_db_semantic_model",
+    coldstart_callbacks: List[bolt.train.callbacks.Callback] = None,
+    **kwargs,
+):
+    documents.restart()
+
+    early_stop_callback = EarlyStopWithMinEpochs(
+        min_epochs=min_epochs, tracked_metric=metric, metric_threshold=acc_to_stop
+    )
+
+    progress_callback = ProgressUpdate(
+        max_epochs=max_epochs,
+        progress_callback_fn=on_progress,
+        total_num_batches=(
+            math.ceil(documents.size / batch_size)
+            if batch_size
+            else math.ceil(documents.size / 2048)  # default batch size we use in UDT.
+        ),
+    )
+
+    cancel_training_callback = CancelTraining(cancel_state=cancel_state)
+
+    freeze_hashtable_callback = FreezeHashTable(
+        freeze_before_train=freeze_before_train,
+        freeze_after_epoch=freeze_after_epoch,
+        tracked_metric=metric,
+        metric_threshold=freeze_after_acc,
+    )
+
+    callbacks = [
+        early_stop_callback,
+        progress_callback,
+        cancel_training_callback,
+        freeze_hashtable_callback,
+    ]
+
+    if coldstart_callbacks:
+        callbacks.extend(coldstart_callbacks)
+
+    if training_progress_callback:
+        callbacks.append(training_progress_callback)
+
+    splade_config = None
+    if semantic_enhancement:
+        splade_config = download_semantic_enhancement_model(semantic_model_cache_dir)
+
+    if balancing_samples:
+        model.cold_start_with_balancing_samples(
+            data=documents,
+            strong_column_names=[documents.strong_column],
+            weak_column_names=[documents.weak_column],
+            batch_size=batch_size,
+            learning_rate=learning_rate,
+            epochs=max_epochs,
+            train_metrics=[metric],
+            callbacks=callbacks,
+            variable_length=variable_length,
+        )
+    else:
+        model.cold_start_on_data_source(
+            data_source=documents,
+            strong_column_names=[documents.strong_column],
+            weak_column_names=[documents.weak_column],
+            batch_size=batch_size,
+            learning_rate=learning_rate,
+            epochs=max_epochs,
+            metrics=[metric],
+            callbacks=callbacks,
+            max_in_memory_batches=max_in_memory_batches,
+            variable_length=variable_length,
+            splade_config=splade_config,
+        )
+
+
+def make_balancing_samples(documents: DocumentDataSource):
+    samples = [
+        (". ".join([row.strong, row.weak]), [row.id])
+        for row in documents.row_iterator()
+    ]
+    if len(samples) > 25000:
+        samples = random.sample(samples, k=25000)
+    return samples
+
+
+class Mach(Model):
+    def __init__(
+        self,
+        id_col="DOC_ID",
+        id_delimiter=" ",
+        query_col="QUERY",
+        fhr=50_000,
+        embedding_dimension=2048,
+        extreme_output_dim=50_000,
+        extreme_num_hashes=8,
+        tokenizer="char-4",
+        hidden_bias=False,
+        model_config=None,
+        hybrid=True,
+        mach_index_seed: int = 341,
+        index_max_shard_size=8_000_000,
+        **kwargs,
+    ):
+        self.id_col = id_col
+        self.id_delimiter = id_delimiter
+        self.tokenizer = tokenizer
+        self.query_col = query_col
+        self.fhr = fhr
+        self.embedding_dimension = embedding_dimension
+        self.extreme_output_dim = extreme_output_dim
+        self.extreme_num_hashes = extreme_num_hashes
+        self.hidden_bias = hidden_bias
+        self.n_ids = 0
+        self.model = None
+        self.balancing_samples = []
+        self.model_config = model_config
+        self.mach_index_seed = mach_index_seed
+
+        if hybrid:
+            self.finetunable_retriever = FinetunableRetriever()
+        else:
+            self.finetunable_retriever = None
+
+    def set_mach_sampling_threshold(self, threshold: float):
+        if self.model is None:
+            raise Exception(
+                "Cannot set Sampling Threshold for a model that has not been"
+                " initialized"
+            )
+        self.model.set_mach_sampling_threshold(threshold)
+
+    def reset_model(self, new_model: Mach):
+        self.id_col = new_model.id_col
+        self.id_delimiter = new_model.id_delimiter
+        self.tokenizer = new_model.tokenizer
+        self.query_col = new_model.query_col
+        self.fhr = new_model.fhr
+        self.embedding_dimension = new_model.embedding_dimension
+        self.extreme_output_dim = new_model.extreme_output_dim
+        self.extreme_num_hashes = new_model.extreme_num_hashes
+        self.hidden_bias = new_model.hidden_bias
+        self.n_ids = new_model.n_ids
+        self.model = new_model.model
+        self.balancing_samples = new_model.balancing_samples
+        self.model_config = new_model.model_config
+        self.finetunable_retriever = new_model.finetunable_retriever
+
+    def save(self, path: Path):
+        pickle_to(self, filepath=path)
+
+    def get_model(self) -> bolt.UniversalDeepTransformer:
+        return self.model
+
+    def set_model(self, model):
+        self.model = model
+
+    def save_meta(self, directory: Path):
+        pass
+
+    def load_meta(self, directory: Path):
+        pass
+
+    def set_n_ids(self, n_ids: int):
+        self.n_ids = n_ids
+
+    def get_query_col(self) -> str:
+        return self.query_col
+
+    def get_id_col(self) -> str:
+        return self.id_col
+
+    def get_id_delimiter(self) -> str:
+        return self.id_delimiter
+
+    def introduce_documents(
+        self,
+        intro_documents: DocumentDataSource,
+        fast_approximation: bool,
+        num_buckets_to_sample: Optional[int],
+        override_number_classes: int,
+    ):
+        if intro_documents.id_column != self.id_col:
+            raise ValueError(
+                f"Model configured to use id_col={self.id_col}, received document with"
+                f" id_col={intro_documents.id_column}"
+            )
+
+        if self.model is None:
+            self.id_col = intro_documents.id_column
+            self.model = self.model_from_scratch(
+                intro_documents, number_classes=override_number_classes
+            )
+        else:
+            if intro_documents.size > 0:
+                doc_id = intro_documents.id_column
+                if doc_id != self.id_col:
+                    raise ValueError(
+                        f"Document has a different id column ({doc_id}) than the model"
+                        f" configuration ({self.id_col})."
+                    )
+
+                num_buckets_to_sample = num_buckets_to_sample or int(
+                    self.model.get_index().num_hashes() * 2.0
+                )
+
+                self.model.introduce_documents_on_data_source(
+                    data_source=intro_documents,
+                    strong_column_names=[intro_documents.strong_column],
+                    weak_column_names=[intro_documents.weak_column],
+                    fast_approximation=fast_approximation,
+                    num_buckets_to_sample=num_buckets_to_sample,
+                )
+
+        if self.finetunable_retriever:
+            intro_documents.restart()
+            self.finetunable_retriever.index_from_start(intro_documents)
+
+        self.n_ids += intro_documents.size
+
+    def index_documents_impl(
+        self,
+        training_progress_manager: TrainingProgressManager,
+        on_progress: Callable = lambda **kwargs: None,
+        cancel_state: CancelState = None,
+        callbacks: List[bolt.train.callbacks.Callback] = None,
+    ):
+        intro_documents = training_progress_manager.intro_source
+        train_documents = training_progress_manager.train_source
+
+        if not training_progress_manager.is_insert_completed:
+            self.introduce_documents(
+                intro_documents=intro_documents,
+                **training_progress_manager.introduce_arguments(),
+            )
+            training_progress_manager.insert_complete()
+
+        if not training_progress_manager.is_training_completed:
+            train_arguments = training_progress_manager.training_arguments()
+            unsupervised_train_on_docs(
+                model=self.model,
+                documents=train_documents,
+                metric=metric_to_track,
+                acc_to_stop=acc_to_stop,
+                on_progress=on_progress,
+                cancel_state=cancel_state,
+                training_progress_callback=TrainingProgressCallback(
+                    training_progress_manager=training_progress_manager
+                ),
+                coldstart_callbacks=callbacks,
+                **train_arguments,
+            )
+            training_progress_manager.training_complete()
+
+    def resume(
+        self,
+        on_progress: Callable,
+        cancel_state: CancelState,
+        checkpoint_config: CheckpointConfig,
+        callbacks: List[bolt.train.callbacks.Callback] = None,
+    ):
+        # This will load the datasources, model, training config and upload the current model with the loaded one. This updates the underlying UDT MACH of the current model with the one from the checkpoint along with other class attributes.
+        training_progress_manager = TrainingProgressManager.from_checkpoint(
+            self, checkpoint_config=checkpoint_config, for_supervised=False
+        )
+
+        self.index_documents_impl(
+            training_progress_manager=training_progress_manager,
+            on_progress=on_progress,
+            cancel_state=cancel_state,
+            callbacks=callbacks,
+        )
+
+    def index_from_start(
+        self,
+        intro_documents: DocumentDataSource,
+        train_documents: DocumentDataSource,
+        should_train: bool,
+        fast_approximation: bool = True,
+        num_buckets_to_sample: Optional[int] = None,
+        on_progress: Callable = lambda **kwargs: None,
+        cancel_state: CancelState = None,
+        max_in_memory_batches: int = None,
+        override_number_classes: int = None,
+        variable_length: Optional[
+            data.transformations.VariableLengthConfig
+        ] = data.transformations.VariableLengthConfig(),
+        checkpoint_config: CheckpointConfig = None,
+        callbacks: List[bolt.train.callbacks.Callback] = None,
+        **kwargs,
+    ):
+        """
+        override_number_classes : The number of classes for the Mach model
+
+        Note: Given the datasources for introduction and training, we initialize a Mach model that has number_classes set to the size of introduce documents. But if we want to use this Mach model in our mixture of Models, this will not work because each Mach will be initialized with number of classes equal to the size of the datasource shard. Hence, we add override_number_classes parameters which if set, will initialize Mach Model with number of classes passed by the Mach Mixture.
+        """
+
+        training_progress_manager = (
+            TrainingProgressManager.from_scratch_for_unsupervised(
+                model=self,
+                intro_documents=intro_documents,
+                train_documents=train_documents,
+                should_train=should_train,
+                fast_approximation=fast_approximation,
+                num_buckets_to_sample=num_buckets_to_sample,
+                max_in_memory_batches=max_in_memory_batches,
+                override_number_classes=override_number_classes,
+                variable_length=variable_length,
+                checkpoint_config=checkpoint_config,
+                **kwargs,
+            )
+        )
+
+        training_progress_manager.make_preindexing_checkpoint()
+        self.index_documents_impl(
+            training_progress_manager=training_progress_manager,
+            on_progress=on_progress,
+            cancel_state=cancel_state,
+            callbacks=callbacks,
+        )
+
+    def add_balancing_samples(self, documents: DocumentDataSource):
+        samples = make_balancing_samples(documents)
+        self.balancing_samples += samples
+        if len(self.balancing_samples) > 25000:
+            self.balancing_samples = random.sample(self.balancing_samples, k=25000)
+
+    def delete_entities(self, entities) -> None:
+        for entity in entities:
+            self.get_model().forget(entity)
+
+        if self.finetunable_retriever:
+            self.finetunable_retriever.delete_entities(entities)
+
+    def model_from_scratch(
+        self, documents: DocumentDataSource, number_classes: int = None
+    ):
+        model = bolt.UniversalDeepTransformer(
+            data_types={
+                self.query_col: bolt.types.text(tokenizer=self.tokenizer),
+                self.id_col: bolt.types.categorical(delimiter=self.id_delimiter),
+            },
+            target=self.id_col,
+            n_target_classes=(
+                documents.size if number_classes is None else number_classes
+            ),
+            integer_target=True,
+            options={
+                "extreme_classification": True,
+                "extreme_output_dim": self.extreme_output_dim,
+                "fhr": self.fhr,
+                "embedding_dimension": self.embedding_dimension,
+                "extreme_num_hashes": self.extreme_num_hashes,
+                "hidden_bias": self.hidden_bias,
+                "rlhf": True,
+                "mach_index_seed": self.mach_index_seed,
+            },
+            model_config=self.model_config,
+        )
+        model.insert_new_doc_ids(documents)
+        return model
+
+    def forget_documents(self) -> None:
+        if self.model is not None:
+            self.model.clear_index()
+        self.n_ids = 0
+        self.balancing_samples = []
+
+        if self.finetunable_retriever:
+            self.finetunable_retriever.forget_documents()
+
+    @property
+    def searchable(self) -> bool:
+        return self.n_ids != 0
+
+    def query_mach(self, samples, n_results):
+        self.model.set_decode_params(min(self.n_ids, n_results), min(self.n_ids, 100))
+        infer_batch = self.infer_samples_to_infer_batch(samples)
+        return add_retriever_tag(
+            results=self.model.predict_batch(infer_batch), tag="mach"
+        )
+
+    def query_finetunable_retriever(self, samples, n_results):
+        return self.finetunable_retriever.infer_labels(
+            samples=samples, n_results=min(self.n_ids, n_results)
+        )
+
+    def infer_labels(
+        self,
+        samples: InferSamples,
+        n_results: int,
+        retriever=None,
+        mach_first=False,
+        **kwargs,
+    ) -> Predictions:
+        if not retriever:
+            if not self.finetunable_retriever:
+                retriever = "mach"
+            else:
+                mach_results = self.query_mach(samples=samples, n_results=n_results)
+                index_results = self.query_finetunable_retriever(
+                    samples=samples, n_results=n_results
+                )
+                return [
+                    (
+                        merge_results(mach_res, index_res, n_results)
+                        if mach_first
+                        # Prioritize retriver results.
+                        else merge_results(index_res, mach_res, n_results)
+                    )
+                    for mach_res, index_res in zip(mach_results, index_results)
+                ]
+
+        if retriever == "mach":
+            return self.query_mach(samples=samples, n_results=n_results)
+
+        if retriever == "finetunable_retriever":
+            if not self.finetunable_retriever:
+                raise ValueError(
+                    "Cannot use retriever 'finetunable_retriever' since the retriever is None."
+                )
+            return self.query_finetunable_retriever(
+                samples=samples, n_results=n_results
+            )
+
+        raise ValueError(
+            f"Invalid retriever '{retriever}'. Please use 'mach', 'finetunable_retriever', "
+            "or pass None to allow the model to autotune which is used."
+        )
+
+    def score(
+        self, samples: InferSamples, entities: List[List[int]], n_results: int = None
+    ) -> Predictions:
+        infer_batch = self.infer_samples_to_infer_batch(samples)
+        results = self.model.score_batch(infer_batch, classes=entities, top_k=n_results)
+        return add_retriever_tag(results=results, tag="mach")
+
+    def _format_associate_samples(self, pairs: List[Tuple[str, str]]):
+        return [(clean_text(source), clean_text(target)) for source, target in pairs]
+
+    def associate(
+        self,
+        pairs: List[Tuple[str, str]],
+        n_buckets: int,
+        n_association_samples: int = 16,
+        n_balancing_samples: int = 50,
+        learning_rate: float = 0.001,
+        epochs: int = 3,
+        **kwargs,
+    ):
+        self.model.associate(
+            source_target_samples=self._format_associate_samples(pairs),
+            n_buckets=n_buckets,
+            n_association_samples=n_association_samples,
+            n_balancing_samples=n_balancing_samples,
+            learning_rate=learning_rate,
+            epochs=epochs,
+            force_non_empty=kwargs.get("force_non_empty", True),
+        )
+
+        if self.finetunable_retriever:
+            self.finetunable_retriever.associate(pairs)
+
+    def upvote(
+        self,
+        pairs: List[Tuple[str, int]],
+        n_upvote_samples: int = 16,
+        n_balancing_samples: int = 50,
+        learning_rate: float = 0.001,
+        epochs: int = 3,
+    ):
+        samples = [(clean_text(text), label) for text, label in pairs]
+
+        self.model.upvote(
+            source_target_samples=samples,
+            n_upvote_samples=n_upvote_samples,
+            n_balancing_samples=n_balancing_samples,
+            learning_rate=learning_rate,
+            epochs=epochs,
+        )
+
+        if self.finetunable_retriever:
+            self.finetunable_retriever.upvote(pairs)
+
+    def retrain(
+        self,
+        balancing_data: DocumentDataSource,
+        source_target_pairs: List[Tuple[str, str]],
+        n_buckets: int,
+        learning_rate: float,
+        epochs: int,
+    ):
+        self.model.associate_cold_start_data_source(
+            balancing_data=balancing_data,
+            strong_column_names=[balancing_data.strong_column],
+            weak_column_names=[balancing_data.weak_column],
+            source_target_samples=self._format_associate_samples(source_target_pairs),
+            n_buckets=n_buckets,
+            n_association_samples=1,
+            learning_rate=learning_rate,
+            epochs=epochs,
+            metrics=["hash_precision@5"],
+            options=bolt.TrainOptions(),
+        )
+
+    def __setstate__(self, state):
+        if "model_config" not in state:
+            # Add model_config field if an older model is being loaded.
+            state["model_config"] = None
+        if "finetunable_retriever" not in state:
+            state["finetunable_retriever"] = None
+        if "inverted_index" in state:
+            state["finetunable_retriever"] = FinetunableRetriever(
+                search.FinetunableRetriever.train_from(state["inverted_index"].export())
+            )
+        self.__dict__.update(state)
+
+    def supervised_training_impl(
+        self,
+        supervised_progress_manager: TrainingProgressManager,
+        callbacks: List[bolt.train.callbacks.Callback],
+    ):
+        if not supervised_progress_manager.is_training_completed:
+            train_args = supervised_progress_manager.training_arguments()
+            self.model.train_on_data_source(
+                data_source=supervised_progress_manager.train_source,
+                callbacks=callbacks
+                + [
+                    TrainingProgressCallback(
+                        training_progress_manager=supervised_progress_manager
+                    )
+                ],
+                **train_args,
+            )
+
+            if (
+                supervised_progress_manager.tracker._train_state.disable_finetunable_retriever
+            ):
+                self.finetunable_retriever = None
+            elif self.finetunable_retriever:
+                supervised_progress_manager.train_source.restart()
+                self.finetunable_retriever.train_on_supervised_data_source(
+                    supervised_progress_manager.train_source
+                )
+
+            supervised_progress_manager.training_complete()
+
+    def train_on_supervised_data_source(
+        self,
+        supervised_data_source: SupDataSource,
+        learning_rate: float,
+        epochs: int,
+        batch_size: Optional[int],
+        max_in_memory_batches: Optional[int],
+        metrics: List[str],
+        callbacks: List[bolt.train.callbacks.Callback],
+        disable_finetunable_retriever: bool,
+        checkpoint_config: Optional[CheckpointConfig] = None,
+    ):
+        if (
+            checkpoint_config is None
+            or checkpoint_config.resume_from_checkpoint is False
+        ):
+            training_manager = TrainingProgressManager.from_scratch_for_supervised(
+                model=self,
+                supervised_datasource=supervised_data_source,
+                learning_rate=learning_rate,
+                epochs=epochs,
+                batch_size=batch_size,
+                max_in_memory_batches=max_in_memory_batches,
+                metrics=metrics,
+                disable_finetunable_retriever=disable_finetunable_retriever,
+                checkpoint_config=checkpoint_config,
+            )
+            training_manager.make_preindexing_checkpoint(save_datasource=True)
+        else:
+            training_manager = TrainingProgressManager.from_checkpoint(
+                self, checkpoint_config, for_supervised=True
+            )
+
+        self.supervised_training_impl(training_manager, callbacks=callbacks)
```

## thirdai/neural_db/models/mach_defaults.py

 * *Ordering differences only*

```diff
@@ -1,52 +1,52 @@
-# This file contains default functions and variables that Mach uses.
-
-metric_to_track = "hash_precision@5"
-acc_to_stop = 0.95
-
-
-def autotune_from_scratch_min_max_epochs(size):
-    if size < 10000:
-        return 10, 15
-    if size < 100000:
-        return 5, 10
-    if size < 1000000:
-        return 3, 8
-    return 1, 5
-
-
-def autotune_from_base_min_max_epochs(size):
-    if size < 100000:
-        return 5, 10
-    if size < 1000000:
-        return 3, 8
-    return 1, 5
-
-
-def training_arguments_from_scratch(size):
-    min_epochs, max_epochs = autotune_from_scratch_min_max_epochs(size)
-
-    # 0.005 was an artifact of the original pocketllm/playground which was
-    # tested with small docs but not yet thoroughly benchmarked
-    # 0.001 is what we use for all of our benchmarks and gives us the best numbers
-    learning_rate = 0.005 if size < 1000 else 0.001
-    freeze_before_train = False
-
-    return {
-        "min_epochs": min_epochs,
-        "max_epochs": max_epochs,
-        "learning_rate": learning_rate,
-        "freeze_before_train": freeze_before_train,
-    }
-
-
-def training_arguments_from_base(size):
-    min_epochs, max_epochs = autotune_from_base_min_max_epochs(size)
-    learning_rate = 0.001
-    freeze_before_train = True
-
-    return {
-        "min_epochs": min_epochs,
-        "max_epochs": max_epochs,
-        "learning_rate": learning_rate,
-        "freeze_before_train": freeze_before_train,
-    }
+# This file contains default functions and variables that Mach uses.
+
+metric_to_track = "hash_precision@5"
+acc_to_stop = 0.95
+
+
+def autotune_from_scratch_min_max_epochs(size):
+    if size < 10000:
+        return 10, 15
+    if size < 100000:
+        return 5, 10
+    if size < 1000000:
+        return 3, 8
+    return 1, 5
+
+
+def autotune_from_base_min_max_epochs(size):
+    if size < 100000:
+        return 5, 10
+    if size < 1000000:
+        return 3, 8
+    return 1, 5
+
+
+def training_arguments_from_scratch(size):
+    min_epochs, max_epochs = autotune_from_scratch_min_max_epochs(size)
+
+    # 0.005 was an artifact of the original pocketllm/playground which was
+    # tested with small docs but not yet thoroughly benchmarked
+    # 0.001 is what we use for all of our benchmarks and gives us the best numbers
+    learning_rate = 0.005 if size < 1000 else 0.001
+    freeze_before_train = False
+
+    return {
+        "min_epochs": min_epochs,
+        "max_epochs": max_epochs,
+        "learning_rate": learning_rate,
+        "freeze_before_train": freeze_before_train,
+    }
+
+
+def training_arguments_from_base(size):
+    min_epochs, max_epochs = autotune_from_base_min_max_epochs(size)
+    learning_rate = 0.001
+    freeze_before_train = True
+
+    return {
+        "min_epochs": min_epochs,
+        "max_epochs": max_epochs,
+        "learning_rate": learning_rate,
+        "freeze_before_train": freeze_before_train,
+    }
```

## thirdai/neural_db/models/mach_mixture_model.py

 * *Ordering differences only*

```diff
@@ -1,690 +1,690 @@
-from collections import defaultdict
-from pathlib import Path
-from typing import Callable, List, Optional, Sequence, Tuple
-
-from thirdai import bolt, data
-
-from ..documents import DocumentDataSource
-from ..sharded_documents import shard_data_source
-from ..supervised_datasource import SupDataSource
-from ..trainer.checkpoint_config import (
-    CheckpointConfig,
-    generate_checkpoint_configs_for_ensembles,
-)
-from ..trainer.training_progress_manager import TrainingProgressManager
-from ..utils import clean_text, pickle_to, requires_condition, unpickle_from
-from .mach import Mach
-from .model_interface import CancelState, Model, add_retriever_tag, merge_results
-from .multi_mach import MultiMach, aggregate_ensemble_results
-
-InferSamples = List
-Predictions = Sequence
-TrainLabels = List
-TrainSamples = List
-
-
-class MachMixture(Model):
-    def __init__(
-        self,
-        num_shards: int,
-        num_models_per_shard: int = 1,
-        id_col: str = "DOC_ID",
-        id_delimiter: str = " ",
-        query_col: str = "QUERY",
-        fhr: int = 50_000,
-        embedding_dimension: int = 2048,
-        extreme_output_dim: int = 10_000,  # for Mach Mixture, we use default dim of 10k
-        extreme_num_hashes: int = 8,
-        tokenizer="char-4",
-        hidden_bias=False,
-        model_config=None,
-        hybrid=True,
-        label_to_segment_map: defaultdict = None,
-        seed_for_sharding: int = 0,
-        **kwargs,
-    ):
-        self.id_col = id_col
-        self.id_delimiter = id_delimiter
-        self.query_col = query_col
-
-        # These parameters are specific to Mach Mixture
-        self.num_shards = num_shards
-        self.num_models_per_shard = num_models_per_shard
-
-        if label_to_segment_map == None:
-            self.label_to_segment_map = defaultdict(list)
-        else:
-            self.label_to_segment_map = label_to_segment_map
-
-        self.seed_for_sharding = seed_for_sharding
-
-        self.ensembles: List[MultiMach] = [
-            MultiMach(
-                number_models=num_models_per_shard,
-                id_col=id_col,
-                id_delimiter=id_delimiter,
-                query_col=query_col,
-                fhr=fhr,
-                embedding_dimension=embedding_dimension,
-                extreme_output_dim=extreme_output_dim,
-                extreme_num_hashes=extreme_num_hashes,
-                tokenizer=tokenizer,
-                hidden_bias=hidden_bias,
-                hybrid=hybrid,
-                model_config=model_config,
-                mach_index_seed_offset=j * 341,
-            )
-            for j in range(self.num_shards)
-        ]
-
-    @property
-    def shards_data_source(self):
-        return self.num_shards > 1
-
-    @property
-    def n_ids(self):
-        # We assume that the label spaces of underlying ensembles are disjoint (True as of now.)
-        n_ids = 0
-        for ensemble in self.ensembles:
-            n_ids += ensemble.n_ids
-        return n_ids
-
-    def set_mach_sampling_threshold(self, threshold: float):
-        for ensemble in self.ensembles:
-            ensemble.set_mach_sampling_threshold(threshold)
-
-    def get_model(self) -> List[MultiMach]:
-        for ensemble in self.ensembles:
-            if not ensemble.get_model():
-                return None
-        return self.ensembles
-
-    def set_model(self, ensembles):
-        self.ensembles = ensembles
-
-    def save_meta(self, directory: Path):
-        if self.ensembles is not None:
-            for ensemble in self.ensembles:
-                ensemble.save_meta(directory)
-
-        pickle_to(
-            [self.label_to_segment_map, self.seed_for_sharding],
-            directory / "segment_map_and_seed.pkl",
-        )
-
-    def load_meta(self, directory: Path):
-        if self.ensembles is not None:
-            for ensemble in self.ensembles:
-                ensemble.load_meta(directory)
-        self.label_to_segment_map, self.seed_for_sharding = unpickle_from(
-            directory / "segment_map_and_seed.pkl"
-        )
-
-    def get_query_col(self) -> str:
-        return self.query_col
-
-    def get_id_col(self) -> str:
-        return self.id_col
-
-    def get_id_delimiter(self) -> str:
-        return self.id_delimiter
-
-    def index_documents_impl(
-        self,
-        training_progress_managers: List[List[TrainingProgressManager]],
-        on_progress: Callable,
-        cancel_state: CancelState,
-        callbacks: List[bolt.train.callbacks.Callback] = None,
-    ):
-        # This function is the entrypoint to underlying mach models in the mixture. The training progress manager becomes the absolute source of truth in this routine and holds all the data needed to index documents into a model irrespective of whether we are checkpointing or not.
-        for progress_manager, ensemble in zip(
-            training_progress_managers, self.ensembles
-        ):
-            ensemble.index_documents_impl(
-                training_progress_managers=progress_manager,
-                on_progress=on_progress,
-                cancel_state=cancel_state,
-                callbacks=callbacks,
-            )
-
-    def resume(
-        self,
-        on_progress: Callable,
-        cancel_state: CancelState,
-        checkpoint_config: CheckpointConfig,
-        callbacks: List[bolt.train.callbacks.Callback] = None,
-    ):
-        # If checkpoint_dir in checkpoint_config is /john/doe and number of models is 2, the underlying mach models will make checkpoint at /john/doe/0 and /john/doe/1 depending on model ids.
-        ensemble_checkpoint_configs = generate_checkpoint_configs_for_ensembles(
-            config=checkpoint_config,
-            number_ensembles=self.num_shards,
-            number_models_per_ensemble=self.num_models_per_shard,
-        )
-
-        self.load_meta(checkpoint_config.checkpoint_dir)
-
-        # The training manager corresponding to a model loads all the needed to complete the training such as model, document sources, tracker, etc.
-        training_managers = []
-        for ensemble, config in zip(self.ensembles, ensemble_checkpoint_configs):
-            ensemble_training_managers: List[TrainingProgressManager] = []
-            for model_id, model in enumerate(ensemble.models):
-                # the intro/train shards are only saved for the first model in each ensemble
-                if model_id == 0:
-                    modelwise_training_manager = (
-                        TrainingProgressManager.from_checkpoint(
-                            original_mach_model=model,
-                            checkpoint_config=config[model_id],
-                            for_supervised=False,
-                        )
-                    )
-                else:
-                    # for every model other than the first in the ensemble,
-                    # manually pass in the loaded intro and train source from
-                    # the first model
-                    modelwise_training_manager = (
-                        TrainingProgressManager.from_checkpoint(
-                            original_mach_model=model,
-                            checkpoint_config=config[model_id],
-                            for_supervised=False,
-                            datasource_manager=ensemble_training_managers[
-                                0
-                            ].datasource_manager,
-                        )
-                    )
-                ensemble_training_managers.append(modelwise_training_manager)
-            training_managers.append(ensemble_training_managers)
-
-        self.index_documents_impl(
-            training_progress_managers=training_managers,
-            on_progress=on_progress,
-            cancel_state=cancel_state,
-            callbacks=callbacks,
-        )
-
-    def index_from_start(
-        self,
-        intro_documents: DocumentDataSource,
-        train_documents: DocumentDataSource,
-        should_train: bool,
-        fast_approximation: bool = True,
-        num_buckets_to_sample: Optional[int] = None,
-        on_progress: Callable = lambda **kwargs: None,
-        cancel_state: CancelState = None,
-        max_in_memory_batches: int = None,
-        variable_length: Optional[
-            data.transformations.VariableLengthConfig
-        ] = data.transformations.VariableLengthConfig(),
-        checkpoint_config: CheckpointConfig = None,
-        callbacks: List[bolt.train.callbacks.Callback] = None,
-        **kwargs,
-    ) -> None:
-        # We need the original number of classes from the original data source so that we can initialize the Mach models this mixture will have
-        number_classes = intro_documents.size
-
-        # Make a sharded data source with introduce documents. When we call shard_data_source, this will shard the introduce data source, return a list of data sources, and modify the label index to keep track of what label goes to what shard
-        introduce_data_sources = shard_data_source(
-            data_source=intro_documents,
-            label_to_segment_map=self.label_to_segment_map,
-            number_shards=self.num_shards,
-            update_segment_map=True,
-        )
-
-        # Once the introduce datasource has been sharded, we can use the update label index to shard the training data source ( We do not want training samples to go to a Mach model that does not contain their labels)
-        train_data_sources = shard_data_source(
-            train_documents,
-            label_to_segment_map=self.label_to_segment_map,
-            number_shards=self.num_shards,
-            update_segment_map=False,
-        )
-
-        # Before we start training individual mach models, we need to save the label to segment map of the current mach mixture so that we can resume in case the training fails.
-        if checkpoint_config:
-            self.save_meta(checkpoint_config.checkpoint_dir)
-
-        ensemble_checkpoint_configs = generate_checkpoint_configs_for_ensembles(
-            config=checkpoint_config,
-            number_ensembles=self.num_shards,
-            number_models_per_ensemble=self.num_models_per_shard,
-        )
-
-        training_managers = []
-        for ensemble_id, (intro_shard, train_shard, ensemble, config) in enumerate(
-            zip(
-                introduce_data_sources,
-                train_data_sources,
-                self.ensembles,
-                ensemble_checkpoint_configs,
-            )
-        ):
-            ensemble_training_managers = []
-            for model_id, model in enumerate(ensemble.models):
-                modelwise_training_manager = (
-                    TrainingProgressManager.from_scratch_for_unsupervised(
-                        model=model,
-                        intro_documents=intro_shard,
-                        train_documents=train_shard,
-                        should_train=should_train,
-                        fast_approximation=fast_approximation,
-                        num_buckets_to_sample=num_buckets_to_sample,
-                        max_in_memory_batches=max_in_memory_batches,
-                        override_number_classes=number_classes,
-                        variable_length=variable_length,
-                        checkpoint_config=config[model_id],
-                        **kwargs,
-                    )
-                )
-                ensemble_training_managers.append(modelwise_training_manager)
-                # When we want to start from scratch, we will have to checkpoint the intro, train sources, the model, tracker,etc. so that the training can be resumed from the checkpoint.
-                # only save the intro and train shards for the first model to avoid data duplication. When loading we will load the first and set the intro and train shards for other models in the multimach
-                modelwise_training_manager.make_preindexing_checkpoint(
-                    save_datasource=model_id == 0
-                )  # no-op when checkpoint_config is None.
-
-            training_managers.append(ensemble_training_managers)
-
-        self.index_documents_impl(
-            training_progress_managers=training_managers,
-            on_progress=on_progress,
-            cancel_state=cancel_state,
-            callbacks=callbacks,
-        )
-
-    def delete_entities(self, entities) -> None:
-        if self.shards_data_source:
-            segment_to_label_map = defaultdict(list)
-            for label in entities:
-                segments = self.label_to_segment_map.get(
-                    label, []
-                )  # Get segments corresponding to the entity
-                for segment in segments:
-                    segment_to_label_map[segment].append(label)
-        else:
-            segment_to_label_map = {
-                model_id: entities for model_id in range(self.num_shards)
-            }
-
-        # Delete entities for each segment
-        for i, ensemble in enumerate(self.ensembles):
-            ensemble.delete_entities(segment_to_label_map[i])
-
-    def forget_documents(self) -> None:
-        for ensemble in self.ensembles:
-            ensemble.forget_documents()
-
-    @property
-    def searchable(self) -> bool:
-        return self.n_ids != 0
-
-    def aggregate_results(self, results, n_results):
-        joined_results = []
-        for i in range(len(results[0])):
-            joined_result = []
-            for result in results:
-                joined_result.extend(result[i])
-
-            joined_result.sort(key=lambda x: x[1], reverse=True)
-            joined_result = joined_result[:n_results]
-
-            joined_results.append(joined_result)
-        return joined_results
-
-    def query_mach(self, samples: List, n_results: int, label_probing: bool):
-        for ensemble in self.ensembles:
-            for model in ensemble.models:
-                model.model.set_decode_params(
-                    min(model.n_ids, n_results),
-                    min(model.n_ids, 100),
-                )
-
-        if not label_probing or self.ensembles[0].models[0].extreme_num_hashes != 1:
-            ensemble_results = []
-            for ensemble in self.ensembles:
-                mach_results = bolt.UniversalDeepTransformer.parallel_inference(
-                    models=[model.model for model in ensemble.models],
-                    batch=[{self.query_col: clean_text(text)} for text in samples],
-                )
-                ensemble_results.append(aggregate_ensemble_results(mach_results))
-
-        else:
-            ensemble_results = (
-                bolt.UniversalDeepTransformer.label_probe_multiple_shards(
-                    shards=[
-                        [model.model for model in ensemble.models]
-                        for ensemble in self.ensembles
-                    ],
-                    batch=[{self.query_col: clean_text(text)} for text in samples],
-                )
-            )
-
-        return add_retriever_tag(
-            self.aggregate_results(ensemble_results, n_results),
-            tag="mach",
-        )
-
-    def query_finetunable_retriever(self, samples, n_results):
-        results = []
-        for ensemble in self.ensembles:
-            ensemble_result = ensemble.query_finetunable_retriever(samples, n_results)
-            if ensemble_result:
-                results.append(ensemble_result)
-
-        if not results:
-            return None
-
-        return self.aggregate_results(results, n_results)
-
-    def infer_labels(
-        self,
-        samples: InferSamples,
-        n_results: int,
-        retriever=None,
-        label_probing=True,
-        mach_first=False,
-        **kwargs,
-    ) -> Predictions:
-        if not retriever:
-            retriever_results = self.query_finetunable_retriever(
-                samples, n_results=n_results
-            )
-            if not retriever_results:
-                retriever = "mach"
-            else:
-                mach_results = self.query_mach(
-                    samples, n_results=n_results, label_probing=label_probing
-                )
-                return [
-                    (
-                        merge_results(mach_res, retriever_res, n_results)
-                        if mach_first
-                        # Prioritize retriever_results.
-                        else merge_results(retriever_res, mach_res, n_results)
-                    )
-                    for mach_res, retriever_res in zip(mach_results, retriever_results)
-                ]
-
-        if retriever == "mach":
-            return self.query_mach(
-                samples=samples, n_results=n_results, label_probing=label_probing
-            )
-
-        if retriever == "finetunable_retriever":
-            results = self.query_finetunable_retriever(
-                samples=samples, n_results=n_results
-            )
-            if not results:
-                raise ValueError(
-                    "Cannot use retriever 'finetunable_retriever' since the retriever is None."
-                )
-            return results
-
-        raise ValueError(
-            f"Invalid retriever '{retriever}'. Please use 'mach', 'finetunable_retriever', "
-            "or pass None to allow the model to autotune which is used."
-        )
-
-    def _shard_label_constraints(
-        self, entities: List[List[int]]
-    ) -> List[List[List[int]]]:
-        shards = [[[] for _ in range(len(entities))] for _ in range(self.num_shards)]
-        for i in range(len(entities)):
-            for label in entities[i]:
-                model_ids = self.label_to_segment_map.get(label)
-                if model_ids is None:
-                    raise Exception(f"The Label {label} is not a part of Label Index")
-                for model_id in model_ids:
-                    shards[model_id][i].append(label)
-        return shards
-
-    def score(
-        self, samples: InferSamples, entities: List[List[int]], n_results: int = None
-    ) -> Predictions:
-        if self.shards_data_source:
-            sharded_entities = self._shard_label_constraints(entities=entities)
-        else:
-            sharded_entities = [entities] * self.num_shards
-
-        model_scores = [
-            ensemble.score(samples=samples, entities=shard_entity, n_results=n_results)
-            for ensemble, shard_entity in zip(self.ensembles, sharded_entities)
-        ]
-
-        aggregated_scores = [defaultdict(int) for _ in range(len(samples))]
-
-        for i in range(len(samples)):
-            for score in model_scores:
-                for label, value, tag in score[i]:
-                    aggregated_scores[i][label] += value
-                    assert tag == "mach", (
-                        "We ignore the retriever tag returned by each ensemble. "
-                        "This was inconsequential at the time of writing since "
-                        "the MultiMach.score() always returns the 'mach' retriever "
-                        "tag. We assert this condition so we reevaluate this "
-                        "decision if the condition no longer holds."
-                    )
-
-        # Sort the aggregated scores and keep only the top k results
-        top_k_results = []
-        for i in range(len(samples)):
-            sorted_scores = sorted(
-                [
-                    (label, score, "mach")
-                    for label, score in aggregated_scores[i].items()
-                ],
-                key=lambda x: x[1],
-                reverse=True,
-            )
-            top_k_results.append(
-                sorted_scores[:n_results] if n_results else sorted_scores
-            )
-
-        return top_k_results
-
-    def associate(
-        self,
-        pairs: List[Tuple[str, str]],
-        n_buckets: int,
-        n_association_samples: int = 16,
-        n_balancing_samples: int = 50,
-        learning_rate: float = 0.001,
-        epochs: int = 3,
-        **kwargs,
-    ):
-        for ensemble in self.ensembles:
-            ensemble.associate(
-                pairs=pairs,
-                n_buckets=n_buckets,
-                n_association_samples=n_association_samples,
-                n_balancing_samples=n_balancing_samples,
-                learning_rate=learning_rate,
-                epochs=epochs,
-                force_non_empty=kwargs.get("force_non_empty", True),
-            )
-
-    def _shard_upvote_pairs(
-        self, source_target_pairs: List[Tuple[str, int]]
-    ) -> List[List[Tuple[str, int]]]:
-        shards = [[] for _ in range(self.num_shards)]
-        for pair in source_target_pairs:
-            model_ids = self.label_to_segment_map.get(pair[1])
-            if model_ids is None:
-                raise Exception(f"The Label {pair[1]} is not a part of Label Index")
-            for model_id in model_ids:
-                shards[model_id].append(pair)
-        return shards
-
-    def upvote(
-        self,
-        pairs: List[Tuple[str, int]],
-        n_upvote_samples: int = 16,
-        n_balancing_samples: int = 50,
-        learning_rate: float = 0.001,
-        epochs: int = 3,
-    ):
-        sharded_pairs = self._shard_upvote_pairs(pairs)
-
-        for ensemble, shard in zip(self.ensembles, sharded_pairs):
-            if len(shard) == 0:
-                continue
-            ensemble.upvote(
-                pairs=shard,
-                n_upvote_samples=n_upvote_samples,
-                n_balancing_samples=n_balancing_samples,
-                learning_rate=learning_rate,
-                epochs=epochs,
-            )
-
-    def retrain(
-        self,
-        balancing_data: DocumentDataSource,
-        source_target_pairs: List[Tuple[str, str]],
-        n_buckets: int,
-        learning_rate: float,
-        epochs: int,
-    ):
-        balancing_data_shards = shard_data_source(
-            data_source=balancing_data,
-            number_shards=self.num_shards,
-            label_to_segment_map=self.label_to_segment_map,
-            update_segment_map=False,
-        )
-        for ensemble, shard in zip(self.ensembles, balancing_data_shards):
-            ensemble.retrain(
-                balancing_data=shard,
-                source_target_pairs=source_target_pairs,
-                n_buckets=n_buckets,
-                learning_rate=learning_rate,
-                epochs=epochs,
-            )
-
-    def __setstate__(self, state):
-        if "model_config" not in state:
-            # Add model_config field if an older model is being loaded.
-            state["model_config"] = None
-        self.__dict__.update(state)
-
-    def _resume_supervised(
-        self,
-        checkpoint_config: Optional[CheckpointConfig],
-        callbacks: List[bolt.train.callbacks.Callback],
-    ):
-        ensemble_checkpoint_configs = generate_checkpoint_configs_for_ensembles(
-            config=checkpoint_config,
-            number_ensembles=self.num_shards,
-            number_models_per_ensemble=self.num_models_per_shard,
-        )
-
-        training_managers = []
-
-        for ensemble, config in zip(self.ensembles, ensemble_checkpoint_configs):
-            ensemble_training_managers: List[TrainingProgressManager] = []
-            for model_id, model in enumerate(ensemble.models):
-                if model_id == 0:
-                    modelwise_training_manager = (
-                        TrainingProgressManager.from_checkpoint(
-                            original_mach_model=model,
-                            checkpoint_config=config[model_id],
-                            for_supervised=True,
-                        )
-                    )
-                else:
-                    modelwise_training_manager = (
-                        TrainingProgressManager.from_checkpoint(
-                            original_mach_model=model,
-                            checkpoint_config=config[model_id],
-                            for_supervised=True,
-                            datasource_manager=ensemble_training_managers[
-                                0
-                            ].datasource_manager,
-                        )
-                    )
-                ensemble_training_managers.append(modelwise_training_manager)
-            training_managers.append(ensemble_training_managers)
-
-        for ensemble, managers in zip(self.ensembles, training_managers):
-            ensemble.supervised_training_impl(managers, callbacks=callbacks)
-
-    def _supervised_from_start(
-        self,
-        supervised_data_source,
-        learning_rate,
-        epochs,
-        batch_size,
-        max_in_memory_batches,
-        metrics,
-        callbacks,
-        disable_finetunable_retriever,
-        checkpoint_config,
-    ):
-
-        supervised_data_source_shards = shard_data_source(
-            data_source=supervised_data_source,
-            number_shards=self.num_shards,
-            label_to_segment_map=self.label_to_segment_map,
-            update_segment_map=False,
-        )
-
-        ensemble_checkpoint_configs = generate_checkpoint_configs_for_ensembles(
-            config=checkpoint_config,
-            number_ensembles=self.num_shards,
-            number_models_per_ensemble=self.num_models_per_shard,
-        )
-        training_managers = []
-
-        for ensemble, config, supervised_shard in zip(
-            self.ensembles, ensemble_checkpoint_configs, supervised_data_source_shards
-        ):
-            ensemble_training_managers: List[TrainingProgressManager] = []
-            for model_id, model in enumerate(ensemble.models):
-                modelwise_training_manager = (
-                    TrainingProgressManager.from_scratch_for_supervised(
-                        model=model,
-                        supervised_datasource=supervised_shard,
-                        learning_rate=learning_rate,
-                        epochs=epochs,
-                        batch_size=batch_size,
-                        max_in_memory_batches=max_in_memory_batches,
-                        metrics=metrics,
-                        disable_finetunable_retriever=disable_finetunable_retriever,
-                        checkpoint_config=config[model_id],
-                    )
-                )
-                ensemble_training_managers.append(modelwise_training_manager)
-                modelwise_training_manager.make_preindexing_checkpoint(
-                    save_datasource=model_id == 0
-                )
-            training_managers.append(ensemble_training_managers)
-
-        for ensemble, managers in zip(self.ensembles, training_managers):
-            ensemble.supervised_training_impl(managers, callbacks=callbacks)
-
-    def train_on_supervised_data_source(
-        self,
-        supervised_data_source: SupDataSource,
-        learning_rate: float,
-        epochs: int,
-        batch_size: Optional[int],
-        max_in_memory_batches: Optional[int],
-        metrics: List[str],
-        callbacks: List[bolt.train.callbacks.Callback],
-        disable_finetunable_retriever: bool,
-        checkpoint_config: Optional[CheckpointConfig] = None,
-    ):
-        if (
-            checkpoint_config is None
-            or checkpoint_config.resume_from_checkpoint is False
-        ):
-            self._supervised_from_start(
-                supervised_data_source=supervised_data_source,
-                learning_rate=learning_rate,
-                epochs=epochs,
-                batch_size=batch_size,
-                max_in_memory_batches=max_in_memory_batches,
-                metrics=metrics,
-                callbacks=callbacks,
-                disable_finetunable_retriever=disable_finetunable_retriever,
-                checkpoint_config=checkpoint_config,
-            )
-        else:
-            self._resume_supervised(
-                checkpoint_config=checkpoint_config, callbacks=callbacks
-            )
+from collections import defaultdict
+from pathlib import Path
+from typing import Callable, List, Optional, Sequence, Tuple
+
+from thirdai import bolt, data
+
+from ..documents import DocumentDataSource
+from ..sharded_documents import shard_data_source
+from ..supervised_datasource import SupDataSource
+from ..trainer.checkpoint_config import (
+    CheckpointConfig,
+    generate_checkpoint_configs_for_ensembles,
+)
+from ..trainer.training_progress_manager import TrainingProgressManager
+from ..utils import clean_text, pickle_to, requires_condition, unpickle_from
+from .mach import Mach
+from .model_interface import CancelState, Model, add_retriever_tag, merge_results
+from .multi_mach import MultiMach, aggregate_ensemble_results
+
+InferSamples = List
+Predictions = Sequence
+TrainLabels = List
+TrainSamples = List
+
+
+class MachMixture(Model):
+    def __init__(
+        self,
+        num_shards: int,
+        num_models_per_shard: int = 1,
+        id_col: str = "DOC_ID",
+        id_delimiter: str = " ",
+        query_col: str = "QUERY",
+        fhr: int = 50_000,
+        embedding_dimension: int = 2048,
+        extreme_output_dim: int = 10_000,  # for Mach Mixture, we use default dim of 10k
+        extreme_num_hashes: int = 8,
+        tokenizer="char-4",
+        hidden_bias=False,
+        model_config=None,
+        hybrid=True,
+        label_to_segment_map: defaultdict = None,
+        seed_for_sharding: int = 0,
+        **kwargs,
+    ):
+        self.id_col = id_col
+        self.id_delimiter = id_delimiter
+        self.query_col = query_col
+
+        # These parameters are specific to Mach Mixture
+        self.num_shards = num_shards
+        self.num_models_per_shard = num_models_per_shard
+
+        if label_to_segment_map == None:
+            self.label_to_segment_map = defaultdict(list)
+        else:
+            self.label_to_segment_map = label_to_segment_map
+
+        self.seed_for_sharding = seed_for_sharding
+
+        self.ensembles: List[MultiMach] = [
+            MultiMach(
+                number_models=num_models_per_shard,
+                id_col=id_col,
+                id_delimiter=id_delimiter,
+                query_col=query_col,
+                fhr=fhr,
+                embedding_dimension=embedding_dimension,
+                extreme_output_dim=extreme_output_dim,
+                extreme_num_hashes=extreme_num_hashes,
+                tokenizer=tokenizer,
+                hidden_bias=hidden_bias,
+                hybrid=hybrid,
+                model_config=model_config,
+                mach_index_seed_offset=j * 341,
+            )
+            for j in range(self.num_shards)
+        ]
+
+    @property
+    def shards_data_source(self):
+        return self.num_shards > 1
+
+    @property
+    def n_ids(self):
+        # We assume that the label spaces of underlying ensembles are disjoint (True as of now.)
+        n_ids = 0
+        for ensemble in self.ensembles:
+            n_ids += ensemble.n_ids
+        return n_ids
+
+    def set_mach_sampling_threshold(self, threshold: float):
+        for ensemble in self.ensembles:
+            ensemble.set_mach_sampling_threshold(threshold)
+
+    def get_model(self) -> List[MultiMach]:
+        for ensemble in self.ensembles:
+            if not ensemble.get_model():
+                return None
+        return self.ensembles
+
+    def set_model(self, ensembles):
+        self.ensembles = ensembles
+
+    def save_meta(self, directory: Path):
+        if self.ensembles is not None:
+            for ensemble in self.ensembles:
+                ensemble.save_meta(directory)
+
+        pickle_to(
+            [self.label_to_segment_map, self.seed_for_sharding],
+            directory / "segment_map_and_seed.pkl",
+        )
+
+    def load_meta(self, directory: Path):
+        if self.ensembles is not None:
+            for ensemble in self.ensembles:
+                ensemble.load_meta(directory)
+        self.label_to_segment_map, self.seed_for_sharding = unpickle_from(
+            directory / "segment_map_and_seed.pkl"
+        )
+
+    def get_query_col(self) -> str:
+        return self.query_col
+
+    def get_id_col(self) -> str:
+        return self.id_col
+
+    def get_id_delimiter(self) -> str:
+        return self.id_delimiter
+
+    def index_documents_impl(
+        self,
+        training_progress_managers: List[List[TrainingProgressManager]],
+        on_progress: Callable,
+        cancel_state: CancelState,
+        callbacks: List[bolt.train.callbacks.Callback] = None,
+    ):
+        # This function is the entrypoint to underlying mach models in the mixture. The training progress manager becomes the absolute source of truth in this routine and holds all the data needed to index documents into a model irrespective of whether we are checkpointing or not.
+        for progress_manager, ensemble in zip(
+            training_progress_managers, self.ensembles
+        ):
+            ensemble.index_documents_impl(
+                training_progress_managers=progress_manager,
+                on_progress=on_progress,
+                cancel_state=cancel_state,
+                callbacks=callbacks,
+            )
+
+    def resume(
+        self,
+        on_progress: Callable,
+        cancel_state: CancelState,
+        checkpoint_config: CheckpointConfig,
+        callbacks: List[bolt.train.callbacks.Callback] = None,
+    ):
+        # If checkpoint_dir in checkpoint_config is /john/doe and number of models is 2, the underlying mach models will make checkpoint at /john/doe/0 and /john/doe/1 depending on model ids.
+        ensemble_checkpoint_configs = generate_checkpoint_configs_for_ensembles(
+            config=checkpoint_config,
+            number_ensembles=self.num_shards,
+            number_models_per_ensemble=self.num_models_per_shard,
+        )
+
+        self.load_meta(checkpoint_config.checkpoint_dir)
+
+        # The training manager corresponding to a model loads all the needed to complete the training such as model, document sources, tracker, etc.
+        training_managers = []
+        for ensemble, config in zip(self.ensembles, ensemble_checkpoint_configs):
+            ensemble_training_managers: List[TrainingProgressManager] = []
+            for model_id, model in enumerate(ensemble.models):
+                # the intro/train shards are only saved for the first model in each ensemble
+                if model_id == 0:
+                    modelwise_training_manager = (
+                        TrainingProgressManager.from_checkpoint(
+                            original_mach_model=model,
+                            checkpoint_config=config[model_id],
+                            for_supervised=False,
+                        )
+                    )
+                else:
+                    # for every model other than the first in the ensemble,
+                    # manually pass in the loaded intro and train source from
+                    # the first model
+                    modelwise_training_manager = (
+                        TrainingProgressManager.from_checkpoint(
+                            original_mach_model=model,
+                            checkpoint_config=config[model_id],
+                            for_supervised=False,
+                            datasource_manager=ensemble_training_managers[
+                                0
+                            ].datasource_manager,
+                        )
+                    )
+                ensemble_training_managers.append(modelwise_training_manager)
+            training_managers.append(ensemble_training_managers)
+
+        self.index_documents_impl(
+            training_progress_managers=training_managers,
+            on_progress=on_progress,
+            cancel_state=cancel_state,
+            callbacks=callbacks,
+        )
+
+    def index_from_start(
+        self,
+        intro_documents: DocumentDataSource,
+        train_documents: DocumentDataSource,
+        should_train: bool,
+        fast_approximation: bool = True,
+        num_buckets_to_sample: Optional[int] = None,
+        on_progress: Callable = lambda **kwargs: None,
+        cancel_state: CancelState = None,
+        max_in_memory_batches: int = None,
+        variable_length: Optional[
+            data.transformations.VariableLengthConfig
+        ] = data.transformations.VariableLengthConfig(),
+        checkpoint_config: CheckpointConfig = None,
+        callbacks: List[bolt.train.callbacks.Callback] = None,
+        **kwargs,
+    ) -> None:
+        # We need the original number of classes from the original data source so that we can initialize the Mach models this mixture will have
+        number_classes = intro_documents.size
+
+        # Make a sharded data source with introduce documents. When we call shard_data_source, this will shard the introduce data source, return a list of data sources, and modify the label index to keep track of what label goes to what shard
+        introduce_data_sources = shard_data_source(
+            data_source=intro_documents,
+            label_to_segment_map=self.label_to_segment_map,
+            number_shards=self.num_shards,
+            update_segment_map=True,
+        )
+
+        # Once the introduce datasource has been sharded, we can use the update label index to shard the training data source ( We do not want training samples to go to a Mach model that does not contain their labels)
+        train_data_sources = shard_data_source(
+            train_documents,
+            label_to_segment_map=self.label_to_segment_map,
+            number_shards=self.num_shards,
+            update_segment_map=False,
+        )
+
+        # Before we start training individual mach models, we need to save the label to segment map of the current mach mixture so that we can resume in case the training fails.
+        if checkpoint_config:
+            self.save_meta(checkpoint_config.checkpoint_dir)
+
+        ensemble_checkpoint_configs = generate_checkpoint_configs_for_ensembles(
+            config=checkpoint_config,
+            number_ensembles=self.num_shards,
+            number_models_per_ensemble=self.num_models_per_shard,
+        )
+
+        training_managers = []
+        for ensemble_id, (intro_shard, train_shard, ensemble, config) in enumerate(
+            zip(
+                introduce_data_sources,
+                train_data_sources,
+                self.ensembles,
+                ensemble_checkpoint_configs,
+            )
+        ):
+            ensemble_training_managers = []
+            for model_id, model in enumerate(ensemble.models):
+                modelwise_training_manager = (
+                    TrainingProgressManager.from_scratch_for_unsupervised(
+                        model=model,
+                        intro_documents=intro_shard,
+                        train_documents=train_shard,
+                        should_train=should_train,
+                        fast_approximation=fast_approximation,
+                        num_buckets_to_sample=num_buckets_to_sample,
+                        max_in_memory_batches=max_in_memory_batches,
+                        override_number_classes=number_classes,
+                        variable_length=variable_length,
+                        checkpoint_config=config[model_id],
+                        **kwargs,
+                    )
+                )
+                ensemble_training_managers.append(modelwise_training_manager)
+                # When we want to start from scratch, we will have to checkpoint the intro, train sources, the model, tracker,etc. so that the training can be resumed from the checkpoint.
+                # only save the intro and train shards for the first model to avoid data duplication. When loading we will load the first and set the intro and train shards for other models in the multimach
+                modelwise_training_manager.make_preindexing_checkpoint(
+                    save_datasource=model_id == 0
+                )  # no-op when checkpoint_config is None.
+
+            training_managers.append(ensemble_training_managers)
+
+        self.index_documents_impl(
+            training_progress_managers=training_managers,
+            on_progress=on_progress,
+            cancel_state=cancel_state,
+            callbacks=callbacks,
+        )
+
+    def delete_entities(self, entities) -> None:
+        if self.shards_data_source:
+            segment_to_label_map = defaultdict(list)
+            for label in entities:
+                segments = self.label_to_segment_map.get(
+                    label, []
+                )  # Get segments corresponding to the entity
+                for segment in segments:
+                    segment_to_label_map[segment].append(label)
+        else:
+            segment_to_label_map = {
+                model_id: entities for model_id in range(self.num_shards)
+            }
+
+        # Delete entities for each segment
+        for i, ensemble in enumerate(self.ensembles):
+            ensemble.delete_entities(segment_to_label_map[i])
+
+    def forget_documents(self) -> None:
+        for ensemble in self.ensembles:
+            ensemble.forget_documents()
+
+    @property
+    def searchable(self) -> bool:
+        return self.n_ids != 0
+
+    def aggregate_results(self, results, n_results):
+        joined_results = []
+        for i in range(len(results[0])):
+            joined_result = []
+            for result in results:
+                joined_result.extend(result[i])
+
+            joined_result.sort(key=lambda x: x[1], reverse=True)
+            joined_result = joined_result[:n_results]
+
+            joined_results.append(joined_result)
+        return joined_results
+
+    def query_mach(self, samples: List, n_results: int, label_probing: bool):
+        for ensemble in self.ensembles:
+            for model in ensemble.models:
+                model.model.set_decode_params(
+                    min(model.n_ids, n_results),
+                    min(model.n_ids, 100),
+                )
+
+        if not label_probing or self.ensembles[0].models[0].extreme_num_hashes != 1:
+            ensemble_results = []
+            for ensemble in self.ensembles:
+                mach_results = bolt.UniversalDeepTransformer.parallel_inference(
+                    models=[model.model for model in ensemble.models],
+                    batch=[{self.query_col: clean_text(text)} for text in samples],
+                )
+                ensemble_results.append(aggregate_ensemble_results(mach_results))
+
+        else:
+            ensemble_results = (
+                bolt.UniversalDeepTransformer.label_probe_multiple_shards(
+                    shards=[
+                        [model.model for model in ensemble.models]
+                        for ensemble in self.ensembles
+                    ],
+                    batch=[{self.query_col: clean_text(text)} for text in samples],
+                )
+            )
+
+        return add_retriever_tag(
+            self.aggregate_results(ensemble_results, n_results),
+            tag="mach",
+        )
+
+    def query_finetunable_retriever(self, samples, n_results):
+        results = []
+        for ensemble in self.ensembles:
+            ensemble_result = ensemble.query_finetunable_retriever(samples, n_results)
+            if ensemble_result:
+                results.append(ensemble_result)
+
+        if not results:
+            return None
+
+        return self.aggregate_results(results, n_results)
+
+    def infer_labels(
+        self,
+        samples: InferSamples,
+        n_results: int,
+        retriever=None,
+        label_probing=True,
+        mach_first=False,
+        **kwargs,
+    ) -> Predictions:
+        if not retriever:
+            retriever_results = self.query_finetunable_retriever(
+                samples, n_results=n_results
+            )
+            if not retriever_results:
+                retriever = "mach"
+            else:
+                mach_results = self.query_mach(
+                    samples, n_results=n_results, label_probing=label_probing
+                )
+                return [
+                    (
+                        merge_results(mach_res, retriever_res, n_results)
+                        if mach_first
+                        # Prioritize retriever_results.
+                        else merge_results(retriever_res, mach_res, n_results)
+                    )
+                    for mach_res, retriever_res in zip(mach_results, retriever_results)
+                ]
+
+        if retriever == "mach":
+            return self.query_mach(
+                samples=samples, n_results=n_results, label_probing=label_probing
+            )
+
+        if retriever == "finetunable_retriever":
+            results = self.query_finetunable_retriever(
+                samples=samples, n_results=n_results
+            )
+            if not results:
+                raise ValueError(
+                    "Cannot use retriever 'finetunable_retriever' since the retriever is None."
+                )
+            return results
+
+        raise ValueError(
+            f"Invalid retriever '{retriever}'. Please use 'mach', 'finetunable_retriever', "
+            "or pass None to allow the model to autotune which is used."
+        )
+
+    def _shard_label_constraints(
+        self, entities: List[List[int]]
+    ) -> List[List[List[int]]]:
+        shards = [[[] for _ in range(len(entities))] for _ in range(self.num_shards)]
+        for i in range(len(entities)):
+            for label in entities[i]:
+                model_ids = self.label_to_segment_map.get(label)
+                if model_ids is None:
+                    raise Exception(f"The Label {label} is not a part of Label Index")
+                for model_id in model_ids:
+                    shards[model_id][i].append(label)
+        return shards
+
+    def score(
+        self, samples: InferSamples, entities: List[List[int]], n_results: int = None
+    ) -> Predictions:
+        if self.shards_data_source:
+            sharded_entities = self._shard_label_constraints(entities=entities)
+        else:
+            sharded_entities = [entities] * self.num_shards
+
+        model_scores = [
+            ensemble.score(samples=samples, entities=shard_entity, n_results=n_results)
+            for ensemble, shard_entity in zip(self.ensembles, sharded_entities)
+        ]
+
+        aggregated_scores = [defaultdict(int) for _ in range(len(samples))]
+
+        for i in range(len(samples)):
+            for score in model_scores:
+                for label, value, tag in score[i]:
+                    aggregated_scores[i][label] += value
+                    assert tag == "mach", (
+                        "We ignore the retriever tag returned by each ensemble. "
+                        "This was inconsequential at the time of writing since "
+                        "the MultiMach.score() always returns the 'mach' retriever "
+                        "tag. We assert this condition so we reevaluate this "
+                        "decision if the condition no longer holds."
+                    )
+
+        # Sort the aggregated scores and keep only the top k results
+        top_k_results = []
+        for i in range(len(samples)):
+            sorted_scores = sorted(
+                [
+                    (label, score, "mach")
+                    for label, score in aggregated_scores[i].items()
+                ],
+                key=lambda x: x[1],
+                reverse=True,
+            )
+            top_k_results.append(
+                sorted_scores[:n_results] if n_results else sorted_scores
+            )
+
+        return top_k_results
+
+    def associate(
+        self,
+        pairs: List[Tuple[str, str]],
+        n_buckets: int,
+        n_association_samples: int = 16,
+        n_balancing_samples: int = 50,
+        learning_rate: float = 0.001,
+        epochs: int = 3,
+        **kwargs,
+    ):
+        for ensemble in self.ensembles:
+            ensemble.associate(
+                pairs=pairs,
+                n_buckets=n_buckets,
+                n_association_samples=n_association_samples,
+                n_balancing_samples=n_balancing_samples,
+                learning_rate=learning_rate,
+                epochs=epochs,
+                force_non_empty=kwargs.get("force_non_empty", True),
+            )
+
+    def _shard_upvote_pairs(
+        self, source_target_pairs: List[Tuple[str, int]]
+    ) -> List[List[Tuple[str, int]]]:
+        shards = [[] for _ in range(self.num_shards)]
+        for pair in source_target_pairs:
+            model_ids = self.label_to_segment_map.get(pair[1])
+            if model_ids is None:
+                raise Exception(f"The Label {pair[1]} is not a part of Label Index")
+            for model_id in model_ids:
+                shards[model_id].append(pair)
+        return shards
+
+    def upvote(
+        self,
+        pairs: List[Tuple[str, int]],
+        n_upvote_samples: int = 16,
+        n_balancing_samples: int = 50,
+        learning_rate: float = 0.001,
+        epochs: int = 3,
+    ):
+        sharded_pairs = self._shard_upvote_pairs(pairs)
+
+        for ensemble, shard in zip(self.ensembles, sharded_pairs):
+            if len(shard) == 0:
+                continue
+            ensemble.upvote(
+                pairs=shard,
+                n_upvote_samples=n_upvote_samples,
+                n_balancing_samples=n_balancing_samples,
+                learning_rate=learning_rate,
+                epochs=epochs,
+            )
+
+    def retrain(
+        self,
+        balancing_data: DocumentDataSource,
+        source_target_pairs: List[Tuple[str, str]],
+        n_buckets: int,
+        learning_rate: float,
+        epochs: int,
+    ):
+        balancing_data_shards = shard_data_source(
+            data_source=balancing_data,
+            number_shards=self.num_shards,
+            label_to_segment_map=self.label_to_segment_map,
+            update_segment_map=False,
+        )
+        for ensemble, shard in zip(self.ensembles, balancing_data_shards):
+            ensemble.retrain(
+                balancing_data=shard,
+                source_target_pairs=source_target_pairs,
+                n_buckets=n_buckets,
+                learning_rate=learning_rate,
+                epochs=epochs,
+            )
+
+    def __setstate__(self, state):
+        if "model_config" not in state:
+            # Add model_config field if an older model is being loaded.
+            state["model_config"] = None
+        self.__dict__.update(state)
+
+    def _resume_supervised(
+        self,
+        checkpoint_config: Optional[CheckpointConfig],
+        callbacks: List[bolt.train.callbacks.Callback],
+    ):
+        ensemble_checkpoint_configs = generate_checkpoint_configs_for_ensembles(
+            config=checkpoint_config,
+            number_ensembles=self.num_shards,
+            number_models_per_ensemble=self.num_models_per_shard,
+        )
+
+        training_managers = []
+
+        for ensemble, config in zip(self.ensembles, ensemble_checkpoint_configs):
+            ensemble_training_managers: List[TrainingProgressManager] = []
+            for model_id, model in enumerate(ensemble.models):
+                if model_id == 0:
+                    modelwise_training_manager = (
+                        TrainingProgressManager.from_checkpoint(
+                            original_mach_model=model,
+                            checkpoint_config=config[model_id],
+                            for_supervised=True,
+                        )
+                    )
+                else:
+                    modelwise_training_manager = (
+                        TrainingProgressManager.from_checkpoint(
+                            original_mach_model=model,
+                            checkpoint_config=config[model_id],
+                            for_supervised=True,
+                            datasource_manager=ensemble_training_managers[
+                                0
+                            ].datasource_manager,
+                        )
+                    )
+                ensemble_training_managers.append(modelwise_training_manager)
+            training_managers.append(ensemble_training_managers)
+
+        for ensemble, managers in zip(self.ensembles, training_managers):
+            ensemble.supervised_training_impl(managers, callbacks=callbacks)
+
+    def _supervised_from_start(
+        self,
+        supervised_data_source,
+        learning_rate,
+        epochs,
+        batch_size,
+        max_in_memory_batches,
+        metrics,
+        callbacks,
+        disable_finetunable_retriever,
+        checkpoint_config,
+    ):
+
+        supervised_data_source_shards = shard_data_source(
+            data_source=supervised_data_source,
+            number_shards=self.num_shards,
+            label_to_segment_map=self.label_to_segment_map,
+            update_segment_map=False,
+        )
+
+        ensemble_checkpoint_configs = generate_checkpoint_configs_for_ensembles(
+            config=checkpoint_config,
+            number_ensembles=self.num_shards,
+            number_models_per_ensemble=self.num_models_per_shard,
+        )
+        training_managers = []
+
+        for ensemble, config, supervised_shard in zip(
+            self.ensembles, ensemble_checkpoint_configs, supervised_data_source_shards
+        ):
+            ensemble_training_managers: List[TrainingProgressManager] = []
+            for model_id, model in enumerate(ensemble.models):
+                modelwise_training_manager = (
+                    TrainingProgressManager.from_scratch_for_supervised(
+                        model=model,
+                        supervised_datasource=supervised_shard,
+                        learning_rate=learning_rate,
+                        epochs=epochs,
+                        batch_size=batch_size,
+                        max_in_memory_batches=max_in_memory_batches,
+                        metrics=metrics,
+                        disable_finetunable_retriever=disable_finetunable_retriever,
+                        checkpoint_config=config[model_id],
+                    )
+                )
+                ensemble_training_managers.append(modelwise_training_manager)
+                modelwise_training_manager.make_preindexing_checkpoint(
+                    save_datasource=model_id == 0
+                )
+            training_managers.append(ensemble_training_managers)
+
+        for ensemble, managers in zip(self.ensembles, training_managers):
+            ensemble.supervised_training_impl(managers, callbacks=callbacks)
+
+    def train_on_supervised_data_source(
+        self,
+        supervised_data_source: SupDataSource,
+        learning_rate: float,
+        epochs: int,
+        batch_size: Optional[int],
+        max_in_memory_batches: Optional[int],
+        metrics: List[str],
+        callbacks: List[bolt.train.callbacks.Callback],
+        disable_finetunable_retriever: bool,
+        checkpoint_config: Optional[CheckpointConfig] = None,
+    ):
+        if (
+            checkpoint_config is None
+            or checkpoint_config.resume_from_checkpoint is False
+        ):
+            self._supervised_from_start(
+                supervised_data_source=supervised_data_source,
+                learning_rate=learning_rate,
+                epochs=epochs,
+                batch_size=batch_size,
+                max_in_memory_batches=max_in_memory_batches,
+                metrics=metrics,
+                callbacks=callbacks,
+                disable_finetunable_retriever=disable_finetunable_retriever,
+                checkpoint_config=checkpoint_config,
+            )
+        else:
+            self._resume_supervised(
+                checkpoint_config=checkpoint_config, callbacks=callbacks
+            )
```

## thirdai/neural_db/models/models.py

 * *Ordering differences only*

```diff
@@ -1,11 +1,11 @@
-"""
-models.py used to contain Mach and the Model interface. Mach was moved to mach.py 
-to resolve a circular import issue since Mach needed to import FinetunableRetriever
-which in turn needed to import the Model interface class. However this caused issues
-when loading models becuase pickle was looking for the import models.models.Mach, 
-however it was changed to either models.mach.Mach or models.Mach. This adds back 
-the classes that were originally in models.py so that any older imports still work.
-"""
-
-from .mach import Mach
-from .model_interface import *
+"""
+models.py used to contain Mach and the Model interface. Mach was moved to mach.py 
+to resolve a circular import issue since Mach needed to import FinetunableRetriever
+which in turn needed to import the Model interface class. However this caused issues
+when loading models becuase pickle was looking for the import models.models.Mach, 
+however it was changed to either models.mach.Mach or models.Mach. This adds back 
+the classes that were originally in models.py so that any older imports still work.
+"""
+
+from .mach import Mach
+from .model_interface import *
```

## thirdai/neural_db/models/model_interface.py

 * *Ordering differences only*

```diff
@@ -1,191 +1,191 @@
-from __future__ import annotations
-
-from pathlib import Path
-from typing import Callable, List, Optional, Sequence, Tuple
-
-import numpy as np
-from thirdai import bolt, data
-
-from ..documents import DocumentDataSource
-from ..supervised_datasource import SupDataSource
-from ..trainer.checkpoint_config import CheckpointConfig
-from ..utils import clean_text
-
-InferSamples = List
-Predictions = Sequence
-TrainLabels = List
-TrainSamples = List
-
-
-# This class can be constructed by clients that use neural_db.
-# The object can then be passed into Model.index_documents(), and if
-# the client calls CancelState.cancel() on the object, training will halt.
-class CancelState:
-    def __init__(self, canceled=False):
-        self.canceled = canceled
-
-    def cancel(self):
-        self.canceled = True
-
-    def uncancel(self):
-        self.canceled = False
-
-    def is_canceled(self):
-        return self.canceled
-
-
-class Model:
-    def get_model(self) -> bolt.UniversalDeepTransformer:
-        raise NotImplementedError()
-
-    def index_documents(
-        self,
-        intro_documents: DocumentDataSource,
-        train_documents: DocumentDataSource,
-        should_train: bool,
-        fast_approximation: bool = True,
-        num_buckets_to_sample: Optional[int] = None,
-        on_progress: Callable = lambda **kwargs: None,
-        cancel_state: CancelState = None,
-        max_in_memory_batches: int = None,
-        override_number_classes: int = None,
-        variable_length: Optional[
-            data.transformations.VariableLengthConfig
-        ] = data.transformations.VariableLengthConfig(),
-        checkpoint_config: CheckpointConfig = None,
-        **kwargs,
-    ) -> None:
-        raise NotImplementedError()
-
-    def forget_documents(self) -> None:
-        raise NotImplementedError()
-
-    def delete_entities(self, entities) -> None:
-        raise NotImplementedError()
-
-    @property
-    def searchable(self) -> bool:
-        raise NotImplementedError()
-
-    def get_query_col(self) -> str:
-        raise NotImplementedError()
-
-    def set_n_ids(self, n_ids: int):
-        raise NotImplementedError()
-
-    def get_id_col(self) -> str:
-        raise NotImplementedError()
-
-    def get_id_delimiter(self) -> str:
-        raise NotImplementedError()
-
-    def infer_samples_to_infer_batch(self, samples: InferSamples):
-        query_col = self.get_query_col()
-        return [{query_col: clean_text(text)} for text in samples]
-
-    def infer_labels(
-        self,
-        samples: InferSamples,
-        n_results: int,
-        retriever: Optional[str] = None,
-        **kwargs,
-    ) -> Predictions:
-        raise NotImplementedError()
-
-    def score(
-        self, samples: InferSamples, entities: List[List[int]], n_results: int = None
-    ) -> Predictions:
-        raise NotImplementedError()
-
-    def save_meta(self, directory: Path) -> None:
-        raise NotImplementedError()
-
-    def load_meta(self, directory: Path):
-        raise NotImplementedError()
-
-    def associate(
-        self,
-        pairs: List[Tuple[str, str]],
-        n_buckets: int,
-        n_association_samples: int = 16,
-        n_balancing_samples: int = 50,
-        learning_rate: float = 0.001,
-        epochs: int = 3,
-        **kwargs,
-    ):
-        raise NotImplementedError()
-
-    def upvote(
-        self,
-        pairs: List[Tuple[str, int]],
-        n_upvote_samples: int = 16,
-        n_balancing_samples: int = 50,
-        learning_rate: float = 0.001,
-        epochs: int = 3,
-    ):
-        raise NotImplementedError()
-
-    def retrain(
-        self,
-        balancing_data: DocumentDataSource,
-        source_target_pairs: List[Tuple[str, str]],
-        n_buckets: int,
-        learning_rate: float,
-        epochs: int,
-    ):
-        raise NotImplementedError()
-
-    def train_on_supervised_data_source(
-        self,
-        supervised_data_source: SupDataSource,
-        learning_rate: float,
-        epochs: int,
-        batch_size: Optional[int],
-        max_in_memory_batches: Optional[int],
-        metrics: List[str],
-        callbacks: List[bolt.train.callbacks.Callback],
-        disable_finetunable_retriever: bool,
-    ):
-        raise NotImplementedError()
-
-
-def normalize_scores(results):
-    if len(results) == 0:
-        return results
-    if len(results) == 1:
-        return [(results[0][0], 1.0, results[0][2])]
-    ids, scores, retriever = zip(*results)
-    scores = np.array(scores)
-    scores -= np.min(scores)
-    scores /= np.max(scores)
-    return list(zip(ids, scores, retriever))
-
-
-def merge_results(results_a, results_b, k):
-    results_a = normalize_scores(results_a)
-    results_b = normalize_scores(results_b)
-    results = []
-    cache = set()
-
-    min_len = min(len(results_a), len(results_b))
-    for a, b in zip(results_a, results_b):
-        if a[0] not in cache:
-            results.append(a)
-            cache.add(a[0])
-        if b[0] not in cache:
-            results.append(b)
-            cache.add(b[0])
-
-    if len(results) < k:
-        for i in range(min_len, len(results_a)):
-            if results_a[i][0] not in cache:
-                results.append(results_a[i])
-        for i in range(min_len, len(results_b)):
-            if results_b[i][0] not in cache:
-                results.append(results_b[i])
-
-    return results[:k]
-
-
-def add_retriever_tag(results, tag):
-    return [[(id, score, tag) for id, score in result] for result in results]
+from __future__ import annotations
+
+from pathlib import Path
+from typing import Callable, List, Optional, Sequence, Tuple
+
+import numpy as np
+from thirdai import bolt, data
+
+from ..documents import DocumentDataSource
+from ..supervised_datasource import SupDataSource
+from ..trainer.checkpoint_config import CheckpointConfig
+from ..utils import clean_text
+
+InferSamples = List
+Predictions = Sequence
+TrainLabels = List
+TrainSamples = List
+
+
+# This class can be constructed by clients that use neural_db.
+# The object can then be passed into Model.index_documents(), and if
+# the client calls CancelState.cancel() on the object, training will halt.
+class CancelState:
+    def __init__(self, canceled=False):
+        self.canceled = canceled
+
+    def cancel(self):
+        self.canceled = True
+
+    def uncancel(self):
+        self.canceled = False
+
+    def is_canceled(self):
+        return self.canceled
+
+
+class Model:
+    def get_model(self) -> bolt.UniversalDeepTransformer:
+        raise NotImplementedError()
+
+    def index_documents(
+        self,
+        intro_documents: DocumentDataSource,
+        train_documents: DocumentDataSource,
+        should_train: bool,
+        fast_approximation: bool = True,
+        num_buckets_to_sample: Optional[int] = None,
+        on_progress: Callable = lambda **kwargs: None,
+        cancel_state: CancelState = None,
+        max_in_memory_batches: int = None,
+        override_number_classes: int = None,
+        variable_length: Optional[
+            data.transformations.VariableLengthConfig
+        ] = data.transformations.VariableLengthConfig(),
+        checkpoint_config: CheckpointConfig = None,
+        **kwargs,
+    ) -> None:
+        raise NotImplementedError()
+
+    def forget_documents(self) -> None:
+        raise NotImplementedError()
+
+    def delete_entities(self, entities) -> None:
+        raise NotImplementedError()
+
+    @property
+    def searchable(self) -> bool:
+        raise NotImplementedError()
+
+    def get_query_col(self) -> str:
+        raise NotImplementedError()
+
+    def set_n_ids(self, n_ids: int):
+        raise NotImplementedError()
+
+    def get_id_col(self) -> str:
+        raise NotImplementedError()
+
+    def get_id_delimiter(self) -> str:
+        raise NotImplementedError()
+
+    def infer_samples_to_infer_batch(self, samples: InferSamples):
+        query_col = self.get_query_col()
+        return [{query_col: clean_text(text)} for text in samples]
+
+    def infer_labels(
+        self,
+        samples: InferSamples,
+        n_results: int,
+        retriever: Optional[str] = None,
+        **kwargs,
+    ) -> Predictions:
+        raise NotImplementedError()
+
+    def score(
+        self, samples: InferSamples, entities: List[List[int]], n_results: int = None
+    ) -> Predictions:
+        raise NotImplementedError()
+
+    def save_meta(self, directory: Path) -> None:
+        raise NotImplementedError()
+
+    def load_meta(self, directory: Path):
+        raise NotImplementedError()
+
+    def associate(
+        self,
+        pairs: List[Tuple[str, str]],
+        n_buckets: int,
+        n_association_samples: int = 16,
+        n_balancing_samples: int = 50,
+        learning_rate: float = 0.001,
+        epochs: int = 3,
+        **kwargs,
+    ):
+        raise NotImplementedError()
+
+    def upvote(
+        self,
+        pairs: List[Tuple[str, int]],
+        n_upvote_samples: int = 16,
+        n_balancing_samples: int = 50,
+        learning_rate: float = 0.001,
+        epochs: int = 3,
+    ):
+        raise NotImplementedError()
+
+    def retrain(
+        self,
+        balancing_data: DocumentDataSource,
+        source_target_pairs: List[Tuple[str, str]],
+        n_buckets: int,
+        learning_rate: float,
+        epochs: int,
+    ):
+        raise NotImplementedError()
+
+    def train_on_supervised_data_source(
+        self,
+        supervised_data_source: SupDataSource,
+        learning_rate: float,
+        epochs: int,
+        batch_size: Optional[int],
+        max_in_memory_batches: Optional[int],
+        metrics: List[str],
+        callbacks: List[bolt.train.callbacks.Callback],
+        disable_finetunable_retriever: bool,
+    ):
+        raise NotImplementedError()
+
+
+def normalize_scores(results):
+    if len(results) == 0:
+        return results
+    if len(results) == 1:
+        return [(results[0][0], 1.0, results[0][2])]
+    ids, scores, retriever = zip(*results)
+    scores = np.array(scores)
+    scores -= np.min(scores)
+    scores /= np.max(scores)
+    return list(zip(ids, scores, retriever))
+
+
+def merge_results(results_a, results_b, k):
+    results_a = normalize_scores(results_a)
+    results_b = normalize_scores(results_b)
+    results = []
+    cache = set()
+
+    min_len = min(len(results_a), len(results_b))
+    for a, b in zip(results_a, results_b):
+        if a[0] not in cache:
+            results.append(a)
+            cache.add(a[0])
+        if b[0] not in cache:
+            results.append(b)
+            cache.add(b[0])
+
+    if len(results) < k:
+        for i in range(min_len, len(results_a)):
+            if results_a[i][0] not in cache:
+                results.append(results_a[i])
+        for i in range(min_len, len(results_b)):
+            if results_b[i][0] not in cache:
+                results.append(results_b[i])
+
+    return results[:k]
+
+
+def add_retriever_tag(results, tag):
+    return [[(id, score, tag) for id, score in result] for result in results]
```

## thirdai/neural_db/models/multi_mach.py

 * *Ordering differences only*

```diff
@@ -1,227 +1,227 @@
-from collections import defaultdict
-from pathlib import Path
-from typing import Callable, List, Optional, Tuple
-
-from thirdai import bolt
-
-from ..documents import DocumentDataSource
-from ..supervised_datasource import SupDataSource
-from ..trainer.training_progress_manager import TrainingProgressManager
-from ..utils import clean_text
-from .mach import Mach
-from .model_interface import CancelState
-
-
-def aggregate_ensemble_results(results):
-    final_results = []
-    for i in range(len(results[0])):
-        sample_result = defaultdict(float)
-        for model_result in results:
-            for res in model_result[i]:
-                sample_result[res[0]] += res[1]
-
-        result = [(key, value) for key, value in sample_result.items()]
-        result.sort(key=lambda x: x[1], reverse=True)
-        final_results.append(result)
-    return final_results
-
-
-class MultiMach:
-    def __init__(
-        self,
-        number_models: int,
-        id_col: str,
-        id_delimiter: str,
-        query_col: str,
-        fhr: int,
-        embedding_dimension: int,
-        extreme_output_dim: int,
-        extreme_num_hashes: int,
-        tokenizer: int,
-        hidden_bias: bool,
-        hybrid: bool,
-        model_config,
-        mach_index_seed_offset: int,
-    ):
-        if number_models < 1:
-            raise ValueError(
-                "Cannot initialize a MultiMach with less than one Mach model"
-            )
-        self.query_col = query_col
-        self.models = [
-            Mach(
-                id_col=id_col,
-                id_delimiter=id_delimiter,
-                query_col=query_col,
-                fhr=fhr,
-                embedding_dimension=embedding_dimension,
-                extreme_output_dim=extreme_output_dim,
-                extreme_num_hashes=extreme_num_hashes,
-                tokenizer=tokenizer,
-                hidden_bias=hidden_bias,
-                model_config=model_config,
-                hybrid=(
-                    hybrid if j == 0 else False
-                ),  # retriever will be the same for all models in the ensemble
-                mach_index_seed=(mach_index_seed_offset + j * 17),
-            )
-            for j in range(number_models)
-        ]
-
-    @property
-    def n_ids(self):
-        return self.models[0].n_ids
-
-    def set_mach_sampling_threshold(self, threshold: float):
-        for model in self.models:
-            model.set_mach_sampling_threshold(threshold)
-
-    def get_model(self) -> List[bolt.UniversalDeepTransformer]:
-        for model in self.models:
-            if not model.get_model():
-                return None
-        return [model.get_model() for model in self.models]
-
-    def set_model(self, models: List[bolt.UniversalDeepTransformer]):
-        for udt_model, ndb_mach in zip(models, self.models):
-            ndb_mach.set_model(udt_model)
-
-    def save_meta(self, directory: Path):
-        pass
-
-    def load_meta(self, directory: Path):
-        pass
-
-    def index_documents_impl(
-        self,
-        training_progress_managers: List[TrainingProgressManager],
-        on_progress: Callable,
-        cancel_state: CancelState,
-        callbacks: List[bolt.train.callbacks.Callback] = None,
-    ):
-        for progress_manager, model in zip(training_progress_managers, self.models):
-            model.index_documents_impl(
-                training_progress_manager=progress_manager,
-                on_progress=on_progress,
-                cancel_state=cancel_state,
-                callbacks=callbacks,
-            )
-
-    def delete_entities(self, entities) -> None:
-        for model in self.models:
-            model.delete_entities(entities)
-
-    def forget_documents(self) -> None:
-        for model in self.models:
-            model.forget_documents()
-
-    @property
-    def searchable(self) -> bool:
-        return self.n_ids != 0
-
-    def query_finetunable_retriever(self, samples, n_results):
-        # only the first model in the ensemble can have the retriever
-        model = self.models[0]
-        if model.finetunable_retriever:
-            model.query_finetunable_retriever(samples=samples, n_results=n_results)
-        else:
-            return None
-
-    def score(self, samples: List, entities: List[List[int]], n_results: int = None):
-        model_scores = [
-            model.score(samples=samples, entities=entities, n_results=n_results)
-            for model in self.models
-        ]
-        aggregated_scores = [defaultdict(int) for _ in range(len(samples))]
-
-        for i in range(len(samples)):
-            for score in model_scores:
-                for label, value, tag in score[i]:
-                    aggregated_scores[i][label] += value
-                    assert tag == "mach", (
-                        "We ignore the retriever tag returned by each ensemble. "
-                        "This was inconsequential at the time of writing since "
-                        "the Mach.score() always returns the 'mach' retriever "
-                        "tag. We assert this condition so we reevaluate this "
-                        "decision if the condition no longer holds."
-                    )
-
-        # Sort the aggregated scores and keep only the top k results
-        top_k_results = []
-        for i in range(len(samples)):
-            sorted_scores = sorted(
-                [
-                    (label, score, "mach")
-                    for label, score in aggregated_scores[i].items()
-                ],
-                key=lambda x: x[1],
-                reverse=True,
-            )
-            top_k_results.append(
-                sorted_scores[:n_results] if n_results else sorted_scores
-            )
-
-        return top_k_results
-
-    def associate(
-        self,
-        pairs: List[Tuple[str, str]],
-        n_buckets: int,
-        n_association_samples: int,
-        n_balancing_samples: int,
-        learning_rate: float,
-        epochs: int,
-        **kwargs,
-    ):
-        for model in self.models:
-            model.associate(
-                pairs=pairs,
-                n_buckets=n_buckets,
-                n_association_samples=n_association_samples,
-                n_balancing_samples=n_balancing_samples,
-                learning_rate=learning_rate,
-                epochs=epochs,
-                force_non_empty=kwargs.get("force_non_empty", True),
-            )
-
-    def upvote(
-        self,
-        pairs: List[Tuple[str, int]],
-        n_upvote_samples: int,
-        n_balancing_samples: int,
-        learning_rate: float,
-        epochs: int,
-    ):
-        for model in self.models:
-            model.upvote(
-                pairs=pairs,
-                n_upvote_samples=n_upvote_samples,
-                n_balancing_samples=n_balancing_samples,
-                learning_rate=learning_rate,
-                epochs=epochs,
-            )
-
-    def retrain(
-        self,
-        balancing_data: DocumentDataSource,
-        source_target_pairs: List[Tuple[str, str]],
-        n_buckets: int,
-        learning_rate: float,
-        epochs: int,
-    ):
-        for model in self.models:
-            model.retrain(
-                balancing_data=balancing_data,
-                source_target_pairs=source_target_pairs,
-                n_buckets=n_buckets,
-                learning_rate=learning_rate,
-                epochs=epochs,
-            )
-
-    def supervised_training_impl(
-        self,
-        supervised_progress_managers: List[TrainingProgressManager],
-        callbacks: List[bolt.train.callbacks.Callback],
-    ):
-        for manager, model in zip(supervised_progress_managers, self.models):
-            model.supervised_training_impl(manager, callbacks)
+from collections import defaultdict
+from pathlib import Path
+from typing import Callable, List, Optional, Tuple
+
+from thirdai import bolt
+
+from ..documents import DocumentDataSource
+from ..supervised_datasource import SupDataSource
+from ..trainer.training_progress_manager import TrainingProgressManager
+from ..utils import clean_text
+from .mach import Mach
+from .model_interface import CancelState
+
+
+def aggregate_ensemble_results(results):
+    final_results = []
+    for i in range(len(results[0])):
+        sample_result = defaultdict(float)
+        for model_result in results:
+            for res in model_result[i]:
+                sample_result[res[0]] += res[1]
+
+        result = [(key, value) for key, value in sample_result.items()]
+        result.sort(key=lambda x: x[1], reverse=True)
+        final_results.append(result)
+    return final_results
+
+
+class MultiMach:
+    def __init__(
+        self,
+        number_models: int,
+        id_col: str,
+        id_delimiter: str,
+        query_col: str,
+        fhr: int,
+        embedding_dimension: int,
+        extreme_output_dim: int,
+        extreme_num_hashes: int,
+        tokenizer: int,
+        hidden_bias: bool,
+        hybrid: bool,
+        model_config,
+        mach_index_seed_offset: int,
+    ):
+        if number_models < 1:
+            raise ValueError(
+                "Cannot initialize a MultiMach with less than one Mach model"
+            )
+        self.query_col = query_col
+        self.models = [
+            Mach(
+                id_col=id_col,
+                id_delimiter=id_delimiter,
+                query_col=query_col,
+                fhr=fhr,
+                embedding_dimension=embedding_dimension,
+                extreme_output_dim=extreme_output_dim,
+                extreme_num_hashes=extreme_num_hashes,
+                tokenizer=tokenizer,
+                hidden_bias=hidden_bias,
+                model_config=model_config,
+                hybrid=(
+                    hybrid if j == 0 else False
+                ),  # retriever will be the same for all models in the ensemble
+                mach_index_seed=(mach_index_seed_offset + j * 17),
+            )
+            for j in range(number_models)
+        ]
+
+    @property
+    def n_ids(self):
+        return self.models[0].n_ids
+
+    def set_mach_sampling_threshold(self, threshold: float):
+        for model in self.models:
+            model.set_mach_sampling_threshold(threshold)
+
+    def get_model(self) -> List[bolt.UniversalDeepTransformer]:
+        for model in self.models:
+            if not model.get_model():
+                return None
+        return [model.get_model() for model in self.models]
+
+    def set_model(self, models: List[bolt.UniversalDeepTransformer]):
+        for udt_model, ndb_mach in zip(models, self.models):
+            ndb_mach.set_model(udt_model)
+
+    def save_meta(self, directory: Path):
+        pass
+
+    def load_meta(self, directory: Path):
+        pass
+
+    def index_documents_impl(
+        self,
+        training_progress_managers: List[TrainingProgressManager],
+        on_progress: Callable,
+        cancel_state: CancelState,
+        callbacks: List[bolt.train.callbacks.Callback] = None,
+    ):
+        for progress_manager, model in zip(training_progress_managers, self.models):
+            model.index_documents_impl(
+                training_progress_manager=progress_manager,
+                on_progress=on_progress,
+                cancel_state=cancel_state,
+                callbacks=callbacks,
+            )
+
+    def delete_entities(self, entities) -> None:
+        for model in self.models:
+            model.delete_entities(entities)
+
+    def forget_documents(self) -> None:
+        for model in self.models:
+            model.forget_documents()
+
+    @property
+    def searchable(self) -> bool:
+        return self.n_ids != 0
+
+    def query_finetunable_retriever(self, samples, n_results):
+        # only the first model in the ensemble can have the retriever
+        model = self.models[0]
+        if model.finetunable_retriever:
+            model.query_finetunable_retriever(samples=samples, n_results=n_results)
+        else:
+            return None
+
+    def score(self, samples: List, entities: List[List[int]], n_results: int = None):
+        model_scores = [
+            model.score(samples=samples, entities=entities, n_results=n_results)
+            for model in self.models
+        ]
+        aggregated_scores = [defaultdict(int) for _ in range(len(samples))]
+
+        for i in range(len(samples)):
+            for score in model_scores:
+                for label, value, tag in score[i]:
+                    aggregated_scores[i][label] += value
+                    assert tag == "mach", (
+                        "We ignore the retriever tag returned by each ensemble. "
+                        "This was inconsequential at the time of writing since "
+                        "the Mach.score() always returns the 'mach' retriever "
+                        "tag. We assert this condition so we reevaluate this "
+                        "decision if the condition no longer holds."
+                    )
+
+        # Sort the aggregated scores and keep only the top k results
+        top_k_results = []
+        for i in range(len(samples)):
+            sorted_scores = sorted(
+                [
+                    (label, score, "mach")
+                    for label, score in aggregated_scores[i].items()
+                ],
+                key=lambda x: x[1],
+                reverse=True,
+            )
+            top_k_results.append(
+                sorted_scores[:n_results] if n_results else sorted_scores
+            )
+
+        return top_k_results
+
+    def associate(
+        self,
+        pairs: List[Tuple[str, str]],
+        n_buckets: int,
+        n_association_samples: int,
+        n_balancing_samples: int,
+        learning_rate: float,
+        epochs: int,
+        **kwargs,
+    ):
+        for model in self.models:
+            model.associate(
+                pairs=pairs,
+                n_buckets=n_buckets,
+                n_association_samples=n_association_samples,
+                n_balancing_samples=n_balancing_samples,
+                learning_rate=learning_rate,
+                epochs=epochs,
+                force_non_empty=kwargs.get("force_non_empty", True),
+            )
+
+    def upvote(
+        self,
+        pairs: List[Tuple[str, int]],
+        n_upvote_samples: int,
+        n_balancing_samples: int,
+        learning_rate: float,
+        epochs: int,
+    ):
+        for model in self.models:
+            model.upvote(
+                pairs=pairs,
+                n_upvote_samples=n_upvote_samples,
+                n_balancing_samples=n_balancing_samples,
+                learning_rate=learning_rate,
+                epochs=epochs,
+            )
+
+    def retrain(
+        self,
+        balancing_data: DocumentDataSource,
+        source_target_pairs: List[Tuple[str, str]],
+        n_buckets: int,
+        learning_rate: float,
+        epochs: int,
+    ):
+        for model in self.models:
+            model.retrain(
+                balancing_data=balancing_data,
+                source_target_pairs=source_target_pairs,
+                n_buckets=n_buckets,
+                learning_rate=learning_rate,
+                epochs=epochs,
+            )
+
+    def supervised_training_impl(
+        self,
+        supervised_progress_managers: List[TrainingProgressManager],
+        callbacks: List[bolt.train.callbacks.Callback],
+    ):
+        for manager, model in zip(supervised_progress_managers, self.models):
+            model.supervised_training_impl(manager, callbacks)
```

## thirdai/neural_db/models/__init__.py

 * *Ordering differences only*

```diff
@@ -1 +1 @@
-from .mach import Mach
+from .mach import Mach
```

## thirdai/neural_db/model_bazaar/bazaar_base.py

 * *Ordering differences only*

```diff
@@ -1,578 +1,578 @@
-import concurrent.futures
-import json
-import os
-import pickle
-import shutil
-import threading
-from dataclasses import dataclass
-from pathlib import Path
-from typing import Callable, List, Optional, Union
-from urllib.parse import urljoin
-
-import requests
-from pydantic import BaseModel, ValidationError
-from requests.auth import HTTPBasicAuth
-from thirdai.neural_db.neural_db import CancelState, NeuralDB
-from tqdm import tqdm
-
-from .utils import (
-    create_model_identifier,
-    get_directory_size,
-    get_file_size,
-    hash_path,
-    http_get_with_error,
-    http_post_with_error,
-    zip_folder,
-)
-
-
-class BazaarEntry(BaseModel):
-    name: str
-    author_username: str
-    identifier: str
-    trained_on: Optional[str] = None
-    num_params: int
-    size: int
-    size_in_memory: int
-    hash: str
-    domain: str
-    description: Optional[str] = None
-    is_indexed: bool = False
-    publish_date: str
-    author_email: str
-    access_level: str = "public"
-    thirdai_version: str
-
-    @staticmethod
-    def from_dict(entry):
-        return BazaarEntry(
-            name=entry["model_name"],
-            author_username=entry["username"],
-            identifier=create_model_identifier(
-                model_name=entry["model_name"], author_username=entry["username"]
-            ),
-            trained_on=entry["trained_on"],
-            num_params=entry["num_params"],
-            size=entry["size"],
-            size_in_memory=entry["size_in_memory"],
-            hash=entry["hash"],
-            domain=entry["domain"],
-            description=entry["description"],
-            is_indexed=entry["is_indexed"],
-            publish_date=entry["publish_date"],
-            author_email=entry["user_email"],
-            access_level=entry["access_level"],
-            thirdai_version=entry["thirdai_version"],
-        )
-
-    @staticmethod
-    def bazaar_entry_from_json(json_entry):
-        try:
-            loaded_entry = BazaarEntry.from_dict(json_entry)
-            return loaded_entry
-        except ValidationError as e:
-            print(f"Validation error: {e}")
-            return None
-
-
-@dataclass
-class Login:
-    base_url: str
-    username: str
-    user_id: str
-    access_token: str
-
-    @staticmethod
-    def with_email(
-        base_url: str,
-        email: str,
-        password: str,
-    ):
-        # We are using HTTPBasic Auth in backend. update this when we change the Authentication in Backend.
-        response = http_get_with_error(
-            urljoin(base_url, "user/email-login"),
-            auth=HTTPBasicAuth(email, password),
-        )
-
-        content = json.loads(response.content)
-        username = content["data"]["user"]["username"]
-        user_id = content["data"]["user"]["user_id"]
-        access_token = content["data"]["access_token"]
-        return Login(base_url, username, user_id, access_token)
-
-
-def auth_header(access_token):
-    return {
-        "Authorization": f"Bearer {access_token}",
-    }
-
-
-def relative_path_depth(child_path: Path, parent_path: Path):
-    child_path, parent_path = child_path.resolve(), parent_path.resolve()
-    relpath = os.path.relpath(child_path, parent_path)
-    if relpath == ".":
-        return 0
-    else:
-        return 1 + relpath.count(os.sep)
-
-
-# Use this decorator for any function to enforce users use only after login.
-def login_required(func):
-    def wrapper(self, *args, **kwargs):
-        if not self.is_logged_in():
-            raise PermissionError(
-                "This method requires login, please use '.login()' first then try again."
-            )
-        return func(self, *args, **kwargs)
-
-    return wrapper
-
-
-class Bazaar:
-    def __init__(
-        self,
-        base_url,
-        cache_dir: Union[Path, str],
-    ):
-        cache_dir = Path(cache_dir)
-        if not os.path.exists(cache_dir):
-            os.makedirs(cache_dir)
-        self._cache_dir = cache_dir
-        if not base_url.endswith("/api/"):
-            raise ValueError("base_url must end with '/api/'.")
-        self._base_url = base_url
-        self._login_instance = None
-
-    def signup(self, email, password, username):
-        json_data = {
-            "username": username,
-            "email": email,
-            "password": password,
-        }
-
-        response = http_post_with_error(
-            urljoin(self._base_url, "user/email-signup-basic"),
-            json=json_data,
-        )
-
-        print(
-            f"Successfully signed up. Please check your email ({email}) to verify your account."
-        )
-
-    def login(self, email, password):
-        self._login_instance = Login.with_email(self._base_url, email, password)
-
-    def is_logged_in(self):
-        return self._login_instance is not None
-
-    def fetch(
-        self,
-        name: str = "",
-        domain: Optional[str] = None,
-        username: Optional[str] = None,
-        access_level: Optional[List[str]] = None,
-    ):
-        if self.is_logged_in():
-            url = urljoin(
-                self._login_instance.base_url,
-                f"bazaar/{self._login_instance.user_id}/list",
-            )
-            response = http_get_with_error(
-                url,
-                params={
-                    "name": name,
-                    "domain": domain,
-                    "username": username,
-                    "access_level": access_level,
-                },
-                headers=auth_header(self._login_instance.access_token),
-            )
-        else:
-            print("Fetching public models, login to fetch all accessible models.")
-            url = urljoin(
-                self._base_url,
-                "bazaar/public-list",
-            )
-            response = http_get_with_error(
-                url,
-                params={
-                    "name": name,
-                    "domain": domain,
-                    "username": username,
-                },
-            )
-        json_entries = json.loads(response.content)["data"]
-
-        bazaar_entries = [
-            BazaarEntry.bazaar_entry_from_json(json_entry)
-            for json_entry in json_entries
-            if json_entry
-        ]
-        return bazaar_entries
-
-    def fetch_from_cache(
-        self,
-        name: str = "",
-        domain: Optional[str] = None,
-        username: Optional[str] = None,
-        access_level: Optional[List[str]] = None,
-        only_check_dir_exists: bool = False,
-    ):
-        bazaar_entries = []
-        # Walk through the directories
-        for dirpath, dirnames, filenames in os.walk(self._cache_dir):
-            depth = relative_path_depth(
-                child_path=Path(dirpath), parent_path=Path(self._cache_dir)
-            )
-
-            if depth == 2:
-                # We're two levels in, which is the level of all checkpoint dirs
-                split_path = dirpath.split(os.path.sep)
-                model_name = split_path[-1]
-                author_username = split_path[-2]
-
-                identifier = f"{author_username}/{model_name}"
-                with open(self._cached_model_metadata_path(identifier), "r") as f:
-                    bazaar_entry = BazaarEntry.from_dict(json.load(f))
-
-                if (
-                    name.lower() in model_name.lower()
-                    and (not username or username == author_username)
-                    and (not domain or domain == bazaar_entry.domain)
-                    and (not access_level or bazaar_entry.access_level in access_level)
-                ):
-                    try:
-                        if self._model_dir_in_cache(
-                            identifier=identifier,
-                            fetched_bazaar_entry=bazaar_entry,
-                            only_check_dir_exists=only_check_dir_exists,
-                        ):
-                            bazaar_entries.append(bazaar_entry)
-                    except:
-                        pass
-
-                dirnames.clear()  # Don't descend any further
-
-            elif depth > 2:
-                # We're too deep, don't process this directory
-                dirnames.clear()
-
-        return bazaar_entries
-
-    def list_model_names(self):
-        return [entry.identifier for entry in self.fetch()]
-
-    def get_neuraldb(
-        self,
-        model_identifier: str,
-        on_progress: Callable = lambda *args, **kwargs: None,
-        cancel_state: CancelState = CancelState(),
-        disable_progress_bar: bool = False,
-    ):
-        model_dir = self.get_model_dir(
-            model_identifier, on_progress, cancel_state, disable_progress_bar
-        )
-        return NeuralDB.from_checkpoint(checkpoint_path=model_dir)
-
-    def get_model_dir(
-        self,
-        model_identifier,
-        on_progress: Callable = lambda *args, **kwargs: None,
-        cancel_state: CancelState = CancelState(),
-        disable_progress_bar: bool = False,
-    ):
-        if self.is_logged_in():
-            url = urljoin(
-                self._login_instance.base_url,
-                f"bazaar/{self._login_instance.user_id}/model",
-            )
-            response = http_get_with_error(
-                url,
-                params={"model_identifier": model_identifier},
-                headers=auth_header(self._login_instance.access_token),
-            )
-        else:
-            url = urljoin(
-                self._base_url,
-                "bazaar/model",
-            )
-            response = http_get_with_error(
-                url,
-                params={"model_identifier": model_identifier},
-            )
-
-        json_entry = json.loads(response.content)["data"]
-        bazaar_entry = BazaarEntry.bazaar_entry_from_json(json_entry)
-
-        cached_model_dir = self._model_dir_in_cache(
-            identifier=model_identifier, fetched_bazaar_entry=bazaar_entry
-        )
-        if cached_model_dir:
-            return cached_model_dir
-
-        self._download(
-            model_identifier,
-            on_progress=on_progress,
-            cancel_state=cancel_state,
-            disable_progress_bar=disable_progress_bar,
-        )
-
-        if not cancel_state.is_canceled():
-            return self._unpack_and_remove_zip(model_identifier)
-        else:
-            try:
-                shutil.rmtree(self._cached_checkpoint_dir(model_identifier))
-            except:
-                pass
-            return None
-
-    # The checkpoint dir is cache_dir/model_identifier/
-    # This is the parent directory for the three paths defined in the following methods
-    def _cached_checkpoint_dir(self, identifier: str):
-        return self._cache_dir / identifier
-
-    # The ndb path is cache_dir/model_identifier/model.ndb
-    def _cached_model_dir_path(self, identifier: str):
-        return self._cached_checkpoint_dir(identifier) / "model.ndb"
-
-    # The ndb zip download path is cache_dir/model_identifier/model.ndb.zip
-    def _cached_model_zip_path(self, identifier: str):
-        return self._cached_checkpoint_dir(identifier) / "model.ndb.zip"
-
-    # The BazaarEntry json metadata path is cache_dir/author_username/model_name/metadata.json
-    def _cached_model_metadata_path(self, identifier: str):
-        return self._cached_checkpoint_dir(identifier) / "metadata.json"
-
-    def _model_dir_in_cache(
-        self,
-        identifier: str,
-        fetched_bazaar_entry: str,
-        only_check_dir_exists: bool = False,
-    ):
-        cached_model_dir = self._cached_model_dir_path(identifier)
-        if cached_model_dir.is_dir():
-            if not only_check_dir_exists:
-                hash_match = hash_path(cached_model_dir) == fetched_bazaar_entry.hash
-                size_match = (
-                    get_directory_size(cached_model_dir) == fetched_bazaar_entry.size
-                )
-                if hash_match and size_match:
-                    return cached_model_dir
-            else:
-                return cached_model_dir
-        return None
-
-    def _unpack_and_remove_zip(self, identifier: str):
-        zip_path = self._cached_model_zip_path(identifier)
-        extract_dir = self._cached_model_dir_path(identifier)
-        shutil.unpack_archive(filename=zip_path, extract_dir=extract_dir)
-        os.remove(zip_path)
-        return extract_dir
-
-    def _download(
-        self,
-        model_identifier: str,
-        on_progress: Callable,
-        cancel_state: CancelState,
-        disable_progress_bar: bool = False,
-    ):
-        if self.is_logged_in():
-            url = urljoin(
-                self._login_instance.base_url,
-                f"bazaar/{self._login_instance.user_id}/download",
-            )
-            response = requests.get(
-                url,
-                params={"model_identifier": model_identifier},
-                headers=auth_header(self._login_instance.access_token),
-                stream=True,
-            )
-        else:
-            url = urljoin(
-                self._base_url,
-                f"bazaar/public-download",
-            )
-            response = requests.get(
-                url, params={"model_identifier": model_identifier}, stream=True
-            )
-        try:
-            shutil.rmtree(self._cached_checkpoint_dir(model_identifier))
-        except:
-            pass
-        os.makedirs(self._cached_checkpoint_dir(model_identifier))
-
-        destination = self._cached_model_zip_path(model_identifier)
-
-        # Try to get the total size from the Content-Length header
-        total_size = int(response.headers.get("Content-Length", 0))
-
-        # Set up a progress bar
-        with tqdm(
-            total=total_size, unit="B", unit_scale=True, desc="Downloading"
-        ) as bar:
-            # Destination file path
-            with open(destination, "wb") as f:
-                for chunk in response.iter_content(8192):  # 8192 bytes or 8KB
-                    f.write(chunk)
-                    bar.update(len(chunk))
-
-    def upload_chunk(self, upload_token, chunk_number, chunk_data, bar, progress_lock):
-        files = {"chunk": chunk_data}
-        response = requests.post(
-            urljoin(
-                self._login_instance.base_url,
-                "bazaar/upload-chunk",
-            ),
-            files=files,
-            params={"chunk_number": chunk_number},
-            headers=auth_header(upload_token),
-        )
-
-        if response.status_code == 200:
-            with progress_lock:
-                # Update the progress bar
-                bar.update(len(chunk_data))
-        else:
-            print(f"Upload failed with status code: {response.status_code}")
-            print(response.text)
-            return False
-
-        return True
-
-    @login_required
-    def push(
-        self,
-        name: str,
-        model_path: Union[Path, str],
-        trained_on: str = "Own Documents",
-        is_indexed: bool = False,
-        access_level: str = "public",
-        description: str = "",
-    ):
-        model_path = Path(model_path)
-        zip_path = zip_folder(model_path)
-
-        model_hash = hash_path(model_path)
-
-        # Generate upload token
-        token_response = http_get_with_error(
-            urljoin(
-                self._login_instance.base_url,
-                f"bazaar/{self._login_instance.user_id}/upload-token",
-            ),
-            headers=auth_header(self._login_instance.access_token),
-            params={
-                "model_name": name,
-                "size": int(get_file_size(zip_path, "MB")),
-            },
-        )
-        upload_token = json.loads(token_response.content)["data"]["token"]
-
-        # Get the total file size for progress bar
-        total_size = os.path.getsize(zip_path)
-
-        # Determine the chunk size you want to upload per request
-        chunk_size = 1024 * 1024  # 1 MB chunk
-
-        # Initialize the progress bar
-        with tqdm(total=total_size, unit="B", unit_scale=True, desc=zip_path) as bar:
-            # Open the file in binary mode
-            with open(zip_path, "rb") as file:
-                chunk_number = 0
-
-                with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:
-                    futures = []
-                    progress_lock = threading.Lock()
-
-                    while True:
-                        # Read a chunk of the file
-                        chunk_data = file.read(chunk_size)
-                        if not chunk_data:
-                            break  # End of file
-
-                        # Increment the chunk number
-                        chunk_number += 1
-
-                        # Submit the task to the thread pool
-                        future = executor.submit(
-                            self.upload_chunk,
-                            upload_token,
-                            chunk_number,
-                            chunk_data,
-                            bar,
-                            progress_lock,
-                        )
-                        futures.append(future)
-
-                    # Collect the return status of all threads
-                    threads_status = [
-                        future.result()
-                        for future in concurrent.futures.as_completed(futures)
-                    ]
-
-                # Check if all uploads were successful
-                if all(threads_status):
-                    print("File upload completed successfully.")
-                else:
-                    print("File upload failed.")
-
-        db = NeuralDB.from_checkpoint(checkpoint_path=model_path)
-        model = db._savable_state.model.model._get_model()
-        num_params = model.num_params()
-        thirdai_version = model.thirdai_version()
-
-        size = get_directory_size(model_path)
-
-        # TODO: Get actual size in memory when db is loaded
-        # This is a temporary approximation of how much RAM a model will take.
-        # Approximation comes from 4x explosion of weights in ADAM optimizer.
-        udt_pickle = model_path / "model.pkl"
-        documents_pickle = model_path / "documents.pkl"
-        logger_pickle = model_path / "logger.pkl"
-        size_in_memory = (
-            os.path.getsize(udt_pickle) * 4
-            + os.path.getsize(documents_pickle)
-            + os.path.getsize(logger_pickle)
-        )
-
-        json_data = {
-            "trained_on": trained_on,
-            "num_params": num_params,
-            "is_indexed": is_indexed,
-            "size": size,
-            "size_in_memory": size_in_memory,
-            "hash": model_hash,
-            "access_level": access_level,
-            "description": description,
-            "thirdai_version": thirdai_version,
-        }
-
-        response = http_post_with_error(
-            urljoin(
-                self._login_instance.base_url,
-                "bazaar/upload-commit",
-            ),
-            params={"total_chunks": chunk_number},
-            json=json_data,
-            headers=auth_header(upload_token),
-        )
-
-        os.remove(zip_path)
-
-    @login_required
-    def delete(
-        self,
-        model_identifier: str,
-    ):
-        delete_response = http_post_with_error(
-            urljoin(
-                self._login_instance.base_url,
-                f"bazaar/{self._login_instance.user_id}/request-delete",
-            ),
-            headers=auth_header(self._login_instance.access_token),
-            json={
-                "model_identifier": model_identifier,
-            },
-        )
-
-        print("Successfully requested admin to delete the model.")
+import concurrent.futures
+import json
+import os
+import pickle
+import shutil
+import threading
+from dataclasses import dataclass
+from pathlib import Path
+from typing import Callable, List, Optional, Union
+from urllib.parse import urljoin
+
+import requests
+from pydantic import BaseModel, ValidationError
+from requests.auth import HTTPBasicAuth
+from thirdai.neural_db.neural_db import CancelState, NeuralDB
+from tqdm import tqdm
+
+from .utils import (
+    create_model_identifier,
+    get_directory_size,
+    get_file_size,
+    hash_path,
+    http_get_with_error,
+    http_post_with_error,
+    zip_folder,
+)
+
+
+class BazaarEntry(BaseModel):
+    name: str
+    author_username: str
+    identifier: str
+    trained_on: Optional[str] = None
+    num_params: int
+    size: int
+    size_in_memory: int
+    hash: str
+    domain: str
+    description: Optional[str] = None
+    is_indexed: bool = False
+    publish_date: str
+    author_email: str
+    access_level: str = "public"
+    thirdai_version: str
+
+    @staticmethod
+    def from_dict(entry):
+        return BazaarEntry(
+            name=entry["model_name"],
+            author_username=entry["username"],
+            identifier=create_model_identifier(
+                model_name=entry["model_name"], author_username=entry["username"]
+            ),
+            trained_on=entry["trained_on"],
+            num_params=entry["num_params"],
+            size=entry["size"],
+            size_in_memory=entry["size_in_memory"],
+            hash=entry["hash"],
+            domain=entry["domain"],
+            description=entry["description"],
+            is_indexed=entry["is_indexed"],
+            publish_date=entry["publish_date"],
+            author_email=entry["user_email"],
+            access_level=entry["access_level"],
+            thirdai_version=entry["thirdai_version"],
+        )
+
+    @staticmethod
+    def bazaar_entry_from_json(json_entry):
+        try:
+            loaded_entry = BazaarEntry.from_dict(json_entry)
+            return loaded_entry
+        except ValidationError as e:
+            print(f"Validation error: {e}")
+            return None
+
+
+@dataclass
+class Login:
+    base_url: str
+    username: str
+    user_id: str
+    access_token: str
+
+    @staticmethod
+    def with_email(
+        base_url: str,
+        email: str,
+        password: str,
+    ):
+        # We are using HTTPBasic Auth in backend. update this when we change the Authentication in Backend.
+        response = http_get_with_error(
+            urljoin(base_url, "user/email-login"),
+            auth=HTTPBasicAuth(email, password),
+        )
+
+        content = json.loads(response.content)
+        username = content["data"]["user"]["username"]
+        user_id = content["data"]["user"]["user_id"]
+        access_token = content["data"]["access_token"]
+        return Login(base_url, username, user_id, access_token)
+
+
+def auth_header(access_token):
+    return {
+        "Authorization": f"Bearer {access_token}",
+    }
+
+
+def relative_path_depth(child_path: Path, parent_path: Path):
+    child_path, parent_path = child_path.resolve(), parent_path.resolve()
+    relpath = os.path.relpath(child_path, parent_path)
+    if relpath == ".":
+        return 0
+    else:
+        return 1 + relpath.count(os.sep)
+
+
+# Use this decorator for any function to enforce users use only after login.
+def login_required(func):
+    def wrapper(self, *args, **kwargs):
+        if not self.is_logged_in():
+            raise PermissionError(
+                "This method requires login, please use '.login()' first then try again."
+            )
+        return func(self, *args, **kwargs)
+
+    return wrapper
+
+
+class Bazaar:
+    def __init__(
+        self,
+        base_url,
+        cache_dir: Union[Path, str],
+    ):
+        cache_dir = Path(cache_dir)
+        if not os.path.exists(cache_dir):
+            os.makedirs(cache_dir)
+        self._cache_dir = cache_dir
+        if not base_url.endswith("/api/"):
+            raise ValueError("base_url must end with '/api/'.")
+        self._base_url = base_url
+        self._login_instance = None
+
+    def signup(self, email, password, username):
+        json_data = {
+            "username": username,
+            "email": email,
+            "password": password,
+        }
+
+        response = http_post_with_error(
+            urljoin(self._base_url, "user/email-signup-basic"),
+            json=json_data,
+        )
+
+        print(
+            f"Successfully signed up. Please check your email ({email}) to verify your account."
+        )
+
+    def login(self, email, password):
+        self._login_instance = Login.with_email(self._base_url, email, password)
+
+    def is_logged_in(self):
+        return self._login_instance is not None
+
+    def fetch(
+        self,
+        name: str = "",
+        domain: Optional[str] = None,
+        username: Optional[str] = None,
+        access_level: Optional[List[str]] = None,
+    ):
+        if self.is_logged_in():
+            url = urljoin(
+                self._login_instance.base_url,
+                f"bazaar/{self._login_instance.user_id}/list",
+            )
+            response = http_get_with_error(
+                url,
+                params={
+                    "name": name,
+                    "domain": domain,
+                    "username": username,
+                    "access_level": access_level,
+                },
+                headers=auth_header(self._login_instance.access_token),
+            )
+        else:
+            print("Fetching public models, login to fetch all accessible models.")
+            url = urljoin(
+                self._base_url,
+                "bazaar/public-list",
+            )
+            response = http_get_with_error(
+                url,
+                params={
+                    "name": name,
+                    "domain": domain,
+                    "username": username,
+                },
+            )
+        json_entries = json.loads(response.content)["data"]
+
+        bazaar_entries = [
+            BazaarEntry.bazaar_entry_from_json(json_entry)
+            for json_entry in json_entries
+            if json_entry
+        ]
+        return bazaar_entries
+
+    def fetch_from_cache(
+        self,
+        name: str = "",
+        domain: Optional[str] = None,
+        username: Optional[str] = None,
+        access_level: Optional[List[str]] = None,
+        only_check_dir_exists: bool = False,
+    ):
+        bazaar_entries = []
+        # Walk through the directories
+        for dirpath, dirnames, filenames in os.walk(self._cache_dir):
+            depth = relative_path_depth(
+                child_path=Path(dirpath), parent_path=Path(self._cache_dir)
+            )
+
+            if depth == 2:
+                # We're two levels in, which is the level of all checkpoint dirs
+                split_path = dirpath.split(os.path.sep)
+                model_name = split_path[-1]
+                author_username = split_path[-2]
+
+                identifier = f"{author_username}/{model_name}"
+                with open(self._cached_model_metadata_path(identifier), "r") as f:
+                    bazaar_entry = BazaarEntry.from_dict(json.load(f))
+
+                if (
+                    name.lower() in model_name.lower()
+                    and (not username or username == author_username)
+                    and (not domain or domain == bazaar_entry.domain)
+                    and (not access_level or bazaar_entry.access_level in access_level)
+                ):
+                    try:
+                        if self._model_dir_in_cache(
+                            identifier=identifier,
+                            fetched_bazaar_entry=bazaar_entry,
+                            only_check_dir_exists=only_check_dir_exists,
+                        ):
+                            bazaar_entries.append(bazaar_entry)
+                    except:
+                        pass
+
+                dirnames.clear()  # Don't descend any further
+
+            elif depth > 2:
+                # We're too deep, don't process this directory
+                dirnames.clear()
+
+        return bazaar_entries
+
+    def list_model_names(self):
+        return [entry.identifier for entry in self.fetch()]
+
+    def get_neuraldb(
+        self,
+        model_identifier: str,
+        on_progress: Callable = lambda *args, **kwargs: None,
+        cancel_state: CancelState = CancelState(),
+        disable_progress_bar: bool = False,
+    ):
+        model_dir = self.get_model_dir(
+            model_identifier, on_progress, cancel_state, disable_progress_bar
+        )
+        return NeuralDB.from_checkpoint(checkpoint_path=model_dir)
+
+    def get_model_dir(
+        self,
+        model_identifier,
+        on_progress: Callable = lambda *args, **kwargs: None,
+        cancel_state: CancelState = CancelState(),
+        disable_progress_bar: bool = False,
+    ):
+        if self.is_logged_in():
+            url = urljoin(
+                self._login_instance.base_url,
+                f"bazaar/{self._login_instance.user_id}/model",
+            )
+            response = http_get_with_error(
+                url,
+                params={"model_identifier": model_identifier},
+                headers=auth_header(self._login_instance.access_token),
+            )
+        else:
+            url = urljoin(
+                self._base_url,
+                "bazaar/model",
+            )
+            response = http_get_with_error(
+                url,
+                params={"model_identifier": model_identifier},
+            )
+
+        json_entry = json.loads(response.content)["data"]
+        bazaar_entry = BazaarEntry.bazaar_entry_from_json(json_entry)
+
+        cached_model_dir = self._model_dir_in_cache(
+            identifier=model_identifier, fetched_bazaar_entry=bazaar_entry
+        )
+        if cached_model_dir:
+            return cached_model_dir
+
+        self._download(
+            model_identifier,
+            on_progress=on_progress,
+            cancel_state=cancel_state,
+            disable_progress_bar=disable_progress_bar,
+        )
+
+        if not cancel_state.is_canceled():
+            return self._unpack_and_remove_zip(model_identifier)
+        else:
+            try:
+                shutil.rmtree(self._cached_checkpoint_dir(model_identifier))
+            except:
+                pass
+            return None
+
+    # The checkpoint dir is cache_dir/model_identifier/
+    # This is the parent directory for the three paths defined in the following methods
+    def _cached_checkpoint_dir(self, identifier: str):
+        return self._cache_dir / identifier
+
+    # The ndb path is cache_dir/model_identifier/model.ndb
+    def _cached_model_dir_path(self, identifier: str):
+        return self._cached_checkpoint_dir(identifier) / "model.ndb"
+
+    # The ndb zip download path is cache_dir/model_identifier/model.ndb.zip
+    def _cached_model_zip_path(self, identifier: str):
+        return self._cached_checkpoint_dir(identifier) / "model.ndb.zip"
+
+    # The BazaarEntry json metadata path is cache_dir/author_username/model_name/metadata.json
+    def _cached_model_metadata_path(self, identifier: str):
+        return self._cached_checkpoint_dir(identifier) / "metadata.json"
+
+    def _model_dir_in_cache(
+        self,
+        identifier: str,
+        fetched_bazaar_entry: str,
+        only_check_dir_exists: bool = False,
+    ):
+        cached_model_dir = self._cached_model_dir_path(identifier)
+        if cached_model_dir.is_dir():
+            if not only_check_dir_exists:
+                hash_match = hash_path(cached_model_dir) == fetched_bazaar_entry.hash
+                size_match = (
+                    get_directory_size(cached_model_dir) == fetched_bazaar_entry.size
+                )
+                if hash_match and size_match:
+                    return cached_model_dir
+            else:
+                return cached_model_dir
+        return None
+
+    def _unpack_and_remove_zip(self, identifier: str):
+        zip_path = self._cached_model_zip_path(identifier)
+        extract_dir = self._cached_model_dir_path(identifier)
+        shutil.unpack_archive(filename=zip_path, extract_dir=extract_dir)
+        os.remove(zip_path)
+        return extract_dir
+
+    def _download(
+        self,
+        model_identifier: str,
+        on_progress: Callable,
+        cancel_state: CancelState,
+        disable_progress_bar: bool = False,
+    ):
+        if self.is_logged_in():
+            url = urljoin(
+                self._login_instance.base_url,
+                f"bazaar/{self._login_instance.user_id}/download",
+            )
+            response = requests.get(
+                url,
+                params={"model_identifier": model_identifier},
+                headers=auth_header(self._login_instance.access_token),
+                stream=True,
+            )
+        else:
+            url = urljoin(
+                self._base_url,
+                f"bazaar/public-download",
+            )
+            response = requests.get(
+                url, params={"model_identifier": model_identifier}, stream=True
+            )
+        try:
+            shutil.rmtree(self._cached_checkpoint_dir(model_identifier))
+        except:
+            pass
+        os.makedirs(self._cached_checkpoint_dir(model_identifier))
+
+        destination = self._cached_model_zip_path(model_identifier)
+
+        # Try to get the total size from the Content-Length header
+        total_size = int(response.headers.get("Content-Length", 0))
+
+        # Set up a progress bar
+        with tqdm(
+            total=total_size, unit="B", unit_scale=True, desc="Downloading"
+        ) as bar:
+            # Destination file path
+            with open(destination, "wb") as f:
+                for chunk in response.iter_content(8192):  # 8192 bytes or 8KB
+                    f.write(chunk)
+                    bar.update(len(chunk))
+
+    def upload_chunk(self, upload_token, chunk_number, chunk_data, bar, progress_lock):
+        files = {"chunk": chunk_data}
+        response = requests.post(
+            urljoin(
+                self._login_instance.base_url,
+                "bazaar/upload-chunk",
+            ),
+            files=files,
+            params={"chunk_number": chunk_number},
+            headers=auth_header(upload_token),
+        )
+
+        if response.status_code == 200:
+            with progress_lock:
+                # Update the progress bar
+                bar.update(len(chunk_data))
+        else:
+            print(f"Upload failed with status code: {response.status_code}")
+            print(response.text)
+            return False
+
+        return True
+
+    @login_required
+    def push(
+        self,
+        name: str,
+        model_path: Union[Path, str],
+        trained_on: str = "Own Documents",
+        is_indexed: bool = False,
+        access_level: str = "public",
+        description: str = "",
+    ):
+        model_path = Path(model_path)
+        zip_path = zip_folder(model_path)
+
+        model_hash = hash_path(model_path)
+
+        # Generate upload token
+        token_response = http_get_with_error(
+            urljoin(
+                self._login_instance.base_url,
+                f"bazaar/{self._login_instance.user_id}/upload-token",
+            ),
+            headers=auth_header(self._login_instance.access_token),
+            params={
+                "model_name": name,
+                "size": int(get_file_size(zip_path, "MB")),
+            },
+        )
+        upload_token = json.loads(token_response.content)["data"]["token"]
+
+        # Get the total file size for progress bar
+        total_size = os.path.getsize(zip_path)
+
+        # Determine the chunk size you want to upload per request
+        chunk_size = 1024 * 1024  # 1 MB chunk
+
+        # Initialize the progress bar
+        with tqdm(total=total_size, unit="B", unit_scale=True, desc=zip_path) as bar:
+            # Open the file in binary mode
+            with open(zip_path, "rb") as file:
+                chunk_number = 0
+
+                with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:
+                    futures = []
+                    progress_lock = threading.Lock()
+
+                    while True:
+                        # Read a chunk of the file
+                        chunk_data = file.read(chunk_size)
+                        if not chunk_data:
+                            break  # End of file
+
+                        # Increment the chunk number
+                        chunk_number += 1
+
+                        # Submit the task to the thread pool
+                        future = executor.submit(
+                            self.upload_chunk,
+                            upload_token,
+                            chunk_number,
+                            chunk_data,
+                            bar,
+                            progress_lock,
+                        )
+                        futures.append(future)
+
+                    # Collect the return status of all threads
+                    threads_status = [
+                        future.result()
+                        for future in concurrent.futures.as_completed(futures)
+                    ]
+
+                # Check if all uploads were successful
+                if all(threads_status):
+                    print("File upload completed successfully.")
+                else:
+                    print("File upload failed.")
+
+        db = NeuralDB.from_checkpoint(checkpoint_path=model_path)
+        model = db._savable_state.model.model._get_model()
+        num_params = model.num_params()
+        thirdai_version = model.thirdai_version()
+
+        size = get_directory_size(model_path)
+
+        # TODO: Get actual size in memory when db is loaded
+        # This is a temporary approximation of how much RAM a model will take.
+        # Approximation comes from 4x explosion of weights in ADAM optimizer.
+        udt_pickle = model_path / "model.pkl"
+        documents_pickle = model_path / "documents.pkl"
+        logger_pickle = model_path / "logger.pkl"
+        size_in_memory = (
+            os.path.getsize(udt_pickle) * 4
+            + os.path.getsize(documents_pickle)
+            + os.path.getsize(logger_pickle)
+        )
+
+        json_data = {
+            "trained_on": trained_on,
+            "num_params": num_params,
+            "is_indexed": is_indexed,
+            "size": size,
+            "size_in_memory": size_in_memory,
+            "hash": model_hash,
+            "access_level": access_level,
+            "description": description,
+            "thirdai_version": thirdai_version,
+        }
+
+        response = http_post_with_error(
+            urljoin(
+                self._login_instance.base_url,
+                "bazaar/upload-commit",
+            ),
+            params={"total_chunks": chunk_number},
+            json=json_data,
+            headers=auth_header(upload_token),
+        )
+
+        os.remove(zip_path)
+
+    @login_required
+    def delete(
+        self,
+        model_identifier: str,
+    ):
+        delete_response = http_post_with_error(
+            urljoin(
+                self._login_instance.base_url,
+                f"bazaar/{self._login_instance.user_id}/request-delete",
+            ),
+            headers=auth_header(self._login_instance.access_token),
+            json={
+                "model_identifier": model_identifier,
+            },
+        )
+
+        print("Successfully requested admin to delete the model.")
```

## thirdai/neural_db/model_bazaar/bazaar_client.py

 * *Ordering differences only*

```diff
@@ -1,978 +1,978 @@
-from __future__ import annotations
-
-import json
-import os
-import re
-import time
-from pathlib import Path
-from typing import Any, Dict, List, Optional, Tuple, Union
-from urllib.parse import urljoin
-
-from .bazaar_base import Bazaar, auth_header
-from .utils import (
-    check_deployment_decorator,
-    construct_deployment_url,
-    create_deployment_identifier,
-    create_model_identifier,
-    http_get_with_error,
-    http_post_with_error,
-    print_progress_dots,
-)
-
-
-class Model:
-    """
-    A class representing a model listed on NeuralDB Enterprise.
-
-    Attributes:
-        _model_identifier (str): The unique identifier for the model.
-
-    Methods:
-        __init__(self, model_identifier: str) -> None:
-            Initializes a new instance of the Model class.
-
-            Parameters:
-                model_identifier (str): An optional model identifier.
-
-        model_identifier(self) -> str:
-            Getter method for accessing the model identifier.
-
-            Returns:
-                str: The model identifier, or None if not set.
-    """
-
-    def __init__(self, model_identifier, model_id=None) -> None:
-        self._model_identifier = model_identifier
-        self._model_id = model_id
-
-    @property
-    def model_identifier(self):
-        return self._model_identifier
-
-    @property
-    def model_id(self):
-        if self._model_id:
-            return self._model_id
-        raise ValueError("Model id is not yet set.")
-
-
-class NeuralDBClient:
-    """
-    A client for interacting with the deployed NeuralDB model.
-
-    Attributes:
-        deployment_identifier (str): The identifier for the deployment.
-        deployment_id (str): The deployment ID for the deployed NeuralDB model.
-        bazaar (thirdai.neural_db.ModelBazaar): The bazaar object corresponding to a NeuralDB Enterprise installation
-
-    Methods:
-        __init__(self, deployment_identifier: str, deployment_id: str, bazaar: ModelBazaar) -> None:
-            Initializes a new instance of the NeuralDBClient.
-
-        search(self, query, top_k=5, constraints: Optional[dict[str, dict[str, str]]]=None) -> List[dict]:
-            Searches the ndb model for relevant search results.
-
-        insert(self, documents: list[dict[str, Any]]) -> None:
-            Inserts documents into the ndb model.
-
-        delete(self, source_ids: List[str]) -> None:
-            Deletes documents from the ndb model
-
-        associate(self, text_pairs (List[Dict[str, str]])) -> None:
-            Associates source and target string pairs in the ndb model.
-
-        upvote(self, text_id_pairs: List[Dict[str, Union[str, int]]]) -> None:
-            Upvotes a response in the ndb model.
-
-        downvote(self, text_id_pairs: List[Dict[str, Union[str, int]]]) -> None:
-            Downvotes a response in the ndb model.
-
-        chat(self, user_input: str, session_id: str) -> Dict[str, str]:
-            Returns a reply given the user_input and the chat history associated with session_id
-
-        get_chat_history(self, session_id: str) -> Dict[List[Dict[str, str]]]:
-            Returns chat history associated with session_id
-
-        sources(self) -> List[Dict[str, str]]:
-            Gets the source names and ids of documents in the ndb model
-    """
-
-    def __init__(
-        self, deployment_identifier: str, deployment_id: str, bazaar: ModelBazaar
-    ):
-        """
-        Initializes a new instance of the NeuralDBClient.
-
-        Args:
-            deployment_identifier (str): The identifier for the deployment.
-            deployment_id (str): The deployment ID for the deployed NeuralDB model.
-            bazaar (thirdai.neural_db.ModelBazaar): The bazaar object corresponding to a NeuralDB Enterprise installation
-        """
-        self.deployment_identifier = deployment_identifier
-        self.base_url = construct_deployment_url(
-            re.sub(r"api/$", "", bazaar._base_url), deployment_id
-        )
-        self.bazaar = bazaar
-
-    @check_deployment_decorator
-    def search(
-        self, query, top_k=5, constraints: Optional[dict[str, dict[str, str]]] = None
-    ):
-        """
-        Searches the ndb model for similar queries.
-
-        Args:
-            query (str): The query to search for.
-            top_k (int): The number of top results to retrieve (default is 10).
-            constraints (Optional[dict[str, dict[str, str]]]): Constraints to filter the search result metadata by.
-                These constraints must be in the following format:
-                {"FIELD_NAME": {"constraint_type": "CONSTRAINT_NAME", **kwargs}} where
-                "FIELD_NAME" is the field that you want to filter over, and "CONSTRAINT_NAME"
-                is one of the following: "AnyOf", "EqualTo", "InRange", "GreaterThan", and "LessThan".
-                The kwargs for the above constraints are shown below:
-
-                class AnyOf(BaseModel):
-                    constraint_type: Literal["AnyOf"]
-                    values: Iterable[Any]
-
-                class EqualTo(BaseModel):
-                    constraint_type: Literal["EqualTo"]
-                    value: Any
-
-                class InRange(BaseModel):
-                    constraint_type: Literal["InRange"]
-                    minimum: Any
-                    maximum: Any
-                    inclusive_min: bool = True
-                    inclusive_max: bool = True
-
-                class GreaterThan(BaseModel):
-                    constraint_type: Literal["GreaterThan"]
-                    minimum: Any
-                    include_equal: bool = False
-
-                class LessThan(BaseModel):
-                    constraint_type: Literal["LessThan"]
-                    maximum: Any
-                    include_equal: bool = False
-
-        Returns:
-            Dict: A dict of search results containing keys: `query_text` and `references`.
-        """
-
-        response = http_post_with_error(
-            urljoin(self.base_url, "predict"),
-            params={"query_text": query, "top_k": top_k},
-            json=constraints,
-            headers=auth_header(self.bazaar._access_token),
-        )
-
-        return json.loads(response.content)["data"]
-
-    @check_deployment_decorator
-    def insert(self, documents: list[dict[str, Any]]):
-        """
-        Inserts documents into the ndb model.
-
-        Args:
-            documents (List[dict[str, Any]]): A list of dictionaries that represent documents to be inserted to the ndb model.
-                The document dictionaries must be in the following format:
-                {"document_type": "DOCUMENT_TYPE", **kwargs} where "DOCUMENT_TYPE" is one of the following:
-                "PDF", "CSV", "DOCX", "URL", "SentenceLevelPDF", "SentenceLevelDOCX", "Unstructured", "InMemoryText".
-                The kwargs for each document type are shown below:
-
-                class PDF(Document):
-                    document_type: Literal["PDF"]
-                    path: str
-                    metadata: Optional[dict[str, Any]] = None
-                    on_disk: bool = False
-                    version: str = "v1"
-                    chunk_size: int = 100
-                    stride: int = 40
-                    emphasize_first_words: int = 0
-                    ignore_header_footer: bool = True
-                    ignore_nonstandard_orientation: bool = True
-
-                class CSV(Document):
-                    document_type: Literal["CSV"]
-                    path: str
-                    id_column: Optional[str] = None
-                    strong_columns: Optional[List[str]] = None
-                    weak_columns: Optional[List[str]] = None
-                    reference_columns: Optional[List[str]] = None
-                    save_extra_info: bool = True
-                    metadata: Optional[dict[str, Any]] = None
-                    has_offset: bool = False
-                    on_disk: bool = False
-
-                class DOCX(Document):
-                    document_type: Literal["DOCX"]
-                    path: str
-                    metadata: Optional[dict[str, Any]] = None
-                    on_disk: bool = False
-
-                class URL(Document):
-                    document_type: Literal["URL"]
-                    url: str
-                    save_extra_info: bool = True
-                    title_is_strong: bool = False
-                    metadata: Optional[dict[str, Any]] = None
-                    on_disk: bool = False
-
-                class SentenceLevelPDF(Document):
-                    document_type: Literal["SentenceLevelPDF"]
-                    path: str
-                    metadata: Optional[dict[str, Any]] = None
-                    on_disk: bool = False
-
-                class SentenceLevelDOCX(Document):
-                    document_type: Literal["SentenceLevelDOCX"]
-                    path: str
-                    metadata: Optional[dict[str, Any]] = None
-                    on_disk: bool = False
-
-                class Unstructured(Document):
-                    document_type: Literal["Unstructured"]
-                    path: str
-                    save_extra_info: bool = True
-                    metadata: Optional[dict[str, Any]] = None
-                    on_disk: bool = False
-
-                class InMemoryText(Document):
-                    document_type: Literal["InMemoryText"]
-                    name: str
-                    texts: list[str]
-                    metadatas: Optional[list[dict[str, Any]]] = None
-                    global_metadata: Optional[dict[str, Any]] = None
-                    on_disk: bool = False
-
-                For Document types with the arg "path", ensure that the path exists on your local machine.
-        """
-
-        if not documents:
-            raise ValueError("Documents cannot be empty.")
-
-        files = []
-        for doc in documents:
-            if "path" in doc and ("location" not in doc or doc["location"] == "local"):
-                if not os.path.exists(doc["path"]):
-                    raise ValueError(
-                        f"Path {doc['path']} was provided but doesn't exist on the machine."
-                    )
-                files.append(("files", open(doc["path"], "rb")))
-
-        files.append(("documents", (None, json.dumps(documents), "application/json")))
-
-        response = http_post_with_error(
-            urljoin(self.base_url, "insert"),
-            files=files,
-            headers=auth_header(self.bazaar._access_token),
-        )
-
-        return json.loads(response.content)["data"]
-
-    @check_deployment_decorator
-    def delete(self, source_ids: List[str]):
-        """
-        Deletes documents from the ndb model using source ids.
-
-        Args:
-            files (List[str]): A list of source ids to delete from the ndb model.
-        """
-        response = http_post_with_error(
-            urljoin(self.base_url, "delete"),
-            json={"source_ids": source_ids},
-            headers=auth_header(self.bazaar._access_token),
-        )
-
-    @check_deployment_decorator
-    def associate(self, text_pairs: List[Dict[str, str]]):
-        """
-        Associates source and target string pairs in the ndb model.
-
-        Args:
-            text_pairs (List[Dict[str, str]]): List of dictionaries where each dictionary has 'source' and 'target' keys.
-        """
-        response = http_post_with_error(
-            urljoin(self.base_url, "associate"),
-            json={"text_pairs": text_pairs},
-            headers=auth_header(self.bazaar._access_token),
-        )
-
-    @check_deployment_decorator
-    def save_model(self, override: bool = True, model_name: Optional[str] = None):
-
-        response = http_post_with_error(
-            urljoin(self.base_url, "save"),
-            json={"override": override, "model_name": model_name},
-            headers=auth_header(self.bazaar._access_token),
-        )
-
-        print("Successfully saved the model.")
-
-        content = response.json()["data"]
-
-        if content["new_model_id"]:
-            return Model(
-                model_identifier=create_model_identifier(
-                    model_name, self.bazaar._username
-                ),
-                model_id=content["new_model_id"],
-            )
-
-        return None
-
-    @check_deployment_decorator
-    def upvote(self, text_id_pairs: List[Dict[str, Union[str, int]]]):
-        """
-        Upvote response with 'reference_id' corresponding to 'query_text' in the ndb model.
-
-        Args:
-            text_id_pairs: (List[Dict[str, Union[str, int]]]): List of dictionaries where each dictionary has 'query_text' and 'reference_id' keys.
-        """
-        response = http_post_with_error(
-            urljoin(self.base_url, "upvote"),
-            json={"text_id_pairs": text_id_pairs},
-            headers=auth_header(self.bazaar._access_token),
-        )
-
-        print("Successfully upvoted the specified search result.")
-
-    @check_deployment_decorator
-    def downvote(self, text_id_pairs: List[Dict[str, Union[str, int]]]):
-        """
-        Downvote response with 'reference_id' corresponding to 'query_text' in the ndb model.
-
-        Args:
-            text_id_pairs: (List[Dict[str, Union[str, int]]]): List of dictionaries where each dictionary has 'query_text' and 'reference_id' keys.
-        """
-        response = http_post_with_error(
-            urljoin(self.base_url, "downvote"),
-            json={"text_id_pairs": text_id_pairs},
-            headers=auth_header(self.bazaar._access_token),
-        )
-
-        print("Successfully downvoted the specified search result.")
-
-    @check_deployment_decorator
-    def chat(self, user_input: str, session_id: str) -> Dict[str, str]:
-        """
-        Returns a reply given the user_input and the chat history associated with session_id
-
-        Args:
-            user_input (str): The user input for the chatbot to respond to
-            session_id (str): The session id corresponding to a specific chat session
-        """
-        response = http_post_with_error(
-            urljoin(self.base_url, "chat"),
-            json={"user_input": user_input, "session_id": session_id},
-            headers=auth_header(self.bazaar._access_token),
-        )
-
-        return response.json()["data"]
-
-    @check_deployment_decorator
-    def get_chat_history(self, session_id: str) -> Dict[List[Dict[str, str]]]:
-        """
-        Returns chat history associated with session_id
-
-        Args:
-            session_id (str): The session id corresponding to a specific chat session
-        """
-        response = http_post_with_error(
-            urljoin(self.base_url, "get-chat-hisory"),
-            json={"session_id": session_id},
-            headers=auth_header(self.bazaar._access_token),
-        )
-
-        return response.json()["data"]
-
-    @check_deployment_decorator
-    def sources(self) -> List[Dict[str, str]]:
-        """
-        Gets the source names and ids of documents in the ndb model
-
-        """
-        response = http_get_with_error(
-            urljoin(self.base_url, "sources"),
-            headers=auth_header(self.bazaar._access_token),
-        )
-
-        return response.json()["data"]
-
-
-class ModelBazaar(Bazaar):
-    """
-    A class representing ModelBazaar, providing functionality for managing models and deployments.
-
-    Attributes:
-        _base_url (str): The base URL for the Model Bazaar.
-        _cache_dir (Union[Path, str]): The directory for caching downloads.
-
-    Methods:
-        __init__(self, base_url: str, cache_dir: Union[Path, str] = "./bazaar_cache") -> None:
-            Initializes a new instance of the ModelBazaar class.
-
-        sign_up(self, email: str, password: str, username: str) -> None:
-            Signs up a user and sets the username for the ModelBazaar instance.
-
-        log_in(self, email: str, password: str) -> None:
-            Logs in a user and sets user-related attributes for the ModelBazaar instance.
-
-        push_model(self, model_name: str, local_path: str, access_level: str = "private") -> None:
-            Pushes a model to the Model Bazaar.
-
-        pull_model(self, model_identifier: str) -> NeuralDBClient:
-            Pulls a model from the Model Bazaar and returns a NeuralDBClient instance.
-
-        list_models(self) -> List[dict]:
-            Lists available models in the Model Bazaar.
-
-        train(self,
-            model_name: str,
-            unsupervised_docs: Optional[List[str]] = None,
-            supervised_docs: Optional[List[Tuple[str, str]]] = None,
-            test_doc: Optional[str] = None,
-            doc_type: str = "local",
-            sharded: bool = False,
-            is_async: bool = False,
-            base_model_identifier: str = None,
-            train_extra_options: Optional[dict] = None,
-            metadata: Optional[List[Dict[str, str]]] = None
-        ) -> Model:
-            Initiates training for a model and returns a Model instance.
-
-        await_train(self, model: Model) -> None:
-            Waits for the training of a model to complete.
-
-        test(self,
-            model_identifier: str,
-            test_doc: str,
-            doc_type: str = "local",
-            test_extra_options: dict = {},
-            is_async: bool = False,
-        ) -> str:
-            Starts the Model testing on given test file.
-
-        await_test(self, model_identifier: str, test_id: str) -> None:
-            Waits for the testing of a model on that test_id to complete.
-
-        deploy(self, model_identifier: str, deployment_name: str, is_async: bool = False) -> NeuralDBClient:
-            Deploys a model and returns a NeuralDBClient instance.
-
-        await_deploy(self, ndb_client: NeuralDBClient) -> None:
-            Waits for the deployment of a model to complete.
-
-        undeploy(self, ndb_client: NeuralDBClient) -> None:
-            Undeploys a deployed model.
-
-        list_deployments(self) -> List[dict]:
-            Lists the deployments in the Model Bazaar.
-
-        connect(self, deployment_identifier: str) -> NeuralDBClient:
-            Connects to a deployed model and returns a NeuralDBClient instance.
-    """
-
-    def __init__(
-        self,
-        base_url: str,
-        cache_dir: Union[Path, str] = "./bazaar_cache",
-    ):
-        """
-        Initializes a new instance of the ModelBazaar class.
-
-        Args:
-            base_url (str): The base URL for the Model Bazaar.
-            cache_dir (Union[Path, str]): The directory for caching downloads.
-        """
-        super().__init__(base_url, cache_dir)
-        self._username = None
-        self._user_id = None
-        self._access_token = None
-        self._doc_types = ["local", "nfs", "s3"]
-
-    def sign_up(self, email, password, username):
-        """
-        Signs up a user and sets the username for the ModelBazaar instance.
-
-        Args:
-            email (str): The email of the user.
-            password (str): The password of the user.
-            username (str): The desired username.
-        """
-        self.signup(email=email, password=password, username=username)
-        self._username = username
-
-    def log_in(self, email, password):
-        """
-        Logs in a user and sets user-related attributes for the ModelBazaar instance.
-
-        Args:
-            email (str): The email of the user.
-            password (str): The password of the user.
-        """
-        self.login(email=email, password=password)
-        self._user_id = self._login_instance.user_id
-        self._access_token = self._login_instance.access_token
-        self._username = self._login_instance.username
-
-    def push_model(
-        self, model_name: str, local_path: str, access_level: str = "private"
-    ):
-        """
-        Pushes a model to the Model Bazaar.
-
-        Args:
-            model_name (str): The name of the model.
-            local_path (str): The local path of the model.
-            access_level (str): The access level for the model (default is "private").
-        """
-        self.push(
-            name=model_name,
-            model_path=local_path,
-            trained_on="Own Documents",
-            access_level=access_level,
-            is_indexed=True,
-            description="",
-        )
-
-    def pull_model(self, model_identifier: str):
-        """
-        Pulls a model from the Model Bazaar and returns a NeuralDBClient instance.
-
-        Args:
-            model_identifier (str): The identifier of the model.
-
-        Returns:
-            NeuralDBClient: A NeuralDBClient instance.
-        """
-        return self.get_neuraldb(model_identifier=model_identifier)
-
-    def list_models(self):
-        """
-        Lists available models in the Model Bazaar.
-
-        Returns:
-            List[dict]: A list of dictionaries containing information about available models.
-        """
-        return self.fetch()
-
-    def train(
-        self,
-        model_name: str,
-        unsupervised_docs: Optional[List[str]] = None,
-        supervised_docs: Optional[List[Tuple[str, str]]] = None,
-        test_doc: Optional[str] = None,
-        doc_type: str = "local",
-        sharded: bool = False,
-        is_async: bool = False,
-        base_model_identifier: Optional[str] = None,
-        train_extra_options: Optional[dict] = None,
-        metadata: Optional[List[Dict[str, str]]] = None,
-    ):
-        """
-        Initiates training for a model and returns a Model instance.
-
-        Args:
-            model_name (str): The name of the model.
-            unsupervised_docs (Optional[List[str]]): A list of document paths for unsupervised training.
-            supervised_docs (Optional[List[Tuple[str, str]]]): A list of document path and source id pairs.
-            test_doc (Optional[str]): A path to a test file for evaluating the trained NeuralDB.
-            doc_type (str): Specifies document location type : "local"(default), "nfs" or "s3".
-            sharded (bool): Whether NeuralDB training will be distributed over NeuralDB shards.
-            is_async (bool): Whether training should be asynchronous (default is False).
-            train_extra_options: (Optional[dict])
-            base_model_identifier (Optional[str]): The identifier of the base model.
-            metadata (Optional[List[Dict[str, str]]]): A list metadata dicts. Each dict corresponds to an unsupervised file.
-
-        Returns:
-            Model: A Model instance.
-        """
-        if doc_type not in self._doc_types:
-            raise ValueError(
-                f"Invalid doc_type value. Supported doc_type are {self._doc_types}"
-            )
-
-        if not unsupervised_docs and not supervised_docs:
-            raise ValueError("Both the unsupervised and supervised docs are empty.")
-
-        if metadata and unsupervised_docs:
-            if len(metadata) != len(unsupervised_docs):
-                raise ValueError("Metadata is not provided for all unsupervised files.")
-
-        file_details_list = []
-        docs = []
-
-        if unsupervised_docs and metadata:
-            for doc, meta in zip(unsupervised_docs, metadata):
-                docs.append(doc)
-                file_details_list.append(
-                    {"mode": "unsupervised", "location": doc_type, "metadata": meta}
-                )
-        elif unsupervised_docs:
-            for doc in unsupervised_docs:
-                docs.append(doc)
-                file_details_list.append({"mode": "unsupervised", "location": doc_type})
-
-        if supervised_docs:
-            for sup_file, source_id in supervised_docs:
-                docs.append(sup_file)
-                file_details_list.append(
-                    {"mode": "supervised", "location": doc_type, "source_id": source_id}
-                )
-
-        if test_doc:
-            docs.append(test_doc)
-            file_details_list.append({"mode": "test", "location": doc_type})
-
-        url = urljoin(self._base_url, f"jobs/{self._user_id}/train")
-        files = [
-            (
-                ("files", open(file_path, "rb"))
-                if doc_type == "local"
-                else ("files", (file_path, "don't care"))
-            )
-            for file_path in docs
-        ]
-        if train_extra_options:
-            files.append(
-                (
-                    "extra_options_form",
-                    (None, json.dumps(train_extra_options), "application/json"),
-                )
-            )
-
-        files.append(
-            (
-                "file_details_list",
-                (
-                    None,
-                    json.dumps({"file_details": file_details_list}),
-                    "application/json",
-                ),
-            )
-        )
-
-        response = http_post_with_error(
-            url,
-            params={
-                "model_name": model_name,
-                "doc_type": doc_type,
-                "sharded": sharded,
-                "base_model_identifier": base_model_identifier,
-            },
-            files=files,
-            headers=auth_header(self._access_token),
-        )
-        print(response.content)
-        response_content = json.loads(response.content)
-        if response_content["status"] != "success":
-            raise Exception(response_content["message"])
-
-        model = Model(
-            model_identifier=create_model_identifier(
-                model_name=model_name, author_username=self._username
-            ),
-            model_id=response_content["data"]["model_id"],
-        )
-
-        if is_async:
-            return model
-
-        self.await_train(model)
-        return model
-
-    def test(
-        self,
-        model_identifier: str,
-        test_doc: str,
-        doc_type: str = "local",
-        test_extra_options: dict = {},
-        is_async: bool = False,
-    ):
-        """
-        Initiates testing for a model and returns the test_id (unique identifier for this test)
-
-        Args:
-            model_identifier (str): The identifier of the model.
-            test_doc (str): A path to a test file for evaluating the trained NeuralDB.
-            doc_type (str): Specifies document location type : "local"(default), "nfs" or "s3".
-            test_extra_options: (Optional[dict])
-            is_async (bool): Whether testing should be asynchronous (default is False).
-
-        Returns:
-            str: The test_id which is unique for given testing.
-        """
-        url = urljoin(self._base_url, f"test/test")
-
-        files = [
-            (
-                ("file", open(test_doc, "rb"))
-                if doc_type == "local"
-                else ("file", (test_doc, "don't care"))
-            )
-        ]
-        if test_extra_options:
-            files.append(
-                (
-                    "extra_options_form",
-                    (None, json.dumps(test_extra_options), "application/json"),
-                )
-            )
-
-        response = http_post_with_error(
-            url,
-            params={
-                "doc_type": doc_type,
-                "model_identifier": model_identifier,
-            },
-            files=files,
-            headers=auth_header(self._access_token),
-        )
-        print(response.content)
-
-        response_content = json.loads(response.content)
-        if response_content["status"] != "success":
-            raise Exception(response_content["message"])
-
-        if is_async:
-            return response_content["data"]["data_id"]
-
-        self.await_test(model_identifier, response_content["data"]["data_id"])
-        return response_content["data"]["data_id"]
-
-    def test_status(self, test_id: str):
-        """
-        Checks for the status of the model testing
-
-        Args:
-            test_id (str): The unique id with which we can recognize the test,
-            the user will get this id in the response when they trigger the test.
-        """
-
-        url = urljoin(self._base_url, f"test/test-status")
-
-        response = http_get_with_error(
-            url,
-            params={"test_id": test_id},
-            headers=auth_header(self._access_token),
-        )
-
-        response_data = json.loads(response.content)["data"]
-
-        return response_data
-
-    def await_test(self, model_identifier: str, test_id: str):
-        """
-        Waits for the testing of the model to complete.
-
-        Args:
-            model_identifier: The identifier of the model.
-            test_id: Unique id for the test.
-        """
-
-        while True:
-            response_data = self.test_status(test_id)
-
-            if response_data["status"] == "complete":
-                print("\nTesting completed")
-                return response_data["results"]
-
-            if response_data["status"] == "failed":
-                print("\nTesting Failed")
-                raise ValueError(f"Test Failed for {model_identifier} and {test_id}")
-
-            print("Testing: In progress", end="", flush=True)
-            print_progress_dots(duration=10)
-
-    def train_status(self, model: Model):
-        """
-        Checks for the status of the model training
-
-        Args:
-            model (Model): The Model instance.
-        """
-
-        url = urljoin(self._base_url, f"jobs/{self._user_id}/train-status")
-
-        response = http_get_with_error(
-            url,
-            params={"model_identifier": model.model_identifier},
-            headers=auth_header(self._access_token),
-        )
-
-        response_data = json.loads(response.content)["data"]
-
-        return response_data
-
-    def await_train(self, model: Model):
-        """
-        Waits for the training of a model to complete.
-
-        Args:
-            model (Model): The Model instance.
-        """
-        while True:
-            response_data = self.train_status(model)
-
-            if response_data["status"] == "complete":
-                print("\nTraining completed")
-                return
-
-            if response_data["status"] == "failed":
-                print("\nTraining Failed")
-                raise ValueError(f"Training Failed for {model.model_identifier}")
-
-            print("Training: In progress", end="", flush=True)
-            print_progress_dots(duration=10)
-
-    def deploy(
-        self,
-        model_identifier: str,
-        deployment_name: str,
-        memory: Optional[int] = None,
-        is_async=False,
-    ):
-        """
-        Deploys a model and returns a NeuralDBClient instance.
-
-        Args:
-            model_identifier (str): The identifier of the model.
-            deployment_name (str): The name for the deployment.
-            is_async (bool): Whether deployment should be asynchronous (default is False).
-
-        Returns:
-            NeuralDBClient: A NeuralDBClient instance.
-        """
-        url = urljoin(self._base_url, f"jobs/{self._user_id}/deploy")
-        params = {
-            "user_id": self._user_id,
-            "model_identifier": model_identifier,
-            "deployment_name": deployment_name,
-            "memory": memory,
-        }
-        response = http_post_with_error(
-            url, params=params, headers=auth_header(self._access_token)
-        )
-        response_data = json.loads(response.content)["data"]
-
-        ndb_client = NeuralDBClient(
-            deployment_identifier=create_deployment_identifier(
-                model_identifier=model_identifier,
-                deployment_name=deployment_name,
-                deployment_username=self._username,
-            ),
-            deployment_id=response_data["deployment_id"],
-            bazaar=self,
-        )
-        if is_async:
-            return ndb_client
-
-        time.sleep(5)
-        self.await_deploy(ndb_client)
-        return ndb_client
-
-    def await_deploy(self, ndb_client: NeuralDBClient):
-        """
-        Waits for the deployment of a model to complete.
-
-        Args:
-            ndb_client (NeuralDBClient): The NeuralDBClient instance.
-        """
-        url = urljoin(self._base_url, f"jobs/{self._user_id}/deploy-status")
-
-        params = {"deployment_identifier": ndb_client.deployment_identifier}
-        while True:
-            response = http_get_with_error(
-                url, params=params, headers=auth_header(self._access_token)
-            )
-            response_data = json.loads(response.content)["data"]
-
-            if response_data["status"] == "complete":
-                print("\nDeployment completed")
-                return
-
-            print("Deployment: In progress", end="", flush=True)
-            print_progress_dots(duration=5)
-
-    def undeploy(self, ndb_client: NeuralDBClient):
-        """
-        Undeploys a deployed model.
-
-        Args:
-            ndb_client (NeuralDBClient): The NeuralDBClient instance.
-        """
-        url = urljoin(self._base_url, f"jobs/{self._user_id}/undeploy")
-        params = {
-            "deployment_identifier": ndb_client.deployment_identifier,
-        }
-        response = http_post_with_error(
-            url, params=params, headers=auth_header(self._access_token)
-        )
-
-        print("Deployment is shutting down.")
-
-    def list_deployments(self):
-        """
-        Lists the deployments in the Model Bazaar.
-
-        Returns:
-            List[dict]: A list of dictionaries containing information about deployments.
-        """
-        url = urljoin(self._base_url, f"jobs/{self._user_id}/list-deployments")
-        response = http_get_with_error(
-            url,
-            params={
-                "user_id": self._user_id,
-            },
-            headers=auth_header(self._access_token),
-        )
-
-        response_data = json.loads(response.content)["data"]
-        deployments = []
-        for deployment in response_data:
-            model_identifier = create_model_identifier(
-                model_name=deployment["model_name"],
-                author_username=deployment["model_username"],
-            )
-            deployment_info = {
-                "deployment_identifier": create_deployment_identifier(
-                    model_identifier=model_identifier,
-                    deployment_name=deployment["name"],
-                    deployment_username=deployment["deployment_username"],
-                ),
-                "status": deployment["status"],
-            }
-            deployments.append(deployment_info)
-
-        return deployments
-
-    def connect(self, deployment_identifier: str):
-        """
-        Connects to a deployed model and returns a NeuralDBClient instance.
-
-        Args:
-            deployment_identifier (str): The identifier of the deployment.
-
-        Returns:
-            NeuralDBClient: A NeuralDBClient instance.
-        """
-        url = urljoin(self._base_url, f"jobs/{self._user_id}/deploy-status")
-
-        response = http_get_with_error(
-            url,
-            params={"deployment_identifier": deployment_identifier},
-            headers=auth_header(self._access_token),
-        )
-
-        response_data = json.loads(response.content)["data"]
-
-        if response_data["status"] == "complete":
-            print("Connection obtained...")
-            return NeuralDBClient(
-                deployment_identifier=deployment_identifier,
-                deployment_id=response_data["deployment_id"],
-                bazaar=self,
-            )
-
-        raise Exception("The model isn't deployed...")
+from __future__ import annotations
+
+import json
+import os
+import re
+import time
+from pathlib import Path
+from typing import Any, Dict, List, Optional, Tuple, Union
+from urllib.parse import urljoin
+
+from .bazaar_base import Bazaar, auth_header
+from .utils import (
+    check_deployment_decorator,
+    construct_deployment_url,
+    create_deployment_identifier,
+    create_model_identifier,
+    http_get_with_error,
+    http_post_with_error,
+    print_progress_dots,
+)
+
+
+class Model:
+    """
+    A class representing a model listed on NeuralDB Enterprise.
+
+    Attributes:
+        _model_identifier (str): The unique identifier for the model.
+
+    Methods:
+        __init__(self, model_identifier: str) -> None:
+            Initializes a new instance of the Model class.
+
+            Parameters:
+                model_identifier (str): An optional model identifier.
+
+        model_identifier(self) -> str:
+            Getter method for accessing the model identifier.
+
+            Returns:
+                str: The model identifier, or None if not set.
+    """
+
+    def __init__(self, model_identifier, model_id=None) -> None:
+        self._model_identifier = model_identifier
+        self._model_id = model_id
+
+    @property
+    def model_identifier(self):
+        return self._model_identifier
+
+    @property
+    def model_id(self):
+        if self._model_id:
+            return self._model_id
+        raise ValueError("Model id is not yet set.")
+
+
+class NeuralDBClient:
+    """
+    A client for interacting with the deployed NeuralDB model.
+
+    Attributes:
+        deployment_identifier (str): The identifier for the deployment.
+        deployment_id (str): The deployment ID for the deployed NeuralDB model.
+        bazaar (thirdai.neural_db.ModelBazaar): The bazaar object corresponding to a NeuralDB Enterprise installation
+
+    Methods:
+        __init__(self, deployment_identifier: str, deployment_id: str, bazaar: ModelBazaar) -> None:
+            Initializes a new instance of the NeuralDBClient.
+
+        search(self, query, top_k=5, constraints: Optional[dict[str, dict[str, str]]]=None) -> List[dict]:
+            Searches the ndb model for relevant search results.
+
+        insert(self, documents: list[dict[str, Any]]) -> None:
+            Inserts documents into the ndb model.
+
+        delete(self, source_ids: List[str]) -> None:
+            Deletes documents from the ndb model
+
+        associate(self, text_pairs (List[Dict[str, str]])) -> None:
+            Associates source and target string pairs in the ndb model.
+
+        upvote(self, text_id_pairs: List[Dict[str, Union[str, int]]]) -> None:
+            Upvotes a response in the ndb model.
+
+        downvote(self, text_id_pairs: List[Dict[str, Union[str, int]]]) -> None:
+            Downvotes a response in the ndb model.
+
+        chat(self, user_input: str, session_id: str) -> Dict[str, str]:
+            Returns a reply given the user_input and the chat history associated with session_id
+
+        get_chat_history(self, session_id: str) -> Dict[List[Dict[str, str]]]:
+            Returns chat history associated with session_id
+
+        sources(self) -> List[Dict[str, str]]:
+            Gets the source names and ids of documents in the ndb model
+    """
+
+    def __init__(
+        self, deployment_identifier: str, deployment_id: str, bazaar: ModelBazaar
+    ):
+        """
+        Initializes a new instance of the NeuralDBClient.
+
+        Args:
+            deployment_identifier (str): The identifier for the deployment.
+            deployment_id (str): The deployment ID for the deployed NeuralDB model.
+            bazaar (thirdai.neural_db.ModelBazaar): The bazaar object corresponding to a NeuralDB Enterprise installation
+        """
+        self.deployment_identifier = deployment_identifier
+        self.base_url = construct_deployment_url(
+            re.sub(r"api/$", "", bazaar._base_url), deployment_id
+        )
+        self.bazaar = bazaar
+
+    @check_deployment_decorator
+    def search(
+        self, query, top_k=5, constraints: Optional[dict[str, dict[str, str]]] = None
+    ):
+        """
+        Searches the ndb model for similar queries.
+
+        Args:
+            query (str): The query to search for.
+            top_k (int): The number of top results to retrieve (default is 10).
+            constraints (Optional[dict[str, dict[str, str]]]): Constraints to filter the search result metadata by.
+                These constraints must be in the following format:
+                {"FIELD_NAME": {"constraint_type": "CONSTRAINT_NAME", **kwargs}} where
+                "FIELD_NAME" is the field that you want to filter over, and "CONSTRAINT_NAME"
+                is one of the following: "AnyOf", "EqualTo", "InRange", "GreaterThan", and "LessThan".
+                The kwargs for the above constraints are shown below:
+
+                class AnyOf(BaseModel):
+                    constraint_type: Literal["AnyOf"]
+                    values: Iterable[Any]
+
+                class EqualTo(BaseModel):
+                    constraint_type: Literal["EqualTo"]
+                    value: Any
+
+                class InRange(BaseModel):
+                    constraint_type: Literal["InRange"]
+                    minimum: Any
+                    maximum: Any
+                    inclusive_min: bool = True
+                    inclusive_max: bool = True
+
+                class GreaterThan(BaseModel):
+                    constraint_type: Literal["GreaterThan"]
+                    minimum: Any
+                    include_equal: bool = False
+
+                class LessThan(BaseModel):
+                    constraint_type: Literal["LessThan"]
+                    maximum: Any
+                    include_equal: bool = False
+
+        Returns:
+            Dict: A dict of search results containing keys: `query_text` and `references`.
+        """
+
+        response = http_post_with_error(
+            urljoin(self.base_url, "predict"),
+            params={"query_text": query, "top_k": top_k},
+            json=constraints,
+            headers=auth_header(self.bazaar._access_token),
+        )
+
+        return json.loads(response.content)["data"]
+
+    @check_deployment_decorator
+    def insert(self, documents: list[dict[str, Any]]):
+        """
+        Inserts documents into the ndb model.
+
+        Args:
+            documents (List[dict[str, Any]]): A list of dictionaries that represent documents to be inserted to the ndb model.
+                The document dictionaries must be in the following format:
+                {"document_type": "DOCUMENT_TYPE", **kwargs} where "DOCUMENT_TYPE" is one of the following:
+                "PDF", "CSV", "DOCX", "URL", "SentenceLevelPDF", "SentenceLevelDOCX", "Unstructured", "InMemoryText".
+                The kwargs for each document type are shown below:
+
+                class PDF(Document):
+                    document_type: Literal["PDF"]
+                    path: str
+                    metadata: Optional[dict[str, Any]] = None
+                    on_disk: bool = False
+                    version: str = "v1"
+                    chunk_size: int = 100
+                    stride: int = 40
+                    emphasize_first_words: int = 0
+                    ignore_header_footer: bool = True
+                    ignore_nonstandard_orientation: bool = True
+
+                class CSV(Document):
+                    document_type: Literal["CSV"]
+                    path: str
+                    id_column: Optional[str] = None
+                    strong_columns: Optional[List[str]] = None
+                    weak_columns: Optional[List[str]] = None
+                    reference_columns: Optional[List[str]] = None
+                    save_extra_info: bool = True
+                    metadata: Optional[dict[str, Any]] = None
+                    has_offset: bool = False
+                    on_disk: bool = False
+
+                class DOCX(Document):
+                    document_type: Literal["DOCX"]
+                    path: str
+                    metadata: Optional[dict[str, Any]] = None
+                    on_disk: bool = False
+
+                class URL(Document):
+                    document_type: Literal["URL"]
+                    url: str
+                    save_extra_info: bool = True
+                    title_is_strong: bool = False
+                    metadata: Optional[dict[str, Any]] = None
+                    on_disk: bool = False
+
+                class SentenceLevelPDF(Document):
+                    document_type: Literal["SentenceLevelPDF"]
+                    path: str
+                    metadata: Optional[dict[str, Any]] = None
+                    on_disk: bool = False
+
+                class SentenceLevelDOCX(Document):
+                    document_type: Literal["SentenceLevelDOCX"]
+                    path: str
+                    metadata: Optional[dict[str, Any]] = None
+                    on_disk: bool = False
+
+                class Unstructured(Document):
+                    document_type: Literal["Unstructured"]
+                    path: str
+                    save_extra_info: bool = True
+                    metadata: Optional[dict[str, Any]] = None
+                    on_disk: bool = False
+
+                class InMemoryText(Document):
+                    document_type: Literal["InMemoryText"]
+                    name: str
+                    texts: list[str]
+                    metadatas: Optional[list[dict[str, Any]]] = None
+                    global_metadata: Optional[dict[str, Any]] = None
+                    on_disk: bool = False
+
+                For Document types with the arg "path", ensure that the path exists on your local machine.
+        """
+
+        if not documents:
+            raise ValueError("Documents cannot be empty.")
+
+        files = []
+        for doc in documents:
+            if "path" in doc and ("location" not in doc or doc["location"] == "local"):
+                if not os.path.exists(doc["path"]):
+                    raise ValueError(
+                        f"Path {doc['path']} was provided but doesn't exist on the machine."
+                    )
+                files.append(("files", open(doc["path"], "rb")))
+
+        files.append(("documents", (None, json.dumps(documents), "application/json")))
+
+        response = http_post_with_error(
+            urljoin(self.base_url, "insert"),
+            files=files,
+            headers=auth_header(self.bazaar._access_token),
+        )
+
+        return json.loads(response.content)["data"]
+
+    @check_deployment_decorator
+    def delete(self, source_ids: List[str]):
+        """
+        Deletes documents from the ndb model using source ids.
+
+        Args:
+            files (List[str]): A list of source ids to delete from the ndb model.
+        """
+        response = http_post_with_error(
+            urljoin(self.base_url, "delete"),
+            json={"source_ids": source_ids},
+            headers=auth_header(self.bazaar._access_token),
+        )
+
+    @check_deployment_decorator
+    def associate(self, text_pairs: List[Dict[str, str]]):
+        """
+        Associates source and target string pairs in the ndb model.
+
+        Args:
+            text_pairs (List[Dict[str, str]]): List of dictionaries where each dictionary has 'source' and 'target' keys.
+        """
+        response = http_post_with_error(
+            urljoin(self.base_url, "associate"),
+            json={"text_pairs": text_pairs},
+            headers=auth_header(self.bazaar._access_token),
+        )
+
+    @check_deployment_decorator
+    def save_model(self, override: bool = True, model_name: Optional[str] = None):
+
+        response = http_post_with_error(
+            urljoin(self.base_url, "save"),
+            json={"override": override, "model_name": model_name},
+            headers=auth_header(self.bazaar._access_token),
+        )
+
+        print("Successfully saved the model.")
+
+        content = response.json()["data"]
+
+        if content["new_model_id"]:
+            return Model(
+                model_identifier=create_model_identifier(
+                    model_name, self.bazaar._username
+                ),
+                model_id=content["new_model_id"],
+            )
+
+        return None
+
+    @check_deployment_decorator
+    def upvote(self, text_id_pairs: List[Dict[str, Union[str, int]]]):
+        """
+        Upvote response with 'reference_id' corresponding to 'query_text' in the ndb model.
+
+        Args:
+            text_id_pairs: (List[Dict[str, Union[str, int]]]): List of dictionaries where each dictionary has 'query_text' and 'reference_id' keys.
+        """
+        response = http_post_with_error(
+            urljoin(self.base_url, "upvote"),
+            json={"text_id_pairs": text_id_pairs},
+            headers=auth_header(self.bazaar._access_token),
+        )
+
+        print("Successfully upvoted the specified search result.")
+
+    @check_deployment_decorator
+    def downvote(self, text_id_pairs: List[Dict[str, Union[str, int]]]):
+        """
+        Downvote response with 'reference_id' corresponding to 'query_text' in the ndb model.
+
+        Args:
+            text_id_pairs: (List[Dict[str, Union[str, int]]]): List of dictionaries where each dictionary has 'query_text' and 'reference_id' keys.
+        """
+        response = http_post_with_error(
+            urljoin(self.base_url, "downvote"),
+            json={"text_id_pairs": text_id_pairs},
+            headers=auth_header(self.bazaar._access_token),
+        )
+
+        print("Successfully downvoted the specified search result.")
+
+    @check_deployment_decorator
+    def chat(self, user_input: str, session_id: str) -> Dict[str, str]:
+        """
+        Returns a reply given the user_input and the chat history associated with session_id
+
+        Args:
+            user_input (str): The user input for the chatbot to respond to
+            session_id (str): The session id corresponding to a specific chat session
+        """
+        response = http_post_with_error(
+            urljoin(self.base_url, "chat"),
+            json={"user_input": user_input, "session_id": session_id},
+            headers=auth_header(self.bazaar._access_token),
+        )
+
+        return response.json()["data"]
+
+    @check_deployment_decorator
+    def get_chat_history(self, session_id: str) -> Dict[List[Dict[str, str]]]:
+        """
+        Returns chat history associated with session_id
+
+        Args:
+            session_id (str): The session id corresponding to a specific chat session
+        """
+        response = http_post_with_error(
+            urljoin(self.base_url, "get-chat-hisory"),
+            json={"session_id": session_id},
+            headers=auth_header(self.bazaar._access_token),
+        )
+
+        return response.json()["data"]
+
+    @check_deployment_decorator
+    def sources(self) -> List[Dict[str, str]]:
+        """
+        Gets the source names and ids of documents in the ndb model
+
+        """
+        response = http_get_with_error(
+            urljoin(self.base_url, "sources"),
+            headers=auth_header(self.bazaar._access_token),
+        )
+
+        return response.json()["data"]
+
+
+class ModelBazaar(Bazaar):
+    """
+    A class representing ModelBazaar, providing functionality for managing models and deployments.
+
+    Attributes:
+        _base_url (str): The base URL for the Model Bazaar.
+        _cache_dir (Union[Path, str]): The directory for caching downloads.
+
+    Methods:
+        __init__(self, base_url: str, cache_dir: Union[Path, str] = "./bazaar_cache") -> None:
+            Initializes a new instance of the ModelBazaar class.
+
+        sign_up(self, email: str, password: str, username: str) -> None:
+            Signs up a user and sets the username for the ModelBazaar instance.
+
+        log_in(self, email: str, password: str) -> None:
+            Logs in a user and sets user-related attributes for the ModelBazaar instance.
+
+        push_model(self, model_name: str, local_path: str, access_level: str = "private") -> None:
+            Pushes a model to the Model Bazaar.
+
+        pull_model(self, model_identifier: str) -> NeuralDBClient:
+            Pulls a model from the Model Bazaar and returns a NeuralDBClient instance.
+
+        list_models(self) -> List[dict]:
+            Lists available models in the Model Bazaar.
+
+        train(self,
+            model_name: str,
+            unsupervised_docs: Optional[List[str]] = None,
+            supervised_docs: Optional[List[Tuple[str, str]]] = None,
+            test_doc: Optional[str] = None,
+            doc_type: str = "local",
+            sharded: bool = False,
+            is_async: bool = False,
+            base_model_identifier: str = None,
+            train_extra_options: Optional[dict] = None,
+            metadata: Optional[List[Dict[str, str]]] = None
+        ) -> Model:
+            Initiates training for a model and returns a Model instance.
+
+        await_train(self, model: Model) -> None:
+            Waits for the training of a model to complete.
+
+        test(self,
+            model_identifier: str,
+            test_doc: str,
+            doc_type: str = "local",
+            test_extra_options: dict = {},
+            is_async: bool = False,
+        ) -> str:
+            Starts the Model testing on given test file.
+
+        await_test(self, model_identifier: str, test_id: str) -> None:
+            Waits for the testing of a model on that test_id to complete.
+
+        deploy(self, model_identifier: str, deployment_name: str, is_async: bool = False) -> NeuralDBClient:
+            Deploys a model and returns a NeuralDBClient instance.
+
+        await_deploy(self, ndb_client: NeuralDBClient) -> None:
+            Waits for the deployment of a model to complete.
+
+        undeploy(self, ndb_client: NeuralDBClient) -> None:
+            Undeploys a deployed model.
+
+        list_deployments(self) -> List[dict]:
+            Lists the deployments in the Model Bazaar.
+
+        connect(self, deployment_identifier: str) -> NeuralDBClient:
+            Connects to a deployed model and returns a NeuralDBClient instance.
+    """
+
+    def __init__(
+        self,
+        base_url: str,
+        cache_dir: Union[Path, str] = "./bazaar_cache",
+    ):
+        """
+        Initializes a new instance of the ModelBazaar class.
+
+        Args:
+            base_url (str): The base URL for the Model Bazaar.
+            cache_dir (Union[Path, str]): The directory for caching downloads.
+        """
+        super().__init__(base_url, cache_dir)
+        self._username = None
+        self._user_id = None
+        self._access_token = None
+        self._doc_types = ["local", "nfs", "s3"]
+
+    def sign_up(self, email, password, username):
+        """
+        Signs up a user and sets the username for the ModelBazaar instance.
+
+        Args:
+            email (str): The email of the user.
+            password (str): The password of the user.
+            username (str): The desired username.
+        """
+        self.signup(email=email, password=password, username=username)
+        self._username = username
+
+    def log_in(self, email, password):
+        """
+        Logs in a user and sets user-related attributes for the ModelBazaar instance.
+
+        Args:
+            email (str): The email of the user.
+            password (str): The password of the user.
+        """
+        self.login(email=email, password=password)
+        self._user_id = self._login_instance.user_id
+        self._access_token = self._login_instance.access_token
+        self._username = self._login_instance.username
+
+    def push_model(
+        self, model_name: str, local_path: str, access_level: str = "private"
+    ):
+        """
+        Pushes a model to the Model Bazaar.
+
+        Args:
+            model_name (str): The name of the model.
+            local_path (str): The local path of the model.
+            access_level (str): The access level for the model (default is "private").
+        """
+        self.push(
+            name=model_name,
+            model_path=local_path,
+            trained_on="Own Documents",
+            access_level=access_level,
+            is_indexed=True,
+            description="",
+        )
+
+    def pull_model(self, model_identifier: str):
+        """
+        Pulls a model from the Model Bazaar and returns a NeuralDBClient instance.
+
+        Args:
+            model_identifier (str): The identifier of the model.
+
+        Returns:
+            NeuralDBClient: A NeuralDBClient instance.
+        """
+        return self.get_neuraldb(model_identifier=model_identifier)
+
+    def list_models(self):
+        """
+        Lists available models in the Model Bazaar.
+
+        Returns:
+            List[dict]: A list of dictionaries containing information about available models.
+        """
+        return self.fetch()
+
+    def train(
+        self,
+        model_name: str,
+        unsupervised_docs: Optional[List[str]] = None,
+        supervised_docs: Optional[List[Tuple[str, str]]] = None,
+        test_doc: Optional[str] = None,
+        doc_type: str = "local",
+        sharded: bool = False,
+        is_async: bool = False,
+        base_model_identifier: Optional[str] = None,
+        train_extra_options: Optional[dict] = None,
+        metadata: Optional[List[Dict[str, str]]] = None,
+    ):
+        """
+        Initiates training for a model and returns a Model instance.
+
+        Args:
+            model_name (str): The name of the model.
+            unsupervised_docs (Optional[List[str]]): A list of document paths for unsupervised training.
+            supervised_docs (Optional[List[Tuple[str, str]]]): A list of document path and source id pairs.
+            test_doc (Optional[str]): A path to a test file for evaluating the trained NeuralDB.
+            doc_type (str): Specifies document location type : "local"(default), "nfs" or "s3".
+            sharded (bool): Whether NeuralDB training will be distributed over NeuralDB shards.
+            is_async (bool): Whether training should be asynchronous (default is False).
+            train_extra_options: (Optional[dict])
+            base_model_identifier (Optional[str]): The identifier of the base model.
+            metadata (Optional[List[Dict[str, str]]]): A list metadata dicts. Each dict corresponds to an unsupervised file.
+
+        Returns:
+            Model: A Model instance.
+        """
+        if doc_type not in self._doc_types:
+            raise ValueError(
+                f"Invalid doc_type value. Supported doc_type are {self._doc_types}"
+            )
+
+        if not unsupervised_docs and not supervised_docs:
+            raise ValueError("Both the unsupervised and supervised docs are empty.")
+
+        if metadata and unsupervised_docs:
+            if len(metadata) != len(unsupervised_docs):
+                raise ValueError("Metadata is not provided for all unsupervised files.")
+
+        file_details_list = []
+        docs = []
+
+        if unsupervised_docs and metadata:
+            for doc, meta in zip(unsupervised_docs, metadata):
+                docs.append(doc)
+                file_details_list.append(
+                    {"mode": "unsupervised", "location": doc_type, "metadata": meta}
+                )
+        elif unsupervised_docs:
+            for doc in unsupervised_docs:
+                docs.append(doc)
+                file_details_list.append({"mode": "unsupervised", "location": doc_type})
+
+        if supervised_docs:
+            for sup_file, source_id in supervised_docs:
+                docs.append(sup_file)
+                file_details_list.append(
+                    {"mode": "supervised", "location": doc_type, "source_id": source_id}
+                )
+
+        if test_doc:
+            docs.append(test_doc)
+            file_details_list.append({"mode": "test", "location": doc_type})
+
+        url = urljoin(self._base_url, f"jobs/{self._user_id}/train")
+        files = [
+            (
+                ("files", open(file_path, "rb"))
+                if doc_type == "local"
+                else ("files", (file_path, "don't care"))
+            )
+            for file_path in docs
+        ]
+        if train_extra_options:
+            files.append(
+                (
+                    "extra_options_form",
+                    (None, json.dumps(train_extra_options), "application/json"),
+                )
+            )
+
+        files.append(
+            (
+                "file_details_list",
+                (
+                    None,
+                    json.dumps({"file_details": file_details_list}),
+                    "application/json",
+                ),
+            )
+        )
+
+        response = http_post_with_error(
+            url,
+            params={
+                "model_name": model_name,
+                "doc_type": doc_type,
+                "sharded": sharded,
+                "base_model_identifier": base_model_identifier,
+            },
+            files=files,
+            headers=auth_header(self._access_token),
+        )
+        print(response.content)
+        response_content = json.loads(response.content)
+        if response_content["status"] != "success":
+            raise Exception(response_content["message"])
+
+        model = Model(
+            model_identifier=create_model_identifier(
+                model_name=model_name, author_username=self._username
+            ),
+            model_id=response_content["data"]["model_id"],
+        )
+
+        if is_async:
+            return model
+
+        self.await_train(model)
+        return model
+
+    def test(
+        self,
+        model_identifier: str,
+        test_doc: str,
+        doc_type: str = "local",
+        test_extra_options: dict = {},
+        is_async: bool = False,
+    ):
+        """
+        Initiates testing for a model and returns the test_id (unique identifier for this test)
+
+        Args:
+            model_identifier (str): The identifier of the model.
+            test_doc (str): A path to a test file for evaluating the trained NeuralDB.
+            doc_type (str): Specifies document location type : "local"(default), "nfs" or "s3".
+            test_extra_options: (Optional[dict])
+            is_async (bool): Whether testing should be asynchronous (default is False).
+
+        Returns:
+            str: The test_id which is unique for given testing.
+        """
+        url = urljoin(self._base_url, f"test/test")
+
+        files = [
+            (
+                ("file", open(test_doc, "rb"))
+                if doc_type == "local"
+                else ("file", (test_doc, "don't care"))
+            )
+        ]
+        if test_extra_options:
+            files.append(
+                (
+                    "extra_options_form",
+                    (None, json.dumps(test_extra_options), "application/json"),
+                )
+            )
+
+        response = http_post_with_error(
+            url,
+            params={
+                "doc_type": doc_type,
+                "model_identifier": model_identifier,
+            },
+            files=files,
+            headers=auth_header(self._access_token),
+        )
+        print(response.content)
+
+        response_content = json.loads(response.content)
+        if response_content["status"] != "success":
+            raise Exception(response_content["message"])
+
+        if is_async:
+            return response_content["data"]["data_id"]
+
+        self.await_test(model_identifier, response_content["data"]["data_id"])
+        return response_content["data"]["data_id"]
+
+    def test_status(self, test_id: str):
+        """
+        Checks for the status of the model testing
+
+        Args:
+            test_id (str): The unique id with which we can recognize the test,
+            the user will get this id in the response when they trigger the test.
+        """
+
+        url = urljoin(self._base_url, f"test/test-status")
+
+        response = http_get_with_error(
+            url,
+            params={"test_id": test_id},
+            headers=auth_header(self._access_token),
+        )
+
+        response_data = json.loads(response.content)["data"]
+
+        return response_data
+
+    def await_test(self, model_identifier: str, test_id: str):
+        """
+        Waits for the testing of the model to complete.
+
+        Args:
+            model_identifier: The identifier of the model.
+            test_id: Unique id for the test.
+        """
+
+        while True:
+            response_data = self.test_status(test_id)
+
+            if response_data["status"] == "complete":
+                print("\nTesting completed")
+                return response_data["results"]
+
+            if response_data["status"] == "failed":
+                print("\nTesting Failed")
+                raise ValueError(f"Test Failed for {model_identifier} and {test_id}")
+
+            print("Testing: In progress", end="", flush=True)
+            print_progress_dots(duration=10)
+
+    def train_status(self, model: Model):
+        """
+        Checks for the status of the model training
+
+        Args:
+            model (Model): The Model instance.
+        """
+
+        url = urljoin(self._base_url, f"jobs/{self._user_id}/train-status")
+
+        response = http_get_with_error(
+            url,
+            params={"model_identifier": model.model_identifier},
+            headers=auth_header(self._access_token),
+        )
+
+        response_data = json.loads(response.content)["data"]
+
+        return response_data
+
+    def await_train(self, model: Model):
+        """
+        Waits for the training of a model to complete.
+
+        Args:
+            model (Model): The Model instance.
+        """
+        while True:
+            response_data = self.train_status(model)
+
+            if response_data["status"] == "complete":
+                print("\nTraining completed")
+                return
+
+            if response_data["status"] == "failed":
+                print("\nTraining Failed")
+                raise ValueError(f"Training Failed for {model.model_identifier}")
+
+            print("Training: In progress", end="", flush=True)
+            print_progress_dots(duration=10)
+
+    def deploy(
+        self,
+        model_identifier: str,
+        deployment_name: str,
+        memory: Optional[int] = None,
+        is_async=False,
+    ):
+        """
+        Deploys a model and returns a NeuralDBClient instance.
+
+        Args:
+            model_identifier (str): The identifier of the model.
+            deployment_name (str): The name for the deployment.
+            is_async (bool): Whether deployment should be asynchronous (default is False).
+
+        Returns:
+            NeuralDBClient: A NeuralDBClient instance.
+        """
+        url = urljoin(self._base_url, f"jobs/{self._user_id}/deploy")
+        params = {
+            "user_id": self._user_id,
+            "model_identifier": model_identifier,
+            "deployment_name": deployment_name,
+            "memory": memory,
+        }
+        response = http_post_with_error(
+            url, params=params, headers=auth_header(self._access_token)
+        )
+        response_data = json.loads(response.content)["data"]
+
+        ndb_client = NeuralDBClient(
+            deployment_identifier=create_deployment_identifier(
+                model_identifier=model_identifier,
+                deployment_name=deployment_name,
+                deployment_username=self._username,
+            ),
+            deployment_id=response_data["deployment_id"],
+            bazaar=self,
+        )
+        if is_async:
+            return ndb_client
+
+        time.sleep(5)
+        self.await_deploy(ndb_client)
+        return ndb_client
+
+    def await_deploy(self, ndb_client: NeuralDBClient):
+        """
+        Waits for the deployment of a model to complete.
+
+        Args:
+            ndb_client (NeuralDBClient): The NeuralDBClient instance.
+        """
+        url = urljoin(self._base_url, f"jobs/{self._user_id}/deploy-status")
+
+        params = {"deployment_identifier": ndb_client.deployment_identifier}
+        while True:
+            response = http_get_with_error(
+                url, params=params, headers=auth_header(self._access_token)
+            )
+            response_data = json.loads(response.content)["data"]
+
+            if response_data["status"] == "complete":
+                print("\nDeployment completed")
+                return
+
+            print("Deployment: In progress", end="", flush=True)
+            print_progress_dots(duration=5)
+
+    def undeploy(self, ndb_client: NeuralDBClient):
+        """
+        Undeploys a deployed model.
+
+        Args:
+            ndb_client (NeuralDBClient): The NeuralDBClient instance.
+        """
+        url = urljoin(self._base_url, f"jobs/{self._user_id}/undeploy")
+        params = {
+            "deployment_identifier": ndb_client.deployment_identifier,
+        }
+        response = http_post_with_error(
+            url, params=params, headers=auth_header(self._access_token)
+        )
+
+        print("Deployment is shutting down.")
+
+    def list_deployments(self):
+        """
+        Lists the deployments in the Model Bazaar.
+
+        Returns:
+            List[dict]: A list of dictionaries containing information about deployments.
+        """
+        url = urljoin(self._base_url, f"jobs/{self._user_id}/list-deployments")
+        response = http_get_with_error(
+            url,
+            params={
+                "user_id": self._user_id,
+            },
+            headers=auth_header(self._access_token),
+        )
+
+        response_data = json.loads(response.content)["data"]
+        deployments = []
+        for deployment in response_data:
+            model_identifier = create_model_identifier(
+                model_name=deployment["model_name"],
+                author_username=deployment["model_username"],
+            )
+            deployment_info = {
+                "deployment_identifier": create_deployment_identifier(
+                    model_identifier=model_identifier,
+                    deployment_name=deployment["name"],
+                    deployment_username=deployment["deployment_username"],
+                ),
+                "status": deployment["status"],
+            }
+            deployments.append(deployment_info)
+
+        return deployments
+
+    def connect(self, deployment_identifier: str):
+        """
+        Connects to a deployed model and returns a NeuralDBClient instance.
+
+        Args:
+            deployment_identifier (str): The identifier of the deployment.
+
+        Returns:
+            NeuralDBClient: A NeuralDBClient instance.
+        """
+        url = urljoin(self._base_url, f"jobs/{self._user_id}/deploy-status")
+
+        response = http_get_with_error(
+            url,
+            params={"deployment_identifier": deployment_identifier},
+            headers=auth_header(self._access_token),
+        )
+
+        response_data = json.loads(response.content)["data"]
+
+        if response_data["status"] == "complete":
+            print("Connection obtained...")
+            return NeuralDBClient(
+                deployment_identifier=deployment_identifier,
+                deployment_id=response_data["deployment_id"],
+                bazaar=self,
+            )
+
+        raise Exception("The model isn't deployed...")
```

## thirdai/neural_db/model_bazaar/utils.py

 * *Ordering differences only*

```diff
@@ -1,145 +1,145 @@
-import hashlib
-import json
-import os
-import shutil
-import sys
-import time
-from functools import wraps
-from pathlib import Path
-from urllib.parse import urljoin
-
-import requests
-from IPython.display import clear_output
-
-
-def print_progress_dots(duration: int):
-    for _ in range(duration):
-        sys.stdout.write(".")
-        sys.stdout.flush()
-        time.sleep(1)
-    clear_output(wait=True)
-
-
-def create_model_identifier(model_name: str, author_username: str):
-    return author_username + "/" + model_name
-
-
-def create_deployment_identifier(
-    model_identifier: str, deployment_name: str, deployment_username: str
-):
-    return model_identifier + ":" + deployment_username + "/" + deployment_name
-
-
-def construct_deployment_url(host, deployment_id):
-    return urljoin(host, deployment_id) + "/"
-
-
-def check_deployment_decorator(func):
-    """
-    A decorator function to check if deployment is complete before executing the decorated method.
-
-    Args:
-        func (callable): The function to be decorated.
-
-    Returns:
-        callable: The decorated function.
-    """
-
-    @wraps(func)
-    def wrapper(self, *args, **kwargs):
-        try:
-            return func(self, *args, **kwargs)
-        except requests.RequestException as e:
-            print(f"Error during HTTP request: {str(e)}")
-            print(
-                "Deployment might not be complete yet. Call `list_deployments()` to check status of your deployment."
-            )
-            return None
-
-    return wrapper
-
-
-def chunks(path: Path):
-    def get_name(dir_entry: os.DirEntry):
-        return Path(dir_entry.path).name
-
-    if path.is_dir():
-        for entry in sorted(os.scandir(path), key=get_name):
-            yield bytes(Path(entry.path).name, "utf-8")
-            for chunk in chunks(Path(entry.path)):
-                yield chunk
-    elif path.is_file():
-        with open(path, "rb") as file:
-            for chunk in iter(lambda: file.read(4096), b""):
-                yield chunk
-
-
-def hash_path(path: Path):
-    # Create a SHA-256 hash object
-    sha256_hash = hashlib.sha256()
-    if not path.exists():
-        raise ValueError("Cannot hash an invalid path.")
-    for chunk in chunks(path):
-        sha256_hash.update(chunk)
-    return sha256_hash.hexdigest()
-
-
-def get_directory_size(directory: Path):
-    size = 0
-    for root, dirs, files in os.walk(directory):
-        for name in files:
-            size += os.stat(Path(root) / name).st_size
-    return size
-
-
-def check_response(response):
-    if not (200 <= response.status_code < 300):
-        print(response.content)
-        raise requests.exceptions.HTTPError(
-            "Failed with status code:", response.status_code
-        )
-
-    content = json.loads(response.content)
-    print(content)
-
-    status = content["status"]
-
-    if status != "success":
-        error = content["message"]
-        raise requests.exceptions.HTTPError(f"error: {error}")
-
-
-def http_get_with_error(*args, **kwargs):
-    """Makes an HTTP GET request and raises an error if status code is not
-    2XX.
-    """
-    response = requests.get(*args, **kwargs)
-    check_response(response)
-    return response
-
-
-def http_post_with_error(*args, **kwargs):
-    """Makes an HTTP POST request and raises an error if status code is not
-    2XX.
-    """
-    response = requests.post(*args, **kwargs)
-    check_response(response)
-    return response
-
-
-def zip_folder(folder_path):
-    shutil.make_archive(folder_path, "zip", folder_path)
-    return str(folder_path) + ".zip"
-
-
-def get_file_size(file_path, unit="B"):
-    file_size = os.path.getsize(file_path)
-    exponents_map = {"B": 0, "KB": 1, "MB": 2, "GB": 3}
-    if unit not in exponents_map:
-        raise ValueError(
-            "Must select from \
-        ['B', 'KB', 'MB', 'GB']"
-        )
-
-    size = file_size / 1024 ** exponents_map[unit]
-    return round(size, 3)
+import hashlib
+import json
+import os
+import shutil
+import sys
+import time
+from functools import wraps
+from pathlib import Path
+from urllib.parse import urljoin
+
+import requests
+from IPython.display import clear_output
+
+
+def print_progress_dots(duration: int):
+    for _ in range(duration):
+        sys.stdout.write(".")
+        sys.stdout.flush()
+        time.sleep(1)
+    clear_output(wait=True)
+
+
+def create_model_identifier(model_name: str, author_username: str):
+    return author_username + "/" + model_name
+
+
+def create_deployment_identifier(
+    model_identifier: str, deployment_name: str, deployment_username: str
+):
+    return model_identifier + ":" + deployment_username + "/" + deployment_name
+
+
+def construct_deployment_url(host, deployment_id):
+    return urljoin(host, deployment_id) + "/"
+
+
+def check_deployment_decorator(func):
+    """
+    A decorator function to check if deployment is complete before executing the decorated method.
+
+    Args:
+        func (callable): The function to be decorated.
+
+    Returns:
+        callable: The decorated function.
+    """
+
+    @wraps(func)
+    def wrapper(self, *args, **kwargs):
+        try:
+            return func(self, *args, **kwargs)
+        except requests.RequestException as e:
+            print(f"Error during HTTP request: {str(e)}")
+            print(
+                "Deployment might not be complete yet. Call `list_deployments()` to check status of your deployment."
+            )
+            return None
+
+    return wrapper
+
+
+def chunks(path: Path):
+    def get_name(dir_entry: os.DirEntry):
+        return Path(dir_entry.path).name
+
+    if path.is_dir():
+        for entry in sorted(os.scandir(path), key=get_name):
+            yield bytes(Path(entry.path).name, "utf-8")
+            for chunk in chunks(Path(entry.path)):
+                yield chunk
+    elif path.is_file():
+        with open(path, "rb") as file:
+            for chunk in iter(lambda: file.read(4096), b""):
+                yield chunk
+
+
+def hash_path(path: Path):
+    # Create a SHA-256 hash object
+    sha256_hash = hashlib.sha256()
+    if not path.exists():
+        raise ValueError("Cannot hash an invalid path.")
+    for chunk in chunks(path):
+        sha256_hash.update(chunk)
+    return sha256_hash.hexdigest()
+
+
+def get_directory_size(directory: Path):
+    size = 0
+    for root, dirs, files in os.walk(directory):
+        for name in files:
+            size += os.stat(Path(root) / name).st_size
+    return size
+
+
+def check_response(response):
+    if not (200 <= response.status_code < 300):
+        print(response.content)
+        raise requests.exceptions.HTTPError(
+            "Failed with status code:", response.status_code
+        )
+
+    content = json.loads(response.content)
+    print(content)
+
+    status = content["status"]
+
+    if status != "success":
+        error = content["message"]
+        raise requests.exceptions.HTTPError(f"error: {error}")
+
+
+def http_get_with_error(*args, **kwargs):
+    """Makes an HTTP GET request and raises an error if status code is not
+    2XX.
+    """
+    response = requests.get(*args, **kwargs)
+    check_response(response)
+    return response
+
+
+def http_post_with_error(*args, **kwargs):
+    """Makes an HTTP POST request and raises an error if status code is not
+    2XX.
+    """
+    response = requests.post(*args, **kwargs)
+    check_response(response)
+    return response
+
+
+def zip_folder(folder_path):
+    shutil.make_archive(folder_path, "zip", folder_path)
+    return str(folder_path) + ".zip"
+
+
+def get_file_size(file_path, unit="B"):
+    file_size = os.path.getsize(file_path)
+    exponents_map = {"B": 0, "KB": 1, "MB": 2, "GB": 3}
+    if unit not in exponents_map:
+        raise ValueError(
+            "Must select from \
+        ['B', 'KB', 'MB', 'GB']"
+        )
+
+    size = file_size / 1024 ** exponents_map[unit]
+    return round(size, 3)
```

## thirdai/neural_db/model_bazaar/__init__.py

 * *Ordering differences only*

```diff
@@ -1,2 +1,2 @@
-from .bazaar_base import Login
-from .bazaar_client import ModelBazaar, NeuralDBClient
+from .bazaar_base import Login
+from .bazaar_client import ModelBazaar, NeuralDBClient
```

## thirdai/neural_db/parsing_utils/doc_parse.py

 * *Ordering differences only*

```diff
@@ -1,46 +1,46 @@
-from pathlib import Path
-
-import pandas as pd
-from docx import Document
-from nltk.tokenize import sent_tokenize, word_tokenize
-
-from .utils import ATTACH_N_WORD_THRESHOLD, chunk_text, ensure_valid_encoding
-
-
-def get_elements(filename):
-    temp = []
-    document = Document(filename)
-    prev_short = False
-    for p in document.paragraphs:
-        if len(p.text.strip()) > 3:
-            if prev_short:
-                temp[-1] = (temp[-1][0] + " " + p.text.strip(), Path(filename).name)
-            else:
-                temp.append((p.text.strip(), Path(filename).name))
-            prev_short = len(word_tokenize(p.text.strip())) < ATTACH_N_WORD_THRESHOLD
-    temp = [
-        (chunk, filename) for passage, filename in temp for chunk in chunk_text(passage)
-    ]
-    return temp, True
-
-
-def create_train_df(elements):
-    df = pd.DataFrame(
-        index=range(len(elements)),
-        columns=["para", "filename", "display"],
-    )
-    for i, elem in enumerate(elements):
-        sents = sent_tokenize(str(elem[0]))
-        sents = [
-            sent.replace("\t", " ").replace(",", " ").replace("\n", " ").strip().lower()
-            for sent in sents
-        ]
-        para = " ".join(sents)
-        df.iloc[i] = [
-            para,
-            elem[1],
-            str(elem[0].replace("\n", " ")),
-        ]
-    for column in ["para", "display"]:
-        df[column] = df[column].apply(ensure_valid_encoding)
-    return df
+from pathlib import Path
+
+import pandas as pd
+from docx import Document
+from nltk.tokenize import sent_tokenize, word_tokenize
+
+from .utils import ATTACH_N_WORD_THRESHOLD, chunk_text, ensure_valid_encoding
+
+
+def get_elements(filename):
+    temp = []
+    document = Document(filename)
+    prev_short = False
+    for p in document.paragraphs:
+        if len(p.text.strip()) > 3:
+            if prev_short:
+                temp[-1] = (temp[-1][0] + " " + p.text.strip(), Path(filename).name)
+            else:
+                temp.append((p.text.strip(), Path(filename).name))
+            prev_short = len(word_tokenize(p.text.strip())) < ATTACH_N_WORD_THRESHOLD
+    temp = [
+        (chunk, filename) for passage, filename in temp for chunk in chunk_text(passage)
+    ]
+    return temp, True
+
+
+def create_train_df(elements):
+    df = pd.DataFrame(
+        index=range(len(elements)),
+        columns=["para", "filename", "display"],
+    )
+    for i, elem in enumerate(elements):
+        sents = sent_tokenize(str(elem[0]))
+        sents = [
+            sent.replace("\t", " ").replace(",", " ").replace("\n", " ").strip().lower()
+            for sent in sents
+        ]
+        para = " ".join(sents)
+        df.iloc[i] = [
+            para,
+            elem[1],
+            str(elem[0].replace("\n", " ")),
+        ]
+    for column in ["para", "display"]:
+        df[column] = df[column].apply(ensure_valid_encoding)
+    return df
```

## thirdai/neural_db/parsing_utils/pdf_parse.py

 * *Ordering differences only*

```diff
@@ -1,189 +1,189 @@
-import functools
-import re
-from dataclasses import dataclass
-from enum import IntEnum
-from pathlib import Path
-from typing import Dict, List, Union
-
-import fitz
-import pandas as pd
-from nltk.tokenize import sent_tokenize
-
-from .utils import ATTACH_N_WORD_THRESHOLD, chunk_text, ensure_valid_encoding
-
-# TODO: Remove senttokenize
-# TODO: Limit paragraph length
-
-
-class BlockType(IntEnum):
-    Text = 0
-    Image = 1
-
-
-@dataclass
-class Block:
-    x0: float
-    y0: float
-    x1: float
-    y1: float
-    lines: str
-    block_no: int
-    block_type: BlockType
-
-    def __init__(self, block: tuple):
-        self.x0 = block[0]
-        self.y0 = block[1]
-        self.x1 = block[2]
-        self.y1 = block[3]
-        self.lines = block[4]
-        self.block_no = block[5]
-        self.block_type = BlockType.Image if block[6] else BlockType.Text
-
-
-@dataclass
-class PDFparagraph:
-    text: str
-    page_no: int
-    filename: str
-    block_nums: Union[
-        str, Dict[int, List[int]]
-    ]  # [Page no. -> Block No(s) Dictionary] in Dict or string format
-
-
-def para_is_complete(para):
-    endings = [".", "?", "!", '."', ".'"]
-    return functools.reduce(
-        lambda a, b: a or b,
-        [para.endswith(end) for end in endings],
-    )
-
-
-# paragraph = {"page_no": [block_id,...], "pagen_no_2":[blicksids, ...]}
-def process_pdf_file(filename):
-    try:
-        rows = []
-        prev = ""
-        prev_n_words = float("inf")
-        doc = fitz.open(filename)
-        paras = []
-        for page_no, page in enumerate(doc):
-            blocks = [Block(block) for block in page.get_text("blocks")]
-            for block in blocks:
-                if block.block_type == BlockType.Text:
-                    current_block_nums = {}
-                    current_block_nums[page_no] = [block.block_no]
-                    current = sent_tokenize(
-                        block.lines.strip().replace("\r\n", " ").replace("\n", " ")
-                    )
-                    current = " ".join(current)
-
-                    if (
-                        len(paras) > 0
-                        and prev != ""
-                        and (
-                            not para_is_complete(paras[-1].text)
-                            or prev_n_words < ATTACH_N_WORD_THRESHOLD
-                        )
-                    ):
-                        attach = True
-                    else:
-                        attach = False
-
-                    if attach and len(paras) > 0:
-                        prev_blocks = paras[-1].block_nums
-                        if page_no in prev_blocks.keys():
-                            prev_blocks[page_no].extend(current_block_nums[page_no])
-                        else:
-                            prev_blocks[page_no] = current_block_nums[page_no]
-
-                        prev_para = paras[-1]
-                        prev_para.text += f" {current}"
-                        prev_para.block_nums = prev_blocks
-
-                    else:
-                        prev = current
-                        paras.append(
-                            PDFparagraph(
-                                text=current,
-                                page_no=page_no,
-                                filename=Path(filename).name,
-                                block_nums=current_block_nums,
-                            )
-                        )
-
-                    # Occurrences of space is proxy for number of words.
-                    # If there are 10 words or less, this paragraph is
-                    # probably just a header.
-                    prev_n_words = len(current.split(" "))
-
-        paras = [
-            PDFparagraph(
-                text=chunk,
-                page_no=paragraph.page_no,
-                filename=paragraph.filename,
-                block_nums=paragraph.block_nums,
-            )
-            for paragraph in paras
-            for chunk in chunk_text(paragraph.text)
-        ]
-        for para in paras:
-            if len(para.text) > 0:
-                sent = re.sub(
-                    " +",
-                    " ",
-                    str(para.text)
-                    .replace("\t", " ")
-                    .replace(",", " ")
-                    .replace("\n", " ")
-                    .strip(),
-                )
-                if len(sent) > 0:
-                    rows.append(
-                        PDFparagraph(
-                            text=sent,
-                            page_no=para.page_no,
-                            filename=para.filename,
-                            block_nums=str(para.block_nums),
-                        )
-                    )
-        return rows, True
-    except Exception as e:
-        print(e.__str__())
-        return "Cannot process pdf file:" + filename, False
-
-
-def create_train_df(elements):
-    df = pd.DataFrame(
-        index=range(len(elements)),
-        columns=["para", "filename", "page", "display", "highlight"],
-    )
-    for i, paragraph in enumerate(elements):
-        sents = sent_tokenize(paragraph.text)
-        sents = list(map(lambda x: x.lower(), sents))
-        para = " ".join(sents)
-        # elem[-1] is id
-        df.iloc[i] = [
-            para,
-            paragraph.filename,
-            paragraph.page_no,
-            paragraph.text,
-            paragraph.block_nums,
-        ]
-    for column in ["para", "display"]:
-        df[column] = df[column].apply(ensure_valid_encoding)
-    return df
-
-
-def highlighted_doc(source, columns):
-    if not "highlight" in columns:
-        return None
-    highlight = eval(columns["highlight"])
-    doc = fitz.open(source)
-    for key, val in highlight.items():
-        page = doc[key]
-        blocks = page.get_text("blocks")
-        for i, b in enumerate(blocks):
-            if i in val:
-                rect = fitz.Rect(b[:4])
-                page.add_highlight_annot(rect)
-    return doc
+import functools
+import re
+from dataclasses import dataclass
+from enum import IntEnum
+from pathlib import Path
+from typing import Dict, List, Union
+
+import fitz
+import pandas as pd
+from nltk.tokenize import sent_tokenize
+
+from .utils import ATTACH_N_WORD_THRESHOLD, chunk_text, ensure_valid_encoding
+
+# TODO: Remove senttokenize
+# TODO: Limit paragraph length
+
+
+class BlockType(IntEnum):
+    Text = 0
+    Image = 1
+
+
+@dataclass
+class Block:
+    x0: float
+    y0: float
+    x1: float
+    y1: float
+    lines: str
+    block_no: int
+    block_type: BlockType
+
+    def __init__(self, block: tuple):
+        self.x0 = block[0]
+        self.y0 = block[1]
+        self.x1 = block[2]
+        self.y1 = block[3]
+        self.lines = block[4]
+        self.block_no = block[5]
+        self.block_type = BlockType.Image if block[6] else BlockType.Text
+
+
+@dataclass
+class PDFparagraph:
+    text: str
+    page_no: int
+    filename: str
+    block_nums: Union[
+        str, Dict[int, List[int]]
+    ]  # [Page no. -> Block No(s) Dictionary] in Dict or string format
+
+
+def para_is_complete(para):
+    endings = [".", "?", "!", '."', ".'"]
+    return functools.reduce(
+        lambda a, b: a or b,
+        [para.endswith(end) for end in endings],
+    )
+
+
+# paragraph = {"page_no": [block_id,...], "pagen_no_2":[blicksids, ...]}
+def process_pdf_file(filename):
+    try:
+        rows = []
+        prev = ""
+        prev_n_words = float("inf")
+        doc = fitz.open(filename)
+        paras = []
+        for page_no, page in enumerate(doc):
+            blocks = [Block(block) for block in page.get_text("blocks")]
+            for block in blocks:
+                if block.block_type == BlockType.Text:
+                    current_block_nums = {}
+                    current_block_nums[page_no] = [block.block_no]
+                    current = sent_tokenize(
+                        block.lines.strip().replace("\r\n", " ").replace("\n", " ")
+                    )
+                    current = " ".join(current)
+
+                    if (
+                        len(paras) > 0
+                        and prev != ""
+                        and (
+                            not para_is_complete(paras[-1].text)
+                            or prev_n_words < ATTACH_N_WORD_THRESHOLD
+                        )
+                    ):
+                        attach = True
+                    else:
+                        attach = False
+
+                    if attach and len(paras) > 0:
+                        prev_blocks = paras[-1].block_nums
+                        if page_no in prev_blocks.keys():
+                            prev_blocks[page_no].extend(current_block_nums[page_no])
+                        else:
+                            prev_blocks[page_no] = current_block_nums[page_no]
+
+                        prev_para = paras[-1]
+                        prev_para.text += f" {current}"
+                        prev_para.block_nums = prev_blocks
+
+                    else:
+                        prev = current
+                        paras.append(
+                            PDFparagraph(
+                                text=current,
+                                page_no=page_no,
+                                filename=Path(filename).name,
+                                block_nums=current_block_nums,
+                            )
+                        )
+
+                    # Occurrences of space is proxy for number of words.
+                    # If there are 10 words or less, this paragraph is
+                    # probably just a header.
+                    prev_n_words = len(current.split(" "))
+
+        paras = [
+            PDFparagraph(
+                text=chunk,
+                page_no=paragraph.page_no,
+                filename=paragraph.filename,
+                block_nums=paragraph.block_nums,
+            )
+            for paragraph in paras
+            for chunk in chunk_text(paragraph.text)
+        ]
+        for para in paras:
+            if len(para.text) > 0:
+                sent = re.sub(
+                    " +",
+                    " ",
+                    str(para.text)
+                    .replace("\t", " ")
+                    .replace(",", " ")
+                    .replace("\n", " ")
+                    .strip(),
+                )
+                if len(sent) > 0:
+                    rows.append(
+                        PDFparagraph(
+                            text=sent,
+                            page_no=para.page_no,
+                            filename=para.filename,
+                            block_nums=str(para.block_nums),
+                        )
+                    )
+        return rows, True
+    except Exception as e:
+        print(e.__str__())
+        return "Cannot process pdf file:" + filename, False
+
+
+def create_train_df(elements):
+    df = pd.DataFrame(
+        index=range(len(elements)),
+        columns=["para", "filename", "page", "display", "highlight"],
+    )
+    for i, paragraph in enumerate(elements):
+        sents = sent_tokenize(paragraph.text)
+        sents = list(map(lambda x: x.lower(), sents))
+        para = " ".join(sents)
+        # elem[-1] is id
+        df.iloc[i] = [
+            para,
+            paragraph.filename,
+            paragraph.page_no,
+            paragraph.text,
+            paragraph.block_nums,
+        ]
+    for column in ["para", "display"]:
+        df[column] = df[column].apply(ensure_valid_encoding)
+    return df
+
+
+def highlighted_doc(source, columns):
+    if not "highlight" in columns:
+        return None
+    highlight = eval(columns["highlight"])
+    doc = fitz.open(source)
+    for key, val in highlight.items():
+        page = doc[key]
+        blocks = page.get_text("blocks")
+        for i, b in enumerate(blocks):
+            if i in val:
+                rect = fitz.Rect(b[:4])
+                page.add_highlight_annot(rect)
+    return doc
```

## thirdai/neural_db/parsing_utils/sliding_pdf_parse.py

 * *Ordering differences only*

```diff
@@ -1,355 +1,355 @@
-from collections import Counter, defaultdict
-
-import fitz
-import numpy as np
-import pandas as pd
-import pdfplumber
-import unidecode
-from sklearn.cluster import DBSCAN
-
-
-def get_fitz_blocks(filename):
-    doc = fitz.open(filename)
-    blocks = [
-        {**block, "page_num": num}
-        for num, page in enumerate(doc)
-        for block in page.get_text("dict")["blocks"]
-    ]
-    doc.close()
-    return blocks
-
-
-def remove_images(blocks):
-    return [block for block in blocks if block["type"] == 0]
-
-
-def get_text_len(block):
-    return sum(len(span["text"]) for line in block["lines"] for span in line["spans"])
-
-
-def remove_header_footer(blocks):
-    # Inspired by
-    # https://github.com/pymupdf/PyMuPDF/discussions/2259#discussioncomment-6669190
-    # TL;DR:
-    # 1. Vectorize each block by their rectangle coordinates + length of text
-    # 2. Use DBSCAN to cheaply cluster these vectors
-    # 3. Only headers and footers are repetitive enough to form its own
-    # cluster, so they will have a unique label while the majority of blocks are
-    # classified as clusterless.
-    # 4. Remove blocks that belong to minority clusters.
-    dbscan = DBSCAN()
-    samples = np.array([(*block["bbox"], get_text_len(block)) for block in blocks])
-    dbscan.fit(samples)
-    labels = dbscan.labels_
-    label_counter = Counter(labels)
-    most_common_label = label_counter.most_common(1)[0][0]
-    return [block for block, label in zip(blocks, labels) if label == most_common_label]
-
-
-def remove_nonstandard_orientation(blocks):
-    orient_to_count = defaultdict(lambda: 0)
-    for block in blocks:
-        for line in block["lines"]:
-            orient_to_count[line["dir"]] += 1
-    #
-    sorted_count_orient_pairs = sorted(
-        [(count, orient) for orient, count in orient_to_count.items()]
-    )
-    _count, most_frequent_orientation = sorted_count_orient_pairs[-1]
-    #
-    return [
-        {
-            **block,
-            "lines": [
-                line
-                for line in block["lines"]
-                if line["dir"] == most_frequent_orientation
-            ],
-        }
-        for block in blocks
-    ]
-
-
-def strip_spaces(blocks):
-    return [
-        {
-            **block,
-            "lines": [
-                {
-                    **line,
-                    "spans": [
-                        {**span, "text": span["text"].strip()} for span in line["spans"]
-                    ],
-                }
-                for line in block["lines"]
-            ],
-        }
-        for block in blocks
-    ]
-
-
-def remove_empty_spans(blocks):
-    return [
-        {
-            **block,
-            "lines": [
-                {
-                    **line,
-                    "spans": [span for span in line["spans"] if len(span["text"]) > 0],
-                }
-                for line in block["lines"]
-            ],
-        }
-        for block in blocks
-    ]
-
-
-def remove_empty_lines(blocks):
-    return [
-        {**block, "lines": [line for line in block["lines"] if len(line["spans"]) > 0]}
-        for block in blocks
-    ]
-
-
-def remove_empty_blocks(blocks):
-    blocks = strip_spaces(blocks)
-    blocks = remove_empty_spans(blocks)
-    blocks = remove_empty_lines(blocks)
-    return [block for block in blocks if len(block["lines"]) > 0]
-
-
-def get_lines(blocks):
-    return [
-        {**line, "page_num": block["page_num"]}
-        for block in blocks
-        for line in block["lines"]
-    ]
-
-
-def set_line_text(lines):
-    """Assumes span texts are stripped of leading and trailing whitespaces."""
-    return [
-        {**line, "text": " ".join(span["text"] for span in line["spans"])}
-        for line in lines
-    ]
-
-
-def set_line_word_counts(lines):
-    """Assumes line texts are set"""
-    return [{**line, "word_count": line["text"].count(" ") + 1} for line in lines]
-
-
-def get_lines_with_first_n_words(lines, n):
-    total_len = 0
-    end = 0
-    while total_len < n:
-        total_len += lines[end]["word_count"]
-        end += 1
-    return " ".join(line["text"] for line in lines[:end])
-
-
-def estimate_section_titles(lines):
-    text_size_freq = defaultdict(int)
-    for line in lines:
-        for span in line["spans"]:
-            text_size_freq[int(span["size"])] += 1
-    most_common_text_size = sorted(text_size_freq.items(), key=lambda x: x[1])[-1][0]
-    current_section_title = ""
-    for i in range(len(lines)):
-        if lines[i]["spans"][0]["size"] > most_common_text_size:
-            current_section_title = lines[i]["text"]
-        lines[i]["section_title"] = current_section_title
-    return lines
-
-
-def process_tables(filename, lines):
-    pdf = pdfplumber.open(filename)
-    tables_by_page = [page.find_tables() for page in pdf.pages]
-
-    def within_by_bbox(a, b):
-        ax1, ay1, ax2, ay2 = a
-        bx1, by1, bx2, by2 = b
-
-        def within(val, lower, upper):
-            return lower < val and val < upper
-
-        return (
-            within(ax1, bx1, bx2)
-            and within(ax2, bx1, bx2)
-            and within(ay1, by1, by2)
-            and within(ay2, by1, by2)
-        )
-
-    new_lines = []
-    seen = defaultdict(bool)
-    for line in lines:
-        is_contained_by_table = False
-        for table in tables_by_page[line["page_num"]]:
-            if within_by_bbox(line["bbox"], table.bbox):
-                is_contained_by_table = True
-                if not seen[table]:
-                    # The format we want the table in for cold_start is different
-                    # than the format we'd like to display so we modify the "text"
-                    # field and we include some display metadata in the line
-                    # Chunking of text within tables is left for future work
-                    extracted_rows = table.extract()
-                    table_as_text = " ".join(
-                        [item or "" for row in extracted_rows for item in row]
-                    )
-                    new_lines.append(
-                        {
-                            **line,
-                            "text": table_as_text,
-                            "bbox": table.bbox,
-                            "table": extracted_rows,
-                        }
-                    )
-                    seen[table] = True
-        if not is_contained_by_table:
-            new_lines.append(line)
-
-    return new_lines
-
-
-def get_chunks_from_lines(lines, chunk_words, stride_words):
-    chunk_start = 0
-    chunks = []
-    chunk_boxes = []
-    section_titles = []
-    display = []
-    while chunk_start < len(lines):
-        chunk_end = chunk_start
-        chunk_size = 0
-        while chunk_size < chunk_words and chunk_end < len(lines):
-            chunk_size += lines[chunk_end]["word_count"]
-            chunk_end += 1
-        stride_end = chunk_start
-        stride_size = 0
-        while stride_size < stride_words and stride_end < len(lines):
-            stride_size += lines[stride_end]["word_count"]
-            stride_end += 1
-        # combine the chunks
-        chunks.append(" ".join(line["text"] for line in lines[chunk_start:chunk_end]))
-        # metadata for highlighting
-        chunk_boxes.append(
-            [(line["page_num"], line["bbox"]) for line in lines[chunk_start:chunk_end]]
-        )
-        # combine unique section titles
-        if "section_title" in lines[0]:
-            unique_section_titles = set(
-                [line["section_title"] for line in lines[chunk_start:chunk_end]]
-            )
-            section_titles.append(" ".join(unique_section_titles))
-        # formulate display text (separating out tables)
-        display.append(
-            " ".join(
-                [
-                    str(line["table"]) if "table" in line else line["text"]
-                    for line in lines[chunk_start:chunk_end]
-                ]
-            )
-        )
-        chunk_start = stride_end
-    return chunks, chunk_boxes, display, section_titles
-
-
-def clean_encoding(text):
-    return unidecode.unidecode(text.encode("utf-8", "replace").decode("utf-8"))
-
-
-def get_chunks(
-    filename,
-    chunk_words,
-    stride_words,
-    emphasize_first_n_words,
-    ignore_header_footer,
-    ignore_nonstandard_orientation,
-    emphasize_section_titles,
-    table_parsing,
-):
-    blocks = get_fitz_blocks(filename)
-    blocks = remove_images(blocks)
-    if ignore_header_footer:
-        blocks = remove_header_footer(blocks)
-    if ignore_nonstandard_orientation:
-        blocks = remove_nonstandard_orientation(blocks)
-    blocks = remove_empty_blocks(blocks)
-    lines = get_lines(blocks)
-    lines = set_line_text(lines)
-    if emphasize_section_titles:
-        lines = estimate_section_titles(lines)
-    if table_parsing:
-        lines = process_tables(filename, lines)
-    lines = set_line_word_counts(lines)
-    first_n_words = get_lines_with_first_n_words(lines, emphasize_first_n_words)
-    chunks, chunk_boxes, display, section_titles = get_chunks_from_lines(
-        lines, chunk_words, stride_words
-    )
-    chunks = [clean_encoding(text) for text in chunks]
-    section_titles = [clean_encoding(section_title) for section_title in section_titles]
-    return chunks, chunk_boxes, display, first_n_words, section_titles
-
-
-def make_df(
-    filename,
-    chunk_words,
-    stride_words,
-    emphasize_first_n_words,
-    ignore_header_footer,
-    ignore_nonstandard_orientation,
-    doc_keywords,
-    emphasize_section_titles,
-    table_parsing,
-):
-    """Arguments:
-    chunk_size: number of words in each chunk of text.
-    stride: number of words between each chunk of text.
-    emphasize_first_words: number of words at the beginning of the file
-        that will be used as strong column for all rows of the resulting
-        dataframe. We do this so that every row can capture important signals
-        like file titles or introductory paragraphs.
-    table_parsing: Whether to enable separate parsing of tables
-    """
-    chunks, chunk_boxes, display, first_n_words, section_titles = get_chunks(
-        filename,
-        chunk_words,
-        stride_words,
-        emphasize_first_n_words,
-        ignore_header_footer,
-        ignore_nonstandard_orientation,
-        emphasize_section_titles,
-        table_parsing,
-    )
-
-    emphasis = [
-        (
-            first_n_words
-            + " "
-            + doc_keywords
-            + " "
-            + (section_titles[i] if emphasize_section_titles else "")
-        )
-        for i in range(len(chunks))
-    ]
-
-    return pd.DataFrame(
-        {
-            "para": [c.lower() for c in chunks],
-            "display": display,
-            "emphasis": emphasis,
-            # chunk_boxes is a list of lists of (page_num, bbox) pairs
-            "chunk_boxes": [str(chunk_box) for chunk_box in chunk_boxes],
-            # get the first element of the first pair in the list of
-            # (page_num, bbox) pairs for each chunk.
-            "page": [chunk_box[0][0] for chunk_box in chunk_boxes],
-        }
-    )
-
-
-def highlighted_doc(source, columns):
-    if not "chunk_boxes" in columns:
-        return None
-    doc = fitz.open(source)
-    for page, box in eval(columns["chunk_boxes"]):
-        doc[page].add_highlight_annot(fitz.Rect(box))
-    return doc
+from collections import Counter, defaultdict
+
+import fitz
+import numpy as np
+import pandas as pd
+import pdfplumber
+import unidecode
+from sklearn.cluster import DBSCAN
+
+
+def get_fitz_blocks(filename):
+    doc = fitz.open(filename)
+    blocks = [
+        {**block, "page_num": num}
+        for num, page in enumerate(doc)
+        for block in page.get_text("dict")["blocks"]
+    ]
+    doc.close()
+    return blocks
+
+
+def remove_images(blocks):
+    return [block for block in blocks if block["type"] == 0]
+
+
+def get_text_len(block):
+    return sum(len(span["text"]) for line in block["lines"] for span in line["spans"])
+
+
+def remove_header_footer(blocks):
+    # Inspired by
+    # https://github.com/pymupdf/PyMuPDF/discussions/2259#discussioncomment-6669190
+    # TL;DR:
+    # 1. Vectorize each block by their rectangle coordinates + length of text
+    # 2. Use DBSCAN to cheaply cluster these vectors
+    # 3. Only headers and footers are repetitive enough to form its own
+    # cluster, so they will have a unique label while the majority of blocks are
+    # classified as clusterless.
+    # 4. Remove blocks that belong to minority clusters.
+    dbscan = DBSCAN()
+    samples = np.array([(*block["bbox"], get_text_len(block)) for block in blocks])
+    dbscan.fit(samples)
+    labels = dbscan.labels_
+    label_counter = Counter(labels)
+    most_common_label = label_counter.most_common(1)[0][0]
+    return [block for block, label in zip(blocks, labels) if label == most_common_label]
+
+
+def remove_nonstandard_orientation(blocks):
+    orient_to_count = defaultdict(lambda: 0)
+    for block in blocks:
+        for line in block["lines"]:
+            orient_to_count[line["dir"]] += 1
+    #
+    sorted_count_orient_pairs = sorted(
+        [(count, orient) for orient, count in orient_to_count.items()]
+    )
+    _count, most_frequent_orientation = sorted_count_orient_pairs[-1]
+    #
+    return [
+        {
+            **block,
+            "lines": [
+                line
+                for line in block["lines"]
+                if line["dir"] == most_frequent_orientation
+            ],
+        }
+        for block in blocks
+    ]
+
+
+def strip_spaces(blocks):
+    return [
+        {
+            **block,
+            "lines": [
+                {
+                    **line,
+                    "spans": [
+                        {**span, "text": span["text"].strip()} for span in line["spans"]
+                    ],
+                }
+                for line in block["lines"]
+            ],
+        }
+        for block in blocks
+    ]
+
+
+def remove_empty_spans(blocks):
+    return [
+        {
+            **block,
+            "lines": [
+                {
+                    **line,
+                    "spans": [span for span in line["spans"] if len(span["text"]) > 0],
+                }
+                for line in block["lines"]
+            ],
+        }
+        for block in blocks
+    ]
+
+
+def remove_empty_lines(blocks):
+    return [
+        {**block, "lines": [line for line in block["lines"] if len(line["spans"]) > 0]}
+        for block in blocks
+    ]
+
+
+def remove_empty_blocks(blocks):
+    blocks = strip_spaces(blocks)
+    blocks = remove_empty_spans(blocks)
+    blocks = remove_empty_lines(blocks)
+    return [block for block in blocks if len(block["lines"]) > 0]
+
+
+def get_lines(blocks):
+    return [
+        {**line, "page_num": block["page_num"]}
+        for block in blocks
+        for line in block["lines"]
+    ]
+
+
+def set_line_text(lines):
+    """Assumes span texts are stripped of leading and trailing whitespaces."""
+    return [
+        {**line, "text": " ".join(span["text"] for span in line["spans"])}
+        for line in lines
+    ]
+
+
+def set_line_word_counts(lines):
+    """Assumes line texts are set"""
+    return [{**line, "word_count": line["text"].count(" ") + 1} for line in lines]
+
+
+def get_lines_with_first_n_words(lines, n):
+    total_len = 0
+    end = 0
+    while total_len < n:
+        total_len += lines[end]["word_count"]
+        end += 1
+    return " ".join(line["text"] for line in lines[:end])
+
+
+def estimate_section_titles(lines):
+    text_size_freq = defaultdict(int)
+    for line in lines:
+        for span in line["spans"]:
+            text_size_freq[int(span["size"])] += 1
+    most_common_text_size = sorted(text_size_freq.items(), key=lambda x: x[1])[-1][0]
+    current_section_title = ""
+    for i in range(len(lines)):
+        if lines[i]["spans"][0]["size"] > most_common_text_size:
+            current_section_title = lines[i]["text"]
+        lines[i]["section_title"] = current_section_title
+    return lines
+
+
+def process_tables(filename, lines):
+    pdf = pdfplumber.open(filename)
+    tables_by_page = [page.find_tables() for page in pdf.pages]
+
+    def within_by_bbox(a, b):
+        ax1, ay1, ax2, ay2 = a
+        bx1, by1, bx2, by2 = b
+
+        def within(val, lower, upper):
+            return lower < val and val < upper
+
+        return (
+            within(ax1, bx1, bx2)
+            and within(ax2, bx1, bx2)
+            and within(ay1, by1, by2)
+            and within(ay2, by1, by2)
+        )
+
+    new_lines = []
+    seen = defaultdict(bool)
+    for line in lines:
+        is_contained_by_table = False
+        for table in tables_by_page[line["page_num"]]:
+            if within_by_bbox(line["bbox"], table.bbox):
+                is_contained_by_table = True
+                if not seen[table]:
+                    # The format we want the table in for cold_start is different
+                    # than the format we'd like to display so we modify the "text"
+                    # field and we include some display metadata in the line
+                    # Chunking of text within tables is left for future work
+                    extracted_rows = table.extract()
+                    table_as_text = " ".join(
+                        [item or "" for row in extracted_rows for item in row]
+                    )
+                    new_lines.append(
+                        {
+                            **line,
+                            "text": table_as_text,
+                            "bbox": table.bbox,
+                            "table": extracted_rows,
+                        }
+                    )
+                    seen[table] = True
+        if not is_contained_by_table:
+            new_lines.append(line)
+
+    return new_lines
+
+
+def get_chunks_from_lines(lines, chunk_words, stride_words):
+    chunk_start = 0
+    chunks = []
+    chunk_boxes = []
+    section_titles = []
+    display = []
+    while chunk_start < len(lines):
+        chunk_end = chunk_start
+        chunk_size = 0
+        while chunk_size < chunk_words and chunk_end < len(lines):
+            chunk_size += lines[chunk_end]["word_count"]
+            chunk_end += 1
+        stride_end = chunk_start
+        stride_size = 0
+        while stride_size < stride_words and stride_end < len(lines):
+            stride_size += lines[stride_end]["word_count"]
+            stride_end += 1
+        # combine the chunks
+        chunks.append(" ".join(line["text"] for line in lines[chunk_start:chunk_end]))
+        # metadata for highlighting
+        chunk_boxes.append(
+            [(line["page_num"], line["bbox"]) for line in lines[chunk_start:chunk_end]]
+        )
+        # combine unique section titles
+        if "section_title" in lines[0]:
+            unique_section_titles = set(
+                [line["section_title"] for line in lines[chunk_start:chunk_end]]
+            )
+            section_titles.append(" ".join(unique_section_titles))
+        # formulate display text (separating out tables)
+        display.append(
+            " ".join(
+                [
+                    str(line["table"]) if "table" in line else line["text"]
+                    for line in lines[chunk_start:chunk_end]
+                ]
+            )
+        )
+        chunk_start = stride_end
+    return chunks, chunk_boxes, display, section_titles
+
+
+def clean_encoding(text):
+    return unidecode.unidecode(text.encode("utf-8", "replace").decode("utf-8"))
+
+
+def get_chunks(
+    filename,
+    chunk_words,
+    stride_words,
+    emphasize_first_n_words,
+    ignore_header_footer,
+    ignore_nonstandard_orientation,
+    emphasize_section_titles,
+    table_parsing,
+):
+    blocks = get_fitz_blocks(filename)
+    blocks = remove_images(blocks)
+    if ignore_header_footer:
+        blocks = remove_header_footer(blocks)
+    if ignore_nonstandard_orientation:
+        blocks = remove_nonstandard_orientation(blocks)
+    blocks = remove_empty_blocks(blocks)
+    lines = get_lines(blocks)
+    lines = set_line_text(lines)
+    if emphasize_section_titles:
+        lines = estimate_section_titles(lines)
+    if table_parsing:
+        lines = process_tables(filename, lines)
+    lines = set_line_word_counts(lines)
+    first_n_words = get_lines_with_first_n_words(lines, emphasize_first_n_words)
+    chunks, chunk_boxes, display, section_titles = get_chunks_from_lines(
+        lines, chunk_words, stride_words
+    )
+    chunks = [clean_encoding(text) for text in chunks]
+    section_titles = [clean_encoding(section_title) for section_title in section_titles]
+    return chunks, chunk_boxes, display, first_n_words, section_titles
+
+
+def make_df(
+    filename,
+    chunk_words,
+    stride_words,
+    emphasize_first_n_words,
+    ignore_header_footer,
+    ignore_nonstandard_orientation,
+    doc_keywords,
+    emphasize_section_titles,
+    table_parsing,
+):
+    """Arguments:
+    chunk_size: number of words in each chunk of text.
+    stride: number of words between each chunk of text.
+    emphasize_first_words: number of words at the beginning of the file
+        that will be used as strong column for all rows of the resulting
+        dataframe. We do this so that every row can capture important signals
+        like file titles or introductory paragraphs.
+    table_parsing: Whether to enable separate parsing of tables
+    """
+    chunks, chunk_boxes, display, first_n_words, section_titles = get_chunks(
+        filename,
+        chunk_words,
+        stride_words,
+        emphasize_first_n_words,
+        ignore_header_footer,
+        ignore_nonstandard_orientation,
+        emphasize_section_titles,
+        table_parsing,
+    )
+
+    emphasis = [
+        (
+            first_n_words
+            + " "
+            + doc_keywords
+            + " "
+            + (section_titles[i] if emphasize_section_titles else "")
+        )
+        for i in range(len(chunks))
+    ]
+
+    return pd.DataFrame(
+        {
+            "para": [c.lower() for c in chunks],
+            "display": display,
+            "emphasis": emphasis,
+            # chunk_boxes is a list of lists of (page_num, bbox) pairs
+            "chunk_boxes": [str(chunk_box) for chunk_box in chunk_boxes],
+            # get the first element of the first pair in the list of
+            # (page_num, bbox) pairs for each chunk.
+            "page": [chunk_box[0][0] for chunk_box in chunk_boxes],
+        }
+    )
+
+
+def highlighted_doc(source, columns):
+    if not "chunk_boxes" in columns:
+        return None
+    doc = fitz.open(source)
+    for page, box in eval(columns["chunk_boxes"]):
+        doc[page].add_highlight_annot(fitz.Rect(box))
+    return doc
```

## thirdai/neural_db/parsing_utils/unstructured_parse.py

 * *Ordering differences only*

```diff
@@ -1,232 +1,232 @@
-# https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/unstructured_file.html
-import re
-from dataclasses import dataclass
-from pathlib import Path
-from typing import List, Optional, Tuple, Type, TypeVar, Union, final
-
-import pandas as pd
-from langchain_community.document_loaders import (
-    UnstructuredEmailLoader,
-    UnstructuredFileLoader,
-    UnstructuredPowerPointLoader,
-)
-
-try:
-    from unstructured.cleaners.core import (
-        clean_bullets,
-        clean_extra_whitespace,
-        clean_ligatures,
-        clean_non_ascii_chars,
-        replace_mime_encodings,
-        replace_unicode_quotes,
-    )
-except Exception as e:
-    raise ModuleNotFoundError(
-        "To use NeuralDB with these document types please run: pip3 install unstructured[all-docs]"
-    )
-
-
-from .utils import (
-    chunk_text,
-    clean_text,
-    clean_text_and_remove_urls,
-    ensure_valid_encoding,
-)
-
-PPTX_CHUNK_THRESHOLD: final = 30
-
-
-@dataclass
-class UnstructuredParagraph:
-    para: str
-    filename: str
-    filetype: str
-    page: Optional[int]
-    display: str
-
-
-@dataclass
-class EmlParagraph(UnstructuredParagraph):
-    subject: str
-    sent_from: str
-    sent_to: str
-
-
-class UnstructuredParse:
-    def __init__(self, filepath: str):
-        self._filepath = filepath
-        self._filename = str(Path(filepath).name)
-        self._ext = Path(filepath).suffix[1:]  # Removing '.' from the extension
-        self._post_processors = [
-            clean_extra_whitespace,
-            clean_non_ascii_chars,
-            clean_bullets,
-            clean_ligatures,
-            replace_unicode_quotes,
-            replace_mime_encodings,
-        ]
-        self._error_msg = f"Cannot process {self._ext} file: {self._filepath}"
-
-    def process_elements(
-        self,
-    ) -> Tuple[Union[List[Type[UnstructuredParagraph]], str], bool]:
-        raise NotImplementedError()
-
-    def create_train_df(
-        self, paragraphs: List[Type[UnstructuredParagraph]]
-    ) -> pd.DataFrame:
-        columns = paragraphs[0].__dict__.keys()
-        df = pd.DataFrame(index=range(len(paragraphs)), columns=columns)
-
-        for i, elem in enumerate(paragraphs):
-            df.iloc[i] = elem.__dict__
-
-        for column in ["para", "display"]:
-            df[column] = df[column].apply(ensure_valid_encoding)
-        return df
-
-
-class PptxParse(UnstructuredParse):
-    def __init__(self, filepath: str):
-        super().__init__(filepath)
-        try:
-            self.PptxLoader = UnstructuredPowerPointLoader(
-                file_path=self._filepath,
-                mode="paged",
-                post_processors=self._post_processors,
-            )
-        except Exception as e:
-            print(str(e))
-            print(self._error_msg)
-
-    def process_elements(
-        self,
-    ) -> Tuple[Union[List[Type[UnstructuredParagraph]], str], bool]:
-        paragraphs = []
-        try:
-            docs = self.PptxLoader.load()
-            current_text = ""
-            last_page_no = len(docs)
-            for doc in docs:
-                text = clean_text(text=current_text + " " + doc.page_content)
-                chunks = chunk_text(text)
-                if len(chunks[-1]) < PPTX_CHUNK_THRESHOLD:
-                    if len(chunks) == 1:
-                        if last_page_no != doc.metadata["page_number"]:
-                            current_text = text
-                            continue
-                        elif len(paragraphs) > 0:
-                            paragraphs[-1].para += " " + text
-                            continue
-                    else:
-                        chunks[-2] += " " + chunks[-1]
-                        chunks.pop()
-
-                rows = [
-                    UnstructuredParagraph(
-                        para=chunk,
-                        filename=self._filename,
-                        filetype=self._ext,
-                        page=doc.metadata["page_number"],
-                        display=str(chunk.replace("\n", " ")),
-                    )
-                    for chunk in chunks
-                ]
-                paragraphs.extend(rows)
-
-            return (
-                (paragraphs, True)
-                if len(paragraphs) > 0
-                else (f"Empty pptx file OR {self._error_msg}", False)
-            )
-        except Exception as e:
-            print(str(e))
-            return self._error_msg, False
-
-
-class EmlParse(UnstructuredParse):
-    def __init__(self, filepath: str):
-        super().__init__(filepath)
-        try:
-            self.EmlLoader = UnstructuredEmailLoader(
-                file_path=self._filepath,
-                mode="elements",
-                post_processors=self._post_processors,
-            )
-        except Exception as e:
-            print(str(e))
-            print(self._error_msg)
-
-    def process_elements(
-        self,
-    ) -> Tuple[Union[List[Type[UnstructuredParagraph]], str], bool]:
-        try:
-            docs = self.EmlLoader.load()
-            text = ""
-            for doc in docs:
-                content = doc.page_content
-                text += clean_text_and_remove_urls(content) + " "
-            text = re.sub(pattern=r"\s+", repl=" ", string=text).strip()
-
-            paragraphs = [
-                EmlParagraph(
-                    para=chunk,
-                    filename=self._filename,
-                    filetype=self._ext,
-                    page=None,
-                    display=chunk,
-                    subject=doc.metadata["subject"],
-                    sent_from=",".join(doc.metadata["sent_from"]),
-                    sent_to=",".join(doc.metadata["sent_to"]),
-                )
-                for chunk in chunk_text(text)
-            ]
-
-            return (
-                (paragraphs, True)
-                if len(paragraphs) > 0
-                else (f"Empty eml file OR {self._error_msg}", False)
-            )
-        except Exception as e:
-            print(str(e))
-            return self._error_msg, False
-
-
-class TxtParse(UnstructuredParse):
-    def __init__(self, filepath: str):
-        super().__init__(filepath)
-        try:
-            self.TxtLoader = UnstructuredFileLoader(
-                file_path=self._filepath,
-                mode="single",
-                post_processors=self._post_processors,
-            )
-        except Exception as e:
-            print(str(e))
-            print(self._error_msg)
-
-    def process_elements(
-        self,
-    ) -> Tuple[Union[List[Type[UnstructuredParagraph]], str], bool]:
-        try:
-            doc = self.TxtLoader.load()
-            content = clean_text(doc[0].page_content)
-
-            paragraphs = [
-                UnstructuredParagraph(
-                    para=chunk,
-                    filename=self._filename,
-                    filetype=self._ext,
-                    page=None,
-                    display=chunk,
-                )
-                for chunk in chunk_text(content)
-            ]
-            return (
-                (paragraphs, True)
-                if len(paragraphs) > 0
-                else (f"Empty txt file OR {self._error_msg}", False)
-            )
-        except Exception as e:
-            print(str(e))
-            return self._error_msg, False
+# https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/unstructured_file.html
+import re
+from dataclasses import dataclass
+from pathlib import Path
+from typing import List, Optional, Tuple, Type, TypeVar, Union, final
+
+import pandas as pd
+from langchain_community.document_loaders import (
+    UnstructuredEmailLoader,
+    UnstructuredFileLoader,
+    UnstructuredPowerPointLoader,
+)
+
+try:
+    from unstructured.cleaners.core import (
+        clean_bullets,
+        clean_extra_whitespace,
+        clean_ligatures,
+        clean_non_ascii_chars,
+        replace_mime_encodings,
+        replace_unicode_quotes,
+    )
+except Exception as e:
+    raise ModuleNotFoundError(
+        "To use NeuralDB with these document types please run: pip3 install unstructured[all-docs]"
+    )
+
+
+from .utils import (
+    chunk_text,
+    clean_text,
+    clean_text_and_remove_urls,
+    ensure_valid_encoding,
+)
+
+PPTX_CHUNK_THRESHOLD: final = 30
+
+
+@dataclass
+class UnstructuredParagraph:
+    para: str
+    filename: str
+    filetype: str
+    page: Optional[int]
+    display: str
+
+
+@dataclass
+class EmlParagraph(UnstructuredParagraph):
+    subject: str
+    sent_from: str
+    sent_to: str
+
+
+class UnstructuredParse:
+    def __init__(self, filepath: str):
+        self._filepath = filepath
+        self._filename = str(Path(filepath).name)
+        self._ext = Path(filepath).suffix[1:]  # Removing '.' from the extension
+        self._post_processors = [
+            clean_extra_whitespace,
+            clean_non_ascii_chars,
+            clean_bullets,
+            clean_ligatures,
+            replace_unicode_quotes,
+            replace_mime_encodings,
+        ]
+        self._error_msg = f"Cannot process {self._ext} file: {self._filepath}"
+
+    def process_elements(
+        self,
+    ) -> Tuple[Union[List[Type[UnstructuredParagraph]], str], bool]:
+        raise NotImplementedError()
+
+    def create_train_df(
+        self, paragraphs: List[Type[UnstructuredParagraph]]
+    ) -> pd.DataFrame:
+        columns = paragraphs[0].__dict__.keys()
+        df = pd.DataFrame(index=range(len(paragraphs)), columns=columns)
+
+        for i, elem in enumerate(paragraphs):
+            df.iloc[i] = elem.__dict__
+
+        for column in ["para", "display"]:
+            df[column] = df[column].apply(ensure_valid_encoding)
+        return df
+
+
+class PptxParse(UnstructuredParse):
+    def __init__(self, filepath: str):
+        super().__init__(filepath)
+        try:
+            self.PptxLoader = UnstructuredPowerPointLoader(
+                file_path=self._filepath,
+                mode="paged",
+                post_processors=self._post_processors,
+            )
+        except Exception as e:
+            print(str(e))
+            print(self._error_msg)
+
+    def process_elements(
+        self,
+    ) -> Tuple[Union[List[Type[UnstructuredParagraph]], str], bool]:
+        paragraphs = []
+        try:
+            docs = self.PptxLoader.load()
+            current_text = ""
+            last_page_no = len(docs)
+            for doc in docs:
+                text = clean_text(text=current_text + " " + doc.page_content)
+                chunks = chunk_text(text)
+                if len(chunks[-1]) < PPTX_CHUNK_THRESHOLD:
+                    if len(chunks) == 1:
+                        if last_page_no != doc.metadata["page_number"]:
+                            current_text = text
+                            continue
+                        elif len(paragraphs) > 0:
+                            paragraphs[-1].para += " " + text
+                            continue
+                    else:
+                        chunks[-2] += " " + chunks[-1]
+                        chunks.pop()
+
+                rows = [
+                    UnstructuredParagraph(
+                        para=chunk,
+                        filename=self._filename,
+                        filetype=self._ext,
+                        page=doc.metadata["page_number"],
+                        display=str(chunk.replace("\n", " ")),
+                    )
+                    for chunk in chunks
+                ]
+                paragraphs.extend(rows)
+
+            return (
+                (paragraphs, True)
+                if len(paragraphs) > 0
+                else (f"Empty pptx file OR {self._error_msg}", False)
+            )
+        except Exception as e:
+            print(str(e))
+            return self._error_msg, False
+
+
+class EmlParse(UnstructuredParse):
+    def __init__(self, filepath: str):
+        super().__init__(filepath)
+        try:
+            self.EmlLoader = UnstructuredEmailLoader(
+                file_path=self._filepath,
+                mode="elements",
+                post_processors=self._post_processors,
+            )
+        except Exception as e:
+            print(str(e))
+            print(self._error_msg)
+
+    def process_elements(
+        self,
+    ) -> Tuple[Union[List[Type[UnstructuredParagraph]], str], bool]:
+        try:
+            docs = self.EmlLoader.load()
+            text = ""
+            for doc in docs:
+                content = doc.page_content
+                text += clean_text_and_remove_urls(content) + " "
+            text = re.sub(pattern=r"\s+", repl=" ", string=text).strip()
+
+            paragraphs = [
+                EmlParagraph(
+                    para=chunk,
+                    filename=self._filename,
+                    filetype=self._ext,
+                    page=None,
+                    display=chunk,
+                    subject=doc.metadata["subject"],
+                    sent_from=",".join(doc.metadata["sent_from"]),
+                    sent_to=",".join(doc.metadata["sent_to"]),
+                )
+                for chunk in chunk_text(text)
+            ]
+
+            return (
+                (paragraphs, True)
+                if len(paragraphs) > 0
+                else (f"Empty eml file OR {self._error_msg}", False)
+            )
+        except Exception as e:
+            print(str(e))
+            return self._error_msg, False
+
+
+class TxtParse(UnstructuredParse):
+    def __init__(self, filepath: str):
+        super().__init__(filepath)
+        try:
+            self.TxtLoader = UnstructuredFileLoader(
+                file_path=self._filepath,
+                mode="single",
+                post_processors=self._post_processors,
+            )
+        except Exception as e:
+            print(str(e))
+            print(self._error_msg)
+
+    def process_elements(
+        self,
+    ) -> Tuple[Union[List[Type[UnstructuredParagraph]], str], bool]:
+        try:
+            doc = self.TxtLoader.load()
+            content = clean_text(doc[0].page_content)
+
+            paragraphs = [
+                UnstructuredParagraph(
+                    para=chunk,
+                    filename=self._filename,
+                    filetype=self._ext,
+                    page=None,
+                    display=chunk,
+                )
+                for chunk in chunk_text(content)
+            ]
+            return (
+                (paragraphs, True)
+                if len(paragraphs) > 0
+                else (f"Empty txt file OR {self._error_msg}", False)
+            )
+        except Exception as e:
+            print(str(e))
+            return self._error_msg, False
```

## thirdai/neural_db/parsing_utils/url_parse.py

 * *Ordering differences only*

```diff
@@ -1,127 +1,127 @@
-from collections import deque
-from urllib.parse import urljoin
-
-import pandas as pd
-import requests
-from bs4 import BeautifulSoup
-from langchain.text_splitter import RecursiveCharacterTextSplitter
-from nltk.tokenize import sent_tokenize
-from trafilatura import extract
-from trafilatura.settings import use_config
-from url_normalize import url_normalize
-
-from .utils import ensure_valid_encoding
-
-# Set headers of request to mimic a browser to retrieve the same html rendered in a browser
-# https://stackoverflow.com/questions/27652543/how-to-use-python-requests-to-fake-a-browser-visit-a-k-a-and-generate-user-agent
-HEADERS = {
-    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36"
-}
-
-config = use_config()
-config.set("DEFAULT", "EXTRACTION_TIMEOUT", "0")
-
-
-def recursive_url_scrape(base_url, max_crawl_depth, match_url_prefix=True):
-    crawled_urls = set()
-    valid_urls = set()
-    queue = deque([(base_url, 0)])
-    crawled_urls.add(base_url)
-    print(f"crawl depth: {max_crawl_depth}", flush=True)
-
-    while queue:
-        url, depth = queue.popleft()
-
-        if depth > max_crawl_depth:
-            continue
-
-        try:
-            response = requests.get(url, headers=HEADERS)
-            if response.status_code != 200:
-                print(
-                    f"Skipping {url} (status code {response.status_code})", flush=True
-                )
-                continue
-            else:
-                print(f"Adding {url}", flush=True)
-                valid_urls.add((url, response))
-        except Exception as error:
-            print(f"Skipping {url} (with error {error})", flush=True)
-            continue
-
-        soup = BeautifulSoup(response.content, "html.parser")
-        for a_tag in soup.find_all("a", href=True):
-            href = a_tag["href"]
-            next_url = url_normalize(urljoin(base_url, href))
-
-            if next_url not in crawled_urls and (
-                next_url[: len(base_url)] == base_url or not match_url_prefix
-            ):
-                crawled_urls.add(next_url)
-                queue.append((next_url, depth + 1))
-
-    print(f"Total links found: {len(valid_urls)}", flush=True)
-    return valid_urls
-
-
-def process_url(url, response):
-    text_splitter = RecursiveCharacterTextSplitter(
-        chunk_size=500,
-        chunk_overlap=75,
-        length_function=len,
-    )
-    elem = []
-
-    if not response:
-        try:
-            response = requests.get(url, headers=HEADERS)
-        except:
-            return f"cannot extract text from {url}", False
-
-    if response.status_code != 200:
-        return f"cannot extract text from {url}", False
-
-    index = response.text
-    title_start_pos = index.find("<title>") + len("<title>")
-    title_end_pos = index.find("</title>")
-    title = index[title_start_pos:title_end_pos]
-    result = extract(
-        index,
-        include_formatting=False,
-        include_comments=False,
-        include_links=False,
-        include_tables=False,
-        favor_precision=True,
-        config=config,
-    )
-    if not result:
-        return f"cannot extract text from {url}", False
-
-    texts = text_splitter.split_text(result)
-
-    for text in texts:
-        text = str(text).strip().replace("\r\n", " ").replace("\n", " ")
-        row = [text, url, title]
-        elem.append(row)
-
-    return elem, True
-
-
-def create_train_df(elements):
-    df = pd.DataFrame(
-        index=range(len(elements)),
-        columns=["text", "display", "url", "title"],
-    )
-    for i, elem in enumerate(elements):
-        sents = sent_tokenize(elem[0])
-        sents = list(map(lambda x: x.lower(), sents))
-        text = " ".join(sents)
-        df.iloc[i] = [
-            text,
-            elem[0],
-            elem[1],
-            elem[2],
-        ]
-    for column in ["text", "display"]:
-        df[column] = df[column].apply(ensure_valid_encoding)
-    return df
+from collections import deque
+from urllib.parse import urljoin
+
+import pandas as pd
+import requests
+from bs4 import BeautifulSoup
+from langchain.text_splitter import RecursiveCharacterTextSplitter
+from nltk.tokenize import sent_tokenize
+from trafilatura import extract
+from trafilatura.settings import use_config
+from url_normalize import url_normalize
+
+from .utils import ensure_valid_encoding
+
+# Set headers of request to mimic a browser to retrieve the same html rendered in a browser
+# https://stackoverflow.com/questions/27652543/how-to-use-python-requests-to-fake-a-browser-visit-a-k-a-and-generate-user-agent
+HEADERS = {
+    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36"
+}
+
+config = use_config()
+config.set("DEFAULT", "EXTRACTION_TIMEOUT", "0")
+
+
+def recursive_url_scrape(base_url, max_crawl_depth, match_url_prefix=True):
+    crawled_urls = set()
+    valid_urls = set()
+    queue = deque([(base_url, 0)])
+    crawled_urls.add(base_url)
+    print(f"crawl depth: {max_crawl_depth}", flush=True)
+
+    while queue:
+        url, depth = queue.popleft()
+
+        if depth > max_crawl_depth:
+            continue
+
+        try:
+            response = requests.get(url, headers=HEADERS)
+            if response.status_code != 200:
+                print(
+                    f"Skipping {url} (status code {response.status_code})", flush=True
+                )
+                continue
+            else:
+                print(f"Adding {url}", flush=True)
+                valid_urls.add((url, response))
+        except Exception as error:
+            print(f"Skipping {url} (with error {error})", flush=True)
+            continue
+
+        soup = BeautifulSoup(response.content, "html.parser")
+        for a_tag in soup.find_all("a", href=True):
+            href = a_tag["href"]
+            next_url = url_normalize(urljoin(base_url, href))
+
+            if next_url not in crawled_urls and (
+                next_url[: len(base_url)] == base_url or not match_url_prefix
+            ):
+                crawled_urls.add(next_url)
+                queue.append((next_url, depth + 1))
+
+    print(f"Total links found: {len(valid_urls)}", flush=True)
+    return valid_urls
+
+
+def process_url(url, response):
+    text_splitter = RecursiveCharacterTextSplitter(
+        chunk_size=500,
+        chunk_overlap=75,
+        length_function=len,
+    )
+    elem = []
+
+    if not response:
+        try:
+            response = requests.get(url, headers=HEADERS)
+        except:
+            return f"cannot extract text from {url}", False
+
+    if response.status_code != 200:
+        return f"cannot extract text from {url}", False
+
+    index = response.text
+    title_start_pos = index.find("<title>") + len("<title>")
+    title_end_pos = index.find("</title>")
+    title = index[title_start_pos:title_end_pos]
+    result = extract(
+        index,
+        include_formatting=False,
+        include_comments=False,
+        include_links=False,
+        include_tables=False,
+        favor_precision=True,
+        config=config,
+    )
+    if not result:
+        return f"cannot extract text from {url}", False
+
+    texts = text_splitter.split_text(result)
+
+    for text in texts:
+        text = str(text).strip().replace("\r\n", " ").replace("\n", " ")
+        row = [text, url, title]
+        elem.append(row)
+
+    return elem, True
+
+
+def create_train_df(elements):
+    df = pd.DataFrame(
+        index=range(len(elements)),
+        columns=["text", "display", "url", "title"],
+    )
+    for i, elem in enumerate(elements):
+        sents = sent_tokenize(elem[0])
+        sents = list(map(lambda x: x.lower(), sents))
+        text = " ".join(sents)
+        df.iloc[i] = [
+            text,
+            elem[0],
+            elem[1],
+            elem[2],
+        ]
+    for column in ["text", "display"]:
+        df[column] = df[column].apply(ensure_valid_encoding)
+    return df
```

## thirdai/neural_db/parsing_utils/utils.py

 * *Ordering differences only*

```diff
@@ -1,100 +1,100 @@
-import re
-
-import unidecode
-from langchain.text_splitter import RecursiveCharacterTextSplitter
-from nltk.tokenize import sent_tokenize, word_tokenize
-
-ATTACH_N_WORD_THRESHOLD = 20
-MIN_WORDS_PER_CHUNK = 50
-CHUNK_THRESHOLD = 150
-MAX_CHUNK_LEN = 750
-MIN_CHUNK_LEN = 20
-
-
-# Convert a string to a unicode string
-def ensure_valid_encoding(text):
-    return unidecode.unidecode(text.encode("utf-8", "replace").decode("utf-8"))
-
-
-# Validates a given chunk based on some rules
-def valid_chunk(chunk):
-    # Check if chunk is large enough
-    if len(chunk) < MIN_CHUNK_LEN:
-        return False
-
-    # Check if chunk contains enough alphabet characters
-    if sum(c.isalpha() for c in chunk) < len(chunk) / 2:
-        return False
-
-    return True
-
-
-# Split a chunk into smaller chunks if the original chunk size is greater than MAX_CHUNK_LEN
-def split_large_chunk(chunk, max_len, text_splitter):
-    if len(chunk) > max_len:
-        return text_splitter.split_text(chunk)
-    return [chunk]
-
-
-def chunk_text(text: str):
-    sentences = sent_tokenize(text)
-    if len(sentences) == 1:
-        return [text] if valid_chunk(text) else []
-
-    words_per_sentence = [len(word_tokenize(sent)) for sent in sentences]
-    if sum(words_per_sentence) < CHUNK_THRESHOLD:
-        return [text] if valid_chunk(text) else []
-
-    chunks = []
-    cur_word_count = 0
-    start_idx = 0
-
-    for idx in range(len(sentences)):
-        word_count = words_per_sentence[idx]
-        if cur_word_count < MIN_WORDS_PER_CHUNK:
-            cur_word_count += word_count
-        else:
-            chunks.append(" ".join(sentences[start_idx:idx]))
-            start_idx = idx
-            cur_word_count = word_count
-
-    if start_idx != len(sentences):
-        final_chunk = " ".join(sentences[start_idx : len(sentences)])
-        if len(chunks) > 0 and cur_word_count < MIN_WORDS_PER_CHUNK:
-            chunks[-1] += final_chunk
-        else:
-            chunks.append(final_chunk)
-
-    text_splitter = RecursiveCharacterTextSplitter(
-        chunk_size=750,
-        chunk_overlap=0,
-        length_function=len,
-    )
-
-    chunks = [
-        sub_chunk
-        for chunk in chunks
-        for sub_chunk in split_large_chunk(chunk, MAX_CHUNK_LEN, text_splitter)
-    ]
-    chunks = [chunk for chunk in chunks if valid_chunk(chunk)]
-
-    return chunks
-
-
-def clean_text_and_remove_urls(text: str) -> str:
-    text = clean_text(text)
-    text = re.sub(r"http\S+", "", text, flags=re.MULTILINE)
-    return text
-
-
-def clean_text(text: str) -> str:
-    text = (
-        str(text)
-        .strip()
-        .replace("\r\n", " ")
-        .replace("\n", " ")
-        .replace("\t", " ")
-        .lower()
-    )
-
-    return text
+import re
+
+import unidecode
+from langchain.text_splitter import RecursiveCharacterTextSplitter
+from nltk.tokenize import sent_tokenize, word_tokenize
+
+ATTACH_N_WORD_THRESHOLD = 20
+MIN_WORDS_PER_CHUNK = 50
+CHUNK_THRESHOLD = 150
+MAX_CHUNK_LEN = 750
+MIN_CHUNK_LEN = 20
+
+
+# Convert a string to a unicode string
+def ensure_valid_encoding(text):
+    return unidecode.unidecode(text.encode("utf-8", "replace").decode("utf-8"))
+
+
+# Validates a given chunk based on some rules
+def valid_chunk(chunk):
+    # Check if chunk is large enough
+    if len(chunk) < MIN_CHUNK_LEN:
+        return False
+
+    # Check if chunk contains enough alphabet characters
+    if sum(c.isalpha() for c in chunk) < len(chunk) / 2:
+        return False
+
+    return True
+
+
+# Split a chunk into smaller chunks if the original chunk size is greater than MAX_CHUNK_LEN
+def split_large_chunk(chunk, max_len, text_splitter):
+    if len(chunk) > max_len:
+        return text_splitter.split_text(chunk)
+    return [chunk]
+
+
+def chunk_text(text: str):
+    sentences = sent_tokenize(text)
+    if len(sentences) == 1:
+        return [text] if valid_chunk(text) else []
+
+    words_per_sentence = [len(word_tokenize(sent)) for sent in sentences]
+    if sum(words_per_sentence) < CHUNK_THRESHOLD:
+        return [text] if valid_chunk(text) else []
+
+    chunks = []
+    cur_word_count = 0
+    start_idx = 0
+
+    for idx in range(len(sentences)):
+        word_count = words_per_sentence[idx]
+        if cur_word_count < MIN_WORDS_PER_CHUNK:
+            cur_word_count += word_count
+        else:
+            chunks.append(" ".join(sentences[start_idx:idx]))
+            start_idx = idx
+            cur_word_count = word_count
+
+    if start_idx != len(sentences):
+        final_chunk = " ".join(sentences[start_idx : len(sentences)])
+        if len(chunks) > 0 and cur_word_count < MIN_WORDS_PER_CHUNK:
+            chunks[-1] += final_chunk
+        else:
+            chunks.append(final_chunk)
+
+    text_splitter = RecursiveCharacterTextSplitter(
+        chunk_size=750,
+        chunk_overlap=0,
+        length_function=len,
+    )
+
+    chunks = [
+        sub_chunk
+        for chunk in chunks
+        for sub_chunk in split_large_chunk(chunk, MAX_CHUNK_LEN, text_splitter)
+    ]
+    chunks = [chunk for chunk in chunks if valid_chunk(chunk)]
+
+    return chunks
+
+
+def clean_text_and_remove_urls(text: str) -> str:
+    text = clean_text(text)
+    text = re.sub(r"http\S+", "", text, flags=re.MULTILINE)
+    return text
+
+
+def clean_text(text: str) -> str:
+    text = (
+        str(text)
+        .strip()
+        .replace("\r\n", " ")
+        .replace("\n", " ")
+        .replace("\t", " ")
+        .lower()
+    )
+
+    return text
```

## thirdai/neural_db/parsing_utils/__init__.py

 * *Ordering differences only*

```diff
@@ -1 +1 @@
-from .url_parse import recursive_url_scrape
+from .url_parse import recursive_url_scrape
```

## thirdai/neural_db/trainer/checkpoint_config.py

 * *Ordering differences only*

```diff
@@ -1,91 +1,91 @@
-from dataclasses import dataclass
-from pathlib import Path
-
-from ..utils import convert_str_to_path
-
-
-@dataclass
-class CheckpointConfig:
-    """
-    Configuration for checkpointing NeuralDB during document insertion.
-
-    Args:
-        checkpoint_dir (Path): Directory where models and related metadata will be stored.
-        resume_from_checkpoint (bool, optional): If a checkpoint exists, set to True to resume, else False. Defaults to False.
-        checkpoint_interval (int, optional): Number of epochs between checkpoints. Defaults to 1.
-    """
-
-    checkpoint_dir: Path
-    resume_from_checkpoint: bool = False
-    checkpoint_interval: int = 1
-
-    def __post_init__(self):
-        self.checkpoint_dir = convert_str_to_path(self.checkpoint_dir)
-
-        # After the completion of training, we store the trained neural db at ndb_trained_path
-        self._ndb_trained_path = self.checkpoint_dir / "trained.ndb"
-
-        self._pickled_documents_ids_resource_name_path = (
-            self.checkpoint_dir / "documents_ids_resource_name.pkl"
-        )
-
-    @property
-    def ndb_trained_path(self):
-        if self._ndb_trained_path is None:
-            raise Exception(
-                "Invalid Access: The 'ndb_trained_path' property is only"
-                " accessible when called by a NeuralDB object within its valid context."
-                " Currently, this property is set to None. Ensure that you are"
-                " accessing 'ndb_trained_path' from a properly initialized NeuralDB"
-                " instance with a valid configuration."
-            )
-        return self._ndb_trained_path
-
-    @property
-    def pickled_documents_ids_resource_name_path(self):
-        if self._pickled_documents_ids_resource_name_path is None:
-            raise Exception(
-                "Invalid Access: The 'pickled_documents_ids_resource_name_path' property is only"
-                " accessible when called by a NeuralDB object within its valid context."
-                " Currently, this property is set to None. Ensure that you are"
-                " accessing 'pickled_documents_ids_resource_name_path' from a properly"
-                " initialized NeuralDB instance with a valid configuration."
-            )
-        return self._pickled_documents_ids_resource_name_path
-
-    def get_mach_config(self):
-        """
-        This function sets the attributes specific to neural db to None so that we do not make any bad accesses. Ideally, Model object should have no idea about neural db and hence, it should also not be able to access any attributes that disclose any information about neural db
-        """
-        config = CheckpointConfig(
-            checkpoint_dir=self.checkpoint_dir,
-            resume_from_checkpoint=self.resume_from_checkpoint,
-            checkpoint_interval=self.checkpoint_interval,
-        )
-        config._ndb_trained_path = None
-        config._pickled_documents_ids_resource_name_path = None
-        return config
-
-
-def generate_checkpoint_configs_for_ensembles(
-    config: CheckpointConfig, number_ensembles: int, number_models_per_ensemble: int
-):
-    """
-    We maintain a checkpoint config for each Mach model in the Mixture while training. This is designed so that Mach models can maintain their training state independent of their Mixture which is necessary for distributed training.
-    """
-    if config:
-        return [
-            [
-                CheckpointConfig(
-                    config.checkpoint_dir
-                    / f"ensemble_{str(ensemble_id)}"
-                    / str(model_id),
-                    config.resume_from_checkpoint,
-                    config.checkpoint_interval,
-                ).get_mach_config()
-                for model_id in range(number_models_per_ensemble)
-            ]
-            for ensemble_id in range(number_ensembles)
-        ]
-    else:
-        return [[None] * number_models_per_ensemble] * number_ensembles
+from dataclasses import dataclass
+from pathlib import Path
+
+from ..utils import convert_str_to_path
+
+
+@dataclass
+class CheckpointConfig:
+    """
+    Configuration for checkpointing NeuralDB during document insertion.
+
+    Args:
+        checkpoint_dir (Path): Directory where models and related metadata will be stored.
+        resume_from_checkpoint (bool, optional): If a checkpoint exists, set to True to resume, else False. Defaults to False.
+        checkpoint_interval (int, optional): Number of epochs between checkpoints. Defaults to 1.
+    """
+
+    checkpoint_dir: Path
+    resume_from_checkpoint: bool = False
+    checkpoint_interval: int = 1
+
+    def __post_init__(self):
+        self.checkpoint_dir = convert_str_to_path(self.checkpoint_dir)
+
+        # After the completion of training, we store the trained neural db at ndb_trained_path
+        self._ndb_trained_path = self.checkpoint_dir / "trained.ndb"
+
+        self._pickled_documents_ids_resource_name_path = (
+            self.checkpoint_dir / "documents_ids_resource_name.pkl"
+        )
+
+    @property
+    def ndb_trained_path(self):
+        if self._ndb_trained_path is None:
+            raise Exception(
+                "Invalid Access: The 'ndb_trained_path' property is only"
+                " accessible when called by a NeuralDB object within its valid context."
+                " Currently, this property is set to None. Ensure that you are"
+                " accessing 'ndb_trained_path' from a properly initialized NeuralDB"
+                " instance with a valid configuration."
+            )
+        return self._ndb_trained_path
+
+    @property
+    def pickled_documents_ids_resource_name_path(self):
+        if self._pickled_documents_ids_resource_name_path is None:
+            raise Exception(
+                "Invalid Access: The 'pickled_documents_ids_resource_name_path' property is only"
+                " accessible when called by a NeuralDB object within its valid context."
+                " Currently, this property is set to None. Ensure that you are"
+                " accessing 'pickled_documents_ids_resource_name_path' from a properly"
+                " initialized NeuralDB instance with a valid configuration."
+            )
+        return self._pickled_documents_ids_resource_name_path
+
+    def get_mach_config(self):
+        """
+        This function sets the attributes specific to neural db to None so that we do not make any bad accesses. Ideally, Model object should have no idea about neural db and hence, it should also not be able to access any attributes that disclose any information about neural db
+        """
+        config = CheckpointConfig(
+            checkpoint_dir=self.checkpoint_dir,
+            resume_from_checkpoint=self.resume_from_checkpoint,
+            checkpoint_interval=self.checkpoint_interval,
+        )
+        config._ndb_trained_path = None
+        config._pickled_documents_ids_resource_name_path = None
+        return config
+
+
+def generate_checkpoint_configs_for_ensembles(
+    config: CheckpointConfig, number_ensembles: int, number_models_per_ensemble: int
+):
+    """
+    We maintain a checkpoint config for each Mach model in the Mixture while training. This is designed so that Mach models can maintain their training state independent of their Mixture which is necessary for distributed training.
+    """
+    if config:
+        return [
+            [
+                CheckpointConfig(
+                    config.checkpoint_dir
+                    / f"ensemble_{str(ensemble_id)}"
+                    / str(model_id),
+                    config.resume_from_checkpoint,
+                    config.checkpoint_interval,
+                ).get_mach_config()
+                for model_id in range(number_models_per_ensemble)
+            ]
+            for ensemble_id in range(number_ensembles)
+        ]
+    else:
+        return [[None] * number_models_per_ensemble] * number_ensembles
```

## thirdai/neural_db/trainer/training_data_manager.py

 * *Ordering differences only*

```diff
@@ -1,209 +1,209 @@
-from __future__ import annotations
-
-import shutil
-from pathlib import Path
-from typing import Optional, Union
-
-from ..documents import DocumentDataSource
-from ..supervised_datasource import SupDataSource
-from ..utils import assert_file_exists, move_between_directories, unpickle_from
-from .training_progress_tracker import (
-    InsertProgressTracker,
-    NeuralDbProgressTracker,
-    SupervisedProgressTracker,
-)
-
-
-class SupervisedDataManager:
-    def __init__(self, checkpoint_dir: Optional[Path], train_source: SupDataSource):
-        self.checkpoint_dir = checkpoint_dir
-        self.train_source = train_source
-
-        if self.checkpoint_dir:
-            self.train_source_folder = self.checkpoint_dir / "sup_source"
-            self.train_source_folder.mkdir(exist_ok=True, parents=True)
-
-    def save(self):
-        if self.checkpoint_dir:
-            self.train_source.save(self.train_source_folder)
-        else:
-            raise Exception(
-                "Invalid method call: 'save' operation for SupervisedDataManager cannot"
-                " be executed because 'checkpoint_dir' is None. Please provide a valid"
-                " directory path for 'checkpoint_dir' to proceed with the save"
-                " operation."
-            )
-
-    @staticmethod
-    def load(checkpoint_dir: Path):
-        manager = SupervisedDataManager(checkpoint_dir, None)
-        manager.train_source = SupDataSource.load(path=manager.train_source_folder)
-        return manager
-
-
-class InsertDataManager:
-    """
-    This class is used for saving and loading the intro and the train sources for the insert method.
-    """
-
-    def __init__(
-        self,
-        checkpoint_dir: Optional[Path],
-        intro_source: DocumentDataSource,
-        train_source: DocumentDataSource,
-    ):
-        self.checkpoint_dir = checkpoint_dir
-        if self.checkpoint_dir:
-            self.intro_source_folder = self.checkpoint_dir / "intro_source"
-            self.train_source_folder = self.checkpoint_dir / "train_source"
-            self.intro_source_folder.mkdir(exist_ok=True, parents=True)
-            self.train_source_folder.mkdir(exist_ok=True)
-
-        self.intro_source = intro_source
-        self.train_source = train_source
-
-    def save(self):
-        if self.checkpoint_dir:
-            self.intro_source.save(path=self.intro_source_folder)
-            self.train_source.save(path=self.train_source_folder)
-        else:
-            raise Exception(
-                "Invalid method call: 'save' operation for InsertDataManager cannot"
-                " be executed because 'checkpoint_dir' is None. Please provide a valid"
-                " directory path for 'checkpoint_dir' to proceed with the save"
-                " operation."
-            )
-
-    @staticmethod
-    def load(
-        checkpoint_dir: Path,
-    ):
-        manager = InsertDataManager(checkpoint_dir, None, None)
-        manager.intro_source = DocumentDataSource.load(path=manager.intro_source_folder)
-        manager.train_source = DocumentDataSource.load(path=manager.train_source_folder)
-        return manager
-
-
-class TrainingDataManager:
-    """
-    This manager class maintains the data needed by the training progress manager. Supports both saving and loading the data. When the manager is initialized with a checkpoint_dir as None, all save and load throw an error.
-    """
-
-    def __init__(
-        self,
-        checkpoint_dir: Optional[Path],
-        model,
-        datasource_manager: Union[InsertDataManager, SupervisedDataManager],
-        tracker: NeuralDbProgressTracker,
-    ):
-        # Checkpoint dir here refers to model specific directory
-        self.checkpoint_dir = checkpoint_dir
-
-        if self.checkpoint_dir:
-            self.model_location = self.checkpoint_dir / "model.pkl"
-            self.tracker_folder = self.checkpoint_dir / "tracker"
-            self.tracker_folder.mkdir(exist_ok=True, parents=True)
-
-        self.model = model
-        self.datasource_manager = datasource_manager
-        self.tracker = tracker
-
-    def save(self):
-        if self.checkpoint_dir:
-            self.model.save(path=self.model_location)
-            self.tracker.save(path=self.tracker_folder)
-            self.datasource_manager.save()
-        else:
-            raise Exception(
-                "Invalid method call: 'save' operation for TrainingDataManager cannot"
-                " be executed because 'checkpoint_dir' is None. Please provide a valid"
-                " directory path for 'checkpoint_dir' to proceed with the save"
-                " operation."
-            )
-
-    @property
-    def intro_source(self):
-        if isinstance(self.datasource_manager, InsertDataManager):
-            return self.datasource_manager.intro_source
-        else:
-            raise Exception(
-                "Invalid method call: 'intro_source' operation for TrainingDataManager cannot"
-                " be executed because 'datasource_manager' is of the type SupervisedDataManager"
-            )
-
-    @property
-    def train_source(self):
-        return self.datasource_manager.train_source
-
-    def save_without_sources(self):
-        # Checkpoints the model and the tracker without the datasources
-        if self.checkpoint_dir:
-            self.model.save(path=self.model_location)
-            self.tracker.save(path=self.tracker_folder)
-        else:
-            raise Exception(
-                "Invalid method call: 'save_without_sources' operation for"
-                " TrainingDataManager cannot be executed because 'checkpoint_dir' is"
-                " None. Please provide a valid directory path for 'checkpoint_dir' to"
-                " proceed with the save operation."
-            )
-
-    @staticmethod
-    def load(
-        checkpoint_dir: Path,
-        for_supervised: bool,
-        data_manager: Optional[Union[SupervisedDataManager, InsertDataManager]] = None,
-    ):
-        manager = TrainingDataManager(checkpoint_dir, None, None, None)
-
-        try:
-            manager.model = unpickle_from(manager.model_location)
-        except:
-            raise Exception(
-                "Could not find a valid Mach model at the path:"
-                f" {manager.model_location}"
-            )
-
-        if for_supervised:
-            tracker = SupervisedProgressTracker.load(manager.tracker_folder)
-            if not data_manager:
-                data_manager = SupervisedDataManager.load(checkpoint_dir)
-        else:
-            tracker = InsertProgressTracker.load(manager.tracker_folder)
-            if not data_manager:
-                data_manager = InsertDataManager.load(checkpoint_dir)
-
-        manager.datasource_manager = data_manager
-        manager.tracker = tracker
-
-        return manager
-
-    def delete_checkpoint(self):
-        shutil.rmtree(path=self.checkpoint_dir, ignore_errors=True)
-
-    @staticmethod
-    def update_model_and_tracker_from_backup(
-        backup_config: TrainingDataManager,
-        target_config: TrainingDataManager,
-    ):
-        assert_file_exists(path=backup_config.model_location)
-        assert_file_exists(path=backup_config.tracker_folder)
-
-        shutil.move(
-            backup_config.model_location,
-            target_config.model_location,
-        )
-
-        move_between_directories(
-            backup_config.tracker_folder, target_config.tracker_folder
-        )
-
-    def copy_with_new_dir(self, new_directory):
-        # Returns a new TrainingDataManager with the same model, tracker and data source but with a different
-        # checkpoint directory. Used for backing up data.
-        return TrainingDataManager(
-            checkpoint_dir=new_directory,
-            model=self.model,
-            datasource_manager=self.datasource_manager,
-            tracker=self.tracker,
-        )
+from __future__ import annotations
+
+import shutil
+from pathlib import Path
+from typing import Optional, Union
+
+from ..documents import DocumentDataSource
+from ..supervised_datasource import SupDataSource
+from ..utils import assert_file_exists, move_between_directories, unpickle_from
+from .training_progress_tracker import (
+    InsertProgressTracker,
+    NeuralDbProgressTracker,
+    SupervisedProgressTracker,
+)
+
+
+class SupervisedDataManager:
+    def __init__(self, checkpoint_dir: Optional[Path], train_source: SupDataSource):
+        self.checkpoint_dir = checkpoint_dir
+        self.train_source = train_source
+
+        if self.checkpoint_dir:
+            self.train_source_folder = self.checkpoint_dir / "sup_source"
+            self.train_source_folder.mkdir(exist_ok=True, parents=True)
+
+    def save(self):
+        if self.checkpoint_dir:
+            self.train_source.save(self.train_source_folder)
+        else:
+            raise Exception(
+                "Invalid method call: 'save' operation for SupervisedDataManager cannot"
+                " be executed because 'checkpoint_dir' is None. Please provide a valid"
+                " directory path for 'checkpoint_dir' to proceed with the save"
+                " operation."
+            )
+
+    @staticmethod
+    def load(checkpoint_dir: Path):
+        manager = SupervisedDataManager(checkpoint_dir, None)
+        manager.train_source = SupDataSource.load(path=manager.train_source_folder)
+        return manager
+
+
+class InsertDataManager:
+    """
+    This class is used for saving and loading the intro and the train sources for the insert method.
+    """
+
+    def __init__(
+        self,
+        checkpoint_dir: Optional[Path],
+        intro_source: DocumentDataSource,
+        train_source: DocumentDataSource,
+    ):
+        self.checkpoint_dir = checkpoint_dir
+        if self.checkpoint_dir:
+            self.intro_source_folder = self.checkpoint_dir / "intro_source"
+            self.train_source_folder = self.checkpoint_dir / "train_source"
+            self.intro_source_folder.mkdir(exist_ok=True, parents=True)
+            self.train_source_folder.mkdir(exist_ok=True)
+
+        self.intro_source = intro_source
+        self.train_source = train_source
+
+    def save(self):
+        if self.checkpoint_dir:
+            self.intro_source.save(path=self.intro_source_folder)
+            self.train_source.save(path=self.train_source_folder)
+        else:
+            raise Exception(
+                "Invalid method call: 'save' operation for InsertDataManager cannot"
+                " be executed because 'checkpoint_dir' is None. Please provide a valid"
+                " directory path for 'checkpoint_dir' to proceed with the save"
+                " operation."
+            )
+
+    @staticmethod
+    def load(
+        checkpoint_dir: Path,
+    ):
+        manager = InsertDataManager(checkpoint_dir, None, None)
+        manager.intro_source = DocumentDataSource.load(path=manager.intro_source_folder)
+        manager.train_source = DocumentDataSource.load(path=manager.train_source_folder)
+        return manager
+
+
+class TrainingDataManager:
+    """
+    This manager class maintains the data needed by the training progress manager. Supports both saving and loading the data. When the manager is initialized with a checkpoint_dir as None, all save and load throw an error.
+    """
+
+    def __init__(
+        self,
+        checkpoint_dir: Optional[Path],
+        model,
+        datasource_manager: Union[InsertDataManager, SupervisedDataManager],
+        tracker: NeuralDbProgressTracker,
+    ):
+        # Checkpoint dir here refers to model specific directory
+        self.checkpoint_dir = checkpoint_dir
+
+        if self.checkpoint_dir:
+            self.model_location = self.checkpoint_dir / "model.pkl"
+            self.tracker_folder = self.checkpoint_dir / "tracker"
+            self.tracker_folder.mkdir(exist_ok=True, parents=True)
+
+        self.model = model
+        self.datasource_manager = datasource_manager
+        self.tracker = tracker
+
+    def save(self):
+        if self.checkpoint_dir:
+            self.model.save(path=self.model_location)
+            self.tracker.save(path=self.tracker_folder)
+            self.datasource_manager.save()
+        else:
+            raise Exception(
+                "Invalid method call: 'save' operation for TrainingDataManager cannot"
+                " be executed because 'checkpoint_dir' is None. Please provide a valid"
+                " directory path for 'checkpoint_dir' to proceed with the save"
+                " operation."
+            )
+
+    @property
+    def intro_source(self):
+        if isinstance(self.datasource_manager, InsertDataManager):
+            return self.datasource_manager.intro_source
+        else:
+            raise Exception(
+                "Invalid method call: 'intro_source' operation for TrainingDataManager cannot"
+                " be executed because 'datasource_manager' is of the type SupervisedDataManager"
+            )
+
+    @property
+    def train_source(self):
+        return self.datasource_manager.train_source
+
+    def save_without_sources(self):
+        # Checkpoints the model and the tracker without the datasources
+        if self.checkpoint_dir:
+            self.model.save(path=self.model_location)
+            self.tracker.save(path=self.tracker_folder)
+        else:
+            raise Exception(
+                "Invalid method call: 'save_without_sources' operation for"
+                " TrainingDataManager cannot be executed because 'checkpoint_dir' is"
+                " None. Please provide a valid directory path for 'checkpoint_dir' to"
+                " proceed with the save operation."
+            )
+
+    @staticmethod
+    def load(
+        checkpoint_dir: Path,
+        for_supervised: bool,
+        data_manager: Optional[Union[SupervisedDataManager, InsertDataManager]] = None,
+    ):
+        manager = TrainingDataManager(checkpoint_dir, None, None, None)
+
+        try:
+            manager.model = unpickle_from(manager.model_location)
+        except:
+            raise Exception(
+                "Could not find a valid Mach model at the path:"
+                f" {manager.model_location}"
+            )
+
+        if for_supervised:
+            tracker = SupervisedProgressTracker.load(manager.tracker_folder)
+            if not data_manager:
+                data_manager = SupervisedDataManager.load(checkpoint_dir)
+        else:
+            tracker = InsertProgressTracker.load(manager.tracker_folder)
+            if not data_manager:
+                data_manager = InsertDataManager.load(checkpoint_dir)
+
+        manager.datasource_manager = data_manager
+        manager.tracker = tracker
+
+        return manager
+
+    def delete_checkpoint(self):
+        shutil.rmtree(path=self.checkpoint_dir, ignore_errors=True)
+
+    @staticmethod
+    def update_model_and_tracker_from_backup(
+        backup_config: TrainingDataManager,
+        target_config: TrainingDataManager,
+    ):
+        assert_file_exists(path=backup_config.model_location)
+        assert_file_exists(path=backup_config.tracker_folder)
+
+        shutil.move(
+            backup_config.model_location,
+            target_config.model_location,
+        )
+
+        move_between_directories(
+            backup_config.tracker_folder, target_config.tracker_folder
+        )
+
+    def copy_with_new_dir(self, new_directory):
+        # Returns a new TrainingDataManager with the same model, tracker and data source but with a different
+        # checkpoint directory. Used for backing up data.
+        return TrainingDataManager(
+            checkpoint_dir=new_directory,
+            model=self.model,
+            datasource_manager=self.datasource_manager,
+            tracker=self.tracker,
+        )
```

## thirdai/neural_db/trainer/training_progress_manager.py

 * *Ordering differences only*

```diff
@@ -1,298 +1,298 @@
-from __future__ import annotations
-
-from typing import Optional, Union
-
-from thirdai import bolt
-
-from ..documents import DocumentDataSource
-from ..models.mach_defaults import (
-    training_arguments_from_base,
-    training_arguments_from_scratch,
-)
-from ..supervised_datasource import SupDataSource
-from .checkpoint_config import CheckpointConfig
-from .training_data_manager import (
-    InsertDataManager,
-    SupervisedDataManager,
-    TrainingDataManager,
-)
-from .training_progress_tracker import (
-    InsertProgressTracker,
-    InsertTrainState,
-    IntroState,
-    SupervisedProgressTracker,
-    SupervisedTrainState,
-)
-
-
-class TrainingProgressCallback(bolt.train.callbacks.Callback):
-    def __init__(self, training_progress_manager: TrainingProgressManager):
-        super().__init__()
-        self.training_progress_manager = training_progress_manager
-
-    def on_epoch_end(self):
-        self.training_progress_manager.complete_epoch()
-
-
-class TrainingProgressManager:
-    """
-    TrainingProgressManager class is used to maintain the checkpoint while training and is the source of truth for inserting documents into the current model object.
-
-    This is designed the way it is to make sure that that we have identical function calls irrespective of whether we're resuming from a checkpoint/using checkpointing/doing no checkpointing. By making our training progress manager the source of truth for all training related variables/objects, we effectively offload the task of maintaining training state and checkpointing to the manager. And the manager internally decides when it should save what objects.
-
-    Another reason to explain this unified design is that if we have seperate calls for indexing with/out checkpoint, we have to be sure that making changes in one does not break the other.
-    """
-
-    def __init__(
-        self,
-        tracker: Union[SupervisedProgressTracker, InsertProgressTracker],
-        save_load_manager: TrainingDataManager,
-        makes_checkpoint: bool,
-        checkpoint_interval: int = 1,
-    ):
-        self.tracker = tracker
-        self.save_load_manager = save_load_manager
-
-        self.makes_checkpoint = makes_checkpoint
-        self.checkpoint_interval = checkpoint_interval
-
-        if self.makes_checkpoint:
-            # Backup config saves into a different directory but all other model, tracker, source references remain the same as save_load_manager
-            self.backup_config = save_load_manager.copy_with_new_dir(
-                new_directory=save_load_manager.checkpoint_dir / "backup"
-            )
-
-    @property
-    def intro_source(self) -> DocumentDataSource:
-        return self.save_load_manager.intro_source
-
-    @property
-    def train_source(self) -> Union[DocumentDataSource, SupDataSource]:
-        return self.save_load_manager.train_source
-
-    @property
-    def datasource_manager(self) -> Union[InsertDataManager, SupervisedDataManager]:
-        return self.save_load_manager.datasource_manager
-
-    @property
-    def is_insert_completed(self):
-        return self.tracker.is_insert_completed
-
-    def introduce_arguments(self):
-        return self.tracker.introduce_arguments()
-
-    def insert_complete(self):
-        # Updates the tracker state by marking insert as completed and saves the resources (tracker and model)
-        # if makes_checkpoint is True.
-        self.tracker.insert_complete()
-        if not self.makes_checkpoint:
-            return
-        self.checkpoint_without_sources()
-
-    @property
-    def is_training_completed(self):
-        return self.tracker.is_training_completed
-
-    def training_arguments(self):
-        return self.tracker.training_arguments()
-
-    def complete_epoch(self):
-        self.tracker.current_epoch_number += 1
-        if self.tracker.current_epoch_number % self.checkpoint_interval == 0:
-            if self.makes_checkpoint:
-                self.checkpoint_without_sources()
-
-    def training_complete(self):
-        # Updates the tracker state by marking training as completed and saves the resources (tracker and model)
-        # if makes_checkpoint is True.
-        self.tracker.training_complete()
-        if not self.makes_checkpoint:
-            return
-        self.checkpoint_without_sources()
-        self.backup_config.delete_checkpoint()
-
-    def make_preindexing_checkpoint(self, save_datasource=True):
-        # Before starting indexing, save all the resources (datasource, model, tracker) to be able to resume.
-        if not self.makes_checkpoint:
-            return
-        if save_datasource:
-            self.save_load_manager.save()
-        else:
-            self.save_load_manager.save_without_sources()
-
-    def checkpoint_without_sources(self):
-        # First save the model in the backup directory. Once the resources have been successfully saved,
-        # we can move them to their intended checkpoint location. We only need to maintain backups of the
-        # model and the tracker because other resources (intro and train source) are never modified.
-        self.backup_config.save_without_sources()
-
-        # This is used to "hot-swap" the model from backup to original checkpoint location. Saving a model takes longer
-        # than moving it across dir. Hence, if the program terminates while checkpointing, only the backup gets corrupted leaving the
-        # model in the original location in a valid state.
-        TrainingDataManager.update_model_and_tracker_from_backup(
-            backup_config=self.backup_config, target_config=self.save_load_manager
-        )
-
-    @staticmethod
-    def from_scratch_for_unsupervised(
-        model,
-        intro_documents,
-        train_documents,
-        should_train,
-        fast_approximation,
-        num_buckets_to_sample,
-        max_in_memory_batches,
-        override_number_classes,
-        variable_length,
-        checkpoint_config: CheckpointConfig,
-        **kwargs,
-    ) -> TrainingProgressManager:
-        intro_state = IntroState(
-            num_buckets_to_sample=num_buckets_to_sample,
-            fast_approximation=fast_approximation,
-            override_number_classes=override_number_classes,
-            is_insert_completed=False,
-        )
-
-        if model.model is None:
-            train_args = training_arguments_from_scratch(train_documents.size)
-        else:
-            train_args = training_arguments_from_base(train_documents.size)
-
-        train_args["batch_size"] = kwargs.get("batch_size", None)
-        train_args["learning_rate"] = kwargs.get(
-            "learning_rate", train_args["learning_rate"]
-        )
-        train_args["min_epochs"] = kwargs.get("epochs", train_args["min_epochs"])
-        train_args["max_epochs"] = kwargs.get("epochs", train_args["max_epochs"])
-
-        train_args["freeze_after_epoch"] = kwargs.get(
-            "freeze_after_epoch", train_args["max_epochs"] - 1
-        )
-        train_args["freeze_after_acc"] = kwargs.get(
-            "freeze_after_acc", 0.80 if "freeze_after_epoch" not in kwargs else 1
-        )
-        train_args["balancing_samples"] = kwargs.get("balancing_samples", False)
-        train_args["semantic_enhancement"] = kwargs.get("semantic_enhancement", False)
-        train_args["semantic_model_cache_dir"] = kwargs.get(
-            "semantic_model_cache_dir", ".cache/neural_db_semantic_model"
-        )
-
-        train_state = InsertTrainState(
-            max_in_memory_batches=max_in_memory_batches,
-            current_epoch_number=0,
-            is_training_completed=not should_train,
-            **train_args,
-        )
-
-        tracker = InsertProgressTracker(
-            intro_state=intro_state, train_state=train_state, vlc_config=variable_length
-        )
-
-        save_load_manager = TrainingDataManager(
-            checkpoint_dir=(
-                checkpoint_config.checkpoint_dir if checkpoint_config else None
-            ),
-            model=model,
-            datasource_manager=InsertDataManager(
-                checkpoint_config.checkpoint_dir if checkpoint_config else None,
-                intro_source=intro_documents,
-                train_source=train_documents,
-            ),
-            tracker=tracker,
-        )
-
-        training_progress_manager = TrainingProgressManager(
-            tracker=tracker,
-            save_load_manager=save_load_manager,
-            makes_checkpoint=True if checkpoint_config else False,
-            checkpoint_interval=(
-                checkpoint_config.checkpoint_interval if checkpoint_config else 1
-            ),
-        )
-
-        if not should_train:
-            training_progress_manager.tracker.is_training_completed = True
-
-        return training_progress_manager
-
-    @staticmethod
-    def from_scratch_for_supervised(
-        model,
-        supervised_datasource,
-        learning_rate,
-        epochs,
-        batch_size,
-        max_in_memory_batches,
-        metrics,
-        disable_finetunable_retriever,
-        checkpoint_config: CheckpointConfig,
-        **kwargs,
-    ) -> TrainingProgressManager:
-
-        train_state = SupervisedTrainState(
-            is_training_completed=False,
-            current_epoch_number=0,
-            learning_rate=learning_rate,
-            epochs=epochs,
-            batch_size=batch_size,
-            max_in_memory_batches=max_in_memory_batches,
-            metrics=metrics,
-            disable_finetunable_retriever=disable_finetunable_retriever,
-        )
-
-        tracker = SupervisedProgressTracker(train_state=train_state)
-
-        save_load_manager = TrainingDataManager(
-            checkpoint_dir=(
-                checkpoint_config.checkpoint_dir if checkpoint_config else None
-            ),
-            model=model,
-            datasource_manager=SupervisedDataManager(
-                checkpoint_config.checkpoint_dir if checkpoint_config else None,
-                train_source=supervised_datasource,
-            ),
-            tracker=tracker,
-        )
-
-        training_progress_manager = TrainingProgressManager(
-            tracker=tracker,
-            save_load_manager=save_load_manager,
-            makes_checkpoint=True if checkpoint_config else False,
-            checkpoint_interval=(
-                checkpoint_config.checkpoint_interval if checkpoint_config else 1
-            ),
-        )
-
-        return training_progress_manager
-
-    @staticmethod
-    def from_checkpoint(
-        original_mach_model,
-        checkpoint_config: CheckpointConfig,
-        for_supervised: bool,
-        datasource_manager: Optional[
-            Union[InsertDataManager, SupervisedDataManager]
-        ] = None,
-    ) -> TrainingProgressManager:
-        """
-        Given a checkpoint, we will make a save load manager that will load the model, data sources, tracker.
-        """
-        assert checkpoint_config.checkpoint_dir != None
-
-        save_load_manager = TrainingDataManager.load(
-            checkpoint_dir=checkpoint_config.checkpoint_dir,
-            for_supervised=for_supervised,
-            data_manager=datasource_manager,
-        )
-
-        # We need to update the passed model with the state of the loaded model. Since, we need a model reference in the save_load_manager as well, we update the model reference there too.
-        original_mach_model.reset_model(save_load_manager.model)
-        save_load_manager.model = original_mach_model
-        training_progress_manager = TrainingProgressManager(
-            tracker=save_load_manager.tracker,
-            save_load_manager=save_load_manager,
-            makes_checkpoint=True,
-            checkpoint_interval=checkpoint_config.checkpoint_interval,
-        )
-        return training_progress_manager
+from __future__ import annotations
+
+from typing import Optional, Union
+
+from thirdai import bolt
+
+from ..documents import DocumentDataSource
+from ..models.mach_defaults import (
+    training_arguments_from_base,
+    training_arguments_from_scratch,
+)
+from ..supervised_datasource import SupDataSource
+from .checkpoint_config import CheckpointConfig
+from .training_data_manager import (
+    InsertDataManager,
+    SupervisedDataManager,
+    TrainingDataManager,
+)
+from .training_progress_tracker import (
+    InsertProgressTracker,
+    InsertTrainState,
+    IntroState,
+    SupervisedProgressTracker,
+    SupervisedTrainState,
+)
+
+
+class TrainingProgressCallback(bolt.train.callbacks.Callback):
+    def __init__(self, training_progress_manager: TrainingProgressManager):
+        super().__init__()
+        self.training_progress_manager = training_progress_manager
+
+    def on_epoch_end(self):
+        self.training_progress_manager.complete_epoch()
+
+
+class TrainingProgressManager:
+    """
+    TrainingProgressManager class is used to maintain the checkpoint while training and is the source of truth for inserting documents into the current model object.
+
+    This is designed the way it is to make sure that that we have identical function calls irrespective of whether we're resuming from a checkpoint/using checkpointing/doing no checkpointing. By making our training progress manager the source of truth for all training related variables/objects, we effectively offload the task of maintaining training state and checkpointing to the manager. And the manager internally decides when it should save what objects.
+
+    Another reason to explain this unified design is that if we have seperate calls for indexing with/out checkpoint, we have to be sure that making changes in one does not break the other.
+    """
+
+    def __init__(
+        self,
+        tracker: Union[SupervisedProgressTracker, InsertProgressTracker],
+        save_load_manager: TrainingDataManager,
+        makes_checkpoint: bool,
+        checkpoint_interval: int = 1,
+    ):
+        self.tracker = tracker
+        self.save_load_manager = save_load_manager
+
+        self.makes_checkpoint = makes_checkpoint
+        self.checkpoint_interval = checkpoint_interval
+
+        if self.makes_checkpoint:
+            # Backup config saves into a different directory but all other model, tracker, source references remain the same as save_load_manager
+            self.backup_config = save_load_manager.copy_with_new_dir(
+                new_directory=save_load_manager.checkpoint_dir / "backup"
+            )
+
+    @property
+    def intro_source(self) -> DocumentDataSource:
+        return self.save_load_manager.intro_source
+
+    @property
+    def train_source(self) -> Union[DocumentDataSource, SupDataSource]:
+        return self.save_load_manager.train_source
+
+    @property
+    def datasource_manager(self) -> Union[InsertDataManager, SupervisedDataManager]:
+        return self.save_load_manager.datasource_manager
+
+    @property
+    def is_insert_completed(self):
+        return self.tracker.is_insert_completed
+
+    def introduce_arguments(self):
+        return self.tracker.introduce_arguments()
+
+    def insert_complete(self):
+        # Updates the tracker state by marking insert as completed and saves the resources (tracker and model)
+        # if makes_checkpoint is True.
+        self.tracker.insert_complete()
+        if not self.makes_checkpoint:
+            return
+        self.checkpoint_without_sources()
+
+    @property
+    def is_training_completed(self):
+        return self.tracker.is_training_completed
+
+    def training_arguments(self):
+        return self.tracker.training_arguments()
+
+    def complete_epoch(self):
+        self.tracker.current_epoch_number += 1
+        if self.tracker.current_epoch_number % self.checkpoint_interval == 0:
+            if self.makes_checkpoint:
+                self.checkpoint_without_sources()
+
+    def training_complete(self):
+        # Updates the tracker state by marking training as completed and saves the resources (tracker and model)
+        # if makes_checkpoint is True.
+        self.tracker.training_complete()
+        if not self.makes_checkpoint:
+            return
+        self.checkpoint_without_sources()
+        self.backup_config.delete_checkpoint()
+
+    def make_preindexing_checkpoint(self, save_datasource=True):
+        # Before starting indexing, save all the resources (datasource, model, tracker) to be able to resume.
+        if not self.makes_checkpoint:
+            return
+        if save_datasource:
+            self.save_load_manager.save()
+        else:
+            self.save_load_manager.save_without_sources()
+
+    def checkpoint_without_sources(self):
+        # First save the model in the backup directory. Once the resources have been successfully saved,
+        # we can move them to their intended checkpoint location. We only need to maintain backups of the
+        # model and the tracker because other resources (intro and train source) are never modified.
+        self.backup_config.save_without_sources()
+
+        # This is used to "hot-swap" the model from backup to original checkpoint location. Saving a model takes longer
+        # than moving it across dir. Hence, if the program terminates while checkpointing, only the backup gets corrupted leaving the
+        # model in the original location in a valid state.
+        TrainingDataManager.update_model_and_tracker_from_backup(
+            backup_config=self.backup_config, target_config=self.save_load_manager
+        )
+
+    @staticmethod
+    def from_scratch_for_unsupervised(
+        model,
+        intro_documents,
+        train_documents,
+        should_train,
+        fast_approximation,
+        num_buckets_to_sample,
+        max_in_memory_batches,
+        override_number_classes,
+        variable_length,
+        checkpoint_config: CheckpointConfig,
+        **kwargs,
+    ) -> TrainingProgressManager:
+        intro_state = IntroState(
+            num_buckets_to_sample=num_buckets_to_sample,
+            fast_approximation=fast_approximation,
+            override_number_classes=override_number_classes,
+            is_insert_completed=False,
+        )
+
+        if model.model is None:
+            train_args = training_arguments_from_scratch(train_documents.size)
+        else:
+            train_args = training_arguments_from_base(train_documents.size)
+
+        train_args["batch_size"] = kwargs.get("batch_size", None)
+        train_args["learning_rate"] = kwargs.get(
+            "learning_rate", train_args["learning_rate"]
+        )
+        train_args["min_epochs"] = kwargs.get("epochs", train_args["min_epochs"])
+        train_args["max_epochs"] = kwargs.get("epochs", train_args["max_epochs"])
+
+        train_args["freeze_after_epoch"] = kwargs.get(
+            "freeze_after_epoch", train_args["max_epochs"] - 1
+        )
+        train_args["freeze_after_acc"] = kwargs.get(
+            "freeze_after_acc", 0.80 if "freeze_after_epoch" not in kwargs else 1
+        )
+        train_args["balancing_samples"] = kwargs.get("balancing_samples", False)
+        train_args["semantic_enhancement"] = kwargs.get("semantic_enhancement", False)
+        train_args["semantic_model_cache_dir"] = kwargs.get(
+            "semantic_model_cache_dir", ".cache/neural_db_semantic_model"
+        )
+
+        train_state = InsertTrainState(
+            max_in_memory_batches=max_in_memory_batches,
+            current_epoch_number=0,
+            is_training_completed=not should_train,
+            **train_args,
+        )
+
+        tracker = InsertProgressTracker(
+            intro_state=intro_state, train_state=train_state, vlc_config=variable_length
+        )
+
+        save_load_manager = TrainingDataManager(
+            checkpoint_dir=(
+                checkpoint_config.checkpoint_dir if checkpoint_config else None
+            ),
+            model=model,
+            datasource_manager=InsertDataManager(
+                checkpoint_config.checkpoint_dir if checkpoint_config else None,
+                intro_source=intro_documents,
+                train_source=train_documents,
+            ),
+            tracker=tracker,
+        )
+
+        training_progress_manager = TrainingProgressManager(
+            tracker=tracker,
+            save_load_manager=save_load_manager,
+            makes_checkpoint=True if checkpoint_config else False,
+            checkpoint_interval=(
+                checkpoint_config.checkpoint_interval if checkpoint_config else 1
+            ),
+        )
+
+        if not should_train:
+            training_progress_manager.tracker.is_training_completed = True
+
+        return training_progress_manager
+
+    @staticmethod
+    def from_scratch_for_supervised(
+        model,
+        supervised_datasource,
+        learning_rate,
+        epochs,
+        batch_size,
+        max_in_memory_batches,
+        metrics,
+        disable_finetunable_retriever,
+        checkpoint_config: CheckpointConfig,
+        **kwargs,
+    ) -> TrainingProgressManager:
+
+        train_state = SupervisedTrainState(
+            is_training_completed=False,
+            current_epoch_number=0,
+            learning_rate=learning_rate,
+            epochs=epochs,
+            batch_size=batch_size,
+            max_in_memory_batches=max_in_memory_batches,
+            metrics=metrics,
+            disable_finetunable_retriever=disable_finetunable_retriever,
+        )
+
+        tracker = SupervisedProgressTracker(train_state=train_state)
+
+        save_load_manager = TrainingDataManager(
+            checkpoint_dir=(
+                checkpoint_config.checkpoint_dir if checkpoint_config else None
+            ),
+            model=model,
+            datasource_manager=SupervisedDataManager(
+                checkpoint_config.checkpoint_dir if checkpoint_config else None,
+                train_source=supervised_datasource,
+            ),
+            tracker=tracker,
+        )
+
+        training_progress_manager = TrainingProgressManager(
+            tracker=tracker,
+            save_load_manager=save_load_manager,
+            makes_checkpoint=True if checkpoint_config else False,
+            checkpoint_interval=(
+                checkpoint_config.checkpoint_interval if checkpoint_config else 1
+            ),
+        )
+
+        return training_progress_manager
+
+    @staticmethod
+    def from_checkpoint(
+        original_mach_model,
+        checkpoint_config: CheckpointConfig,
+        for_supervised: bool,
+        datasource_manager: Optional[
+            Union[InsertDataManager, SupervisedDataManager]
+        ] = None,
+    ) -> TrainingProgressManager:
+        """
+        Given a checkpoint, we will make a save load manager that will load the model, data sources, tracker.
+        """
+        assert checkpoint_config.checkpoint_dir != None
+
+        save_load_manager = TrainingDataManager.load(
+            checkpoint_dir=checkpoint_config.checkpoint_dir,
+            for_supervised=for_supervised,
+            data_manager=datasource_manager,
+        )
+
+        # We need to update the passed model with the state of the loaded model. Since, we need a model reference in the save_load_manager as well, we update the model reference there too.
+        original_mach_model.reset_model(save_load_manager.model)
+        save_load_manager.model = original_mach_model
+        training_progress_manager = TrainingProgressManager(
+            tracker=save_load_manager.tracker,
+            save_load_manager=save_load_manager,
+            makes_checkpoint=True,
+            checkpoint_interval=checkpoint_config.checkpoint_interval,
+        )
+        return training_progress_manager
```

## thirdai/neural_db/trainer/training_progress_tracker.py

 * *Ordering differences only*

```diff
@@ -1,235 +1,235 @@
-import json
-from dataclasses import dataclass
-from pathlib import Path
-from typing import List, Union
-
-from thirdai import data
-
-from ..utils import pickle_to, unpickle_from
-
-
-@dataclass
-class SupervisedTrainState:
-
-    is_training_completed: bool
-    current_epoch_number: int
-    learning_rate: float
-    epochs: int
-    batch_size: int
-    max_in_memory_batches: int
-    metrics: List[int]
-    disable_finetunable_retriever: bool
-
-
-@dataclass
-class InsertTrainState:
-    max_in_memory_batches: int
-    current_epoch_number: int
-    is_training_completed: bool
-    learning_rate: float
-    min_epochs: int
-    max_epochs: int
-    freeze_before_train: bool
-    batch_size: int
-    freeze_after_epoch: int
-    freeze_after_acc: float
-    balancing_samples: bool
-    semantic_enhancement: bool
-    semantic_model_cache_dir: str
-
-
-class IntroState:
-    def __init__(
-        self,
-        num_buckets_to_sample: int,
-        fast_approximation: bool,
-        override_number_classes: bool,
-        is_insert_completed: bool,
-        **kwargs,
-    ):
-        self.num_buckets_to_sample = num_buckets_to_sample
-        self.fast_approximation = fast_approximation
-        self.override_number_classes = override_number_classes
-        self.is_insert_completed = is_insert_completed
-
-
-class NeuralDbProgressTracker:
-    """
-    This class will be used to track the current training status of a NeuralDB Mach Model.
-    The training state needs to be updated constantly while a model is being trained and
-    hence, this should ideally be used inside a callback.
-
-    Given the NeuralDbProgressTracker of the model and the data sources, we should be able to resume the training.
-    """
-
-    def __init__(self, train_state: Union[SupervisedTrainState, InsertTrainState]):
-        # These are training arguments and are updated while the training is in progress
-        self._train_state = train_state
-
-    @property
-    def is_insert_completed(self):
-        raise NotImplementedError(
-            "Method 'is_insert_completed' not implemented for class NeuralDBProgressTracker."
-        )
-
-    @is_insert_completed.setter
-    def is_insert_completed(self, is_insert_completed: bool):
-        raise NotImplementedError(
-            "Setter Method 'is_insert_completed' not implemented for class NeuralDBProgressTracker."
-        )
-
-    def insert_complete(self):
-        raise NotImplementedError(
-            "Method 'insert_complete' not implemented for class NeuralDBProgressTracker."
-        )
-
-    def introduce_arguments(self):
-        raise NotImplementedError(
-            "Method 'introduce_arguments' not implemented for class NeuralDBProgressTracker."
-        )
-
-    @property
-    def is_training_completed(self):
-        return self._train_state.is_training_completed
-
-    @is_training_completed.setter
-    def is_training_completed(self, is_training_completed: bool):
-        if isinstance(is_training_completed, bool):
-            self._train_state.is_training_completed = is_training_completed
-        else:
-            raise TypeError("Can set the property only with a bool")
-
-    @property
-    def current_epoch_number(self):
-        return self._train_state.current_epoch_number
-
-    @current_epoch_number.setter
-    def current_epoch_number(self, current_epoch_number: int):
-        if isinstance(current_epoch_number, int):
-            self._train_state.current_epoch_number = current_epoch_number
-        else:
-            raise TypeError("Can set the property only with an int")
-
-    def epoch_complete(self):
-        self.current_epoch_number += 1
-
-    def training_complete(self):
-        if self.is_training_completed:
-            raise Exception("Training has already been finished.")
-        self.is_training_completed = True
-
-
-class InsertProgressTracker(NeuralDbProgressTracker):
-    def __init__(
-        self,
-        intro_state: IntroState,
-        train_state: InsertTrainState,
-        vlc_config: data.transformations.VariableLengthConfig,
-    ):
-        super().__init__(train_state=train_state)
-
-        # These are the introduce state arguments and updated once the introduce document is done
-        self._intro_state = intro_state
-
-        # These are training arguments and are updated while the training is in progress
-        self._train_state = train_state
-        self.vlc_config = vlc_config
-
-    @property
-    def is_insert_completed(self):
-        return self._intro_state.is_insert_completed
-
-    @is_insert_completed.setter
-    def is_insert_completed(self, is_insert_completed: bool):
-        if isinstance(is_insert_completed, bool):
-            self._intro_state.is_insert_completed = is_insert_completed
-        else:
-            raise TypeError("Can set the property only with a bool")
-
-    def insert_complete(self):
-        if self.is_insert_completed:
-            raise Exception("Insert has already been finished.")
-        self.is_insert_completed = True
-
-    def introduce_arguments(self):
-        return {
-            "num_buckets_to_sample": self._intro_state.num_buckets_to_sample,
-            "fast_approximation": self._intro_state.fast_approximation,
-            "override_number_classes": self._intro_state.override_number_classes,
-        }
-
-    def training_arguments(self):
-        min_epochs = (
-            self._train_state.min_epochs - self._train_state.current_epoch_number
-        )
-        max_epochs = (
-            self._train_state.max_epochs - self._train_state.current_epoch_number
-        )
-        freeze_after_epochs = (
-            self._train_state.freeze_after_epoch
-            - self._train_state.current_epoch_number
-        )
-
-        args = self._train_state.__dict__.copy()
-
-        args["freeze_after_epochs"] = freeze_after_epochs
-        args["min_epochs"] = min_epochs
-        args["max_epochs"] = max_epochs
-
-        args["variable_length"] = self.vlc_config
-
-        return args
-
-    def save(self, path: Path):
-        path.mkdir(exist_ok=True, parents=True)
-        with open(path / "tracker.json", "w") as f:
-            json.dump(
-                {
-                    "intro_state": self._intro_state.__dict__,
-                    "train_state": self._train_state.__dict__,
-                },
-                f,
-                indent=4,
-            )
-        pickle_to(self.vlc_config, path / "vlc.config")
-
-    @staticmethod
-    def load(path: Path):
-        with open(path / "tracker.json", "r") as f:
-            args = json.load(f)
-
-        vlc_config = unpickle_from(path / "vlc.config")
-
-        return InsertProgressTracker(
-            intro_state=IntroState(**args["intro_state"]),
-            train_state=InsertTrainState(**args["train_state"]),
-            vlc_config=vlc_config,
-        )
-
-
-class SupervisedProgressTracker(NeuralDbProgressTracker):
-    def __init__(self, train_state: SupervisedTrainState):
-        super().__init__(train_state=train_state)
-        self._train_state = train_state
-
-    def training_arguments(self):
-        epochs = self._train_state.epochs - self.current_epoch_number
-        args = self._train_state.__dict__.copy()
-        args["epochs"] = epochs
-        return args
-
-    def save(self, path: Path):
-        path.mkdir(exist_ok=True, parents=True)
-        with open(path / "tracker.json", "w") as f:
-            json.dump(
-                self._train_state.__dict__,
-                f,
-                indent=4,
-            )
-
-    @staticmethod
-    def load(path: Path):
-        with open(path / "tracker.json", "r") as f:
-            args = json.load(f)
-
-        return SupervisedProgressTracker(train_state=SupervisedTrainState(**args))
+import json
+from dataclasses import dataclass
+from pathlib import Path
+from typing import List, Union
+
+from thirdai import data
+
+from ..utils import pickle_to, unpickle_from
+
+
+@dataclass
+class SupervisedTrainState:
+
+    is_training_completed: bool
+    current_epoch_number: int
+    learning_rate: float
+    epochs: int
+    batch_size: int
+    max_in_memory_batches: int
+    metrics: List[int]
+    disable_finetunable_retriever: bool
+
+
+@dataclass
+class InsertTrainState:
+    max_in_memory_batches: int
+    current_epoch_number: int
+    is_training_completed: bool
+    learning_rate: float
+    min_epochs: int
+    max_epochs: int
+    freeze_before_train: bool
+    batch_size: int
+    freeze_after_epoch: int
+    freeze_after_acc: float
+    balancing_samples: bool
+    semantic_enhancement: bool
+    semantic_model_cache_dir: str
+
+
+class IntroState:
+    def __init__(
+        self,
+        num_buckets_to_sample: int,
+        fast_approximation: bool,
+        override_number_classes: bool,
+        is_insert_completed: bool,
+        **kwargs,
+    ):
+        self.num_buckets_to_sample = num_buckets_to_sample
+        self.fast_approximation = fast_approximation
+        self.override_number_classes = override_number_classes
+        self.is_insert_completed = is_insert_completed
+
+
+class NeuralDbProgressTracker:
+    """
+    This class will be used to track the current training status of a NeuralDB Mach Model.
+    The training state needs to be updated constantly while a model is being trained and
+    hence, this should ideally be used inside a callback.
+
+    Given the NeuralDbProgressTracker of the model and the data sources, we should be able to resume the training.
+    """
+
+    def __init__(self, train_state: Union[SupervisedTrainState, InsertTrainState]):
+        # These are training arguments and are updated while the training is in progress
+        self._train_state = train_state
+
+    @property
+    def is_insert_completed(self):
+        raise NotImplementedError(
+            "Method 'is_insert_completed' not implemented for class NeuralDBProgressTracker."
+        )
+
+    @is_insert_completed.setter
+    def is_insert_completed(self, is_insert_completed: bool):
+        raise NotImplementedError(
+            "Setter Method 'is_insert_completed' not implemented for class NeuralDBProgressTracker."
+        )
+
+    def insert_complete(self):
+        raise NotImplementedError(
+            "Method 'insert_complete' not implemented for class NeuralDBProgressTracker."
+        )
+
+    def introduce_arguments(self):
+        raise NotImplementedError(
+            "Method 'introduce_arguments' not implemented for class NeuralDBProgressTracker."
+        )
+
+    @property
+    def is_training_completed(self):
+        return self._train_state.is_training_completed
+
+    @is_training_completed.setter
+    def is_training_completed(self, is_training_completed: bool):
+        if isinstance(is_training_completed, bool):
+            self._train_state.is_training_completed = is_training_completed
+        else:
+            raise TypeError("Can set the property only with a bool")
+
+    @property
+    def current_epoch_number(self):
+        return self._train_state.current_epoch_number
+
+    @current_epoch_number.setter
+    def current_epoch_number(self, current_epoch_number: int):
+        if isinstance(current_epoch_number, int):
+            self._train_state.current_epoch_number = current_epoch_number
+        else:
+            raise TypeError("Can set the property only with an int")
+
+    def epoch_complete(self):
+        self.current_epoch_number += 1
+
+    def training_complete(self):
+        if self.is_training_completed:
+            raise Exception("Training has already been finished.")
+        self.is_training_completed = True
+
+
+class InsertProgressTracker(NeuralDbProgressTracker):
+    def __init__(
+        self,
+        intro_state: IntroState,
+        train_state: InsertTrainState,
+        vlc_config: data.transformations.VariableLengthConfig,
+    ):
+        super().__init__(train_state=train_state)
+
+        # These are the introduce state arguments and updated once the introduce document is done
+        self._intro_state = intro_state
+
+        # These are training arguments and are updated while the training is in progress
+        self._train_state = train_state
+        self.vlc_config = vlc_config
+
+    @property
+    def is_insert_completed(self):
+        return self._intro_state.is_insert_completed
+
+    @is_insert_completed.setter
+    def is_insert_completed(self, is_insert_completed: bool):
+        if isinstance(is_insert_completed, bool):
+            self._intro_state.is_insert_completed = is_insert_completed
+        else:
+            raise TypeError("Can set the property only with a bool")
+
+    def insert_complete(self):
+        if self.is_insert_completed:
+            raise Exception("Insert has already been finished.")
+        self.is_insert_completed = True
+
+    def introduce_arguments(self):
+        return {
+            "num_buckets_to_sample": self._intro_state.num_buckets_to_sample,
+            "fast_approximation": self._intro_state.fast_approximation,
+            "override_number_classes": self._intro_state.override_number_classes,
+        }
+
+    def training_arguments(self):
+        min_epochs = (
+            self._train_state.min_epochs - self._train_state.current_epoch_number
+        )
+        max_epochs = (
+            self._train_state.max_epochs - self._train_state.current_epoch_number
+        )
+        freeze_after_epochs = (
+            self._train_state.freeze_after_epoch
+            - self._train_state.current_epoch_number
+        )
+
+        args = self._train_state.__dict__.copy()
+
+        args["freeze_after_epochs"] = freeze_after_epochs
+        args["min_epochs"] = min_epochs
+        args["max_epochs"] = max_epochs
+
+        args["variable_length"] = self.vlc_config
+
+        return args
+
+    def save(self, path: Path):
+        path.mkdir(exist_ok=True, parents=True)
+        with open(path / "tracker.json", "w") as f:
+            json.dump(
+                {
+                    "intro_state": self._intro_state.__dict__,
+                    "train_state": self._train_state.__dict__,
+                },
+                f,
+                indent=4,
+            )
+        pickle_to(self.vlc_config, path / "vlc.config")
+
+    @staticmethod
+    def load(path: Path):
+        with open(path / "tracker.json", "r") as f:
+            args = json.load(f)
+
+        vlc_config = unpickle_from(path / "vlc.config")
+
+        return InsertProgressTracker(
+            intro_state=IntroState(**args["intro_state"]),
+            train_state=InsertTrainState(**args["train_state"]),
+            vlc_config=vlc_config,
+        )
+
+
+class SupervisedProgressTracker(NeuralDbProgressTracker):
+    def __init__(self, train_state: SupervisedTrainState):
+        super().__init__(train_state=train_state)
+        self._train_state = train_state
+
+    def training_arguments(self):
+        epochs = self._train_state.epochs - self.current_epoch_number
+        args = self._train_state.__dict__.copy()
+        args["epochs"] = epochs
+        return args
+
+    def save(self, path: Path):
+        path.mkdir(exist_ok=True, parents=True)
+        with open(path / "tracker.json", "w") as f:
+            json.dump(
+                self._train_state.__dict__,
+                f,
+                indent=4,
+            )
+
+    @staticmethod
+    def load(path: Path):
+        with open(path / "tracker.json", "r") as f:
+            args = json.load(f)
+
+        return SupervisedProgressTracker(train_state=SupervisedTrainState(**args))
```

## thirdai/neural_db_v2/neural_db.py

```diff
@@ -1,150 +1,151 @@
-import json
-import os
-from typing import Iterable, List, Optional, Tuple, Union
-
-from .chunk_stores import PandasChunkStore, SQLiteChunkStore
-from .core.chunk_store import ChunkStore
-from .core.documents import Document
-from .core.retriever import Retriever
-from .core.supervised import SupervisedDataset
-from .core.types import Chunk, ChunkId, CustomIdSupervisedBatch, NewChunkBatch, Score
-from .documents import document_by_name
-from .retrievers import FinetunableRetriever, Mach
-
-
-class NeuralDB:
-    def __init__(
-        self,
-        chunk_store: Optional[ChunkStore] = None,
-        retriever: Optional[Retriever] = None,
-        **kwargs,
-    ):
-        self.chunk_store = chunk_store or SQLiteChunkStore(**kwargs)
-        self.retriever = retriever or Mach(**kwargs)
-
-    def insert_chunks(self, chunks: Iterable[NewChunkBatch], **kwargs):
-        stored_chunks = self.chunk_store.insert(
-            chunks=chunks,
-            **kwargs,
-        )
-        self.retriever.insert(
-            chunks=stored_chunks,
-            **kwargs,
-        )
-
-    def insert(self, docs: List[Union[str, Document]], **kwargs):
-        docs = [
-            doc if isinstance(doc, Document) else document_by_name(doc) for doc in docs
-        ]
-
-        def chunk_generator():
-            for doc in docs:
-                for chunk in doc.chunks():
-                    yield chunk
-
-        self.insert_chunks(chunk_generator(), **kwargs)
-
-    def search(
-        self, query: str, top_k: int, constraints: dict = None, **kwargs
-    ) -> List[Tuple[Chunk, Score]]:
-        return self.search_batch([query], top_k, constraints, **kwargs)[0]
-
-    def search_batch(
-        self, queries: List[str], top_k: int, constraints: dict = None, **kwargs
-    ) -> List[List[Tuple[Chunk, Score]]]:
-        if not constraints:
-            results = self.retriever.search(queries, top_k, **kwargs)
-        else:
-            choices = self.chunk_store.filter_chunk_ids(constraints, **kwargs)
-            # TODO is there a better way that duplicating the constraints here
-            results = self.retriever.rank(queries, [choices for _ in queries], **kwargs)
-
-        chunk_results = []
-        for query_results in results:
-            chunk_ids, scores = [list(tup) for tup in zip(*query_results)]
-            chunks = self.chunk_store.get_chunks(chunk_ids)
-            chunk_results.append(list(zip(chunks, scores)))
-
-        return chunk_results
-
-    def delete(self, chunk_ids: List[ChunkId]):
-        self.chunk_store.delete(chunk_ids)
-        self.retriever.delete(chunk_ids)
-
-    def upvote(self, queries: List[str], chunk_ids: List[ChunkId], **kwargs):
-        self.retriever.upvote(queries, chunk_ids, **kwargs)
-
-    def associate(self, sources: List[str], targets: List[str], **kwargs):
-        self.retriever.associate(sources, targets, **kwargs)
-
-    def supervised_train(self, supervised: SupervisedDataset, **kwargs):
-        iterable = supervised.samples()
-
-        if isinstance(next(iter(iterable)), CustomIdSupervisedBatch):
-            iterable = self.chunk_store.remap_custom_ids(iterable)
-
-        self.retriever.supervised_train(iterable, **kwargs)
-
-    @staticmethod
-    def chunk_store_path(directory: str) -> str:
-        return os.path.join(directory, "chunk_store")
-
-    @staticmethod
-    def retriever_path(directory: str) -> str:
-        return os.path.join(directory, "retriever")
-
-    @staticmethod
-    def metadata_path(directory: str) -> str:
-        return os.path.join(directory, "metadata.json")
-
-    @staticmethod
-    def load_chunk_store(path: str, chunk_store_name: str):
-        chunk_store_name_map = {
-            "PandasChunkStore": PandasChunkStore,
-            "SQLiteChunkStore": SQLiteChunkStore,
-        }
-        if chunk_store_name not in chunk_store_name_map:
-            raise ValueError(f"Class name {chunk_store_name} not found in registry.")
-
-        return chunk_store_name_map[chunk_store_name].load(path)
-
-    @staticmethod
-    def load_retriever(path: str, retriever_name: str):
-        retriever_name_map = {
-            FinetunableRetriever.__name__: FinetunableRetriever,
-            Mach.__name__: Mach,
-        }
-        if retriever_name not in retriever_name_map:
-            raise ValueError(f"Class name {retriever_name} not found in registry.")
-
-        return retriever_name_map[retriever_name].load(path)
-
-    def save(self, path: str):
-        os.makedirs(path)
-
-        self.chunk_store.save(self.chunk_store_path(path))
-        self.retriever.save(self.retriever_path(path))
-
-        metadata = {
-            "chunk_store_name": self.chunk_store.__class__.__name__,
-            "retriever_name": self.retriever.__class__.__name__,
-        }
-
-        with open(self.metadata_path(path), "w") as f:
-            json.dump(metadata, f)
-
-    @staticmethod
-    def load(path: str):
-        with open(NeuralDB.metadata_path(path), "r") as f:
-            metadata = json.load(f)
-
-        chunk_store = NeuralDB.load_chunk_store(
-            NeuralDB.chunk_store_path(path),
-            chunk_store_name=metadata["chunk_store_name"],
-        )
-        retriever = NeuralDB.load_retriever(
-            NeuralDB.retriever_path(path),
-            retriever_name=metadata["retriever_name"],
-        )
-
-        return NeuralDB(chunk_store=chunk_store, retriever=retriever)
+import json
+import os
+from typing import Iterable, List, Optional, Tuple, Union
+
+from .chunk_stores import PandasChunkStore, SQLiteChunkStore
+from .core.chunk_store import ChunkStore
+from .core.documents import Document
+from .core.retriever import Retriever
+from .core.supervised import SupervisedDataset
+from .core.types import Chunk, ChunkId, CustomIdSupervisedBatch, NewChunkBatch, Score
+from .documents import document_by_name
+from .retrievers import FinetunableRetriever, Mach, MachEnsemble
+
+
+class NeuralDB:
+    def __init__(
+        self,
+        chunk_store: Optional[ChunkStore] = None,
+        retriever: Optional[Retriever] = None,
+        **kwargs,
+    ):
+        self.chunk_store = chunk_store or SQLiteChunkStore(**kwargs)
+        self.retriever = retriever or Mach(**kwargs)
+
+    def insert_chunks(self, chunks: Iterable[NewChunkBatch], **kwargs):
+        stored_chunks = self.chunk_store.insert(
+            chunks=chunks,
+            **kwargs,
+        )
+        self.retriever.insert(
+            chunks=stored_chunks,
+            **kwargs,
+        )
+
+    def insert(self, docs: List[Union[str, Document]], **kwargs):
+        docs = [
+            doc if isinstance(doc, Document) else document_by_name(doc) for doc in docs
+        ]
+
+        def chunk_generator():
+            for doc in docs:
+                for chunk in doc.chunks():
+                    yield chunk
+
+        self.insert_chunks(chunk_generator(), **kwargs)
+
+    def search(
+        self, query: str, top_k: int, constraints: dict = None, **kwargs
+    ) -> List[Tuple[Chunk, Score]]:
+        return self.search_batch([query], top_k, constraints, **kwargs)[0]
+
+    def search_batch(
+        self, queries: List[str], top_k: int, constraints: dict = None, **kwargs
+    ) -> List[List[Tuple[Chunk, Score]]]:
+        if not constraints:
+            results = self.retriever.search(queries, top_k, **kwargs)
+        else:
+            choices = self.chunk_store.filter_chunk_ids(constraints, **kwargs)
+            # TODO is there a better way that duplicating the constraints here
+            results = self.retriever.rank(queries, [choices for _ in queries], **kwargs)
+
+        chunk_results = []
+        for query_results in results:
+            chunk_ids, scores = [list(tup) for tup in zip(*query_results)]
+            chunks = self.chunk_store.get_chunks(chunk_ids)
+            chunk_results.append(list(zip(chunks, scores)))
+
+        return chunk_results
+
+    def delete(self, chunk_ids: List[ChunkId]):
+        self.chunk_store.delete(chunk_ids)
+        self.retriever.delete(chunk_ids)
+
+    def upvote(self, queries: List[str], chunk_ids: List[ChunkId], **kwargs):
+        self.retriever.upvote(queries, chunk_ids, **kwargs)
+
+    def associate(self, sources: List[str], targets: List[str], **kwargs):
+        self.retriever.associate(sources, targets, **kwargs)
+
+    def supervised_train(self, supervised: SupervisedDataset, **kwargs):
+        iterable = supervised.samples()
+
+        if isinstance(next(iter(iterable)), CustomIdSupervisedBatch):
+            iterable = self.chunk_store.remap_custom_ids(iterable)
+
+        self.retriever.supervised_train(iterable, **kwargs)
+
+    @staticmethod
+    def chunk_store_path(directory: str) -> str:
+        return os.path.join(directory, "chunk_store")
+
+    @staticmethod
+    def retriever_path(directory: str) -> str:
+        return os.path.join(directory, "retriever")
+
+    @staticmethod
+    def metadata_path(directory: str) -> str:
+        return os.path.join(directory, "metadata.json")
+
+    @staticmethod
+    def load_chunk_store(path: str, chunk_store_name: str):
+        chunk_store_name_map = {
+            "PandasChunkStore": PandasChunkStore,
+            "SQLiteChunkStore": SQLiteChunkStore,
+        }
+        if chunk_store_name not in chunk_store_name_map:
+            raise ValueError(f"Class name {chunk_store_name} not found in registry.")
+
+        return chunk_store_name_map[chunk_store_name].load(path)
+
+    @staticmethod
+    def load_retriever(path: str, retriever_name: str):
+        retriever_name_map = {
+            FinetunableRetriever.__name__: FinetunableRetriever,
+            Mach.__name__: Mach,
+            MachEnsemble.__name__: MachEnsemble,
+        }
+        if retriever_name not in retriever_name_map:
+            raise ValueError(f"Class name {retriever_name} not found in registry.")
+
+        return retriever_name_map[retriever_name].load(path)
+
+    def save(self, path: str):
+        os.makedirs(path)
+
+        self.chunk_store.save(self.chunk_store_path(path))
+        self.retriever.save(self.retriever_path(path))
+
+        metadata = {
+            "chunk_store_name": self.chunk_store.__class__.__name__,
+            "retriever_name": self.retriever.__class__.__name__,
+        }
+
+        with open(self.metadata_path(path), "w") as f:
+            json.dump(metadata, f)
+
+    @staticmethod
+    def load(path: str):
+        with open(NeuralDB.metadata_path(path), "r") as f:
+            metadata = json.load(f)
+
+        chunk_store = NeuralDB.load_chunk_store(
+            NeuralDB.chunk_store_path(path),
+            chunk_store_name=metadata["chunk_store_name"],
+        )
+        retriever = NeuralDB.load_retriever(
+            NeuralDB.retriever_path(path),
+            retriever_name=metadata["retriever_name"],
+        )
+
+        return NeuralDB(chunk_store=chunk_store, retriever=retriever)
```

## thirdai/neural_db_v2/__init__.py

 * *Ordering differences only*

```diff
@@ -1,13 +1,13 @@
-from .documents import (
-    CSV,
-    DOCX,
-    PDF,
-    PPTX,
-    URL,
-    Document,
-    Email,
-    InMemoryText,
-    TextFile,
-)
-from .neural_db import NeuralDB
-from .supervised import CsvSupervised, InMemorySupervised
+from .documents import (
+    CSV,
+    DOCX,
+    PDF,
+    PPTX,
+    URL,
+    Document,
+    Email,
+    InMemoryText,
+    TextFile,
+)
+from .neural_db import NeuralDB
+from .supervised import CsvSupervised, InMemorySupervised
```

## thirdai/neural_db_v2/chunk_stores/constraints.py

 * *Ordering differences only*

```diff
@@ -1,72 +1,72 @@
-from abc import ABC, abstractmethod
-
-import pandas as pd
-from sqlalchemy import Table
-
-
-class Constraint(ABC):
-    @abstractmethod
-    def sql_condition(self, column_name: str, table: Table):
-        raise NotImplementedError
-
-    @abstractmethod
-    def pd_filter(self, column_name: str, df: pd.DataFrame):
-        raise NotImplementedError
-
-
-class EqualTo(Constraint):
-    def __init__(self, value):
-        super().__init__()
-        self.value = value
-
-    def sql_condition(self, column_name: str, table: Table):
-        return table.c[column_name] == self.value
-
-    def pd_filter(self, column_name: str, df: pd.DataFrame):
-        return df[column_name] == self.value
-
-
-class AnyOf(Constraint):
-    def __init__(self, values):
-        super().__init__()
-        self.values = values
-
-    def sql_condition(self, column_name: str, table: Table):
-        return table.c[column_name].in_(self.values)
-
-    def pd_filter(self, column_name: str, df: pd.DataFrame):
-        return df[column_name].isin(self.values)
-
-
-class GreaterThan(Constraint):
-    def __init__(self, value, inclusive=True):
-        super().__init__()
-        self.value = value
-        self.inclusive = inclusive
-
-    def sql_condition(self, column_name: str, table: Table):
-        if self.inclusive:
-            return table.c[column_name] >= self.value
-        return table.c[column_name] > self.value
-
-    def pd_filter(self, column_name: str, df: pd.DataFrame):
-        if self.inclusive:
-            return df[column_name] >= self.value
-        return df[column_name] > self.value
-
-
-class LessThan(Constraint):
-    def __init__(self, value, inclusive=True):
-        super().__init__()
-        self.value = value
-        self.inclusive = inclusive
-
-    def sql_condition(self, column_name: str, table: Table):
-        if self.inclusive:
-            return table.c[column_name] <= self.value
-        return table.c[column_name] < self.value
-
-    def pd_filter(self, column_name: str, df: pd.DataFrame):
-        if self.inclusive:
-            return df[column_name] <= self.value
-        return df[column_name] < self.value
+from abc import ABC, abstractmethod
+
+import pandas as pd
+from sqlalchemy import Table
+
+
+class Constraint(ABC):
+    @abstractmethod
+    def sql_condition(self, column_name: str, table: Table):
+        raise NotImplementedError
+
+    @abstractmethod
+    def pd_filter(self, column_name: str, df: pd.DataFrame):
+        raise NotImplementedError
+
+
+class EqualTo(Constraint):
+    def __init__(self, value):
+        super().__init__()
+        self.value = value
+
+    def sql_condition(self, column_name: str, table: Table):
+        return table.c[column_name] == self.value
+
+    def pd_filter(self, column_name: str, df: pd.DataFrame):
+        return df[column_name] == self.value
+
+
+class AnyOf(Constraint):
+    def __init__(self, values):
+        super().__init__()
+        self.values = values
+
+    def sql_condition(self, column_name: str, table: Table):
+        return table.c[column_name].in_(self.values)
+
+    def pd_filter(self, column_name: str, df: pd.DataFrame):
+        return df[column_name].isin(self.values)
+
+
+class GreaterThan(Constraint):
+    def __init__(self, value, inclusive=True):
+        super().__init__()
+        self.value = value
+        self.inclusive = inclusive
+
+    def sql_condition(self, column_name: str, table: Table):
+        if self.inclusive:
+            return table.c[column_name] >= self.value
+        return table.c[column_name] > self.value
+
+    def pd_filter(self, column_name: str, df: pd.DataFrame):
+        if self.inclusive:
+            return df[column_name] >= self.value
+        return df[column_name] > self.value
+
+
+class LessThan(Constraint):
+    def __init__(self, value, inclusive=True):
+        super().__init__()
+        self.value = value
+        self.inclusive = inclusive
+
+    def sql_condition(self, column_name: str, table: Table):
+        if self.inclusive:
+            return table.c[column_name] <= self.value
+        return table.c[column_name] < self.value
+
+    def pd_filter(self, column_name: str, df: pd.DataFrame):
+        if self.inclusive:
+            return df[column_name] <= self.value
+        return df[column_name] < self.value
```

## thirdai/neural_db_v2/chunk_stores/pandas_chunk_store.py

 * *Ordering differences only*

```diff
@@ -1,180 +1,180 @@
-import operator
-import os
-from functools import reduce
-from typing import Dict, Iterable, List, Set, Union
-
-import numpy as np
-import pandas as pd
-from thirdai.neural_db.utils import pickle_to, unpickle_from
-
-from ..core.chunk_store import ChunkStore
-from ..core.types import (
-    Chunk,
-    ChunkBatch,
-    ChunkId,
-    CustomIdSupervisedBatch,
-    NewChunkBatch,
-    SupervisedBatch,
-)
-from .constraints import Constraint
-
-
-class PandasChunkStore(ChunkStore):
-    def __init__(self, **kwargs):
-        super().__init__()
-
-        self.chunk_df = pd.DataFrame()
-
-        self.custom_id_map = {}
-
-        self.metadata_df = pd.DataFrame()
-
-        self.next_id = 0
-
-    def _update_custom_ids(self, custom_ids, chunk_ids):
-        self._set_or_validate_custom_id_type(custom_ids)
-
-        if custom_ids is not None:
-            for custom_id, chunk_id in zip(custom_ids, chunk_ids):
-                self.custom_id_map[custom_id] = chunk_id
-
-    def insert(self, chunks: Iterable[NewChunkBatch], **kwargs) -> Iterable[ChunkBatch]:
-        all_chunks = [self.chunk_df]
-        all_metadata = [self.metadata_df]
-        output_batches = []
-        for batch in chunks:
-            chunk_ids = pd.Series(
-                np.arange(self.next_id, self.next_id + len(batch), dtype=np.int64)
-            )
-            self.next_id += len(batch)
-
-            chunk_df = batch.to_df()
-            chunk_df["chunk_id"] = chunk_ids
-
-            all_chunks.append(chunk_df)
-
-            if batch.metadata is not None:
-                metadata = batch.metadata.copy(deep=False)
-                metadata["chunk_id"] = chunk_ids
-                all_metadata.append(metadata)
-
-            self._update_custom_ids(batch.custom_id, chunk_ids)
-
-            output_batches.append(
-                ChunkBatch(chunk_id=chunk_ids, text=batch.text, keywords=batch.keywords)
-            )
-
-        self.chunk_df = pd.concat(all_chunks)
-        self.chunk_df.set_index("chunk_id", inplace=True, drop=False)
-
-        self.metadata_df = pd.concat(all_metadata)
-
-        if not self.metadata_df.empty:
-            # Numpy will default missing values to NaN, however we want missing values
-            # to be None so that it's consistent with the behavior of sqlalchemy.
-            self.metadata_df.replace(to_replace=np.nan, value=None, inplace=True)
-            self.metadata_df.set_index("chunk_id", inplace=True, drop=False)
-
-        return output_batches
-
-    def delete(self, chunk_ids: List[ChunkId]):
-        self.chunk_df.drop(chunk_ids, inplace=True)
-        if not self.metadata_df.empty:
-            self.metadata_df.drop(chunk_ids, inplace=True)
-
-        chunk_ids = set(chunk_ids)
-        for custom_id, chunk_id in list(self.custom_id_map.items()):
-            if chunk_id in chunk_ids:
-                del self.custom_id_map[custom_id]
-
-    def get_chunks(self, chunk_ids: List[ChunkId], **kwargs) -> List[Chunk]:
-        try:
-            chunks = self.chunk_df.loc[chunk_ids]
-            metadatas = (
-                self.metadata_df.loc[chunk_ids] if not self.metadata_df.empty else None
-            )
-        except KeyError:
-            raise ValueError(
-                f"Could not find chunk with one or more ids in {chunk_ids}."
-            )
-        output_chunks = []
-        for i, row in enumerate(chunks.itertuples()):
-            if metadatas is not None:
-                metadata = metadatas.iloc[i].to_dict()
-                del metadata["chunk_id"]
-            else:
-                metadata = None
-            output_chunks.append(
-                Chunk(
-                    custom_id=row.custom_id,
-                    text=row.text,
-                    keywords=row.keywords,
-                    metadata=metadata,
-                    document=row.document,
-                    chunk_id=row.chunk_id,
-                )
-            )
-        return output_chunks
-
-    def filter_chunk_ids(
-        self, constraints: Dict[str, Constraint], **kwargs
-    ) -> Set[ChunkId]:
-        if not len(constraints):
-            raise ValueError("Cannot call filter_chunk_ids with empty constraints.")
-
-        if self.metadata_df.empty:
-            raise ValueError("Cannot filter constraints with no metadata.")
-
-        condition = reduce(
-            operator.and_,
-            [
-                constraint.pd_filter(column_name=column, df=self.metadata_df)
-                for column, constraint in constraints.items()
-            ],
-        )
-
-        return set(self.chunk_df[condition]["chunk_id"])
-
-    def _remap_id(self, custom_id: Union[int, str]) -> int:
-        if custom_id not in self.custom_id_map:
-            reversed_id = (
-                int(custom_id)
-                if isinstance(custom_id, str) and custom_id.isdigit()
-                else str(custom_id)
-            )
-            if reversed_id not in self.custom_id_map:
-                raise ValueError(f"Could not find chunk with custom id {custom_id}.")
-            return self.custom_id_map[reversed_id]
-        return self.custom_id_map[custom_id]
-
-    def remap_custom_ids(
-        self, samples: Iterable[CustomIdSupervisedBatch]
-    ) -> Iterable[SupervisedBatch]:
-
-        if not self.custom_id_map:
-            raise ValueError(f"Chunk Store does not contain custom ids.")
-
-        return [
-            SupervisedBatch(
-                query=batch.query,
-                chunk_id=pd.Series(
-                    [
-                        list(map(self._remap_id, custom_ids))
-                        for custom_ids in batch.custom_id
-                    ]
-                ),
-            )
-            for batch in samples
-        ]
-
-    @staticmethod
-    def object_pickle_path(path):
-        return os.path.join(path, "object.pkl")
-
-    def save(self, path: str):
-        os.makedirs(path)
-        pickle_to(self, self.object_pickle_path(path))
-
-    @classmethod
-    def load(cls, path: str):
-        return unpickle_from(PandasChunkStore.object_pickle_path(path))
+import operator
+import os
+from functools import reduce
+from typing import Dict, Iterable, List, Set, Union
+
+import numpy as np
+import pandas as pd
+from thirdai.neural_db.utils import pickle_to, unpickle_from
+
+from ..core.chunk_store import ChunkStore
+from ..core.types import (
+    Chunk,
+    ChunkBatch,
+    ChunkId,
+    CustomIdSupervisedBatch,
+    NewChunkBatch,
+    SupervisedBatch,
+)
+from .constraints import Constraint
+
+
+class PandasChunkStore(ChunkStore):
+    def __init__(self, **kwargs):
+        super().__init__()
+
+        self.chunk_df = pd.DataFrame()
+
+        self.custom_id_map = {}
+
+        self.metadata_df = pd.DataFrame()
+
+        self.next_id = 0
+
+    def _update_custom_ids(self, custom_ids, chunk_ids):
+        self._set_or_validate_custom_id_type(custom_ids)
+
+        if custom_ids is not None:
+            for custom_id, chunk_id in zip(custom_ids, chunk_ids):
+                self.custom_id_map[custom_id] = chunk_id
+
+    def insert(self, chunks: Iterable[NewChunkBatch], **kwargs) -> Iterable[ChunkBatch]:
+        all_chunks = [self.chunk_df]
+        all_metadata = [self.metadata_df]
+        output_batches = []
+        for batch in chunks:
+            chunk_ids = pd.Series(
+                np.arange(self.next_id, self.next_id + len(batch), dtype=np.int64)
+            )
+            self.next_id += len(batch)
+
+            chunk_df = batch.to_df()
+            chunk_df["chunk_id"] = chunk_ids
+
+            all_chunks.append(chunk_df)
+
+            if batch.metadata is not None:
+                metadata = batch.metadata.copy(deep=False)
+                metadata["chunk_id"] = chunk_ids
+                all_metadata.append(metadata)
+
+            self._update_custom_ids(batch.custom_id, chunk_ids)
+
+            output_batches.append(
+                ChunkBatch(chunk_id=chunk_ids, text=batch.text, keywords=batch.keywords)
+            )
+
+        self.chunk_df = pd.concat(all_chunks)
+        self.chunk_df.set_index("chunk_id", inplace=True, drop=False)
+
+        self.metadata_df = pd.concat(all_metadata)
+
+        if not self.metadata_df.empty:
+            # Numpy will default missing values to NaN, however we want missing values
+            # to be None so that it's consistent with the behavior of sqlalchemy.
+            self.metadata_df.replace(to_replace=np.nan, value=None, inplace=True)
+            self.metadata_df.set_index("chunk_id", inplace=True, drop=False)
+
+        return output_batches
+
+    def delete(self, chunk_ids: List[ChunkId]):
+        self.chunk_df.drop(chunk_ids, inplace=True)
+        if not self.metadata_df.empty:
+            self.metadata_df.drop(chunk_ids, inplace=True)
+
+        chunk_ids = set(chunk_ids)
+        for custom_id, chunk_id in list(self.custom_id_map.items()):
+            if chunk_id in chunk_ids:
+                del self.custom_id_map[custom_id]
+
+    def get_chunks(self, chunk_ids: List[ChunkId], **kwargs) -> List[Chunk]:
+        try:
+            chunks = self.chunk_df.loc[chunk_ids]
+            metadatas = (
+                self.metadata_df.loc[chunk_ids] if not self.metadata_df.empty else None
+            )
+        except KeyError:
+            raise ValueError(
+                f"Could not find chunk with one or more ids in {chunk_ids}."
+            )
+        output_chunks = []
+        for i, row in enumerate(chunks.itertuples()):
+            if metadatas is not None:
+                metadata = metadatas.iloc[i].to_dict()
+                del metadata["chunk_id"]
+            else:
+                metadata = None
+            output_chunks.append(
+                Chunk(
+                    custom_id=row.custom_id,
+                    text=row.text,
+                    keywords=row.keywords,
+                    metadata=metadata,
+                    document=row.document,
+                    chunk_id=row.chunk_id,
+                )
+            )
+        return output_chunks
+
+    def filter_chunk_ids(
+        self, constraints: Dict[str, Constraint], **kwargs
+    ) -> Set[ChunkId]:
+        if not len(constraints):
+            raise ValueError("Cannot call filter_chunk_ids with empty constraints.")
+
+        if self.metadata_df.empty:
+            raise ValueError("Cannot filter constraints with no metadata.")
+
+        condition = reduce(
+            operator.and_,
+            [
+                constraint.pd_filter(column_name=column, df=self.metadata_df)
+                for column, constraint in constraints.items()
+            ],
+        )
+
+        return set(self.chunk_df[condition]["chunk_id"])
+
+    def _remap_id(self, custom_id: Union[int, str]) -> int:
+        if custom_id not in self.custom_id_map:
+            reversed_id = (
+                int(custom_id)
+                if isinstance(custom_id, str) and custom_id.isdigit()
+                else str(custom_id)
+            )
+            if reversed_id not in self.custom_id_map:
+                raise ValueError(f"Could not find chunk with custom id {custom_id}.")
+            return self.custom_id_map[reversed_id]
+        return self.custom_id_map[custom_id]
+
+    def remap_custom_ids(
+        self, samples: Iterable[CustomIdSupervisedBatch]
+    ) -> Iterable[SupervisedBatch]:
+
+        if not self.custom_id_map:
+            raise ValueError(f"Chunk Store does not contain custom ids.")
+
+        return [
+            SupervisedBatch(
+                query=batch.query,
+                chunk_id=pd.Series(
+                    [
+                        list(map(self._remap_id, custom_ids))
+                        for custom_ids in batch.custom_id
+                    ]
+                ),
+            )
+            for batch in samples
+        ]
+
+    @staticmethod
+    def object_pickle_path(path):
+        return os.path.join(path, "object.pkl")
+
+    def save(self, path: str):
+        os.makedirs(path)
+        pickle_to(self, self.object_pickle_path(path))
+
+    @classmethod
+    def load(cls, path: str):
+        return unpickle_from(PandasChunkStore.object_pickle_path(path))
```

## thirdai/neural_db_v2/chunk_stores/sqlite_chunk_store.py

 * *Ordering differences only*

```diff
@@ -1,366 +1,366 @@
-import operator
-import os
-import shutil
-import uuid
-from functools import reduce
-from typing import Dict, Iterable, List, Optional, Set
-
-import numpy as np
-import pandas as pd
-from sqlalchemy import (
-    Column,
-    Engine,
-    Float,
-    Integer,
-    MetaData,
-    String,
-    Table,
-    create_engine,
-    delete,
-    select,
-    text,
-)
-from thirdai.neural_db.utils import pickle_to, unpickle_from
-
-from ..core.chunk_store import ChunkStore, CustomIDType
-from ..core.types import (
-    Chunk,
-    ChunkBatch,
-    ChunkId,
-    CustomIdSupervisedBatch,
-    NewChunkBatch,
-    SupervisedBatch,
-)
-from .constraints import Constraint
-
-
-def get_sql_columns(df: pd.DataFrame):
-    columns = []
-    for col in df.columns:
-        dtype = df[col].dtype
-        if dtype == int:
-            columns.append(Column(col, Integer))
-        elif dtype == float:
-            columns.append(Column(col, Float))
-        elif dtype == object:
-            columns.append(Column(col, String))
-        else:
-            raise ValueError(
-                f"Column {col} has dtype {str(dtype)} which is not a supported type for metadata columns."
-            )
-    return columns
-
-
-class SqlLiteIterator:
-    def __init__(
-        self,
-        table: Table,
-        engine: Engine,
-        min_insertion_chunk_id: int,
-        max_insertion_chunk_id: int,
-        batch_size: int = 100,
-    ):
-        self.chunk_table = table
-        self.engine = engine
-
-        # Since assigned chunk_ids are contiguous, each SqlLiteIterator can search
-        # through a range of chunk_ids. We need a min and a max we do an insertion
-        # while another iterator still exists
-        self.min_insertion_chunk_id = min_insertion_chunk_id
-        self.max_insertion_chunk_id = max_insertion_chunk_id
-
-        self.batch_size = batch_size
-
-    def __next__(self) -> Optional[ChunkBatch]:
-        # The "next" call on the sql_row_iterator returns one row at a time
-        # despite fetching them in "batch_size" quantities from the database.
-        # Thus we call "next" "batch_size" times to pull out all the rows we want
-        sql_lite_batch = []
-        try:
-            for _ in range(self.batch_size):
-                sql_lite_batch.append(next(self.sql_row_iterator))
-        except StopIteration:
-            if not sql_lite_batch:
-                raise StopIteration
-
-        df = pd.DataFrame(sql_lite_batch, columns=self.sql_row_iterator.keys())
-
-        return ChunkBatch(
-            chunk_id=df["chunk_id"],
-            text=df["text"],
-            keywords=df["keywords"],
-        )
-
-    def __iter__(self):
-        stmt = select(self.chunk_table).where(
-            (self.chunk_table.c.chunk_id >= self.min_insertion_chunk_id)
-            & (self.chunk_table.c.chunk_id < self.max_insertion_chunk_id)
-        )
-        with self.engine.connect() as conn:
-            result = conn.execute(stmt)
-            self.sql_row_iterator = result.yield_per(self.batch_size)
-        return self
-
-
-class SQLiteChunkStore(ChunkStore):
-    def __init__(self, **kwargs):
-        super().__init__()
-
-        self.db_name = f"{uuid.uuid4()}.db"
-        self.engine = create_engine(f"sqlite:///{self.db_name}")
-
-        self.metadata = MetaData()
-
-        self.chunk_table = Table(
-            "neural_db_chunks",
-            self.metadata,
-            Column("chunk_id", Integer, primary_key=True),
-            Column("custom_id", Integer),
-            Column("text", String),
-            Column("keywords", String),
-            Column("document", String),
-        )
-        self.metadata.create_all(self.engine)
-
-        self.custom_id_table = None
-
-        self.metadata_table = None
-
-        self.next_id = 0
-
-    def _write_to_table(self, df: pd.DataFrame, table: Table):
-        df.to_sql(
-            table.name,
-            con=self.engine,
-            dtype={c.name: c.type for c in table.columns},
-            if_exists="append",
-            index=False,
-        )
-
-    def _create_custom_id_table(self):
-        custom_id_dtype = (
-            Integer if self.custom_id_type == CustomIDType.Integer else String
-        )
-        self.custom_id_table = Table(
-            "neural_db_custom_ids",
-            self.metadata,
-            Column("custom_id", custom_id_dtype, primary_key=True),
-            Column("chunk_id", Integer),
-        )
-        self.metadata.create_all(self.engine)
-
-    def _update_custom_ids(self, custom_ids, chunk_ids):
-        self._set_or_validate_custom_id_type(custom_ids)
-
-        if custom_ids is not None:
-            if self.custom_id_table is None:
-                self._create_custom_id_table()
-
-            custom_id_df = pd.DataFrame(
-                {"custom_id": custom_ids, "chunk_id": chunk_ids}
-            )
-            self._write_to_table(df=custom_id_df, table=self.custom_id_table)
-
-    def _add_metadata_column(self, column: Column):
-        column_name = column.compile(dialect=self.engine.dialect)
-        column_type = column.type.compile(self.engine.dialect)
-        stmt = text(
-            f"ALTER TABLE {self.metadata_table.name} ADD COLUMN {column_name} {column_type}"
-        )
-
-        with self.engine.begin() as conn:
-            conn.execute(stmt)
-
-        # This is so that sqlalchemy recognizes the new column.
-        self.metadata = MetaData()
-        self.metadata.reflect(bind=self.engine)
-        self.metadata_table = Table(
-            self.metadata_table.name, self.metadata, autoload_with=self.engine
-        )
-
-    def _store_metadata(self, metadata: pd.DataFrame, chunk_ids: pd.Series):
-        metadata_columns = get_sql_columns(metadata)
-        if self.metadata_table is None:
-            self.metadata_table = Table(
-                "neural_db_metadata",
-                self.metadata,
-                Column("chunk_id", Integer, primary_key=True),
-                *metadata_columns,
-            )
-            self.metadata.create_all(self.engine)
-        else:
-            for column in metadata_columns:
-                if column.name not in self.metadata_table.columns:
-                    self._add_metadata_column(column=column)
-                else:
-                    if str(column.type) != str(
-                        self.metadata_table.columns[column.name].type
-                    ):
-                        raise ValueError(
-                            f"Existing metadata for column {column.name} has type {str(self.metadata_table.columns[column.name].type)} but new metadata has type {str(column.type)}."
-                        )
-        metadata["chunk_id"] = chunk_ids
-        self._write_to_table(df=metadata, table=self.metadata_table)
-
-    def insert(self, chunks: Iterable[NewChunkBatch], **kwargs) -> Iterable[ChunkBatch]:
-        min_insertion_chunk_id = self.next_id
-        for batch in chunks:
-            chunk_ids = pd.Series(
-                np.arange(self.next_id, self.next_id + len(batch), dtype=np.int64)
-            )
-            self.next_id += len(batch)
-
-            chunk_df = batch.to_df()
-            chunk_df["chunk_id"] = chunk_ids
-
-            self._update_custom_ids(custom_ids=batch.custom_id, chunk_ids=chunk_ids)
-
-            if batch.metadata is not None:
-                self._store_metadata(batch.metadata, chunk_ids=chunk_ids)
-
-            self._write_to_table(df=chunk_df, table=self.chunk_table)
-
-        max_insertion_chunk_id = self.next_id
-
-        inserted_chunks_iterator = SqlLiteIterator(
-            table=self.chunk_table,
-            engine=self.engine,
-            min_insertion_chunk_id=min_insertion_chunk_id,
-            max_insertion_chunk_id=max_insertion_chunk_id,
-            batch_size=kwargs.get("sql_lite_iterator_batch_size", 100),
-        )
-
-        return inserted_chunks_iterator
-
-    def delete(self, chunk_ids: List[ChunkId]):
-        with self.engine.begin() as conn:
-            delete_chunks = delete(self.chunk_table).where(
-                self.chunk_table.c.chunk_id.in_(chunk_ids)
-            )
-            conn.execute(delete_chunks)
-
-            if self.metadata_table is not None:
-                delete_metadata = delete(self.metadata_table).where(
-                    self.metadata_table.c.chunk_id.in_(chunk_ids)
-                )
-                conn.execute(delete_metadata)
-
-            if self.custom_id_table is not None:
-                delete_chunk_ids = delete(self.custom_id_table).where(
-                    self.custom_id_table.c.chunk_id.in_(chunk_ids)
-                )
-                conn.execute(delete_chunk_ids)
-
-    def get_chunks(self, chunk_ids: List[ChunkId], **kwargs) -> List[Chunk]:
-        id_to_chunk = {}
-
-        with self.engine.connect() as conn:
-            chunk_stmt = select(self.chunk_table).where(
-                self.chunk_table.c.chunk_id.in_(chunk_ids)
-            )
-            for row in conn.execute(chunk_stmt).all():
-                id_to_chunk[row.chunk_id] = Chunk(
-                    custom_id=row.custom_id,
-                    text=row.text,
-                    keywords=row.keywords,
-                    document=row.document,
-                    chunk_id=row.chunk_id,
-                    metadata=None,
-                )
-
-            if self.metadata_table is not None:
-                metadata_stmt = select(self.metadata_table).where(
-                    self.metadata_table.c.chunk_id.in_(chunk_ids)
-                )
-                for row in conn.execute(metadata_stmt).all():
-                    metadata = row._asdict()
-                    del metadata["chunk_id"]
-                    id_to_chunk[row.chunk_id].metadata = metadata
-
-        chunks = []
-        for chunk_id in chunk_ids:
-            if chunk_id not in id_to_chunk:
-                raise ValueError(f"Could not find chunk with id {chunk_id}.")
-            chunks.append(id_to_chunk[chunk_id])
-
-        return chunks
-
-    def filter_chunk_ids(
-        self, constraints: Dict[str, Constraint], **kwargs
-    ) -> Set[ChunkId]:
-        if not len(constraints):
-            raise ValueError("Cannot call filter_chunk_ids with empty constraints.")
-
-        if self.metadata_table is None:
-            raise ValueError("Cannot filter constraints with no metadata.")
-
-        condition = reduce(
-            operator.and_,
-            [
-                constraint.sql_condition(column_name=column, table=self.metadata_table)
-                for column, constraint in constraints.items()
-            ],
-        )
-
-        stmt = select(self.metadata_table.c.chunk_id).where(condition)
-
-        chunk_ids = set()
-        with self.engine.connect() as conn:
-            for row in conn.execute(stmt):
-                chunk_ids.add(row.chunk_id)
-
-        return chunk_ids
-
-    def remap_custom_ids(
-        self, samples: Iterable[CustomIdSupervisedBatch]
-    ) -> Iterable[SupervisedBatch]:
-        remapped_batches = []
-
-        if self.custom_id_table is None:
-            raise ValueError(f"Chunk Store does not contain custom ids.")
-
-        for batch in samples:
-            chunk_ids = []
-            with self.engine.connect() as conn:
-                for custom_ids in batch.custom_id:
-                    sample_ids = []
-                    for custom_id in custom_ids:
-                        stmt = select(self.custom_id_table.c.chunk_id).where(
-                            self.custom_id_table.c.custom_id == custom_id
-                        )
-                        if result := conn.execute(stmt).first():
-                            sample_ids.append(result.chunk_id)
-                        else:
-                            raise ValueError(
-                                f"Could not find chunk with custom id {custom_id}."
-                            )
-                    chunk_ids.append(sample_ids)
-
-            remapped_batches.append(
-                SupervisedBatch(query=batch.query, chunk_id=pd.Series(chunk_ids))
-            )
-
-        return remapped_batches
-
-    def save(self, path: str):
-        os.makedirs(path)
-        db_target_path = os.path.join(path, self.db_name)
-        shutil.copyfile(self.db_name, db_target_path)
-
-        contents = {k: v for k, v in self.__dict__.items() if k != "engine"}
-        pickle_path = os.path.join(path, "object.pkl")
-        pickle_to(contents, pickle_path)
-
-    @classmethod
-    def load(cls, path: str):
-        pickle_path = os.path.join(path, "object.pkl")
-        contents = unpickle_from(pickle_path)
-
-        obj = cls.__new__(cls)
-        obj.__dict__.update(contents)
-
-        db_name = os.path.basename(obj.db_name)
-        obj.db_name = os.path.join(path, db_name)
-        obj.engine = create_engine(f"sqlite:///{obj.db_name}")
-
-        return obj
+import operator
+import os
+import shutil
+import uuid
+from functools import reduce
+from typing import Dict, Iterable, List, Optional, Set
+
+import numpy as np
+import pandas as pd
+from sqlalchemy import (
+    Column,
+    Engine,
+    Float,
+    Integer,
+    MetaData,
+    String,
+    Table,
+    create_engine,
+    delete,
+    select,
+    text,
+)
+from thirdai.neural_db.utils import pickle_to, unpickle_from
+
+from ..core.chunk_store import ChunkStore, CustomIDType
+from ..core.types import (
+    Chunk,
+    ChunkBatch,
+    ChunkId,
+    CustomIdSupervisedBatch,
+    NewChunkBatch,
+    SupervisedBatch,
+)
+from .constraints import Constraint
+
+
+def get_sql_columns(df: pd.DataFrame):
+    columns = []
+    for col in df.columns:
+        dtype = df[col].dtype
+        if dtype == int:
+            columns.append(Column(col, Integer))
+        elif dtype == float:
+            columns.append(Column(col, Float))
+        elif dtype == object:
+            columns.append(Column(col, String))
+        else:
+            raise ValueError(
+                f"Column {col} has dtype {str(dtype)} which is not a supported type for metadata columns."
+            )
+    return columns
+
+
+class SqlLiteIterator:
+    def __init__(
+        self,
+        table: Table,
+        engine: Engine,
+        min_insertion_chunk_id: int,
+        max_insertion_chunk_id: int,
+        batch_size: int = 100,
+    ):
+        self.chunk_table = table
+        self.engine = engine
+
+        # Since assigned chunk_ids are contiguous, each SqlLiteIterator can search
+        # through a range of chunk_ids. We need a min and a max we do an insertion
+        # while another iterator still exists
+        self.min_insertion_chunk_id = min_insertion_chunk_id
+        self.max_insertion_chunk_id = max_insertion_chunk_id
+
+        self.batch_size = batch_size
+
+    def __next__(self) -> Optional[ChunkBatch]:
+        # The "next" call on the sql_row_iterator returns one row at a time
+        # despite fetching them in "batch_size" quantities from the database.
+        # Thus we call "next" "batch_size" times to pull out all the rows we want
+        sql_lite_batch = []
+        try:
+            for _ in range(self.batch_size):
+                sql_lite_batch.append(next(self.sql_row_iterator))
+        except StopIteration:
+            if not sql_lite_batch:
+                raise StopIteration
+
+        df = pd.DataFrame(sql_lite_batch, columns=self.sql_row_iterator.keys())
+
+        return ChunkBatch(
+            chunk_id=df["chunk_id"],
+            text=df["text"],
+            keywords=df["keywords"],
+        )
+
+    def __iter__(self):
+        stmt = select(self.chunk_table).where(
+            (self.chunk_table.c.chunk_id >= self.min_insertion_chunk_id)
+            & (self.chunk_table.c.chunk_id < self.max_insertion_chunk_id)
+        )
+        with self.engine.connect() as conn:
+            result = conn.execute(stmt)
+            self.sql_row_iterator = result.yield_per(self.batch_size)
+        return self
+
+
+class SQLiteChunkStore(ChunkStore):
+    def __init__(self, **kwargs):
+        super().__init__()
+
+        self.db_name = f"{uuid.uuid4()}.db"
+        self.engine = create_engine(f"sqlite:///{self.db_name}")
+
+        self.metadata = MetaData()
+
+        self.chunk_table = Table(
+            "neural_db_chunks",
+            self.metadata,
+            Column("chunk_id", Integer, primary_key=True),
+            Column("custom_id", Integer),
+            Column("text", String),
+            Column("keywords", String),
+            Column("document", String),
+        )
+        self.metadata.create_all(self.engine)
+
+        self.custom_id_table = None
+
+        self.metadata_table = None
+
+        self.next_id = 0
+
+    def _write_to_table(self, df: pd.DataFrame, table: Table):
+        df.to_sql(
+            table.name,
+            con=self.engine,
+            dtype={c.name: c.type for c in table.columns},
+            if_exists="append",
+            index=False,
+        )
+
+    def _create_custom_id_table(self):
+        custom_id_dtype = (
+            Integer if self.custom_id_type == CustomIDType.Integer else String
+        )
+        self.custom_id_table = Table(
+            "neural_db_custom_ids",
+            self.metadata,
+            Column("custom_id", custom_id_dtype, primary_key=True),
+            Column("chunk_id", Integer),
+        )
+        self.metadata.create_all(self.engine)
+
+    def _update_custom_ids(self, custom_ids, chunk_ids):
+        self._set_or_validate_custom_id_type(custom_ids)
+
+        if custom_ids is not None:
+            if self.custom_id_table is None:
+                self._create_custom_id_table()
+
+            custom_id_df = pd.DataFrame(
+                {"custom_id": custom_ids, "chunk_id": chunk_ids}
+            )
+            self._write_to_table(df=custom_id_df, table=self.custom_id_table)
+
+    def _add_metadata_column(self, column: Column):
+        column_name = column.compile(dialect=self.engine.dialect)
+        column_type = column.type.compile(self.engine.dialect)
+        stmt = text(
+            f"ALTER TABLE {self.metadata_table.name} ADD COLUMN {column_name} {column_type}"
+        )
+
+        with self.engine.begin() as conn:
+            conn.execute(stmt)
+
+        # This is so that sqlalchemy recognizes the new column.
+        self.metadata = MetaData()
+        self.metadata.reflect(bind=self.engine)
+        self.metadata_table = Table(
+            self.metadata_table.name, self.metadata, autoload_with=self.engine
+        )
+
+    def _store_metadata(self, metadata: pd.DataFrame, chunk_ids: pd.Series):
+        metadata_columns = get_sql_columns(metadata)
+        if self.metadata_table is None:
+            self.metadata_table = Table(
+                "neural_db_metadata",
+                self.metadata,
+                Column("chunk_id", Integer, primary_key=True),
+                *metadata_columns,
+            )
+            self.metadata.create_all(self.engine)
+        else:
+            for column in metadata_columns:
+                if column.name not in self.metadata_table.columns:
+                    self._add_metadata_column(column=column)
+                else:
+                    if str(column.type) != str(
+                        self.metadata_table.columns[column.name].type
+                    ):
+                        raise ValueError(
+                            f"Existing metadata for column {column.name} has type {str(self.metadata_table.columns[column.name].type)} but new metadata has type {str(column.type)}."
+                        )
+        metadata["chunk_id"] = chunk_ids
+        self._write_to_table(df=metadata, table=self.metadata_table)
+
+    def insert(self, chunks: Iterable[NewChunkBatch], **kwargs) -> Iterable[ChunkBatch]:
+        min_insertion_chunk_id = self.next_id
+        for batch in chunks:
+            chunk_ids = pd.Series(
+                np.arange(self.next_id, self.next_id + len(batch), dtype=np.int64)
+            )
+            self.next_id += len(batch)
+
+            chunk_df = batch.to_df()
+            chunk_df["chunk_id"] = chunk_ids
+
+            self._update_custom_ids(custom_ids=batch.custom_id, chunk_ids=chunk_ids)
+
+            if batch.metadata is not None:
+                self._store_metadata(batch.metadata, chunk_ids=chunk_ids)
+
+            self._write_to_table(df=chunk_df, table=self.chunk_table)
+
+        max_insertion_chunk_id = self.next_id
+
+        inserted_chunks_iterator = SqlLiteIterator(
+            table=self.chunk_table,
+            engine=self.engine,
+            min_insertion_chunk_id=min_insertion_chunk_id,
+            max_insertion_chunk_id=max_insertion_chunk_id,
+            batch_size=kwargs.get("sql_lite_iterator_batch_size", 100),
+        )
+
+        return inserted_chunks_iterator
+
+    def delete(self, chunk_ids: List[ChunkId]):
+        with self.engine.begin() as conn:
+            delete_chunks = delete(self.chunk_table).where(
+                self.chunk_table.c.chunk_id.in_(chunk_ids)
+            )
+            conn.execute(delete_chunks)
+
+            if self.metadata_table is not None:
+                delete_metadata = delete(self.metadata_table).where(
+                    self.metadata_table.c.chunk_id.in_(chunk_ids)
+                )
+                conn.execute(delete_metadata)
+
+            if self.custom_id_table is not None:
+                delete_chunk_ids = delete(self.custom_id_table).where(
+                    self.custom_id_table.c.chunk_id.in_(chunk_ids)
+                )
+                conn.execute(delete_chunk_ids)
+
+    def get_chunks(self, chunk_ids: List[ChunkId], **kwargs) -> List[Chunk]:
+        id_to_chunk = {}
+
+        with self.engine.connect() as conn:
+            chunk_stmt = select(self.chunk_table).where(
+                self.chunk_table.c.chunk_id.in_(chunk_ids)
+            )
+            for row in conn.execute(chunk_stmt).all():
+                id_to_chunk[row.chunk_id] = Chunk(
+                    custom_id=row.custom_id,
+                    text=row.text,
+                    keywords=row.keywords,
+                    document=row.document,
+                    chunk_id=row.chunk_id,
+                    metadata=None,
+                )
+
+            if self.metadata_table is not None:
+                metadata_stmt = select(self.metadata_table).where(
+                    self.metadata_table.c.chunk_id.in_(chunk_ids)
+                )
+                for row in conn.execute(metadata_stmt).all():
+                    metadata = row._asdict()
+                    del metadata["chunk_id"]
+                    id_to_chunk[row.chunk_id].metadata = metadata
+
+        chunks = []
+        for chunk_id in chunk_ids:
+            if chunk_id not in id_to_chunk:
+                raise ValueError(f"Could not find chunk with id {chunk_id}.")
+            chunks.append(id_to_chunk[chunk_id])
+
+        return chunks
+
+    def filter_chunk_ids(
+        self, constraints: Dict[str, Constraint], **kwargs
+    ) -> Set[ChunkId]:
+        if not len(constraints):
+            raise ValueError("Cannot call filter_chunk_ids with empty constraints.")
+
+        if self.metadata_table is None:
+            raise ValueError("Cannot filter constraints with no metadata.")
+
+        condition = reduce(
+            operator.and_,
+            [
+                constraint.sql_condition(column_name=column, table=self.metadata_table)
+                for column, constraint in constraints.items()
+            ],
+        )
+
+        stmt = select(self.metadata_table.c.chunk_id).where(condition)
+
+        chunk_ids = set()
+        with self.engine.connect() as conn:
+            for row in conn.execute(stmt):
+                chunk_ids.add(row.chunk_id)
+
+        return chunk_ids
+
+    def remap_custom_ids(
+        self, samples: Iterable[CustomIdSupervisedBatch]
+    ) -> Iterable[SupervisedBatch]:
+        remapped_batches = []
+
+        if self.custom_id_table is None:
+            raise ValueError(f"Chunk Store does not contain custom ids.")
+
+        for batch in samples:
+            chunk_ids = []
+            with self.engine.connect() as conn:
+                for custom_ids in batch.custom_id:
+                    sample_ids = []
+                    for custom_id in custom_ids:
+                        stmt = select(self.custom_id_table.c.chunk_id).where(
+                            self.custom_id_table.c.custom_id == custom_id
+                        )
+                        if result := conn.execute(stmt).first():
+                            sample_ids.append(result.chunk_id)
+                        else:
+                            raise ValueError(
+                                f"Could not find chunk with custom id {custom_id}."
+                            )
+                    chunk_ids.append(sample_ids)
+
+            remapped_batches.append(
+                SupervisedBatch(query=batch.query, chunk_id=pd.Series(chunk_ids))
+            )
+
+        return remapped_batches
+
+    def save(self, path: str):
+        os.makedirs(path)
+        db_target_path = os.path.join(path, self.db_name)
+        shutil.copyfile(self.db_name, db_target_path)
+
+        contents = {k: v for k, v in self.__dict__.items() if k != "engine"}
+        pickle_path = os.path.join(path, "object.pkl")
+        pickle_to(contents, pickle_path)
+
+    @classmethod
+    def load(cls, path: str):
+        pickle_path = os.path.join(path, "object.pkl")
+        contents = unpickle_from(pickle_path)
+
+        obj = cls.__new__(cls)
+        obj.__dict__.update(contents)
+
+        db_name = os.path.basename(obj.db_name)
+        obj.db_name = os.path.join(path, db_name)
+        obj.engine = create_engine(f"sqlite:///{obj.db_name}")
+
+        return obj
```

## thirdai/neural_db_v2/chunk_stores/__init__.py

 * *Ordering differences only*

```diff
@@ -1,2 +1,2 @@
-from .pandas_chunk_store import PandasChunkStore
-from .sqlite_chunk_store import SQLiteChunkStore
+from .pandas_chunk_store import PandasChunkStore
+from .sqlite_chunk_store import SQLiteChunkStore
```

## thirdai/neural_db_v2/core/chunk_store.py

 * *Ordering differences only*

```diff
@@ -1,68 +1,68 @@
-from abc import ABC, abstractmethod
-from enum import Enum
-from typing import Iterable, List, Set
-
-from pandas.api.types import is_numeric_dtype, is_string_dtype
-
-from .types import (
-    Chunk,
-    ChunkBatch,
-    ChunkId,
-    CustomIdSupervisedBatch,
-    NewChunkBatch,
-    SupervisedBatch,
-)
-
-
-class CustomIDType(Enum):
-    NotSet = 1
-    NoneType = 2
-    String = 3
-    Integer = 4
-
-
-# Calling this ChunkStore instead of DocumentStore because it stores chunks
-# instead of documents.
-class ChunkStore(ABC):
-    def __init__(self):
-        self.custom_id_type = CustomIDType.NotSet
-
-    @abstractmethod
-    def insert(self, chunks: Iterable[NewChunkBatch], **kwargs) -> Iterable[ChunkBatch]:
-        raise NotImplementedError
-
-    @abstractmethod
-    def delete(self, chunk_ids: List[ChunkId], **kwargs):
-        raise NotImplementedError
-
-    @abstractmethod
-    def get_chunks(self, chunk_ids: List[ChunkId], **kwargs) -> List[Chunk]:
-        raise NotImplementedError
-
-    @abstractmethod
-    def filter_chunk_ids(self, constraints: dict, **kwargs) -> Set[ChunkId]:
-        raise NotImplementedError
-
-    @abstractmethod
-    def remap_custom_ids(
-        self, samples: Iterable[CustomIdSupervisedBatch]
-    ) -> Iterable[SupervisedBatch]:
-        raise NotImplementedError
-
-    def _set_or_validate_custom_id_type(self, custom_ids):
-        incoming_custom_id_type = CustomIDType.NotSet
-        if custom_ids is None:
-            incoming_custom_id_type = CustomIDType.NoneType
-        elif is_string_dtype(custom_ids):
-            incoming_custom_id_type = CustomIDType.String
-        elif is_numeric_dtype(custom_ids):
-            incoming_custom_id_type = CustomIDType.Integer
-        else:
-            raise ValueError("Invalid custom id type.")
-
-        if self.custom_id_type == CustomIDType.NotSet:
-            self.custom_id_type = incoming_custom_id_type
-        elif incoming_custom_id_type != self.custom_id_type:
-            raise ValueError(
-                "Custom ids must all have the same type. Must be int, str, or None."
-            )
+from abc import ABC, abstractmethod
+from enum import Enum
+from typing import Iterable, List, Set
+
+from pandas.api.types import is_numeric_dtype, is_string_dtype
+
+from .types import (
+    Chunk,
+    ChunkBatch,
+    ChunkId,
+    CustomIdSupervisedBatch,
+    NewChunkBatch,
+    SupervisedBatch,
+)
+
+
+class CustomIDType(Enum):
+    NotSet = 1
+    NoneType = 2
+    String = 3
+    Integer = 4
+
+
+# Calling this ChunkStore instead of DocumentStore because it stores chunks
+# instead of documents.
+class ChunkStore(ABC):
+    def __init__(self):
+        self.custom_id_type = CustomIDType.NotSet
+
+    @abstractmethod
+    def insert(self, chunks: Iterable[NewChunkBatch], **kwargs) -> Iterable[ChunkBatch]:
+        raise NotImplementedError
+
+    @abstractmethod
+    def delete(self, chunk_ids: List[ChunkId], **kwargs):
+        raise NotImplementedError
+
+    @abstractmethod
+    def get_chunks(self, chunk_ids: List[ChunkId], **kwargs) -> List[Chunk]:
+        raise NotImplementedError
+
+    @abstractmethod
+    def filter_chunk_ids(self, constraints: dict, **kwargs) -> Set[ChunkId]:
+        raise NotImplementedError
+
+    @abstractmethod
+    def remap_custom_ids(
+        self, samples: Iterable[CustomIdSupervisedBatch]
+    ) -> Iterable[SupervisedBatch]:
+        raise NotImplementedError
+
+    def _set_or_validate_custom_id_type(self, custom_ids):
+        incoming_custom_id_type = CustomIDType.NotSet
+        if custom_ids is None:
+            incoming_custom_id_type = CustomIDType.NoneType
+        elif is_string_dtype(custom_ids):
+            incoming_custom_id_type = CustomIDType.String
+        elif is_numeric_dtype(custom_ids):
+            incoming_custom_id_type = CustomIDType.Integer
+        else:
+            raise ValueError("Invalid custom id type.")
+
+        if self.custom_id_type == CustomIDType.NotSet:
+            self.custom_id_type = incoming_custom_id_type
+        elif incoming_custom_id_type != self.custom_id_type:
+            raise ValueError(
+                "Custom ids must all have the same type. Must be int, str, or None."
+            )
```

## thirdai/neural_db_v2/core/documents.py

 * *Ordering differences only*

```diff
@@ -1,10 +1,10 @@
-from abc import ABC, abstractmethod
-from typing import Iterable
-
-from .types import NewChunkBatch
-
-
-class Document(ABC):
-    @abstractmethod
-    def chunks(self) -> Iterable[NewChunkBatch]:
-        raise NotImplementedError
+from abc import ABC, abstractmethod
+from typing import Iterable
+
+from .types import NewChunkBatch
+
+
+class Document(ABC):
+    @abstractmethod
+    def chunks(self) -> Iterable[NewChunkBatch]:
+        raise NotImplementedError
```

## thirdai/neural_db_v2/core/retriever.py

```diff
@@ -1,56 +1,56 @@
-from abc import ABC, abstractmethod
-from typing import Iterable, List, Tuple
-
-from .types import ChunkBatch, ChunkId, Score, SupervisedBatch
-
-
-class Retriever(ABC):
-    @abstractmethod
-    def search(
-        self, queries: List[str], top_k: int, **kwargs
-    ) -> List[List[Tuple[ChunkId, Score]]]:
-        raise NotImplementedError
-
-    @abstractmethod
-    def rank(
-        self, queries: List[str], choices: List[List[ChunkId]], **kwargs
-    ) -> List[List[Tuple[ChunkId, Score]]]:
-        """For constrained search.
-        Note on method signature:
-        Choices are provided as a separate argument from queries. While it may
-        be safer for the function to accept pairs of (query, choices), choices
-        are likely the return value of some function fn(queries) -> choices.
-        Thus, there likely exist separate collections for queries and
-        choices in memory. This function signature preempts the need to reshape
-        these existing data structures.
-        """
-        raise NotImplementedError
-
-    @abstractmethod
-    def upvote(self, queries: List[str], chunk_ids: List[ChunkId], **kwargs):
-        raise NotImplementedError
-
-    @abstractmethod
-    def associate(self, sources: List[str], targets: List[str], **kwargs):
-        raise NotImplementedError
-
-    @abstractmethod
-    def insert(self, chunks: Iterable[ChunkBatch], **kwargs):
-        raise NotImplementedError
-
-    @abstractmethod
-    def supervised_train(self, samples: Iterable[SupervisedBatch], **kwargs):
-        raise NotImplementedError
-
-    @abstractmethod
-    def delete(self, chunk_ids: List[ChunkId], **kwargs):
-        raise NotImplementedError
-
-    @abstractmethod
-    def save(self, path: str):
-        raise NotImplementedError
-
-    @classmethod
-    @abstractmethod
-    def load(path: str):
-        raise NotImplementedError
+from abc import ABC, abstractmethod
+from typing import Iterable, List, Set, Tuple
+
+from .types import ChunkBatch, ChunkId, Score, SupervisedBatch
+
+
+class Retriever(ABC):
+    @abstractmethod
+    def search(
+        self, queries: List[str], top_k: int, **kwargs
+    ) -> List[List[Tuple[ChunkId, Score]]]:
+        raise NotImplementedError
+
+    @abstractmethod
+    def rank(
+        self, queries: List[str], choices: List[Set[ChunkId]], **kwargs
+    ) -> List[List[Tuple[ChunkId, Score]]]:
+        """For constrained search.
+        Note on method signature:
+        Choices are provided as a separate argument from queries. While it may
+        be safer for the function to accept pairs of (query, choices), choices
+        are likely the return value of some function fn(queries) -> choices.
+        Thus, there likely exist separate collections for queries and
+        choices in memory. This function signature preempts the need to reshape
+        these existing data structures.
+        """
+        raise NotImplementedError
+
+    @abstractmethod
+    def upvote(self, queries: List[str], chunk_ids: List[ChunkId], **kwargs):
+        raise NotImplementedError
+
+    @abstractmethod
+    def associate(self, sources: List[str], targets: List[str], **kwargs):
+        raise NotImplementedError
+
+    @abstractmethod
+    def insert(self, chunks: Iterable[ChunkBatch], **kwargs):
+        raise NotImplementedError
+
+    @abstractmethod
+    def supervised_train(self, samples: Iterable[SupervisedBatch], **kwargs):
+        raise NotImplementedError
+
+    @abstractmethod
+    def delete(self, chunk_ids: List[ChunkId], **kwargs):
+        raise NotImplementedError
+
+    @abstractmethod
+    def save(self, path: str):
+        raise NotImplementedError
+
+    @classmethod
+    @abstractmethod
+    def load(path: str):
+        raise NotImplementedError
```

## thirdai/neural_db_v2/core/supervised.py

 * *Ordering differences only*

```diff
@@ -1,27 +1,27 @@
-from abc import ABC, abstractmethod
-from typing import Iterable, List, Union
-
-import pandas as pd
-
-from .types import ChunkId, CustomIdSupervisedBatch, SupervisedBatch
-
-
-class SupervisedDataset(ABC):
-    @abstractmethod
-    def samples(
-        self,
-    ) -> Union[Iterable[SupervisedBatch], Iterable[CustomIdSupervisedBatch]]:
-        raise NotImplementedError
-
-    def supervised_samples(
-        self,
-        queries: List[str],
-        ids: Union[List[List[ChunkId]], List[List[str]], List[List[int]]],
-        uses_db_id: bool,
-    ) -> Union[SupervisedBatch, CustomIdSupervisedBatch]:
-        if uses_db_id:
-            return SupervisedBatch(
-                query=queries,
-                chunk_id=ids,
-            )
-        return CustomIdSupervisedBatch(query=queries, custom_id=ids)
+from abc import ABC, abstractmethod
+from typing import Iterable, List, Union
+
+import pandas as pd
+
+from .types import ChunkId, CustomIdSupervisedBatch, SupervisedBatch
+
+
+class SupervisedDataset(ABC):
+    @abstractmethod
+    def samples(
+        self,
+    ) -> Union[Iterable[SupervisedBatch], Iterable[CustomIdSupervisedBatch]]:
+        raise NotImplementedError
+
+    def supervised_samples(
+        self,
+        queries: List[str],
+        ids: Union[List[List[ChunkId]], List[List[str]], List[List[int]]],
+        uses_db_id: bool,
+    ) -> Union[SupervisedBatch, CustomIdSupervisedBatch]:
+        if uses_db_id:
+            return SupervisedBatch(
+                query=queries,
+                chunk_id=ids,
+            )
+        return CustomIdSupervisedBatch(query=queries, custom_id=ids)
```

## thirdai/neural_db_v2/core/types.py

 * *Ordering differences only*

```diff
@@ -1,208 +1,208 @@
-from dataclasses import dataclass
-from typing import List, Optional, Union
-
-import numpy as np
-import pandas as pd
-from pandera import typing as pt
-from thirdai import data
-
-# We typedef doc ID to anticipate switching over to string IDs
-ChunkId = int
-
-Score = float
-
-
-@dataclass
-class NewChunk:
-    """A chunk that has not been assigned a unique ID."""
-
-    # An optional identifier supplied by the user.
-    custom_id: Union[str, int, None]
-
-    # The text content of the chunk, e.g. a paragraph.
-    text: str
-
-    # Keywords / strong signals.
-    keywords: str
-
-    # Arbitrary metadata related to the chunk.
-    metadata: dict
-
-    # Parent document name
-    document: str
-
-
-@dataclass
-class Chunk(NewChunk):
-    """A chunk that has been assigned a unique ID."""
-
-    # A unique identifier assigned by a chunk store.
-    chunk_id: ChunkId
-
-
-"""Design choices for batch objects:
-- Column oriented so we can efficiently convert it to a ColumnMap
-- Pandas series instead of Columns.
-  - Series can contain dictionaries, which is useful for the metadata field. 
-  - Many libraries natively accept Series or Numpy arrays, which
-    Series can easily convert into, so this is useful for when we implement
-    chunk stores or retrievers using external libraries.
-  - Series are easy to work with in Python, preventing the need to write 
-    more bindings and tools for Columns.
-- Store individual columns as named fields instead of storing a dataframe to
-  prevent errors from column name typos.
-- __getitem__ method to access individual rows for convenience.
-"""
-
-
-@dataclass
-class NewChunkBatch:
-    custom_id: Union[pt.Series[str], pt.Series[int], None]
-    text: pt.Series[str]
-    keywords: pt.Series[str]
-    metadata: Optional[pt.DataFrame]
-    document: pt.Series[str]
-
-    def __post_init__(self):
-        assert isinstance(self.custom_id, pd.Series) or self.custom_id is None
-        assert isinstance(self.text, pd.Series)
-        assert isinstance(self.keywords, pd.Series)
-        assert isinstance(self.metadata, pd.DataFrame) or self.metadata is None
-        assert isinstance(self.document, pd.Series)
-
-        fields_to_check = [self.text, self.keywords, self.document]
-
-        if not self.custom_id is None:
-            fields_to_check.append(self.custom_id)
-
-        if not self.metadata is None:
-            fields_to_check.append(self.metadata)
-
-        lengths = set(len(x) for x in fields_to_check)
-        if len(lengths) != 1:
-            raise ValueError("Must have fields of the same length in NewChunkBatch.")
-
-        if len(self.text) == 0:
-            raise ValueError("Cannot create empty NewChunkBatch.")
-
-    def __len__(self):
-        return len(self.text)
-
-    def __getitem__(self, i: int):
-        return NewChunk(
-            custom_id=self.custom_id[i] if self.custom_id is not None else None,
-            text=self.text[i],
-            keywords=self.keywords[i],
-            metadata=(
-                self.metadata.iloc[i].to_dict() if self.metadata is not None else None
-            ),
-            document=self.document[i],
-        )
-
-    def to_df(self):
-        columns = {
-            "text": self.text,
-            "keywords": self.keywords,
-            "document": self.document,
-        }
-        if self.custom_id is not None:
-            columns["custom_id"] = self.custom_id
-        else:
-            columns["custom_id"] = pd.Series(np.full(len(self.text), None))
-
-        return pd.DataFrame(columns)
-
-
-@dataclass
-class ChunkBatch:
-    chunk_id: pt.Series[ChunkId]
-    text: pt.Series[str]
-    keywords: pt.Series[str]
-
-    def __post_init__(self):
-        assert isinstance(self.chunk_id, pd.Series)
-        assert isinstance(self.text, pd.Series)
-        assert isinstance(self.keywords, pd.Series)
-
-        self.chunk_id = self.chunk_id.reset_index(drop=True)
-        self.text = self.text.reset_index(drop=True)
-        self.keywords = self.keywords.reset_index(drop=True)
-
-        if not (len(self.chunk_id) == len(self.text) == len(self.keywords)):
-            raise ValueError("Must have fields of the same length in ChunkBatch.")
-
-        if len(self.text) == 0:
-            raise ValueError("Cannot create empty ChunkBatch.")
-
-    def to_df(self):
-        return pd.DataFrame(self.__dict__)
-
-
-@dataclass
-class CustomIdSupervisedSample:
-    query: str
-    custom_id: Union[List[str], List[int]]
-
-
-@dataclass
-class SupervisedSample:
-    query: str
-    chunk_id: List[ChunkId]
-
-
-@dataclass
-class CustomIdSupervisedBatch:
-    query: pt.Series[str]
-    custom_id: Union[pt.Series[List[str]], pt.Series[List[int]]]
-
-    def __post_init__(self):
-        assert isinstance(self.custom_id, pd.Series)
-        assert isinstance(self.query, pd.Series)
-
-        self.query = self.query.reset_index(drop=True)
-        self.custom_id = self.custom_id.reset_index(drop=True)
-
-        if len(self.query) != len(self.custom_id):
-            raise ValueError(
-                "Must have fields of the same length in CustomIdSupervisedBatch."
-            )
-
-        if len(self.query) == 0:
-            raise ValueError("Cannot create empty CustomIdSupervisedBatch.")
-
-    def __getitem__(self, i: int):
-        return CustomIdSupervisedSample(
-            query=self.query[i],
-            custom_id=self.custom_id[i],
-        )
-
-    def to_df(self):
-        return pd.DataFrame(self.__dict__)
-
-
-@dataclass
-class SupervisedBatch:
-    query: pt.Series[str]
-    chunk_id: pt.Series[List[ChunkId]]
-
-    def __post_init__(self):
-        assert isinstance(self.chunk_id, pd.Series)
-        assert isinstance(self.query, pd.Series)
-
-        self.query = self.query.reset_index(drop=True)
-        self.chunk_id = self.chunk_id.reset_index(drop=True)
-
-        if len(self.query) != len(self.chunk_id):
-            raise ValueError("Must have fields of the same length in SupervisedBatch.")
-
-        if len(self.query) == 0:
-            raise ValueError("Cannot create empty SupervisedBatch.")
-
-    def __getitem__(self, i: int):
-        return SupervisedSample(
-            query=self.query[i],
-            chunk_id=self.chunk_id[i],
-        )
-
-    def to_df(self):
-        return pd.DataFrame(self.__dict__)
+from dataclasses import dataclass
+from typing import List, Optional, Union
+
+import numpy as np
+import pandas as pd
+from pandera import typing as pt
+from thirdai import data
+
+# We typedef doc ID to anticipate switching over to string IDs
+ChunkId = int
+
+Score = float
+
+
+@dataclass
+class NewChunk:
+    """A chunk that has not been assigned a unique ID."""
+
+    # An optional identifier supplied by the user.
+    custom_id: Union[str, int, None]
+
+    # The text content of the chunk, e.g. a paragraph.
+    text: str
+
+    # Keywords / strong signals.
+    keywords: str
+
+    # Arbitrary metadata related to the chunk.
+    metadata: dict
+
+    # Parent document name
+    document: str
+
+
+@dataclass
+class Chunk(NewChunk):
+    """A chunk that has been assigned a unique ID."""
+
+    # A unique identifier assigned by a chunk store.
+    chunk_id: ChunkId
+
+
+"""Design choices for batch objects:
+- Column oriented so we can efficiently convert it to a ColumnMap
+- Pandas series instead of Columns.
+  - Series can contain dictionaries, which is useful for the metadata field. 
+  - Many libraries natively accept Series or Numpy arrays, which
+    Series can easily convert into, so this is useful for when we implement
+    chunk stores or retrievers using external libraries.
+  - Series are easy to work with in Python, preventing the need to write 
+    more bindings and tools for Columns.
+- Store individual columns as named fields instead of storing a dataframe to
+  prevent errors from column name typos.
+- __getitem__ method to access individual rows for convenience.
+"""
+
+
+@dataclass
+class NewChunkBatch:
+    custom_id: Union[pt.Series[str], pt.Series[int], None]
+    text: pt.Series[str]
+    keywords: pt.Series[str]
+    metadata: Optional[pt.DataFrame]
+    document: pt.Series[str]
+
+    def __post_init__(self):
+        assert isinstance(self.custom_id, pd.Series) or self.custom_id is None
+        assert isinstance(self.text, pd.Series)
+        assert isinstance(self.keywords, pd.Series)
+        assert isinstance(self.metadata, pd.DataFrame) or self.metadata is None
+        assert isinstance(self.document, pd.Series)
+
+        fields_to_check = [self.text, self.keywords, self.document]
+
+        if not self.custom_id is None:
+            fields_to_check.append(self.custom_id)
+
+        if not self.metadata is None:
+            fields_to_check.append(self.metadata)
+
+        lengths = set(len(x) for x in fields_to_check)
+        if len(lengths) != 1:
+            raise ValueError("Must have fields of the same length in NewChunkBatch.")
+
+        if len(self.text) == 0:
+            raise ValueError("Cannot create empty NewChunkBatch.")
+
+    def __len__(self):
+        return len(self.text)
+
+    def __getitem__(self, i: int):
+        return NewChunk(
+            custom_id=self.custom_id[i] if self.custom_id is not None else None,
+            text=self.text[i],
+            keywords=self.keywords[i],
+            metadata=(
+                self.metadata.iloc[i].to_dict() if self.metadata is not None else None
+            ),
+            document=self.document[i],
+        )
+
+    def to_df(self):
+        columns = {
+            "text": self.text,
+            "keywords": self.keywords,
+            "document": self.document,
+        }
+        if self.custom_id is not None:
+            columns["custom_id"] = self.custom_id
+        else:
+            columns["custom_id"] = pd.Series(np.full(len(self.text), None))
+
+        return pd.DataFrame(columns)
+
+
+@dataclass
+class ChunkBatch:
+    chunk_id: pt.Series[ChunkId]
+    text: pt.Series[str]
+    keywords: pt.Series[str]
+
+    def __post_init__(self):
+        assert isinstance(self.chunk_id, pd.Series)
+        assert isinstance(self.text, pd.Series)
+        assert isinstance(self.keywords, pd.Series)
+
+        self.chunk_id = self.chunk_id.reset_index(drop=True)
+        self.text = self.text.reset_index(drop=True)
+        self.keywords = self.keywords.reset_index(drop=True)
+
+        if not (len(self.chunk_id) == len(self.text) == len(self.keywords)):
+            raise ValueError("Must have fields of the same length in ChunkBatch.")
+
+        if len(self.text) == 0:
+            raise ValueError("Cannot create empty ChunkBatch.")
+
+    def to_df(self):
+        return pd.DataFrame(self.__dict__)
+
+
+@dataclass
+class CustomIdSupervisedSample:
+    query: str
+    custom_id: Union[List[str], List[int]]
+
+
+@dataclass
+class SupervisedSample:
+    query: str
+    chunk_id: List[ChunkId]
+
+
+@dataclass
+class CustomIdSupervisedBatch:
+    query: pt.Series[str]
+    custom_id: Union[pt.Series[List[str]], pt.Series[List[int]]]
+
+    def __post_init__(self):
+        assert isinstance(self.custom_id, pd.Series)
+        assert isinstance(self.query, pd.Series)
+
+        self.query = self.query.reset_index(drop=True)
+        self.custom_id = self.custom_id.reset_index(drop=True)
+
+        if len(self.query) != len(self.custom_id):
+            raise ValueError(
+                "Must have fields of the same length in CustomIdSupervisedBatch."
+            )
+
+        if len(self.query) == 0:
+            raise ValueError("Cannot create empty CustomIdSupervisedBatch.")
+
+    def __getitem__(self, i: int):
+        return CustomIdSupervisedSample(
+            query=self.query[i],
+            custom_id=self.custom_id[i],
+        )
+
+    def to_df(self):
+        return pd.DataFrame(self.__dict__)
+
+
+@dataclass
+class SupervisedBatch:
+    query: pt.Series[str]
+    chunk_id: pt.Series[List[ChunkId]]
+
+    def __post_init__(self):
+        assert isinstance(self.chunk_id, pd.Series)
+        assert isinstance(self.query, pd.Series)
+
+        self.query = self.query.reset_index(drop=True)
+        self.chunk_id = self.chunk_id.reset_index(drop=True)
+
+        if len(self.query) != len(self.chunk_id):
+            raise ValueError("Must have fields of the same length in SupervisedBatch.")
+
+        if len(self.query) == 0:
+            raise ValueError("Cannot create empty SupervisedBatch.")
+
+    def __getitem__(self, i: int):
+        return SupervisedSample(
+            query=self.query[i],
+            chunk_id=self.chunk_id[i],
+        )
+
+    def to_df(self):
+        return pd.DataFrame(self.__dict__)
```

## thirdai/neural_db_v2/documents/csv.py

 * *Ordering differences only*

```diff
@@ -1,80 +1,80 @@
-from typing import Iterable, List
-
-import pandas as pd
-
-from ..core.documents import Document
-from ..core.types import NewChunkBatch
-from .utils import join_metadata, series_from_value
-
-
-def is_text_column(column: pd.Series):
-    return (
-        column.dtype == "object"
-        and column[:200].map(lambda x: isinstance(x, str)).all()
-    )
-
-
-def infer_text_columns(df: pd.DataFrame):
-    return [column for column in df.columns if is_text_column(df[column])]
-
-
-def concat_str_columns(df: pd.DataFrame, columns: List[str]):
-    if len(columns) == 0:
-        return series_from_value(value="", n=len(df))
-
-    output = df[columns[0]].fillna("")
-
-    for col in columns[1:]:
-        output = output + " " + df[col].fillna("")
-
-    return output
-
-
-class CSV(Document):
-    def __init__(
-        self,
-        path,
-        text_columns=[],
-        keyword_columns=[],
-        custom_id_column=None,
-        doc_metadata=None,
-        max_rows=10_000_000,
-    ):
-        super().__init__()
-
-        self.path = path
-        self.text_columns = text_columns
-        self.keyword_columns = keyword_columns
-        self.custom_id_column = custom_id_column
-        self.doc_metadata = doc_metadata
-        self.max_rows = max_rows
-
-    def chunks(self) -> Iterable[NewChunkBatch]:
-        data_iter = pd.read_csv(self.path, chunksize=self.max_rows)
-
-        for df in data_iter:
-            df.reset_index(drop=True, inplace=True)
-            custom_id = df[self.custom_id_column] if self.custom_id_column else None
-            if self.custom_id_column:
-                df.drop(self.custom_id_column, axis=1, inplace=True)
-
-            if len(self.text_columns) + len(self.keyword_columns) == 0:
-                self.text_columns = infer_text_columns(df)
-
-            text = concat_str_columns(df, self.text_columns)
-            keywords = concat_str_columns(df, self.keyword_columns)
-
-            chunk_metadata = df.drop(self.text_columns + self.keyword_columns, axis=1)
-            metadata = join_metadata(
-                n_rows=len(text),
-                chunk_metadata=chunk_metadata,
-                doc_metadata=self.doc_metadata,
-            )
-
-            yield NewChunkBatch(
-                custom_id=custom_id,
-                text=text,
-                keywords=keywords,
-                metadata=metadata,
-                document=series_from_value(self.path, n=len(text)),
-            )
+from typing import Iterable, List
+
+import pandas as pd
+
+from ..core.documents import Document
+from ..core.types import NewChunkBatch
+from .utils import join_metadata, series_from_value
+
+
+def is_text_column(column: pd.Series):
+    return (
+        column.dtype == "object"
+        and column[:200].map(lambda x: isinstance(x, str)).all()
+    )
+
+
+def infer_text_columns(df: pd.DataFrame):
+    return [column for column in df.columns if is_text_column(df[column])]
+
+
+def concat_str_columns(df: pd.DataFrame, columns: List[str]):
+    if len(columns) == 0:
+        return series_from_value(value="", n=len(df))
+
+    output = df[columns[0]].fillna("")
+
+    for col in columns[1:]:
+        output = output + " " + df[col].fillna("")
+
+    return output
+
+
+class CSV(Document):
+    def __init__(
+        self,
+        path,
+        text_columns=[],
+        keyword_columns=[],
+        custom_id_column=None,
+        doc_metadata=None,
+        max_rows=10_000_000,
+    ):
+        super().__init__()
+
+        self.path = path
+        self.text_columns = text_columns
+        self.keyword_columns = keyword_columns
+        self.custom_id_column = custom_id_column
+        self.doc_metadata = doc_metadata
+        self.max_rows = max_rows
+
+    def chunks(self) -> Iterable[NewChunkBatch]:
+        data_iter = pd.read_csv(self.path, chunksize=self.max_rows)
+
+        for df in data_iter:
+            df.reset_index(drop=True, inplace=True)
+            custom_id = df[self.custom_id_column] if self.custom_id_column else None
+            if self.custom_id_column:
+                df.drop(self.custom_id_column, axis=1, inplace=True)
+
+            if len(self.text_columns) + len(self.keyword_columns) == 0:
+                self.text_columns = infer_text_columns(df)
+
+            text = concat_str_columns(df, self.text_columns)
+            keywords = concat_str_columns(df, self.keyword_columns)
+
+            chunk_metadata = df.drop(self.text_columns + self.keyword_columns, axis=1)
+            metadata = join_metadata(
+                n_rows=len(text),
+                chunk_metadata=chunk_metadata,
+                doc_metadata=self.doc_metadata,
+            )
+
+            yield NewChunkBatch(
+                custom_id=custom_id,
+                text=text,
+                keywords=keywords,
+                metadata=metadata,
+                document=series_from_value(self.path, n=len(text)),
+            )
```

## thirdai/neural_db_v2/documents/docx.py

 * *Ordering differences only*

```diff
@@ -1,39 +1,39 @@
-from typing import Iterable
-
-import thirdai.neural_db.parsing_utils.doc_parse as doc_parse
-
-from ..core.documents import Document
-from ..core.types import NewChunkBatch
-from .utils import join_metadata, series_from_value
-
-
-class DOCX(Document):
-    def __init__(self, path, doc_metadata=None):
-        super().__init__()
-
-        self.path = path
-        self.doc_metadata = doc_metadata
-
-    def chunks(self) -> Iterable[NewChunkBatch]:
-        elements, success = doc_parse.get_elements(self.path)
-
-        if not success:
-            raise ValueError(f"Unable to parse docx file: '{self.path}'.")
-
-        parsed_chunks = doc_parse.create_train_df(elements)
-
-        text = parsed_chunks["para"]
-
-        metadata = join_metadata(
-            n_rows=len(text), chunk_metadata=None, doc_metadata=self.doc_metadata
-        )
-
-        return [
-            NewChunkBatch(
-                custom_id=None,
-                text=text,
-                keywords=series_from_value("", len(text)),
-                metadata=metadata,
-                document=series_from_value(self.path, len(text)),
-            )
-        ]
+from typing import Iterable
+
+import thirdai.neural_db.parsing_utils.doc_parse as doc_parse
+
+from ..core.documents import Document
+from ..core.types import NewChunkBatch
+from .utils import join_metadata, series_from_value
+
+
+class DOCX(Document):
+    def __init__(self, path, doc_metadata=None):
+        super().__init__()
+
+        self.path = path
+        self.doc_metadata = doc_metadata
+
+    def chunks(self) -> Iterable[NewChunkBatch]:
+        elements, success = doc_parse.get_elements(self.path)
+
+        if not success:
+            raise ValueError(f"Unable to parse docx file: '{self.path}'.")
+
+        parsed_chunks = doc_parse.create_train_df(elements)
+
+        text = parsed_chunks["para"]
+
+        metadata = join_metadata(
+            n_rows=len(text), chunk_metadata=None, doc_metadata=self.doc_metadata
+        )
+
+        return [
+            NewChunkBatch(
+                custom_id=None,
+                text=text,
+                keywords=series_from_value("", len(text)),
+                metadata=metadata,
+                document=series_from_value(self.path, len(text)),
+            )
+        ]
```

## thirdai/neural_db_v2/documents/in_memory_text.py

 * *Ordering differences only*

```diff
@@ -1,44 +1,44 @@
-from typing import Iterable, List
-
-import pandas as pd
-
-from ..core.documents import Document
-from ..core.types import NewChunkBatch
-from .utils import join_metadata, series_from_value
-
-
-class InMemoryText(Document):
-    def __init__(
-        self,
-        document_name,
-        text=[],
-        chunk_metadata=None,
-        doc_metadata=None,
-        custom_id=None,
-    ):
-        super().__init__()
-
-        self.document_name = document_name
-        self.text = pd.Series(text)
-        self.chunk_metadata = (
-            pd.DataFrame.from_records(chunk_metadata) if chunk_metadata else None
-        )
-        self.doc_metadata = doc_metadata
-        self.custom_id = custom_id
-
-    def chunks(self) -> Iterable[NewChunkBatch]:
-        metadata = join_metadata(
-            n_rows=len(self.text),
-            chunk_metadata=self.chunk_metadata,
-            doc_metadata=self.doc_metadata,
-        )
-
-        return [
-            NewChunkBatch(
-                custom_id=pd.Series(self.custom_id) if self.custom_id else None,
-                text=self.text,
-                keywords=series_from_value("", len(self.text)),
-                metadata=metadata,
-                document=series_from_value(self.document_name, len(self.text)),
-            )
-        ]
+from typing import Iterable, List
+
+import pandas as pd
+
+from ..core.documents import Document
+from ..core.types import NewChunkBatch
+from .utils import join_metadata, series_from_value
+
+
+class InMemoryText(Document):
+    def __init__(
+        self,
+        document_name,
+        text=[],
+        chunk_metadata=None,
+        doc_metadata=None,
+        custom_id=None,
+    ):
+        super().__init__()
+
+        self.document_name = document_name
+        self.text = pd.Series(text)
+        self.chunk_metadata = (
+            pd.DataFrame.from_records(chunk_metadata) if chunk_metadata else None
+        )
+        self.doc_metadata = doc_metadata
+        self.custom_id = custom_id
+
+    def chunks(self) -> Iterable[NewChunkBatch]:
+        metadata = join_metadata(
+            n_rows=len(self.text),
+            chunk_metadata=self.chunk_metadata,
+            doc_metadata=self.doc_metadata,
+        )
+
+        return [
+            NewChunkBatch(
+                custom_id=pd.Series(self.custom_id) if self.custom_id else None,
+                text=self.text,
+                keywords=series_from_value("", len(self.text)),
+                metadata=metadata,
+                document=series_from_value(self.document_name, len(self.text)),
+            )
+        ]
```

## thirdai/neural_db_v2/documents/pdf.py

 * *Ordering differences only*

```diff
@@ -1,68 +1,68 @@
-from typing import Iterable
-
-import thirdai.neural_db.parsing_utils.sliding_pdf_parse as pdf_parse
-
-from ..core.documents import Document
-from ..core.types import NewChunkBatch
-from .utils import join_metadata, series_from_value
-
-
-class PDF(Document):
-    def __init__(
-        self,
-        path,
-        chunk_size=100,
-        stride=40,
-        emphasize_first_words=0,
-        ignore_header_footer=True,
-        ignore_nonstandard_orientation=True,
-        doc_metadata=None,
-        doc_keywords="",
-        emphasize_section_titles=False,
-        table_parsing=False,
-    ):
-        super().__init__()
-
-        self.path = path
-        self.chunk_size = chunk_size
-        self.stride = stride
-        self.emphasize_first_words = emphasize_first_words
-        self.ignore_header_footer = ignore_header_footer
-        self.ignore_nonstandard_orientation = ignore_nonstandard_orientation
-        self.table_parsing = table_parsing
-        self.doc_metadata = doc_metadata
-        self.doc_keywords = doc_keywords
-        self.emphasize_section_titles = emphasize_section_titles
-        self.table_parsing = table_parsing
-
-    def chunks(self) -> Iterable[NewChunkBatch]:
-        parsed_chunks = pdf_parse.make_df(
-            filename=self.path,
-            chunk_words=self.chunk_size,
-            stride_words=self.stride,
-            emphasize_first_n_words=self.emphasize_first_words,
-            ignore_header_footer=self.ignore_header_footer,
-            ignore_nonstandard_orientation=self.ignore_nonstandard_orientation,
-            doc_keywords=self.doc_keywords,
-            emphasize_section_titles=self.emphasize_section_titles,
-            table_parsing=self.table_parsing,
-        )
-
-        text = parsed_chunks["para"]
-        keywords = parsed_chunks["emphasis"]
-
-        metadata = join_metadata(
-            n_rows=len(text),
-            chunk_metadata=parsed_chunks[["chunk_boxes", "page"]],
-            doc_metadata=self.doc_metadata,
-        )
-
-        return [
-            NewChunkBatch(
-                custom_id=None,
-                text=text,
-                keywords=keywords,
-                metadata=metadata,
-                document=series_from_value(self.path, len(text)),
-            )
-        ]
+from typing import Iterable
+
+import thirdai.neural_db.parsing_utils.sliding_pdf_parse as pdf_parse
+
+from ..core.documents import Document
+from ..core.types import NewChunkBatch
+from .utils import join_metadata, series_from_value
+
+
+class PDF(Document):
+    def __init__(
+        self,
+        path,
+        chunk_size=100,
+        stride=40,
+        emphasize_first_words=0,
+        ignore_header_footer=True,
+        ignore_nonstandard_orientation=True,
+        doc_metadata=None,
+        doc_keywords="",
+        emphasize_section_titles=False,
+        table_parsing=False,
+    ):
+        super().__init__()
+
+        self.path = path
+        self.chunk_size = chunk_size
+        self.stride = stride
+        self.emphasize_first_words = emphasize_first_words
+        self.ignore_header_footer = ignore_header_footer
+        self.ignore_nonstandard_orientation = ignore_nonstandard_orientation
+        self.table_parsing = table_parsing
+        self.doc_metadata = doc_metadata
+        self.doc_keywords = doc_keywords
+        self.emphasize_section_titles = emphasize_section_titles
+        self.table_parsing = table_parsing
+
+    def chunks(self) -> Iterable[NewChunkBatch]:
+        parsed_chunks = pdf_parse.make_df(
+            filename=self.path,
+            chunk_words=self.chunk_size,
+            stride_words=self.stride,
+            emphasize_first_n_words=self.emphasize_first_words,
+            ignore_header_footer=self.ignore_header_footer,
+            ignore_nonstandard_orientation=self.ignore_nonstandard_orientation,
+            doc_keywords=self.doc_keywords,
+            emphasize_section_titles=self.emphasize_section_titles,
+            table_parsing=self.table_parsing,
+        )
+
+        text = parsed_chunks["para"]
+        keywords = parsed_chunks["emphasis"]
+
+        metadata = join_metadata(
+            n_rows=len(text),
+            chunk_metadata=parsed_chunks[["chunk_boxes", "page"]],
+            doc_metadata=self.doc_metadata,
+        )
+
+        return [
+            NewChunkBatch(
+                custom_id=None,
+                text=text,
+                keywords=keywords,
+                metadata=metadata,
+                document=series_from_value(self.path, len(text)),
+            )
+        ]
```

## thirdai/neural_db_v2/documents/unstructured.py

 * *Ordering differences only*

```diff
@@ -1,87 +1,87 @@
-from typing import Iterable, List
-
-import pandas as pd
-from thirdai.neural_db.parsing_utils.unstructured_parse import (
-    EmlParse,
-    PptxParse,
-    TxtParse,
-    UnstructuredParse,
-)
-
-from ..core.documents import Document
-from ..core.types import NewChunkBatch
-from .utils import join_metadata, series_from_value
-
-
-class Unstructured(Document):
-    def __init__(
-        self,
-        path: str,
-        parser: UnstructuredParse,
-        chunk_metadata_columns: List[str],
-        doc_metadata: dict = None,
-    ):
-        super().__init__()
-
-        self.path = path
-        self.parser = parser
-        self.chunk_metadata_columns = chunk_metadata_columns
-        self.doc_metadata = doc_metadata
-
-    def chunks(self) -> Iterable[NewChunkBatch]:
-        parser = self.parser(self.path)
-
-        elements, success = parser.process_elements()
-
-        if not success:
-            raise ValueError(f"Could not read file: {self.path}")
-
-        contents = parser.create_train_df(elements)
-
-        text = contents["para"]
-
-        metadata = join_metadata(
-            n_rows=len(text),
-            chunk_metadata=contents[self.chunk_metadata_columns],
-            doc_metadata=self.doc_metadata,
-        )
-
-        return [
-            NewChunkBatch(
-                custom_id=None,
-                text=text,
-                keywords=series_from_value("", len(text)),
-                metadata=metadata,
-                document=contents["filename"],
-            )
-        ]
-
-
-class PPTX(Unstructured):
-    def __init__(self, path, doc_metadata):
-        super().__init__(
-            path=path,
-            parser=PptxParse,
-            chunk_metadata_columns=["filetype", "page"],
-            doc_metadata=doc_metadata,
-        )
-
-
-class TextFile(Unstructured):
-    def __init__(self, path, doc_metadata):
-        super().__init__(
-            path=path,
-            parser=TxtParse,
-            chunk_metadata_columns=["filetype"],
-            doc_metadata=doc_metadata,
-        )
-
-
-class Email(Unstructured):
-    def __init__(self, path, doc_metadata):
-        super().__init__(
-            path=path,
-            parser=EmlParse,
-            chunk_metadata_columns=["filetype", "subject", "sent_from", "sent_to"],
-            doc_metadata=doc_metadata,
-        )
+from typing import Iterable, List
+
+import pandas as pd
+from thirdai.neural_db.parsing_utils.unstructured_parse import (
+    EmlParse,
+    PptxParse,
+    TxtParse,
+    UnstructuredParse,
+)
+
+from ..core.documents import Document
+from ..core.types import NewChunkBatch
+from .utils import join_metadata, series_from_value
+
+
+class Unstructured(Document):
+    def __init__(
+        self,
+        path: str,
+        parser: UnstructuredParse,
+        chunk_metadata_columns: List[str],
+        doc_metadata: dict = None,
+    ):
+        super().__init__()
+
+        self.path = path
+        self.parser = parser
+        self.chunk_metadata_columns = chunk_metadata_columns
+        self.doc_metadata = doc_metadata
+
+    def chunks(self) -> Iterable[NewChunkBatch]:
+        parser = self.parser(self.path)
+
+        elements, success = parser.process_elements()
+
+        if not success:
+            raise ValueError(f"Could not read file: {self.path}")
+
+        contents = parser.create_train_df(elements)
+
+        text = contents["para"]
+
+        metadata = join_metadata(
+            n_rows=len(text),
+            chunk_metadata=contents[self.chunk_metadata_columns],
+            doc_metadata=self.doc_metadata,
+        )
+
+        return [
+            NewChunkBatch(
+                custom_id=None,
+                text=text,
+                keywords=series_from_value("", len(text)),
+                metadata=metadata,
+                document=contents["filename"],
+            )
+        ]
+
+
+class PPTX(Unstructured):
+    def __init__(self, path, doc_metadata):
+        super().__init__(
+            path=path,
+            parser=PptxParse,
+            chunk_metadata_columns=["filetype", "page"],
+            doc_metadata=doc_metadata,
+        )
+
+
+class TextFile(Unstructured):
+    def __init__(self, path, doc_metadata):
+        super().__init__(
+            path=path,
+            parser=TxtParse,
+            chunk_metadata_columns=["filetype"],
+            doc_metadata=doc_metadata,
+        )
+
+
+class Email(Unstructured):
+    def __init__(self, path, doc_metadata):
+        super().__init__(
+            path=path,
+            parser=EmlParse,
+            chunk_metadata_columns=["filetype", "subject", "sent_from", "sent_to"],
+            doc_metadata=doc_metadata,
+        )
```

## thirdai/neural_db_v2/documents/url.py

 * *Ordering differences only*

```diff
@@ -1,48 +1,48 @@
-from typing import Iterable
-
-import thirdai.neural_db.parsing_utils.url_parse as url_parse
-from requests.models import Response
-
-from ..core.documents import Document
-from ..core.types import NewChunkBatch
-from .utils import join_metadata
-
-
-class URL(Document):
-    def __init__(
-        self,
-        url: str,
-        response: Response = None,
-        title_is_strong: bool = False,
-        doc_metadata=None,
-    ):
-        super().__init__()
-
-        self.url = url
-        self.response = response
-        self.title_is_strong = title_is_strong
-        self.doc_metadata = doc_metadata
-
-    def chunks(self) -> Iterable[NewChunkBatch]:
-        elements, success = url_parse.process_url(self.url, self.response)
-
-        if not success or not elements:
-            raise ValueError(f"Could not retrieve data from URL: {self.url}")
-
-        content = url_parse.create_train_df(elements)
-
-        text = content["text"]
-        keywords = content["title"] if self.title_is_strong else content["text"]
-
-        metadata = join_metadata(
-            n_rows=len(text), chunk_metadata=None, doc_metadata=self.doc_metadata
-        )
-        return [
-            NewChunkBatch(
-                custom_id=None,
-                text=text,
-                keywords=keywords,
-                metadata=metadata,
-                document=content["url"],
-            )
-        ]
+from typing import Iterable
+
+import thirdai.neural_db.parsing_utils.url_parse as url_parse
+from requests.models import Response
+
+from ..core.documents import Document
+from ..core.types import NewChunkBatch
+from .utils import join_metadata
+
+
+class URL(Document):
+    def __init__(
+        self,
+        url: str,
+        response: Response = None,
+        title_is_strong: bool = False,
+        doc_metadata=None,
+    ):
+        super().__init__()
+
+        self.url = url
+        self.response = response
+        self.title_is_strong = title_is_strong
+        self.doc_metadata = doc_metadata
+
+    def chunks(self) -> Iterable[NewChunkBatch]:
+        elements, success = url_parse.process_url(self.url, self.response)
+
+        if not success or not elements:
+            raise ValueError(f"Could not retrieve data from URL: {self.url}")
+
+        content = url_parse.create_train_df(elements)
+
+        text = content["text"]
+        keywords = content["title"] if self.title_is_strong else content["text"]
+
+        metadata = join_metadata(
+            n_rows=len(text), chunk_metadata=None, doc_metadata=self.doc_metadata
+        )
+        return [
+            NewChunkBatch(
+                custom_id=None,
+                text=text,
+                keywords=keywords,
+                metadata=metadata,
+                document=content["url"],
+            )
+        ]
```

## thirdai/neural_db_v2/documents/utils.py

 * *Ordering differences only*

```diff
@@ -1,29 +1,29 @@
-from typing import Any, Optional
-
-import numpy as np
-import pandas as pd
-
-
-def series_from_value(value: Any, n: int) -> pd.Series:
-    return pd.Series(np.full(n, value))
-
-
-def join_metadata(
-    n_rows,
-    chunk_metadata: Optional[pd.DataFrame] = None,
-    doc_metadata: Optional[dict] = None,
-) -> Optional[pd.DataFrame]:
-    if chunk_metadata is not None and len(chunk_metadata) != n_rows:
-        raise ValueError("Length of chunk metadata must match number of chunks.")
-
-    if chunk_metadata is None:
-        chunk_metadata = pd.DataFrame()
-
-    if doc_metadata:
-        doc_metadata = pd.DataFrame.from_records([doc_metadata] * n_rows)
-    else:
-        doc_metadata = pd.DataFrame()
-
-    metadata = pd.concat([chunk_metadata, doc_metadata], axis=1)
-
-    return metadata if not metadata.empty else None
+from typing import Any, Optional
+
+import numpy as np
+import pandas as pd
+
+
+def series_from_value(value: Any, n: int) -> pd.Series:
+    return pd.Series(np.full(n, value))
+
+
+def join_metadata(
+    n_rows,
+    chunk_metadata: Optional[pd.DataFrame] = None,
+    doc_metadata: Optional[dict] = None,
+) -> Optional[pd.DataFrame]:
+    if chunk_metadata is not None and len(chunk_metadata) != n_rows:
+        raise ValueError("Length of chunk metadata must match number of chunks.")
+
+    if chunk_metadata is None:
+        chunk_metadata = pd.DataFrame()
+
+    if doc_metadata:
+        doc_metadata = pd.DataFrame.from_records([doc_metadata] * n_rows)
+    else:
+        doc_metadata = pd.DataFrame()
+
+    metadata = pd.concat([chunk_metadata, doc_metadata], axis=1)
+
+    return metadata if not metadata.empty else None
```

## thirdai/neural_db_v2/documents/__init__.py

 * *Ordering differences only*

```diff
@@ -1,12 +1,12 @@
-from ..core.documents import Document
-from .csv import CSV
-from .docx import DOCX
-from .in_memory_text import InMemoryText
-from .pdf import PDF
-from .unstructured import PPTX, Email, TextFile
-from .url import URL
-
-
-def document_by_name(name: str, **kwargs) -> Document:
-    # TODO: Add options here
-    raise ValueError(f"Invalid document name {name}")
+from ..core.documents import Document
+from .csv import CSV
+from .docx import DOCX
+from .in_memory_text import InMemoryText
+from .pdf import PDF
+from .unstructured import PPTX, Email, TextFile
+from .url import URL
+
+
+def document_by_name(name: str, **kwargs) -> Document:
+    # TODO: Add options here
+    raise ValueError(f"Invalid document name {name}")
```

## thirdai/neural_db_v2/retrievers/finetunable_retriever.py

 * *Ordering differences only*

```diff
@@ -1,58 +1,58 @@
-from typing import Iterable, List, Set, Tuple
-
-from thirdai import search
-
-from ..core.retriever import Retriever
-from ..core.types import ChunkBatch, ChunkId, Score, SupervisedBatch
-
-
-class FinetunableRetriever(Retriever):
-    def __init__(self, **kwargs):
-        super().__init__()
-        self.retriever = search.FinetunableRetriever()
-
-    def search(
-        self, queries: List[str], top_k: int, **kwargs
-    ) -> List[List[Tuple[ChunkId, Score]]]:
-        return self.retriever.query(queries, k=top_k)
-
-    def rank(
-        self, queries: List[str], choices: List[Set[ChunkId]], top_k: int, **kwargs
-    ) -> List[List[Tuple[ChunkId, Score]]]:
-        return self.retriever.rank(queries, candidates=choices, k=top_k)
-
-    def upvote(self, queries: List[str], chunk_ids: List[ChunkId], **kwargs):
-        self.retriever.finetune(
-            doc_ids=list(map(lambda id: [id], chunk_ids)), queries=queries
-        )
-
-    def associate(
-        self, sources: List[str], targets: List[str], associate_strength=4, **kwargs
-    ):
-        self.retriever.associate(
-            sources=sources, targets=targets, strength=associate_strength
-        )
-
-    def insert(self, chunks: Iterable[ChunkBatch], **kwargs):
-        for batch in chunks:
-            texts = batch.keywords + " " + batch.text
-
-            self.retriever.index(ids=batch.chunk_id.to_list(), docs=texts.to_list())
-
-    def supervised_train(self, samples: Iterable[SupervisedBatch], **kwargs):
-        for batch in samples:
-            self.retriever.finetune(
-                doc_ids=batch.chunk_id.to_list(), queries=batch.query.to_list()
-            )
-
-    def delete(self, chunk_ids: List[ChunkId], **kwargs):
-        self.retriever.remove(ids=chunk_ids)
-
-    def save(self, path: str):
-        self.retriever.save(path)
-
-    @classmethod
-    def load(cls, path: str):
-        instance = cls()
-        instance.retriever = search.FinetunableRetriever.load(path)
-        return instance
+from typing import Iterable, List, Set, Tuple
+
+from thirdai import search
+
+from ..core.retriever import Retriever
+from ..core.types import ChunkBatch, ChunkId, Score, SupervisedBatch
+
+
+class FinetunableRetriever(Retriever):
+    def __init__(self, **kwargs):
+        super().__init__()
+        self.retriever = search.FinetunableRetriever()
+
+    def search(
+        self, queries: List[str], top_k: int, **kwargs
+    ) -> List[List[Tuple[ChunkId, Score]]]:
+        return self.retriever.query(queries, k=top_k)
+
+    def rank(
+        self, queries: List[str], choices: List[Set[ChunkId]], top_k: int, **kwargs
+    ) -> List[List[Tuple[ChunkId, Score]]]:
+        return self.retriever.rank(queries, candidates=choices, k=top_k)
+
+    def upvote(self, queries: List[str], chunk_ids: List[ChunkId], **kwargs):
+        self.retriever.finetune(
+            doc_ids=list(map(lambda id: [id], chunk_ids)), queries=queries
+        )
+
+    def associate(
+        self, sources: List[str], targets: List[str], associate_strength=4, **kwargs
+    ):
+        self.retriever.associate(
+            sources=sources, targets=targets, strength=associate_strength
+        )
+
+    def insert(self, chunks: Iterable[ChunkBatch], **kwargs):
+        for batch in chunks:
+            texts = batch.keywords + " " + batch.text
+
+            self.retriever.index(ids=batch.chunk_id.to_list(), docs=texts.to_list())
+
+    def supervised_train(self, samples: Iterable[SupervisedBatch], **kwargs):
+        for batch in samples:
+            self.retriever.finetune(
+                doc_ids=batch.chunk_id.to_list(), queries=batch.query.to_list()
+            )
+
+    def delete(self, chunk_ids: List[ChunkId], **kwargs):
+        self.retriever.remove(ids=chunk_ids)
+
+    def save(self, path: str):
+        self.retriever.save(path)
+
+    @classmethod
+    def load(cls, path: str):
+        instance = cls()
+        instance.retriever = search.FinetunableRetriever.load(path)
+        return instance
```

## thirdai/neural_db_v2/retrievers/mach.py

```diff
@@ -1,219 +1,225 @@
-from typing import Dict, Iterable, List, Optional, Tuple
-
-from thirdai import bolt, data
-from thirdai.neural_db.models.mach_defaults import autotune_from_scratch_min_max_epochs
-
-from ..core.retriever import Retriever
-from ..core.types import ChunkBatch, ChunkId, Score, SupervisedBatch
-
-
-class ChunkColumnMapIterator(data.ColumnMapIterator):
-    def __init__(
-        self,
-        iterable: Iterable[ChunkBatch],
-        text_columns: Dict[str, str],
-        multi_label=False,
-    ):
-        data.ColumnMapIterator.__init__(self)
-
-        self.iterable = iterable
-        self.iterator = iter(self.iterable)
-        self.text_columns = text_columns
-        self.multi_label = multi_label
-
-    def next(self) -> Optional[data.ColumnMap]:
-        id_column = (
-            data.columns.TokenArrayColumn
-            if self.multi_label
-            else data.columns.TokenColumn
-        )
-
-        try:
-            batch = next(self.iterator)
-            columns = {
-                Mach.ID: id_column(batch.chunk_id.to_list(), dim=data.columns.MAX_DIM)
-            }
-            for name, attr in self.text_columns.items():
-                columns[name] = data.columns.StringColumn(getattr(batch, attr))
-            return data.ColumnMap(columns)
-        except StopIteration:
-            return None
-
-    def restart(self) -> None:
-        self.iterator = iter(self.iterable)
-
-    def resource_name(self):
-        return "ChunkColumnMapIterator"
-
-    def size(self) -> int:
-        return sum(len(batch.text) for batch in self.iterable)
-
-
-class EarlyStopWithMinEpochs(bolt.train.callbacks.Callback):
-    def __init__(self, min_epochs, tracked_metric, metric_threshold):
-        super().__init__()
-
-        self.epoch_count = 0
-        self.min_epochs = min_epochs
-        self.tracked_metric = tracked_metric
-        self.metric_threshold = metric_threshold
-
-    def on_epoch_end(self):
-        self.epoch_count += 1
-
-        if (
-            self.epoch_count > self.min_epochs
-            and self.history[f"train_{self.tracked_metric}"][-1] > self.metric_threshold
-        ):
-            self.train_state.stop_training()
-
-
-class Mach(Retriever):
-    STRONG = "keywords"
-    WEAK = "text"
-    TEXT = "text"
-    ID = "chunk_id"
-
-    def __init__(
-        self,
-        tokenizer: str = "char-4",
-        encoding: str = "none",
-        emb_dim: int = 2000,
-        n_buckets: int = 50000,
-        output_act: str = "sigmoid",
-        emb_bias: bool = True,
-        output_bias: bool = True,
-        **kwargs,
-    ):
-        super().__init__()
-        config = (
-            bolt.MachConfig()
-            .text_col(Mach.TEXT)
-            .id_col(Mach.ID)
-            .tokenizer(tokenizer)
-            .contextual_encoding(encoding)
-            .emb_dim(emb_dim)
-            .n_buckets(n_buckets)
-            .output_activation(output_act)
-            .emb_bias(emb_bias)
-            .output_bias(output_bias)
-        )
-
-        self.model = config.build()
-
-    def search(
-        self, queries: List[str], top_k: int, sparse_inference: bool = False, **kwargs
-    ) -> List[List[Tuple[ChunkId, Score]]]:
-        return self.model.search(
-            queries=queries,
-            top_k=top_k,
-            sparse_inference=sparse_inference,
-        )
-
-    def rank(
-        self,
-        queries: List[str],
-        choices: List[List[ChunkId]],
-        top_k: int,
-        sparse_inference: bool = False,
-        **kwargs,
-    ) -> List[List[Tuple[ChunkId, Score]]]:
-        return self.model.rank(
-            queries=queries,
-            candidates=choices,
-            top_k=top_k,
-            sparse_inference=sparse_inference,
-        )
-
-    def upvote(self, queries: List[str], chunk_ids: List[ChunkId], **kwargs):
-        self.model.upvote(queries=queries, ids=chunk_ids)
-
-    def associate(
-        self, sources: List[str], targets: List[str], n_buckets: int = 7, **kwargs
-    ):
-        self.model.associate(
-            sources=sources,
-            targets=targets,
-            n_buckets=n_buckets,
-        )
-
-    def insert(
-        self,
-        chunks: Iterable[ChunkBatch],
-        learning_rate: float = 0.001,
-        epochs: Optional[int] = None,
-        metrics: Optional[List[str]] = None,
-        callbacks: Optional[List[bolt.train.callbacks.Callback]] = None,
-        max_in_memory_batches: Optional[int] = None,
-        variable_length: Optional[
-            data.transformations.VariableLengthConfig
-        ] = data.transformations.VariableLengthConfig(),
-        batch_size: int = 2000,
-        early_stop_metric: str = "hash_precision@5",
-        early_stop_metric_threshold: float = 0.95,
-        **kwargs,
-    ):
-        train_data = ChunkColumnMapIterator(
-            chunks, text_columns={Mach.STRONG: "keywords", Mach.WEAK: "text"}
-        )
-
-        metrics = metrics or []
-        if "hash_precision@5" not in metrics:
-            metrics.append("hash_precision@5")
-
-        min_epochs, max_epochs = autotune_from_scratch_min_max_epochs(
-            size=train_data.size()
-        )
-
-        early_stop_callback = EarlyStopWithMinEpochs(
-            min_epochs=epochs or min_epochs,
-            tracked_metric=early_stop_metric,
-            metric_threshold=early_stop_metric_threshold,
-        )
-
-        callbacks = callbacks or []
-        callbacks.append(early_stop_callback)
-
-        self.model.coldstart(
-            data=train_data,
-            strong_cols=[Mach.STRONG],
-            weak_cols=[Mach.WEAK],
-            learning_rate=learning_rate,
-            epochs=epochs or max_epochs,
-            metrics=metrics,
-            callbacks=callbacks,
-            max_in_memory_batches=max_in_memory_batches,
-            variable_length=variable_length,
-            batch_size=batch_size,
-        )
-
-    def supervised_train(
-        self,
-        samples: Iterable[SupervisedBatch],
-        learning_rate: float = 0.001,
-        epochs: int = 3,
-        metrics: Optional[List[str]] = None,
-        **kwargs,
-    ):
-        train_data = ChunkColumnMapIterator(
-            samples, text_columns={Mach.TEXT: "query"}, multi_label=True
-        )
-
-        self.model.train(
-            data=train_data,
-            learning_rate=learning_rate,
-            epochs=epochs,
-            metrics=metrics or ["hash_precision@5"],
-        )
-
-    def delete(self, chunk_ids: List[ChunkId], **kwargs):
-        self.model.erase(ids=chunk_ids)
-
-    def save(self, path: str):
-        self.model.save(path)
-
-    @classmethod
-    def load(cls, path: str):
-        instance = cls()
-        instance.model = bolt.MachRetriever.load(path)
-        return instance
+from typing import Dict, Iterable, List, Optional, Set, Tuple
+
+from thirdai import bolt, data
+from thirdai.neural_db.models.mach_defaults import autotune_from_scratch_min_max_epochs
+
+from ..core.retriever import Retriever
+from ..core.types import ChunkBatch, ChunkId, Score, SupervisedBatch
+
+
+class ChunkColumnMapIterator(data.ColumnMapIterator):
+    def __init__(
+        self,
+        iterable: Iterable[ChunkBatch],
+        text_columns: Dict[str, str],
+        multi_label=False,
+    ):
+        data.ColumnMapIterator.__init__(self)
+
+        self.iterable = iterable
+        self.iterator = iter(self.iterable)
+        self.text_columns = text_columns
+        self.multi_label = multi_label
+
+    def next(self) -> Optional[data.ColumnMap]:
+        id_column = (
+            data.columns.TokenArrayColumn
+            if self.multi_label
+            else data.columns.TokenColumn
+        )
+
+        try:
+            batch = next(self.iterator)
+            columns = {
+                Mach.ID: id_column(batch.chunk_id.to_list(), dim=data.columns.MAX_DIM)
+            }
+            for name, attr in self.text_columns.items():
+                columns[name] = data.columns.StringColumn(getattr(batch, attr))
+            return data.ColumnMap(columns)
+        except StopIteration:
+            return None
+
+    def restart(self) -> None:
+        self.iterator = iter(self.iterable)
+
+    def resource_name(self):
+        return "ChunkColumnMapIterator"
+
+    def size(self) -> int:
+        return sum(len(batch.text) for batch in self.iterable)
+
+
+class EarlyStopWithMinEpochs(bolt.train.callbacks.Callback):
+    def __init__(self, min_epochs, tracked_metric, metric_threshold):
+        super().__init__()
+
+        self.epoch_count = 0
+        self.min_epochs = min_epochs
+        self.tracked_metric = tracked_metric
+        self.metric_threshold = metric_threshold
+
+    def on_epoch_end(self):
+        self.epoch_count += 1
+
+        if (
+            self.epoch_count > self.min_epochs
+            and self.history[f"train_{self.tracked_metric}"][-1] > self.metric_threshold
+        ):
+            self.train_state.stop_training()
+
+
+class Mach(Retriever):
+    STRONG = "keywords"
+    WEAK = "text"
+    TEXT = "text"
+    ID = "chunk_id"
+
+    def __init__(
+        self,
+        tokenizer: str = "char-4",
+        encoding: str = "none",
+        emb_dim: int = 2000,
+        n_buckets: int = 50000,
+        output_act: str = "sigmoid",
+        emb_bias: bool = True,
+        output_bias: bool = True,
+        n_hashes: Optional[int] = None,
+        index_seed: Optional[int] = None,
+        **kwargs,
+    ):
+        super().__init__()
+        config = (
+            bolt.MachConfig()
+            .text_col(Mach.TEXT)
+            .id_col(Mach.ID)
+            .tokenizer(tokenizer)
+            .contextual_encoding(encoding)
+            .emb_dim(emb_dim)
+            .n_buckets(n_buckets)
+            .output_activation(output_act)
+            .emb_bias(emb_bias)
+            .output_bias(output_bias)
+        )
+        if index_seed:
+            config = config.index_seed(index_seed)
+        if n_hashes:
+            config = config.n_hashes(n_hashes)
+
+        self.model = config.build()
+
+    def search(
+        self, queries: List[str], top_k: int, sparse_inference: bool = False, **kwargs
+    ) -> List[List[Tuple[ChunkId, Score]]]:
+        return self.model.search(
+            queries=queries,
+            top_k=top_k,
+            sparse_inference=sparse_inference,
+        )
+
+    def rank(
+        self,
+        queries: List[str],
+        choices: List[Set[ChunkId]],
+        top_k: int,
+        sparse_inference: bool = False,
+        **kwargs,
+    ) -> List[List[Tuple[ChunkId, Score]]]:
+        return self.model.rank(
+            queries=queries,
+            candidates=choices,
+            top_k=top_k,
+            sparse_inference=sparse_inference,
+        )
+
+    def upvote(self, queries: List[str], chunk_ids: List[ChunkId], **kwargs):
+        self.model.upvote(queries=queries, ids=chunk_ids)
+
+    def associate(
+        self, sources: List[str], targets: List[str], n_buckets: int = 7, **kwargs
+    ):
+        self.model.associate(
+            sources=sources,
+            targets=targets,
+            n_buckets=n_buckets,
+        )
+
+    def insert(
+        self,
+        chunks: Iterable[ChunkBatch],
+        learning_rate: float = 0.001,
+        epochs: Optional[int] = None,
+        metrics: Optional[List[str]] = None,
+        callbacks: Optional[List[bolt.train.callbacks.Callback]] = None,
+        max_in_memory_batches: Optional[int] = None,
+        variable_length: Optional[
+            data.transformations.VariableLengthConfig
+        ] = data.transformations.VariableLengthConfig(),
+        batch_size: int = 2000,
+        early_stop_metric: str = "hash_precision@5",
+        early_stop_metric_threshold: float = 0.95,
+        **kwargs,
+    ):
+        train_data = ChunkColumnMapIterator(
+            chunks, text_columns={Mach.STRONG: "keywords", Mach.WEAK: "text"}
+        )
+
+        metrics = metrics or []
+        if "hash_precision@5" not in metrics:
+            metrics.append("hash_precision@5")
+
+        min_epochs, max_epochs = autotune_from_scratch_min_max_epochs(
+            size=train_data.size()
+        )
+
+        early_stop_callback = EarlyStopWithMinEpochs(
+            min_epochs=epochs or min_epochs,
+            tracked_metric=early_stop_metric,
+            metric_threshold=early_stop_metric_threshold,
+        )
+
+        callbacks = callbacks or []
+        callbacks.append(early_stop_callback)
+
+        self.model.coldstart(
+            data=train_data,
+            strong_cols=[Mach.STRONG],
+            weak_cols=[Mach.WEAK],
+            learning_rate=learning_rate,
+            epochs=epochs or max_epochs,
+            metrics=metrics,
+            callbacks=callbacks,
+            max_in_memory_batches=max_in_memory_batches,
+            variable_length=variable_length,
+            batch_size=batch_size,
+        )
+
+    def supervised_train(
+        self,
+        samples: Iterable[SupervisedBatch],
+        learning_rate: float = 0.001,
+        epochs: int = 3,
+        metrics: Optional[List[str]] = None,
+        **kwargs,
+    ):
+        train_data = ChunkColumnMapIterator(
+            samples, text_columns={Mach.TEXT: "query"}, multi_label=True
+        )
+
+        self.model.train(
+            data=train_data,
+            learning_rate=learning_rate,
+            epochs=epochs,
+            metrics=metrics or ["hash_precision@5"],
+        )
+
+    def delete(self, chunk_ids: List[ChunkId], **kwargs):
+        self.model.erase(ids=chunk_ids)
+
+    def save(self, path: str):
+        self.model.save(path)
+
+    @classmethod
+    def load(cls, path: str):
+        instance = cls()
+        instance.model = bolt.MachRetriever.load(path)
+        return instance
```

## thirdai/neural_db_v2/retrievers/__init__.py

```diff
@@ -1,4 +1,5 @@
-from pathlib import Path
-
-from .finetunable_retriever import FinetunableRetriever
-from .mach import Mach
+from pathlib import Path
+
+from .finetunable_retriever import FinetunableRetriever
+from .mach import Mach
+from .mach_ensemble import MachEnsemble
```

## thirdai/neural_db_v2/supervised/csv_supervised.py

 * *Ordering differences only*

```diff
@@ -1,40 +1,40 @@
-from typing import Iterable, Optional, Union
-
-import pandas as pd
-
-from ..core.supervised import SupervisedDataset
-from ..core.types import CustomIdSupervisedBatch, SupervisedBatch
-
-
-class CsvSupervised(SupervisedDataset):
-    def __init__(
-        self,
-        path: str,
-        query_column: str,
-        id_column: str,
-        id_delimiter: str,
-        uses_db_id: bool,
-    ):
-        self.path = path
-        self.query_column = query_column
-        self.id_column = id_column
-        self.id_delimiter = id_delimiter
-        self.uses_db_id = uses_db_id
-
-    def samples(
-        self,
-    ) -> Union[Iterable[SupervisedBatch], Iterable[CustomIdSupervisedBatch]]:
-        df = pd.read_csv(self.path)
-
-        ids = df[self.id_column].map(lambda val: str(val).split(self.id_delimiter))
-
-        if self.uses_db_id:
-            ids = pd.Series([list(map(int, row_ids)) for row_ids in ids])
-
-        return [
-            self.supervised_samples(
-                queries=df[self.query_column],
-                ids=ids,
-                uses_db_id=self.uses_db_id,
-            )
-        ]
+from typing import Iterable, Optional, Union
+
+import pandas as pd
+
+from ..core.supervised import SupervisedDataset
+from ..core.types import CustomIdSupervisedBatch, SupervisedBatch
+
+
+class CsvSupervised(SupervisedDataset):
+    def __init__(
+        self,
+        path: str,
+        query_column: str,
+        id_column: str,
+        id_delimiter: str,
+        uses_db_id: bool,
+    ):
+        self.path = path
+        self.query_column = query_column
+        self.id_column = id_column
+        self.id_delimiter = id_delimiter
+        self.uses_db_id = uses_db_id
+
+    def samples(
+        self,
+    ) -> Union[Iterable[SupervisedBatch], Iterable[CustomIdSupervisedBatch]]:
+        df = pd.read_csv(self.path)
+
+        ids = df[self.id_column].map(lambda val: str(val).split(self.id_delimiter))
+
+        if self.uses_db_id:
+            ids = pd.Series([list(map(int, row_ids)) for row_ids in ids])
+
+        return [
+            self.supervised_samples(
+                queries=df[self.query_column],
+                ids=ids,
+                uses_db_id=self.uses_db_id,
+            )
+        ]
```

## thirdai/neural_db_v2/supervised/in_memory_supervised.py

 * *Ordering differences only*

```diff
@@ -1,23 +1,23 @@
-from typing import Iterable, List, Union
-
-import pandas as pd
-
-from ..core.supervised import SupervisedDataset
-from ..core.types import ChunkId, CustomIdSupervisedBatch, SupervisedBatch
-
-
-class InMemorySupervised(SupervisedDataset):
-    def __init__(
-        self,
-        queries: List[str],
-        ids: Union[List[List[ChunkId]], List[List[str]], List[List[int]]],
-        uses_db_id: bool,
-    ):
-        self.queries = pd.Series(queries)
-        self.ids = pd.Series(ids)
-        self.uses_db_id = uses_db_id
-
-    def samples(
-        self,
-    ) -> Union[Iterable[SupervisedBatch], Iterable[CustomIdSupervisedBatch]]:
-        return [self.supervised_samples(self.queries, self.ids, self.uses_db_id)]
+from typing import Iterable, List, Union
+
+import pandas as pd
+
+from ..core.supervised import SupervisedDataset
+from ..core.types import ChunkId, CustomIdSupervisedBatch, SupervisedBatch
+
+
+class InMemorySupervised(SupervisedDataset):
+    def __init__(
+        self,
+        queries: List[str],
+        ids: Union[List[List[ChunkId]], List[List[str]], List[List[int]]],
+        uses_db_id: bool,
+    ):
+        self.queries = pd.Series(queries)
+        self.ids = pd.Series(ids)
+        self.uses_db_id = uses_db_id
+
+    def samples(
+        self,
+    ) -> Union[Iterable[SupervisedBatch], Iterable[CustomIdSupervisedBatch]]:
+        return [self.supervised_samples(self.queries, self.ids, self.uses_db_id)]
```

## thirdai/neural_db_v2/supervised/__init__.py

 * *Ordering differences only*

```diff
@@ -1,2 +1,2 @@
-from .csv_supervised import CsvSupervised
-from .in_memory_supervised import InMemorySupervised
+from .csv_supervised import CsvSupervised
+from .in_memory_supervised import InMemorySupervised
```

## thirdai/telemetry/telemetry_daemon.py

 * *Ordering differences only*

```diff
@@ -1,141 +1,141 @@
-import argparse
-import signal
-import time
-from pathlib import Path
-from urllib.parse import urlparse
-
-import requests
-
-# This daemon gets run whenever a write_dir (which can be local or cloud (for
-# now just s3)) is specified when starting metrics. It reads from the local
-# port where the thirdai prometheus metrics are hosted, and writes those metrics
-# to write_dir/telemetry-<uuid> every DEFAULT_UPLOAD_INTERVAL_SECONDS.
-
-
-# See https://stackoverflow.com/questions/18499497/how-to-process-sigterm-signal-gracefully
-class GracefulKiller:
-    kill_now = False
-
-    def __init__(self):
-        signal.signal(signal.SIGINT, self.exit_gracefully)
-        signal.signal(signal.SIGTERM, self.exit_gracefully)
-
-    def exit_gracefully(self, *args):
-        self.kill_now = True
-
-
-# We will respond to interrupt signals (via the GracefulKiller) at this interval
-DEFAULT_SLEEP_INTERVAL_SECONDS = 0.1
-
-
-def push_to_local_file(parsed_file_path, raw_telemetry):
-    Path(parsed_file_path.path).parent.mkdir(parents=True, exist_ok=True)
-    with open(parsed_file_path.path, "wb") as f:
-        f.write(raw_telemetry)
-
-
-def push_to_s3(parsed_s3_path, raw_telemetry, optional_endpoint_url):
-    import boto3
-
-    if optional_endpoint_url is None:
-        client = boto3.client("s3")
-    else:
-        client = boto3.client("s3", endpoint_url=optional_endpoint_url)
-    key = parsed_s3_path.path
-    if key.startswith("/"):
-        key = key[1:]
-    client.put_object(
-        Bucket=parsed_s3_path.netloc,
-        Key=key,
-        Body=raw_telemetry,
-    )
-
-
-def parse_uuid(raw_telemetry):
-    telemetry_string = raw_telemetry.decode("utf-8")
-    key = "thirdai_instance_uuid"
-    uuid_key_offset = telemetry_string.index(key)
-    # The format of the key value pair is key="<UUID>", so we need to add the
-    # length of the key + 2 to go from the offset of the key to the offset of
-    # the uuid
-    uuid_value_offset = uuid_key_offset + len(key) + 2
-    uuid_length = 32  # 32 hex chars in a UUID
-    uuid = telemetry_string[uuid_value_offset : uuid_value_offset + uuid_length]
-    return uuid
-
-
-def push_telemetry(push_dir, telemetry_url, optional_endpoint_url):
-    raw_telemetry = requests.get(telemetry_url).content
-    # Parsing the UUID from the raw telemetry instead of having it passed in
-    # at program creation ensures that we are much less likely to push a
-    # corrupted or wrong telemetry file to the remote location. If the parent
-    # process is dead, we won't get any telemetry, so either the raw_telemetry
-    # call above will fail or it will be an empty string and the parse_uuid
-    # call will fail. If the parent process has died and a new process has
-    # started in the meantime, we might have missed some of the parent processes
-    # final updates, but we won't overwrite the parent's telemetry file with a
-    # new telemetry file because the parsed uuid will be different.
-    uuid = parse_uuid(raw_telemetry)
-    parsed_push_location = urlparse(push_dir + "/telemetry-" + uuid)
-    if parsed_push_location.scheme == "":
-        push_to_local_file(parsed_push_location, raw_telemetry)
-    elif parsed_push_location.scheme == "s3":
-        push_to_s3(parsed_push_location, raw_telemetry, optional_endpoint_url)
-    else:
-        raise ValueError(f"Unknown location {push_dir}")
-
-
-def launch_daemon(
-    push_dir, telemetry_url, optional_endpoint_url, upload_interval_seconds, killer
-):
-    last_update_time = 0
-    while not killer.kill_now:
-        if time.time() - last_update_time > upload_interval_seconds:
-            push_telemetry(push_dir, telemetry_url, optional_endpoint_url)
-            last_update_time = time.time()
-        # Sleeping for this shorter amount of time instead of
-        # DEFAULT_UPLOAD_INTERVAL_SECONDS ensures we can respond to interrupts
-        # quickly
-        time.sleep(DEFAULT_SLEEP_INTERVAL_SECONDS)
-
-    # We push at the end to make sure the telemetry is flushed (if the parent
-    # thirdai process has been killed this will just throw an error)
-    push_telemetry(push_dir, telemetry_url, optional_endpoint_url)
-
-
-if __name__ == "__main__":
-    parser = argparse.ArgumentParser(
-        description="Start a background daemon thread that pushes telemetry to a remote location."
-    )
-    parser.add_argument(
-        "--telemetry_url",
-        help="The local telemetry server url to scrape from.",
-        required=True,
-    )
-    parser.add_argument(
-        "--push_dir",
-        help="The location (currently local or s3) to push telemetry to.",
-        required=True,
-    )
-    parser.add_argument(
-        "--upload_interval_seconds",
-        help="How often to upload telemetry.",
-        type=int,
-        required=True,
-    )
-    parser.add_argument(
-        "--optional_endpoint_url",
-        help="Optional endpoint url to pass to boto3. Usually not needed (currently used for testing).",
-        default=None,
-    )
-    args = parser.parse_args()
-
-    killer = GracefulKiller()
-
-    launch_daemon(
-        args.push_dir,
-        args.telemetry_url,
-        args.optional_endpoint_url,
-        args.upload_interval_seconds,
-        killer,
-    )
+import argparse
+import signal
+import time
+from pathlib import Path
+from urllib.parse import urlparse
+
+import requests
+
+# This daemon gets run whenever a write_dir (which can be local or cloud (for
+# now just s3)) is specified when starting metrics. It reads from the local
+# port where the thirdai prometheus metrics are hosted, and writes those metrics
+# to write_dir/telemetry-<uuid> every DEFAULT_UPLOAD_INTERVAL_SECONDS.
+
+
+# See https://stackoverflow.com/questions/18499497/how-to-process-sigterm-signal-gracefully
+class GracefulKiller:
+    kill_now = False
+
+    def __init__(self):
+        signal.signal(signal.SIGINT, self.exit_gracefully)
+        signal.signal(signal.SIGTERM, self.exit_gracefully)
+
+    def exit_gracefully(self, *args):
+        self.kill_now = True
+
+
+# We will respond to interrupt signals (via the GracefulKiller) at this interval
+DEFAULT_SLEEP_INTERVAL_SECONDS = 0.1
+
+
+def push_to_local_file(parsed_file_path, raw_telemetry):
+    Path(parsed_file_path.path).parent.mkdir(parents=True, exist_ok=True)
+    with open(parsed_file_path.path, "wb") as f:
+        f.write(raw_telemetry)
+
+
+def push_to_s3(parsed_s3_path, raw_telemetry, optional_endpoint_url):
+    import boto3
+
+    if optional_endpoint_url is None:
+        client = boto3.client("s3")
+    else:
+        client = boto3.client("s3", endpoint_url=optional_endpoint_url)
+    key = parsed_s3_path.path
+    if key.startswith("/"):
+        key = key[1:]
+    client.put_object(
+        Bucket=parsed_s3_path.netloc,
+        Key=key,
+        Body=raw_telemetry,
+    )
+
+
+def parse_uuid(raw_telemetry):
+    telemetry_string = raw_telemetry.decode("utf-8")
+    key = "thirdai_instance_uuid"
+    uuid_key_offset = telemetry_string.index(key)
+    # The format of the key value pair is key="<UUID>", so we need to add the
+    # length of the key + 2 to go from the offset of the key to the offset of
+    # the uuid
+    uuid_value_offset = uuid_key_offset + len(key) + 2
+    uuid_length = 32  # 32 hex chars in a UUID
+    uuid = telemetry_string[uuid_value_offset : uuid_value_offset + uuid_length]
+    return uuid
+
+
+def push_telemetry(push_dir, telemetry_url, optional_endpoint_url):
+    raw_telemetry = requests.get(telemetry_url).content
+    # Parsing the UUID from the raw telemetry instead of having it passed in
+    # at program creation ensures that we are much less likely to push a
+    # corrupted or wrong telemetry file to the remote location. If the parent
+    # process is dead, we won't get any telemetry, so either the raw_telemetry
+    # call above will fail or it will be an empty string and the parse_uuid
+    # call will fail. If the parent process has died and a new process has
+    # started in the meantime, we might have missed some of the parent processes
+    # final updates, but we won't overwrite the parent's telemetry file with a
+    # new telemetry file because the parsed uuid will be different.
+    uuid = parse_uuid(raw_telemetry)
+    parsed_push_location = urlparse(push_dir + "/telemetry-" + uuid)
+    if parsed_push_location.scheme == "":
+        push_to_local_file(parsed_push_location, raw_telemetry)
+    elif parsed_push_location.scheme == "s3":
+        push_to_s3(parsed_push_location, raw_telemetry, optional_endpoint_url)
+    else:
+        raise ValueError(f"Unknown location {push_dir}")
+
+
+def launch_daemon(
+    push_dir, telemetry_url, optional_endpoint_url, upload_interval_seconds, killer
+):
+    last_update_time = 0
+    while not killer.kill_now:
+        if time.time() - last_update_time > upload_interval_seconds:
+            push_telemetry(push_dir, telemetry_url, optional_endpoint_url)
+            last_update_time = time.time()
+        # Sleeping for this shorter amount of time instead of
+        # DEFAULT_UPLOAD_INTERVAL_SECONDS ensures we can respond to interrupts
+        # quickly
+        time.sleep(DEFAULT_SLEEP_INTERVAL_SECONDS)
+
+    # We push at the end to make sure the telemetry is flushed (if the parent
+    # thirdai process has been killed this will just throw an error)
+    push_telemetry(push_dir, telemetry_url, optional_endpoint_url)
+
+
+if __name__ == "__main__":
+    parser = argparse.ArgumentParser(
+        description="Start a background daemon thread that pushes telemetry to a remote location."
+    )
+    parser.add_argument(
+        "--telemetry_url",
+        help="The local telemetry server url to scrape from.",
+        required=True,
+    )
+    parser.add_argument(
+        "--push_dir",
+        help="The location (currently local or s3) to push telemetry to.",
+        required=True,
+    )
+    parser.add_argument(
+        "--upload_interval_seconds",
+        help="How often to upload telemetry.",
+        type=int,
+        required=True,
+    )
+    parser.add_argument(
+        "--optional_endpoint_url",
+        help="Optional endpoint url to pass to boto3. Usually not needed (currently used for testing).",
+        default=None,
+    )
+    args = parser.parse_args()
+
+    killer = GracefulKiller()
+
+    launch_daemon(
+        args.push_dir,
+        args.telemetry_url,
+        args.optional_endpoint_url,
+        args.upload_interval_seconds,
+        killer,
+    )
```

## thirdai/telemetry/telemetry_start_and_stop.py

 * *Ordering differences only*

```diff
@@ -1,147 +1,147 @@
-import atexit
-import pathlib
-import subprocess
-import sys
-import time
-from typing import Optional
-
-from thirdai._thirdai import telemetry
-
-from .telemetry_daemon import push_telemetry
-
-# This file defines methods start and stop, which get added to the
-# thirdai.telemetry module in __init__.py. They wrap the corresponding start and
-# stop methods in thirdai._thirdai.telemetry, which start serving the prometheus
-# metrics on the passed in port. The reason we need to wrap these methods is so
-# that we can additionally start and stop a background thread that *pushes*
-# prometheus metrics from the local Prometheus endpoint (localhost:port) to a
-# file/remote endpoint. See telemetry_daemon.py for the background daemon that
-# gets started. The push location can be an s3 path or a file path, and the
-# daemon will handle pushing to the correct location.
-#
-# telemetry.start() should be called by the user at the top of a script that
-# they want to track telemetry in. telemetry.stop() mostly should never need to
-# be called by the user.
-#
-# We cannot use a python thread for the background push daemon because of the
-# GIL. We cannot easily use a C++ background thread because of the complexity
-# of writing e.g. S3 adapters in C++.
-
-
-daemon_script_path = pathlib.Path(__file__).parent.resolve() / "telemetry_daemon.py"
-
-background_telemetry_push_process = None
-
-# If we have to wait longer than this many seconds when trying to gracefully
-# terminate the background process, we give up and send a kill message, possibly
-# losing telemetry data.
-BACKGROUND_THREAD_TIMEOUT_SECONDS = 2
-
-# Wait this many seconds after starting the background thread before checking if
-# it is still running.
-BACKGROUND_THREAD_HEALTH_CHECK_WAIT = 0.5
-
-# We will upload data to the push dir at this interval (and before the script
-# finishes when the GracefulKiller catches that exception)
-DEFAULT_UPLOAD_INTERVAL_SECONDS = 60 * 20
-
-
-# See https://stackoverflow.com/q/320232/ensuring-subprocesses-are-dead-on-exiting-python-program
-# If a background telemetry push process (as started by a call to start) exists,
-# this function tries to gracefully kill that process by sending a sigkill.
-# If the process doesn't finish quickly enough, this function sends a sigterm,
-# which will force kill it.
-def _kill_background_telemetry_push_process():
-    global background_telemetry_push_process
-    if (
-        background_telemetry_push_process != None
-        and background_telemetry_push_process.poll() is None
-    ):
-        background_telemetry_push_process.terminate()
-        try:
-            background_telemetry_push_process.wait(
-                timeout=BACKGROUND_THREAD_TIMEOUT_SECONDS
-            )
-        except subprocess.TimeoutExpired:
-            background_telemetry_push_process.kill()
-
-    background_telemetry_push_process = None
-
-
-# This will cause _kill_background_telemetry_push_process to get called when
-# the current interpreter session finishes, which will kill the background
-# process and cause it to do one last push. According to
-# https://docs.python.org/3/library/atexit.html, this is not called when the
-# program is killed by a signal not handled by Python, when a Python fatal
-# internal error is detected, or when os._exit() is called. Since this is
-# defined after we define _thirdai, it should get called before the destructors
-# that take down the metrics server.
-atexit.register(_kill_background_telemetry_push_process)
-
-
-wrapped_start_method = telemetry.start
-wrapped_stop_method = telemetry.stop
-
-
-def start(
-    port: Optional[int] = None,
-    write_dir: Optional[str] = None,
-    write_period_seconds: int = DEFAULT_UPLOAD_INTERVAL_SECONDS,
-    optional_endpoint_url: Optional[str] = None,
-):
-    """
-    Start a Prometheus telemetry client on the passed in port. If a port is not
-    specified this method will use the default ThirdAI port of 9929. This
-    function is not thread safe with other ThirdAI code, so you should make sure
-    that no other code is running when this method is called.
-    If a write_dir is passed in, this function will additionally start a
-    background daemon that will push the Prometheus telemetry to write_dir at
-    the path write_dir/telemetry-<instance_uuid>. Currently, write_dir can be a
-    local path or an s3 path. Writes to this write_dir will happen every
-    write_period_seconds, which has a default of 20 minutes.
-    """
-    global background_telemetry_push_process
-    if background_telemetry_push_process != None:
-        raise RuntimeError(
-            "Trying to start telemetry client when one is already running"
-        )
-
-    if port:
-        telemetry_url = wrapped_start_method(port)
-    else:
-        telemetry_url = wrapped_start_method()
-
-    if write_dir == None:
-        return telemetry_url
-
-    # Run serially once to fix most errors
-    push_telemetry(write_dir, telemetry_url, optional_endpoint_url)
-
-    # Could also try using os.fork
-    python_executable = sys.executable
-    args = [
-        python_executable,
-        str(daemon_script_path.resolve()),
-        "--telemetry_url",
-        telemetry_url,
-        "--push_dir",
-        write_dir,
-        "--upload_interval_seconds",
-        str(write_period_seconds),
-    ]
-    if optional_endpoint_url:
-        args += ["--optional_endpoint_url", optional_endpoint_url]
-    background_telemetry_push_process = subprocess.Popen(args)
-
-    push_location = write_dir + f"/telemetry-" + telemetry.uuid()
-    return push_location
-
-
-def stop():
-    """
-    Stops the current Prometheus telemetry client if one is running. This
-    function is not thread safe with other ThirdAI code, so you should make sure
-    that no other code is running when this method is called.
-    """
-    _kill_background_telemetry_push_process()
-    wrapped_stop_method()
+import atexit
+import pathlib
+import subprocess
+import sys
+import time
+from typing import Optional
+
+from thirdai._thirdai import telemetry
+
+from .telemetry_daemon import push_telemetry
+
+# This file defines methods start and stop, which get added to the
+# thirdai.telemetry module in __init__.py. They wrap the corresponding start and
+# stop methods in thirdai._thirdai.telemetry, which start serving the prometheus
+# metrics on the passed in port. The reason we need to wrap these methods is so
+# that we can additionally start and stop a background thread that *pushes*
+# prometheus metrics from the local Prometheus endpoint (localhost:port) to a
+# file/remote endpoint. See telemetry_daemon.py for the background daemon that
+# gets started. The push location can be an s3 path or a file path, and the
+# daemon will handle pushing to the correct location.
+#
+# telemetry.start() should be called by the user at the top of a script that
+# they want to track telemetry in. telemetry.stop() mostly should never need to
+# be called by the user.
+#
+# We cannot use a python thread for the background push daemon because of the
+# GIL. We cannot easily use a C++ background thread because of the complexity
+# of writing e.g. S3 adapters in C++.
+
+
+daemon_script_path = pathlib.Path(__file__).parent.resolve() / "telemetry_daemon.py"
+
+background_telemetry_push_process = None
+
+# If we have to wait longer than this many seconds when trying to gracefully
+# terminate the background process, we give up and send a kill message, possibly
+# losing telemetry data.
+BACKGROUND_THREAD_TIMEOUT_SECONDS = 2
+
+# Wait this many seconds after starting the background thread before checking if
+# it is still running.
+BACKGROUND_THREAD_HEALTH_CHECK_WAIT = 0.5
+
+# We will upload data to the push dir at this interval (and before the script
+# finishes when the GracefulKiller catches that exception)
+DEFAULT_UPLOAD_INTERVAL_SECONDS = 60 * 20
+
+
+# See https://stackoverflow.com/q/320232/ensuring-subprocesses-are-dead-on-exiting-python-program
+# If a background telemetry push process (as started by a call to start) exists,
+# this function tries to gracefully kill that process by sending a sigkill.
+# If the process doesn't finish quickly enough, this function sends a sigterm,
+# which will force kill it.
+def _kill_background_telemetry_push_process():
+    global background_telemetry_push_process
+    if (
+        background_telemetry_push_process != None
+        and background_telemetry_push_process.poll() is None
+    ):
+        background_telemetry_push_process.terminate()
+        try:
+            background_telemetry_push_process.wait(
+                timeout=BACKGROUND_THREAD_TIMEOUT_SECONDS
+            )
+        except subprocess.TimeoutExpired:
+            background_telemetry_push_process.kill()
+
+    background_telemetry_push_process = None
+
+
+# This will cause _kill_background_telemetry_push_process to get called when
+# the current interpreter session finishes, which will kill the background
+# process and cause it to do one last push. According to
+# https://docs.python.org/3/library/atexit.html, this is not called when the
+# program is killed by a signal not handled by Python, when a Python fatal
+# internal error is detected, or when os._exit() is called. Since this is
+# defined after we define _thirdai, it should get called before the destructors
+# that take down the metrics server.
+atexit.register(_kill_background_telemetry_push_process)
+
+
+wrapped_start_method = telemetry.start
+wrapped_stop_method = telemetry.stop
+
+
+def start(
+    port: Optional[int] = None,
+    write_dir: Optional[str] = None,
+    write_period_seconds: int = DEFAULT_UPLOAD_INTERVAL_SECONDS,
+    optional_endpoint_url: Optional[str] = None,
+):
+    """
+    Start a Prometheus telemetry client on the passed in port. If a port is not
+    specified this method will use the default ThirdAI port of 9929. This
+    function is not thread safe with other ThirdAI code, so you should make sure
+    that no other code is running when this method is called.
+    If a write_dir is passed in, this function will additionally start a
+    background daemon that will push the Prometheus telemetry to write_dir at
+    the path write_dir/telemetry-<instance_uuid>. Currently, write_dir can be a
+    local path or an s3 path. Writes to this write_dir will happen every
+    write_period_seconds, which has a default of 20 minutes.
+    """
+    global background_telemetry_push_process
+    if background_telemetry_push_process != None:
+        raise RuntimeError(
+            "Trying to start telemetry client when one is already running"
+        )
+
+    if port:
+        telemetry_url = wrapped_start_method(port)
+    else:
+        telemetry_url = wrapped_start_method()
+
+    if write_dir == None:
+        return telemetry_url
+
+    # Run serially once to fix most errors
+    push_telemetry(write_dir, telemetry_url, optional_endpoint_url)
+
+    # Could also try using os.fork
+    python_executable = sys.executable
+    args = [
+        python_executable,
+        str(daemon_script_path.resolve()),
+        "--telemetry_url",
+        telemetry_url,
+        "--push_dir",
+        write_dir,
+        "--upload_interval_seconds",
+        str(write_period_seconds),
+    ]
+    if optional_endpoint_url:
+        args += ["--optional_endpoint_url", optional_endpoint_url]
+    background_telemetry_push_process = subprocess.Popen(args)
+
+    push_location = write_dir + f"/telemetry-" + telemetry.uuid()
+    return push_location
+
+
+def stop():
+    """
+    Stops the current Prometheus telemetry client if one is running. This
+    function is not thread safe with other ThirdAI code, so you should make sure
+    that no other code is running when this method is called.
+    """
+    _kill_background_telemetry_push_process()
+    wrapped_stop_method()
```

## thirdai/telemetry/__init__.py

 * *Ordering differences only*

```diff
@@ -1 +1 @@
-from .telemetry_start_and_stop import start, stop
+from .telemetry_start_and_stop import start, stop
```

## thirdai/_deps/ColBERT/colbertconfig/base_config.py

 * *Ordering differences only*

```diff
@@ -1,71 +1,71 @@
-import os
-import torch
-import json
-import dataclasses
-
-from typing import Any
-from collections import defaultdict
-from dataclasses import dataclass, fields
-from thirdai._deps.ColBERT.colbertutils.utils import timestamp, torch_load_dnn
-
-from .core_config import *
-
-
-@dataclass
-class BaseConfig(CoreConfig):
-    @classmethod
-    def from_existing(cls, *sources):
-        kw_args = {}
-
-        for source in sources:
-            if source is None:
-                continue
-
-            local_kw_args = dataclasses.asdict(source)
-            local_kw_args = {k: local_kw_args[k] for k in source.assigned}
-            kw_args = {**kw_args, **local_kw_args}
-
-        obj = cls(**kw_args)
-
-        return obj
-
-    @classmethod
-    def from_deprecated_args(cls, args):
-        obj = cls()
-        ignored = obj.configure(ignore_unrecognized=True, **args)
-
-        return obj, ignored
-
-    @classmethod
-    def from_path(cls, name):
-        with open(name) as f:
-            args = json.load(f)
-
-            if "config" in args:
-                args = args["config"]
-
-        return cls.from_deprecated_args(
-            args
-        )  # the new, non-deprecated version functions the same at this level.
-
-    @classmethod
-    def load_from_checkpoint(cls, checkpoint_path):
-        if checkpoint_path.endswith(".dnn"):
-            dnn = torch_load_dnn(checkpoint_path)
-            config, _ = cls.from_deprecated_args(dnn.get("arguments", {}))
-
-            # TODO: FIXME: Decide if the line below will have any unintended consequences. We don't want to overwrite those!
-            config.set("checkpoint", checkpoint_path)
-
-            return config
-
-        loaded_config_path = os.path.join(checkpoint_path, "artifact.metadata")
-        if os.path.exists(loaded_config_path):
-            loaded_config, _ = cls.from_path(loaded_config_path)
-            loaded_config.set("checkpoint", checkpoint_path)
-
-            return loaded_config
-
-        return (
-            None  # can happen if checkpoint_path is something like 'bert-base-uncased'
-        )
+import os
+import torch
+import json
+import dataclasses
+
+from typing import Any
+from collections import defaultdict
+from dataclasses import dataclass, fields
+from thirdai._deps.ColBERT.colbertutils.utils import timestamp, torch_load_dnn
+
+from .core_config import *
+
+
+@dataclass
+class BaseConfig(CoreConfig):
+    @classmethod
+    def from_existing(cls, *sources):
+        kw_args = {}
+
+        for source in sources:
+            if source is None:
+                continue
+
+            local_kw_args = dataclasses.asdict(source)
+            local_kw_args = {k: local_kw_args[k] for k in source.assigned}
+            kw_args = {**kw_args, **local_kw_args}
+
+        obj = cls(**kw_args)
+
+        return obj
+
+    @classmethod
+    def from_deprecated_args(cls, args):
+        obj = cls()
+        ignored = obj.configure(ignore_unrecognized=True, **args)
+
+        return obj, ignored
+
+    @classmethod
+    def from_path(cls, name):
+        with open(name) as f:
+            args = json.load(f)
+
+            if "config" in args:
+                args = args["config"]
+
+        return cls.from_deprecated_args(
+            args
+        )  # the new, non-deprecated version functions the same at this level.
+
+    @classmethod
+    def load_from_checkpoint(cls, checkpoint_path):
+        if checkpoint_path.endswith(".dnn"):
+            dnn = torch_load_dnn(checkpoint_path)
+            config, _ = cls.from_deprecated_args(dnn.get("arguments", {}))
+
+            # TODO: FIXME: Decide if the line below will have any unintended consequences. We don't want to overwrite those!
+            config.set("checkpoint", checkpoint_path)
+
+            return config
+
+        loaded_config_path = os.path.join(checkpoint_path, "artifact.metadata")
+        if os.path.exists(loaded_config_path):
+            loaded_config, _ = cls.from_path(loaded_config_path)
+            loaded_config.set("checkpoint", checkpoint_path)
+
+            return loaded_config
+
+        return (
+            None  # can happen if checkpoint_path is something like 'bert-base-uncased'
+        )
```

## thirdai/_deps/ColBERT/colbertconfig/config.py

 * *Ordering differences only*

```diff
@@ -1,18 +1,18 @@
-from dataclasses import dataclass
-
-from .base_config import BaseConfig
-from .settings import *
-
-
-@dataclass
-class ColBERTConfig(
-    RunSettings,
-    ResourceSettings,
-    DocSettings,
-    QuerySettings,
-    TrainingSettings,
-    IndexingSettings,
-    SearchSettings,
-    BaseConfig,
-):
-    pass
+from dataclasses import dataclass
+
+from .base_config import BaseConfig
+from .settings import *
+
+
+@dataclass
+class ColBERTConfig(
+    RunSettings,
+    ResourceSettings,
+    DocSettings,
+    QuerySettings,
+    TrainingSettings,
+    IndexingSettings,
+    SearchSettings,
+    BaseConfig,
+):
+    pass
```

## thirdai/_deps/ColBERT/colbertconfig/core_config.py

 * *Ordering differences only*

```diff
@@ -1,84 +1,84 @@
-import os
-import torch
-import json
-import dataclasses
-
-from typing import Any
-from collections import defaultdict
-from dataclasses import dataclass, fields
-from thirdai._deps.ColBERT.colbertutils.utils import timestamp, torch_load_dnn
-
-
-@dataclass
-class DefaultVal:
-    val: Any
-
-
-@dataclass
-class CoreConfig:
-    def __post_init__(self):
-        """
-        Source: https://stackoverflow.com/a/58081120/1493011
-        """
-
-        self.assigned = {}
-
-        for field in fields(self):
-            field_val = getattr(self, field.name)
-
-            if isinstance(field_val, DefaultVal) or field_val is None:
-                setattr(self, field.name, field.default.val)
-
-            if not isinstance(field_val, DefaultVal):
-                self.assigned[field.name] = True
-
-    def assign_defaults(self):
-        for field in fields(self):
-            setattr(self, field.name, field.default.val)
-            self.assigned[field.name] = True
-
-    def configure(self, ignore_unrecognized=True, **kw_args):
-        ignored = set()
-
-        for key, value in kw_args.items():
-            self.set(key, value, ignore_unrecognized) or ignored.update({key})
-
-        return ignored
-
-        """
-        # TODO: Take a config object, not kw_args.
-
-        for key in config.assigned:
-            value = getattr(config, key)
-        """
-
-    def set(self, key, value, ignore_unrecognized=False):
-        if hasattr(self, key):
-            setattr(self, key, value)
-            self.assigned[key] = True
-            return True
-
-        if not ignore_unrecognized:
-            raise Exception(f"Unrecognized key `{key}` for {type(self)}")
-
-    def help(self):
-        print(json.dumps(dataclasses.asdict(self), indent=4))
-
-    def __export_value(self, v):
-        v = v.provenance() if hasattr(v, "provenance") else v
-
-        if isinstance(v, list) and len(v) > 100:
-            v = (f"list with {len(v)} elements starting with...", v[:3])
-
-        if isinstance(v, dict) and len(v) > 100:
-            v = (f"dict with {len(v)} keys starting with...", list(v.keys())[:3])
-
-        return v
-
-    def export(self):
-        d = dataclasses.asdict(self)
-
-        for k, v in d.items():
-            d[k] = self.__export_value(v)
-
-        return d
+import os
+import torch
+import json
+import dataclasses
+
+from typing import Any
+from collections import defaultdict
+from dataclasses import dataclass, fields
+from thirdai._deps.ColBERT.colbertutils.utils import timestamp, torch_load_dnn
+
+
+@dataclass
+class DefaultVal:
+    val: Any
+
+
+@dataclass
+class CoreConfig:
+    def __post_init__(self):
+        """
+        Source: https://stackoverflow.com/a/58081120/1493011
+        """
+
+        self.assigned = {}
+
+        for field in fields(self):
+            field_val = getattr(self, field.name)
+
+            if isinstance(field_val, DefaultVal) or field_val is None:
+                setattr(self, field.name, field.default.val)
+
+            if not isinstance(field_val, DefaultVal):
+                self.assigned[field.name] = True
+
+    def assign_defaults(self):
+        for field in fields(self):
+            setattr(self, field.name, field.default.val)
+            self.assigned[field.name] = True
+
+    def configure(self, ignore_unrecognized=True, **kw_args):
+        ignored = set()
+
+        for key, value in kw_args.items():
+            self.set(key, value, ignore_unrecognized) or ignored.update({key})
+
+        return ignored
+
+        """
+        # TODO: Take a config object, not kw_args.
+
+        for key in config.assigned:
+            value = getattr(config, key)
+        """
+
+    def set(self, key, value, ignore_unrecognized=False):
+        if hasattr(self, key):
+            setattr(self, key, value)
+            self.assigned[key] = True
+            return True
+
+        if not ignore_unrecognized:
+            raise Exception(f"Unrecognized key `{key}` for {type(self)}")
+
+    def help(self):
+        print(json.dumps(dataclasses.asdict(self), indent=4))
+
+    def __export_value(self, v):
+        v = v.provenance() if hasattr(v, "provenance") else v
+
+        if isinstance(v, list) and len(v) > 100:
+            v = (f"list with {len(v)} elements starting with...", v[:3])
+
+        if isinstance(v, dict) and len(v) > 100:
+            v = (f"dict with {len(v)} keys starting with...", list(v.keys())[:3])
+
+        return v
+
+    def export(self):
+        d = dataclasses.asdict(self)
+
+        for k, v in d.items():
+            d[k] = self.__export_value(v)
+
+        return d
```

## thirdai/_deps/ColBERT/colbertconfig/settings.py

 * *Ordering differences only*

```diff
@@ -1,158 +1,158 @@
-import os
-import torch
-
-import __main__
-from dataclasses import dataclass
-from thirdai._deps.ColBERT.colbertutils.utils import timestamp
-
-from .core_config import DefaultVal
-
-
-@dataclass
-class RunSettings:
-    """
-    The defaults here have a special status in Run(), which initially calls assign_defaults(),
-    so these aren't soft defaults in that specific context.
-    """
-
-    overwrite: bool = DefaultVal(False)
-
-    root: str = DefaultVal(os.path.join(os.getcwd(), "experiments"))
-    experiment: str = DefaultVal("default")
-
-    index_root: str = DefaultVal(None)
-    name: str = DefaultVal(timestamp(daydir=True))
-
-    rank: int = DefaultVal(0)
-    nranks: int = DefaultVal(1)
-    amp: bool = DefaultVal(True)
-
-    total_visible_gpus = torch.cuda.device_count()
-    gpus: int = DefaultVal(total_visible_gpus)
-
-    @property
-    def gpus_(self):
-        value = 0
-
-        if isinstance(value, int):
-            value = list(range(value))
-
-        if isinstance(value, str):
-            value = value.split(",")
-
-        value = list(map(int, value))
-        value = sorted(list(set(value)))
-
-        return value
-
-    @property
-    def index_root_(self):
-        return self.index_root or os.path.join(self.root, self.experiment, "indexes/")
-
-    @property
-    def script_name_(self):
-        if "__file__" in dir(__main__):
-            cwd = os.path.abspath(os.getcwd())
-            script_path = os.path.abspath(__main__.__file__)
-            root_path = os.path.abspath(self.root)
-
-            if script_path.startswith(cwd):
-                script_path = script_path[len(cwd) :]
-
-            else:
-                try:
-                    commonpath = os.path.commonpath([script_path, root_path])
-                    script_path = script_path[len(commonpath) :]
-                except:
-                    pass
-
-            script_name = script_path.replace("/", ".").strip(".")[:-3]
-
-            return script_name
-
-        return "none"
-
-    @property
-    def path_(self):
-        return os.path.join(self.root, self.experiment, self.script_name_, self.name)
-
-    @property
-    def device_(self):
-        return self.gpus_[self.rank % self.nranks]
-
-
-@dataclass
-class ResourceSettings:
-    checkpoint: str = DefaultVal(None)
-    triples: str = DefaultVal(None)
-    collection: str = DefaultVal(None)
-    queries: str = DefaultVal(None)
-    index_name: str = DefaultVal(None)
-
-
-@dataclass
-class DocSettings:
-    dim: int = DefaultVal(128)
-    doc_maxlen: int = DefaultVal(220)
-    mask_punctuation: bool = DefaultVal(True)
-
-
-@dataclass
-class QuerySettings:
-    query_maxlen: int = DefaultVal(32)
-    attend_to_mask_tokens: bool = DefaultVal(False)
-    interaction: str = DefaultVal("colbert")
-
-
-@dataclass
-class TrainingSettings:
-    similarity: str = DefaultVal("cosine")
-
-    bsize: int = DefaultVal(32)
-
-    accumsteps: int = DefaultVal(1)
-
-    lr: float = DefaultVal(3e-06)
-
-    maxsteps: int = DefaultVal(500_000)
-
-    save_every: int = DefaultVal(None)
-
-    resume: bool = DefaultVal(False)
-
-    ## NEW:
-    warmup: int = DefaultVal(None)
-
-    warmup_bert: int = DefaultVal(None)
-
-    relu: bool = DefaultVal(False)
-
-    nway: int = DefaultVal(2)
-
-    use_ib_negatives: bool = DefaultVal(False)
-
-    reranker: bool = DefaultVal(False)
-
-    distillation_alpha: float = DefaultVal(1.0)
-
-    ignore_scores: bool = DefaultVal(False)
-
-
-@dataclass
-class IndexingSettings:
-    index_path: str = DefaultVal(None)
-
-    nbits: int = DefaultVal(1)
-
-    kmeans_niters: int = DefaultVal(20)
-
-    @property
-    def index_path_(self):
-        return self.index_path or os.path.join(self.index_root_, self.index_name)
-
-
-@dataclass
-class SearchSettings:
-    nprobe: int = DefaultVal(2)
-
-    ncandidates: int = DefaultVal(8192)
+import os
+import torch
+
+import __main__
+from dataclasses import dataclass
+from thirdai._deps.ColBERT.colbertutils.utils import timestamp
+
+from .core_config import DefaultVal
+
+
+@dataclass
+class RunSettings:
+    """
+    The defaults here have a special status in Run(), which initially calls assign_defaults(),
+    so these aren't soft defaults in that specific context.
+    """
+
+    overwrite: bool = DefaultVal(False)
+
+    root: str = DefaultVal(os.path.join(os.getcwd(), "experiments"))
+    experiment: str = DefaultVal("default")
+
+    index_root: str = DefaultVal(None)
+    name: str = DefaultVal(timestamp(daydir=True))
+
+    rank: int = DefaultVal(0)
+    nranks: int = DefaultVal(1)
+    amp: bool = DefaultVal(True)
+
+    total_visible_gpus = torch.cuda.device_count()
+    gpus: int = DefaultVal(total_visible_gpus)
+
+    @property
+    def gpus_(self):
+        value = 0
+
+        if isinstance(value, int):
+            value = list(range(value))
+
+        if isinstance(value, str):
+            value = value.split(",")
+
+        value = list(map(int, value))
+        value = sorted(list(set(value)))
+
+        return value
+
+    @property
+    def index_root_(self):
+        return self.index_root or os.path.join(self.root, self.experiment, "indexes/")
+
+    @property
+    def script_name_(self):
+        if "__file__" in dir(__main__):
+            cwd = os.path.abspath(os.getcwd())
+            script_path = os.path.abspath(__main__.__file__)
+            root_path = os.path.abspath(self.root)
+
+            if script_path.startswith(cwd):
+                script_path = script_path[len(cwd) :]
+
+            else:
+                try:
+                    commonpath = os.path.commonpath([script_path, root_path])
+                    script_path = script_path[len(commonpath) :]
+                except:
+                    pass
+
+            script_name = script_path.replace("/", ".").strip(".")[:-3]
+
+            return script_name
+
+        return "none"
+
+    @property
+    def path_(self):
+        return os.path.join(self.root, self.experiment, self.script_name_, self.name)
+
+    @property
+    def device_(self):
+        return self.gpus_[self.rank % self.nranks]
+
+
+@dataclass
+class ResourceSettings:
+    checkpoint: str = DefaultVal(None)
+    triples: str = DefaultVal(None)
+    collection: str = DefaultVal(None)
+    queries: str = DefaultVal(None)
+    index_name: str = DefaultVal(None)
+
+
+@dataclass
+class DocSettings:
+    dim: int = DefaultVal(128)
+    doc_maxlen: int = DefaultVal(220)
+    mask_punctuation: bool = DefaultVal(True)
+
+
+@dataclass
+class QuerySettings:
+    query_maxlen: int = DefaultVal(32)
+    attend_to_mask_tokens: bool = DefaultVal(False)
+    interaction: str = DefaultVal("colbert")
+
+
+@dataclass
+class TrainingSettings:
+    similarity: str = DefaultVal("cosine")
+
+    bsize: int = DefaultVal(32)
+
+    accumsteps: int = DefaultVal(1)
+
+    lr: float = DefaultVal(3e-06)
+
+    maxsteps: int = DefaultVal(500_000)
+
+    save_every: int = DefaultVal(None)
+
+    resume: bool = DefaultVal(False)
+
+    ## NEW:
+    warmup: int = DefaultVal(None)
+
+    warmup_bert: int = DefaultVal(None)
+
+    relu: bool = DefaultVal(False)
+
+    nway: int = DefaultVal(2)
+
+    use_ib_negatives: bool = DefaultVal(False)
+
+    reranker: bool = DefaultVal(False)
+
+    distillation_alpha: float = DefaultVal(1.0)
+
+    ignore_scores: bool = DefaultVal(False)
+
+
+@dataclass
+class IndexingSettings:
+    index_path: str = DefaultVal(None)
+
+    nbits: int = DefaultVal(1)
+
+    kmeans_niters: int = DefaultVal(20)
+
+    @property
+    def index_path_(self):
+        return self.index_path or os.path.join(self.index_root_, self.index_name)
+
+
+@dataclass
+class SearchSettings:
+    nprobe: int = DefaultVal(2)
+
+    ncandidates: int = DefaultVal(8192)
```

## thirdai/_deps/ColBERT/colbertconfig/__init__.py

 * *Ordering differences only*

```diff
@@ -1,2 +1,2 @@
-from .config import *
-from .settings import *
+from .config import *
+from .settings import *
```

## thirdai/_deps/ColBERT/colbertmodeling/base_colbert.py

 * *Ordering differences only*

```diff
@@ -1,45 +1,45 @@
-import os
-import torch
-
-from thirdai._deps.ColBERT.colbertutils.utils import torch_load_dnn
-
-from transformers import AutoTokenizer
-from thirdai._deps.ColBERT.colbertmodeling.hf_colbert import HF_ColBERT
-from thirdai._deps.ColBERT.colbertconfig import ColBERTConfig
-
-
-class BaseColBERT(torch.nn.Module):
-    """
-    Shallow module that wraps the ColBERT parameters, custom configuration, and underlying tokenizer.
-    This class provides direct instantiation and saving of the model/colbert_config/tokenizer package.
-
-    Like HF, evaluation mode is the default.
-    """
-
-    def __init__(self, name):
-        super().__init__()
-
-        self.name = name
-        self.colbert_config = ColBERTConfig.load_from_checkpoint(name)
-        self.model = HF_ColBERT.from_pretrained(
-            name, colbert_config=self.colbert_config
-        )
-        self.raw_tokenizer = AutoTokenizer.from_pretrained(self.model.base)
-
-        self.eval()
-
-    @property
-    def device(self):
-        return self.model.device
-
-    @property
-    def bert(self):
-        return self.model.bert
-
-    @property
-    def linear(self):
-        return self.model.linear
-
-    @property
-    def score_scaler(self):
-        return self.model.score_scaler
+import os
+import torch
+
+from thirdai._deps.ColBERT.colbertutils.utils import torch_load_dnn
+
+from transformers import AutoTokenizer
+from thirdai._deps.ColBERT.colbertmodeling.hf_colbert import HF_ColBERT
+from thirdai._deps.ColBERT.colbertconfig import ColBERTConfig
+
+
+class BaseColBERT(torch.nn.Module):
+    """
+    Shallow module that wraps the ColBERT parameters, custom configuration, and underlying tokenizer.
+    This class provides direct instantiation and saving of the model/colbert_config/tokenizer package.
+
+    Like HF, evaluation mode is the default.
+    """
+
+    def __init__(self, name):
+        super().__init__()
+
+        self.name = name
+        self.colbert_config = ColBERTConfig.load_from_checkpoint(name)
+        self.model = HF_ColBERT.from_pretrained(
+            name, colbert_config=self.colbert_config
+        )
+        self.raw_tokenizer = AutoTokenizer.from_pretrained(self.model.base)
+
+        self.eval()
+
+    @property
+    def device(self):
+        return self.model.device
+
+    @property
+    def bert(self):
+        return self.model.bert
+
+    @property
+    def linear(self):
+        return self.model.linear
+
+    @property
+    def score_scaler(self):
+        return self.model.score_scaler
```

## thirdai/_deps/ColBERT/colbertmodeling/checkpoint.py

 * *Ordering differences only*

```diff
@@ -1,46 +1,46 @@
-import torch
-
-
-from thirdai._deps.ColBERT.colbertmodeling.tokenization import (
-    QueryTokenizer,
-    DocTokenizer,
-)
-from thirdai._deps.ColBERT.colbertmodeling.colbert import ColBERT
-
-
-class Checkpoint(ColBERT):
-    """
-    Easy inference with ColBERT.
-
-    TODO: Add .cast() accepting [also] an object instance-of(Checkpoint) as first argument.
-    """
-
-    def __init__(self, name):
-        super().__init__(name)
-
-        self.query_tokenizer = QueryTokenizer(self.colbert_config)
-        self.doc_tokenizer = DocTokenizer(self.colbert_config)
-
-    def query(self, *args, to_cpu=True, **kw_args):
-        with torch.no_grad():
-            Q = super().query(*args, **kw_args)
-            return Q.cpu() if to_cpu else Q
-
-    def doc(self, *args, to_cpu=True, **kw_args):
-        with torch.no_grad():
-            D = super().doc(*args, **kw_args)
-
-            if to_cpu:
-                return (D[0].cpu(), *D[1:]) if isinstance(D, tuple) else D.cpu()
-
-            return D
-
-    def queryFromText(self, queries, context=None):
-        input_ids, attention_mask = self.query_tokenizer.tensorize(
-            queries, context=context
-        )
-        return self.query(input_ids, attention_mask)
-
-    def docFromText(self, docs):
-        input_ids, attention_mask = self.doc_tokenizer.tensorize(docs)
-        return self.doc(input_ids, attention_mask, keep_dims=True)
+import torch
+
+
+from thirdai._deps.ColBERT.colbertmodeling.tokenization import (
+    QueryTokenizer,
+    DocTokenizer,
+)
+from thirdai._deps.ColBERT.colbertmodeling.colbert import ColBERT
+
+
+class Checkpoint(ColBERT):
+    """
+    Easy inference with ColBERT.
+
+    TODO: Add .cast() accepting [also] an object instance-of(Checkpoint) as first argument.
+    """
+
+    def __init__(self, name):
+        super().__init__(name)
+
+        self.query_tokenizer = QueryTokenizer(self.colbert_config)
+        self.doc_tokenizer = DocTokenizer(self.colbert_config)
+
+    def query(self, *args, to_cpu=True, **kw_args):
+        with torch.no_grad():
+            Q = super().query(*args, **kw_args)
+            return Q.cpu() if to_cpu else Q
+
+    def doc(self, *args, to_cpu=True, **kw_args):
+        with torch.no_grad():
+            D = super().doc(*args, **kw_args)
+
+            if to_cpu:
+                return (D[0].cpu(), *D[1:]) if isinstance(D, tuple) else D.cpu()
+
+            return D
+
+    def queryFromText(self, queries, context=None):
+        input_ids, attention_mask = self.query_tokenizer.tensorize(
+            queries, context=context
+        )
+        return self.query(input_ids, attention_mask)
+
+    def docFromText(self, docs):
+        input_ids, attention_mask = self.doc_tokenizer.tensorize(docs)
+        return self.doc(input_ids, attention_mask, keep_dims=True)
```

## thirdai/_deps/ColBERT/colbertmodeling/colbert.py

 * *Ordering differences only*

```diff
@@ -1,75 +1,75 @@
-from thirdai._deps.ColBERT.colbertconfig import ColBERTConfig
-from thirdai._deps.ColBERT.colbertmodeling.base_colbert import BaseColBERT
-
-import torch
-import string
-
-
-class ColBERT(BaseColBERT):
-    """
-    This class handles the basic encoding and scoring operations in ColBERT. It is used for training.
-    """
-
-    def __init__(self, name="bert-base-uncased"):
-        super().__init__(name)
-
-        if self.colbert_config.mask_punctuation:
-            self.skiplist = {
-                w: True
-                for symbol in string.punctuation
-                for w in [
-                    symbol,
-                    self.raw_tokenizer.encode(symbol, add_special_tokens=False)[0],
-                ]
-            }
-
-    def query(self, input_ids, attention_mask):
-        input_ids, attention_mask = input_ids.to(self.device), attention_mask.to(
-            self.device
-        )
-        Q = self.bert(input_ids, attention_mask=attention_mask)[0]
-        Q = self.linear(Q)
-
-        mask = (
-            torch.tensor(self.mask(input_ids, skiplist=[]), device=self.device)
-            .unsqueeze(2)
-            .float()
-        )
-        Q = Q * mask
-
-        return torch.nn.functional.normalize(Q, p=2, dim=2)
-
-    def doc(self, input_ids, attention_mask, keep_dims=True):
-
-        input_ids, attention_mask = input_ids.to(self.device), attention_mask.to(
-            self.device
-        )
-        D = self.bert(input_ids, attention_mask=attention_mask)[0]
-        D = self.linear(D)
-
-        mask = (
-            torch.tensor(
-                self.mask(input_ids, skiplist=self.skiplist), device=self.device
-            )
-            .unsqueeze(2)
-            .float()
-        )
-        D = D * mask
-
-        D = torch.nn.functional.normalize(D, p=2, dim=2)
-
-        if keep_dims is False:
-            D, mask = D.cpu(), mask.bool().cpu().squeeze(-1)
-            D = [d[mask[idx]] for idx, d in enumerate(D)]
-
-        elif keep_dims == "return_mask":
-            return D, mask.bool()
-
-        return D
-
-    def mask(self, input_ids, skiplist):
-        mask = [
-            [(x not in skiplist) and (x != 0) for x in d]
-            for d in input_ids.cpu().tolist()
-        ]
-        return mask
+from thirdai._deps.ColBERT.colbertconfig import ColBERTConfig
+from thirdai._deps.ColBERT.colbertmodeling.base_colbert import BaseColBERT
+
+import torch
+import string
+
+
+class ColBERT(BaseColBERT):
+    """
+    This class handles the basic encoding and scoring operations in ColBERT. It is used for training.
+    """
+
+    def __init__(self, name="bert-base-uncased"):
+        super().__init__(name)
+
+        if self.colbert_config.mask_punctuation:
+            self.skiplist = {
+                w: True
+                for symbol in string.punctuation
+                for w in [
+                    symbol,
+                    self.raw_tokenizer.encode(symbol, add_special_tokens=False)[0],
+                ]
+            }
+
+    def query(self, input_ids, attention_mask):
+        input_ids, attention_mask = input_ids.to(self.device), attention_mask.to(
+            self.device
+        )
+        Q = self.bert(input_ids, attention_mask=attention_mask)[0]
+        Q = self.linear(Q)
+
+        mask = (
+            torch.tensor(self.mask(input_ids, skiplist=[]), device=self.device)
+            .unsqueeze(2)
+            .float()
+        )
+        Q = Q * mask
+
+        return torch.nn.functional.normalize(Q, p=2, dim=2)
+
+    def doc(self, input_ids, attention_mask, keep_dims=True):
+
+        input_ids, attention_mask = input_ids.to(self.device), attention_mask.to(
+            self.device
+        )
+        D = self.bert(input_ids, attention_mask=attention_mask)[0]
+        D = self.linear(D)
+
+        mask = (
+            torch.tensor(
+                self.mask(input_ids, skiplist=self.skiplist), device=self.device
+            )
+            .unsqueeze(2)
+            .float()
+        )
+        D = D * mask
+
+        D = torch.nn.functional.normalize(D, p=2, dim=2)
+
+        if keep_dims is False:
+            D, mask = D.cpu(), mask.bool().cpu().squeeze(-1)
+            D = [d[mask[idx]] for idx, d in enumerate(D)]
+
+        elif keep_dims == "return_mask":
+            return D, mask.bool()
+
+        return D
+
+    def mask(self, input_ids, skiplist):
+        mask = [
+            [(x not in skiplist) and (x != 0) for x in d]
+            for d in input_ids.cpu().tolist()
+        ]
+        return mask
```

## thirdai/_deps/ColBERT/colbertmodeling/hf_colbert.py

 * *Ordering differences only*

```diff
@@ -1,64 +1,64 @@
-import torch.nn as nn
-
-from transformers import BertPreTrainedModel, BertModel, AutoTokenizer
-from thirdai._deps.ColBERT.colbertutils.utils import torch_load_dnn
-
-
-class HF_ColBERT(BertPreTrainedModel):
-    """
-    Shallow wrapper around HuggingFace transformers. All new parameters should be defined at this level.
-
-    This makes sure `{from,save}_pretrained` and `init_weights` are applied to new parameters correctly.
-    """
-
-    _keys_to_ignore_on_load_unexpected = [r"cls"]
-
-    def __init__(self, config, colbert_config):
-        super().__init__(config)
-
-        self.dim = colbert_config.dim
-        self.bert = BertModel(config)
-        self.linear = nn.Linear(config.hidden_size, colbert_config.dim, bias=False)
-        self.init_weights()
-
-    @classmethod
-    def from_pretrained(cls, name_or_path, colbert_config):
-        if name_or_path.endswith(".dnn"):
-            dnn = torch_load_dnn(name_or_path)
-            base = dnn.get("arguments", {}).get("model", "bert-base-uncased")
-
-            obj = super().from_pretrained(
-                base, state_dict=dnn["model_state_dict"], colbert_config=colbert_config
-            )
-            obj.base = base
-
-            return obj
-
-        obj = super().from_pretrained(name_or_path, colbert_config=colbert_config)
-        obj.base = name_or_path
-
-        return obj
-
-    @staticmethod
-    def raw_tokenizer_from_pretrained(name_or_path):
-        if name_or_path.endswith(".dnn"):
-            dnn = torch_load_dnn(name_or_path)
-            base = dnn.get("arguments", {}).get("model", "bert-base-uncased")
-
-            obj = AutoTokenizer.from_pretrained(base)
-            obj.base = base
-
-            return obj
-
-        obj = AutoTokenizer.from_pretrained(name_or_path)
-        obj.base = name_or_path
-
-        return obj
-
-
-"""
-TODO: It's easy to write a class generator that takes "name_or_path" and loads AutoConfig to check the Architecture's
-      name, finds that name's *PreTrainedModel and *Model in dir(transformers), and then basically repeats the above.
-
-      It's easy for the BaseColBERT class to instantiate things from there.
-"""
+import torch.nn as nn
+
+from transformers import BertPreTrainedModel, BertModel, AutoTokenizer
+from thirdai._deps.ColBERT.colbertutils.utils import torch_load_dnn
+
+
+class HF_ColBERT(BertPreTrainedModel):
+    """
+    Shallow wrapper around HuggingFace transformers. All new parameters should be defined at this level.
+
+    This makes sure `{from,save}_pretrained` and `init_weights` are applied to new parameters correctly.
+    """
+
+    _keys_to_ignore_on_load_unexpected = [r"cls"]
+
+    def __init__(self, config, colbert_config):
+        super().__init__(config)
+
+        self.dim = colbert_config.dim
+        self.bert = BertModel(config)
+        self.linear = nn.Linear(config.hidden_size, colbert_config.dim, bias=False)
+        self.init_weights()
+
+    @classmethod
+    def from_pretrained(cls, name_or_path, colbert_config):
+        if name_or_path.endswith(".dnn"):
+            dnn = torch_load_dnn(name_or_path)
+            base = dnn.get("arguments", {}).get("model", "bert-base-uncased")
+
+            obj = super().from_pretrained(
+                base, state_dict=dnn["model_state_dict"], colbert_config=colbert_config
+            )
+            obj.base = base
+
+            return obj
+
+        obj = super().from_pretrained(name_or_path, colbert_config=colbert_config)
+        obj.base = name_or_path
+
+        return obj
+
+    @staticmethod
+    def raw_tokenizer_from_pretrained(name_or_path):
+        if name_or_path.endswith(".dnn"):
+            dnn = torch_load_dnn(name_or_path)
+            base = dnn.get("arguments", {}).get("model", "bert-base-uncased")
+
+            obj = AutoTokenizer.from_pretrained(base)
+            obj.base = base
+
+            return obj
+
+        obj = AutoTokenizer.from_pretrained(name_or_path)
+        obj.base = name_or_path
+
+        return obj
+
+
+"""
+TODO: It's easy to write a class generator that takes "name_or_path" and loads AutoConfig to check the Architecture's
+      name, finds that name's *PreTrainedModel and *Model in dir(transformers), and then basically repeats the above.
+
+      It's easy for the BaseColBERT class to instantiate things from there.
+"""
```

## thirdai/_deps/ColBERT/colbertmodeling/tokenization/doc_tokenization.py

 * *Ordering differences only*

```diff
@@ -1,67 +1,67 @@
-import torch
-
-# from transformers import BertTokenizerFast
-
-from thirdai._deps.ColBERT.colbertmodeling.hf_colbert import HF_ColBERT
-from thirdai._deps.ColBERT.colbertconfig import ColBERTConfig
-
-
-class DocTokenizer:
-    def __init__(self, config: ColBERTConfig):
-        self.tok = HF_ColBERT.raw_tokenizer_from_pretrained(config.checkpoint)
-
-        self.config = config
-        self.doc_maxlen = config.doc_maxlen
-
-        (
-            self.D_marker_token,
-            self.D_marker_token_id,
-        ) = "[D]", self.tok.convert_tokens_to_ids("[unused1]")
-        self.cls_token, self.cls_token_id = self.tok.cls_token, self.tok.cls_token_id
-        self.sep_token, self.sep_token_id = self.tok.sep_token, self.tok.sep_token_id
-
-    def tokenize(self, batch_text, add_special_tokens=False):
-
-        tokens = [self.tok.tokenize(x, add_special_tokens=False) for x in batch_text]
-
-        if not add_special_tokens:
-            return tokens
-
-        prefix, suffix = [self.cls_token, self.D_marker_token], [self.sep_token]
-        tokens = [prefix + lst + suffix for lst in tokens]
-
-        return tokens
-
-    def encode(self, batch_text, add_special_tokens=False):
-
-        ids = self.tok(batch_text, add_special_tokens=False)["input_ids"]
-
-        if not add_special_tokens:
-            return ids
-
-        prefix, suffix = [self.cls_token_id, self.D_marker_token_id], [
-            self.sep_token_id
-        ]
-        ids = [prefix + lst + suffix for lst in ids]
-
-        return ids
-
-    def tensorize(self, batch_text):
-
-        # add placehold for the [D] marker
-        batch_text = [". " + x for x in batch_text]
-
-        obj = self.tok(
-            batch_text,
-            padding="longest",
-            truncation="longest_first",
-            return_tensors="pt",
-            max_length=self.doc_maxlen,
-        )
-
-        ids, mask = obj["input_ids"], obj["attention_mask"]
-
-        # postprocess for the [D] marker
-        ids[:, 1] = self.D_marker_token_id
-
-        return ids, mask
+import torch
+
+# from transformers import BertTokenizerFast
+
+from thirdai._deps.ColBERT.colbertmodeling.hf_colbert import HF_ColBERT
+from thirdai._deps.ColBERT.colbertconfig import ColBERTConfig
+
+
+class DocTokenizer:
+    def __init__(self, config: ColBERTConfig):
+        self.tok = HF_ColBERT.raw_tokenizer_from_pretrained(config.checkpoint)
+
+        self.config = config
+        self.doc_maxlen = config.doc_maxlen
+
+        (
+            self.D_marker_token,
+            self.D_marker_token_id,
+        ) = "[D]", self.tok.convert_tokens_to_ids("[unused1]")
+        self.cls_token, self.cls_token_id = self.tok.cls_token, self.tok.cls_token_id
+        self.sep_token, self.sep_token_id = self.tok.sep_token, self.tok.sep_token_id
+
+    def tokenize(self, batch_text, add_special_tokens=False):
+
+        tokens = [self.tok.tokenize(x, add_special_tokens=False) for x in batch_text]
+
+        if not add_special_tokens:
+            return tokens
+
+        prefix, suffix = [self.cls_token, self.D_marker_token], [self.sep_token]
+        tokens = [prefix + lst + suffix for lst in tokens]
+
+        return tokens
+
+    def encode(self, batch_text, add_special_tokens=False):
+
+        ids = self.tok(batch_text, add_special_tokens=False)["input_ids"]
+
+        if not add_special_tokens:
+            return ids
+
+        prefix, suffix = [self.cls_token_id, self.D_marker_token_id], [
+            self.sep_token_id
+        ]
+        ids = [prefix + lst + suffix for lst in ids]
+
+        return ids
+
+    def tensorize(self, batch_text):
+
+        # add placehold for the [D] marker
+        batch_text = [". " + x for x in batch_text]
+
+        obj = self.tok(
+            batch_text,
+            padding="longest",
+            truncation="longest_first",
+            return_tensors="pt",
+            max_length=self.doc_maxlen,
+        )
+
+        ids, mask = obj["input_ids"], obj["attention_mask"]
+
+        # postprocess for the [D] marker
+        ids[:, 1] = self.D_marker_token_id
+
+        return ids, mask
```

## thirdai/_deps/ColBERT/colbertmodeling/tokenization/query_tokenization.py

 * *Ordering differences only*

```diff
@@ -1,108 +1,108 @@
-import torch
-
-from thirdai._deps.ColBERT.colbertmodeling.hf_colbert import HF_ColBERT
-from thirdai._deps.ColBERT.colbertconfig import ColBERTConfig
-
-
-class QueryTokenizer:
-    def __init__(self, config: ColBERTConfig):
-        self.tok = HF_ColBERT.raw_tokenizer_from_pretrained(config.checkpoint)
-
-        self.config = config
-        self.query_maxlen = config.query_maxlen
-        self.background_maxlen = (
-            512 - self.query_maxlen + 1
-        )  # FIXME: Make this configurable
-
-        (
-            self.Q_marker_token,
-            self.Q_marker_token_id,
-        ) = "[Q]", self.tok.convert_tokens_to_ids("[unused0]")
-        self.cls_token, self.cls_token_id = self.tok.cls_token, self.tok.cls_token_id
-        self.sep_token, self.sep_token_id = self.tok.sep_token, self.tok.sep_token_id
-        self.mask_token, self.mask_token_id = (
-            self.tok.mask_token,
-            self.tok.mask_token_id,
-        )
-
-        self.used = False
-
-    def tokenize(self, batch_text, add_special_tokens=False):
-
-        tokens = [self.tok.tokenize(x, add_special_tokens=False) for x in batch_text]
-
-        if not add_special_tokens:
-            return tokens
-
-        prefix, suffix = [self.cls_token, self.Q_marker_token], [self.sep_token]
-        tokens = [
-            prefix
-            + lst
-            + suffix
-            + [self.mask_token] * (self.query_maxlen - (len(lst) + 3))
-            for lst in tokens
-        ]
-
-        return tokens
-
-    def encode(self, batch_text, add_special_tokens=False):
-
-        ids = self.tok(batch_text, add_special_tokens=False)["input_ids"]
-
-        if not add_special_tokens:
-            return ids
-
-        prefix, suffix = [self.cls_token_id, self.Q_marker_token_id], [
-            self.sep_token_id
-        ]
-        ids = [
-            prefix
-            + lst
-            + suffix
-            + [self.mask_token_id] * (self.query_maxlen - (len(lst) + 3))
-            for lst in ids
-        ]
-
-        return ids
-
-    def tensorize(self, batch_text, context=None):
-
-        # add placehold for the [Q] marker
-        batch_text = [". " + x for x in batch_text]
-
-        obj = self.tok(
-            batch_text,
-            padding="max_length",
-            truncation=True,
-            return_tensors="pt",
-            max_length=self.query_maxlen,
-        )
-
-        ids, mask = obj["input_ids"], obj["attention_mask"]
-
-        # postprocess for the [Q] marker and the [MASK] augmentation
-        ids[:, 1] = self.Q_marker_token_id
-        ids[ids == 0] = self.mask_token_id
-
-        if context is not None:
-
-            obj_2 = self.tok(
-                context,
-                padding="longest",
-                truncation=True,
-                return_tensors="pt",
-                max_length=self.background_maxlen,
-            )
-
-            ids_2, mask_2 = (
-                obj_2["input_ids"][:, 1:],
-                obj_2["attention_mask"][:, 1:],
-            )  # Skip the first [SEP]
-
-            ids = torch.cat((ids, ids_2), dim=-1)
-            mask = torch.cat((mask, mask_2), dim=-1)
-
-        if self.config.attend_to_mask_tokens:
-            mask[ids == self.mask_token_id] = 1
-
-        return ids, mask
+import torch
+
+from thirdai._deps.ColBERT.colbertmodeling.hf_colbert import HF_ColBERT
+from thirdai._deps.ColBERT.colbertconfig import ColBERTConfig
+
+
+class QueryTokenizer:
+    def __init__(self, config: ColBERTConfig):
+        self.tok = HF_ColBERT.raw_tokenizer_from_pretrained(config.checkpoint)
+
+        self.config = config
+        self.query_maxlen = config.query_maxlen
+        self.background_maxlen = (
+            512 - self.query_maxlen + 1
+        )  # FIXME: Make this configurable
+
+        (
+            self.Q_marker_token,
+            self.Q_marker_token_id,
+        ) = "[Q]", self.tok.convert_tokens_to_ids("[unused0]")
+        self.cls_token, self.cls_token_id = self.tok.cls_token, self.tok.cls_token_id
+        self.sep_token, self.sep_token_id = self.tok.sep_token, self.tok.sep_token_id
+        self.mask_token, self.mask_token_id = (
+            self.tok.mask_token,
+            self.tok.mask_token_id,
+        )
+
+        self.used = False
+
+    def tokenize(self, batch_text, add_special_tokens=False):
+
+        tokens = [self.tok.tokenize(x, add_special_tokens=False) for x in batch_text]
+
+        if not add_special_tokens:
+            return tokens
+
+        prefix, suffix = [self.cls_token, self.Q_marker_token], [self.sep_token]
+        tokens = [
+            prefix
+            + lst
+            + suffix
+            + [self.mask_token] * (self.query_maxlen - (len(lst) + 3))
+            for lst in tokens
+        ]
+
+        return tokens
+
+    def encode(self, batch_text, add_special_tokens=False):
+
+        ids = self.tok(batch_text, add_special_tokens=False)["input_ids"]
+
+        if not add_special_tokens:
+            return ids
+
+        prefix, suffix = [self.cls_token_id, self.Q_marker_token_id], [
+            self.sep_token_id
+        ]
+        ids = [
+            prefix
+            + lst
+            + suffix
+            + [self.mask_token_id] * (self.query_maxlen - (len(lst) + 3))
+            for lst in ids
+        ]
+
+        return ids
+
+    def tensorize(self, batch_text, context=None):
+
+        # add placehold for the [Q] marker
+        batch_text = [". " + x for x in batch_text]
+
+        obj = self.tok(
+            batch_text,
+            padding="max_length",
+            truncation=True,
+            return_tensors="pt",
+            max_length=self.query_maxlen,
+        )
+
+        ids, mask = obj["input_ids"], obj["attention_mask"]
+
+        # postprocess for the [Q] marker and the [MASK] augmentation
+        ids[:, 1] = self.Q_marker_token_id
+        ids[ids == 0] = self.mask_token_id
+
+        if context is not None:
+
+            obj_2 = self.tok(
+                context,
+                padding="longest",
+                truncation=True,
+                return_tensors="pt",
+                max_length=self.background_maxlen,
+            )
+
+            ids_2, mask_2 = (
+                obj_2["input_ids"][:, 1:],
+                obj_2["attention_mask"][:, 1:],
+            )  # Skip the first [SEP]
+
+            ids = torch.cat((ids, ids_2), dim=-1)
+            mask = torch.cat((mask, mask_2), dim=-1)
+
+        if self.config.attend_to_mask_tokens:
+            mask[ids == self.mask_token_id] = 1
+
+        return ids, mask
```

## thirdai/_deps/ColBERT/colbertmodeling/tokenization/__init__.py

 * *Ordering differences only*

```diff
@@ -1,2 +1,2 @@
-from thirdai._deps.ColBERT.colbertmodeling.tokenization.query_tokenization import *
-from thirdai._deps.ColBERT.colbertmodeling.tokenization.doc_tokenization import *
+from thirdai._deps.ColBERT.colbertmodeling.tokenization.query_tokenization import *
+from thirdai._deps.ColBERT.colbertmodeling.tokenization.doc_tokenization import *
```

## thirdai/_deps/ColBERT/colbertutils/utils.py

 * *Ordering differences only*

```diff
@@ -1,13 +1,13 @@
-import os
-import torch
-import datetime
-
-
-def timestamp(daydir=False):
-    format_str = f"%Y-%m{'/' if daydir else '-'}%d{'/' if daydir else '_'}%H.%M.%S"
-    result = datetime.datetime.now().strftime(format_str)
-    return result
-
-
-def torch_load_dnn(path):
-    return torch.load(path, map_location="cpu")
+import os
+import torch
+import datetime
+
+
+def timestamp(daydir=False):
+    format_str = f"%Y-%m{'/' if daydir else '-'}%d{'/' if daydir else '_'}%H.%M.%S"
+    result = datetime.datetime.now().strftime(format_str)
+    return result
+
+
+def torch_load_dnn(path):
+    return torch.load(path, map_location="cpu")
```

## thirdai/_distributed_bolt/distributed.py

 * *Ordering differences only*

```diff
@@ -1,124 +1,124 @@
-import os
-
-import numpy as np
-import thirdai
-from thirdai._thirdai import bolt
-
-from .utils import check_torch_installed, timed
-
-
-class Communication(bolt.train.Communication):
-    def __init__(self):
-        # For trampoline classes, we need to explicitly call
-        # __init__ of the object rather than just using super()
-        bolt.train.Communication.__init__(self)
-        check_torch_installed()
-
-    @timed
-    def synchronize_workers(self):
-        import torch.distributed as dist
-
-        dist.barrier()
-
-    @timed
-    def communicate(self, model):
-        import torch
-        import torch.distributed as dist
-        from ray import train
-
-        self.synchronize_workers()
-
-        num_workers = train.get_context().get_world_size()
-        gradients = torch.from_numpy(np.array(model.get_gradients()))
-
-        dist.all_reduce(gradients)
-
-        gradients = gradients.numpy() / num_workers
-        model.set_gradients(gradients)
-
-    @timed
-    def min_num_batches(self, num_batches):
-        import torch
-        import torch.distributed as dist
-
-        dist.barrier()
-        all_reduce_num_batches = torch.tensor(num_batches)
-        dist.all_reduce(all_reduce_num_batches, op=dist.ReduceOp.MIN)
-        return all_reduce_num_batches
-
-    @timed
-    def broadcast_metrics(self, train_metrics):
-        import torch.distributed as dist
-
-        dist.barrier()
-        dist.broadcast_object_list(train_metrics, src=0)
-
-        return train_metrics
-
-
-# Note: We need to disable sparse updates neural network updates as after allreduce
-# during sparse training, we only update the parameters selected by hash tables, rather we
-# need to update all the parameters, since during all-reduce some other neuron could be non-zero
-# too.
-def adds_distributed_to_bolt():
-    def train_distributed(self, *args, **kwargs):
-        self.model.disable_sparse_parameter_updates()
-
-        kwargs["comm"] = Communication()
-        metrics = self.train(*args, **kwargs)
-
-        self.model.enable_sparse_parameter_updates()
-
-        return metrics
-
-    bolt.train.Trainer.train_distributed = train_distributed
-
-    def udt_train_distributed(self, *args, **kwargs):
-        self._get_model().disable_sparse_parameter_updates()
-
-        kwargs["comm"] = Communication()
-        metrics = self.train(*args, **kwargs)
-
-        self._get_model().enable_sparse_parameter_updates()
-
-        return metrics
-
-    bolt.UniversalDeepTransformer.train_distributed = udt_train_distributed
-
-    def udt_coldstart_distributed(self, *args, **kwargs):
-        self._get_model().disable_sparse_parameter_updates()
-
-        kwargs["comm"] = Communication()
-        metrics = self.cold_start(*args, **kwargs)
-
-        self._get_model().enable_sparse_parameter_updates()
-
-        return metrics
-
-    def udt_coldstart_distributed_on_data_source(self, *args, **kwargs):
-        self._get_model().disable_sparse_parameter_updates()
-
-        kwargs["comm"] = Communication()
-        metrics = self.cold_start_on_data_source(*args, **kwargs)
-
-        self._get_model().enable_sparse_parameter_updates()
-
-        return metrics
-
-    bolt.UniversalDeepTransformer.coldstart_distributed = udt_coldstart_distributed
-
-    bolt.UniversalDeepTransformer.coldstart_distributed_on_data_source = (
-        udt_coldstart_distributed_on_data_source
-    )
-
-    def generative_model_train_distributed(self, *args, **kwargs):
-        self.model.disable_sparse_parameter_updates()
-
-        kwargs["comm"] = Communication()
-        metrics = self.train(*args, **kwargs)
-
-        self.model.enable_sparse_parameter_updates()
-
-        return metrics
-
-    bolt.GenerativeModel.train_distributed = generative_model_train_distributed
+import os
+
+import numpy as np
+import thirdai
+from thirdai._thirdai import bolt
+
+from .utils import check_torch_installed, timed
+
+
+class Communication(bolt.train.Communication):
+    def __init__(self):
+        # For trampoline classes, we need to explicitly call
+        # __init__ of the object rather than just using super()
+        bolt.train.Communication.__init__(self)
+        check_torch_installed()
+
+    @timed
+    def synchronize_workers(self):
+        import torch.distributed as dist
+
+        dist.barrier()
+
+    @timed
+    def communicate(self, model):
+        import torch
+        import torch.distributed as dist
+        from ray import train
+
+        self.synchronize_workers()
+
+        num_workers = train.get_context().get_world_size()
+        gradients = torch.from_numpy(np.array(model.get_gradients()))
+
+        dist.all_reduce(gradients)
+
+        gradients = gradients.numpy() / num_workers
+        model.set_gradients(gradients)
+
+    @timed
+    def min_num_batches(self, num_batches):
+        import torch
+        import torch.distributed as dist
+
+        dist.barrier()
+        all_reduce_num_batches = torch.tensor(num_batches)
+        dist.all_reduce(all_reduce_num_batches, op=dist.ReduceOp.MIN)
+        return all_reduce_num_batches
+
+    @timed
+    def broadcast_metrics(self, train_metrics):
+        import torch.distributed as dist
+
+        dist.barrier()
+        dist.broadcast_object_list(train_metrics, src=0)
+
+        return train_metrics
+
+
+# Note: We need to disable sparse updates neural network updates as after allreduce
+# during sparse training, we only update the parameters selected by hash tables, rather we
+# need to update all the parameters, since during all-reduce some other neuron could be non-zero
+# too.
+def adds_distributed_to_bolt():
+    def train_distributed(self, *args, **kwargs):
+        self.model.disable_sparse_parameter_updates()
+
+        kwargs["comm"] = Communication()
+        metrics = self.train(*args, **kwargs)
+
+        self.model.enable_sparse_parameter_updates()
+
+        return metrics
+
+    bolt.train.Trainer.train_distributed = train_distributed
+
+    def udt_train_distributed(self, *args, **kwargs):
+        self._get_model().disable_sparse_parameter_updates()
+
+        kwargs["comm"] = Communication()
+        metrics = self.train(*args, **kwargs)
+
+        self._get_model().enable_sparse_parameter_updates()
+
+        return metrics
+
+    bolt.UniversalDeepTransformer.train_distributed = udt_train_distributed
+
+    def udt_coldstart_distributed(self, *args, **kwargs):
+        self._get_model().disable_sparse_parameter_updates()
+
+        kwargs["comm"] = Communication()
+        metrics = self.cold_start(*args, **kwargs)
+
+        self._get_model().enable_sparse_parameter_updates()
+
+        return metrics
+
+    def udt_coldstart_distributed_on_data_source(self, *args, **kwargs):
+        self._get_model().disable_sparse_parameter_updates()
+
+        kwargs["comm"] = Communication()
+        metrics = self.cold_start_on_data_source(*args, **kwargs)
+
+        self._get_model().enable_sparse_parameter_updates()
+
+        return metrics
+
+    bolt.UniversalDeepTransformer.coldstart_distributed = udt_coldstart_distributed
+
+    bolt.UniversalDeepTransformer.coldstart_distributed_on_data_source = (
+        udt_coldstart_distributed_on_data_source
+    )
+
+    def generative_model_train_distributed(self, *args, **kwargs):
+        self.model.disable_sparse_parameter_updates()
+
+        kwargs["comm"] = Communication()
+        metrics = self.train(*args, **kwargs)
+
+        self.model.enable_sparse_parameter_updates()
+
+        return metrics
+
+    bolt.GenerativeModel.train_distributed = generative_model_train_distributed
```

## thirdai/_distributed_bolt/utils.py

 * *Ordering differences only*

```diff
@@ -1,36 +1,36 @@
-import importlib
-from functools import wraps
-from time import time
-
-from thirdai import data, logging
-
-
-def get_num_cpus():
-    try:
-        import multiprocessing
-
-        return multiprocessing.cpu_count()
-    except ImportError:
-        print("Could not find num_cpus, setting num_cpus to DEFAULT=1")
-        return 1
-
-
-def check_torch_installed():
-    try:
-        importlib.import_module("torch")
-    except ImportError as e:
-        raise ImportError(
-            "Distributed Bolt requires Torch Distributed as its communication backend. Please ensure that Torch is installed to enable distributed training with Bolt.t"
-        ) from e
-
-
-def timed(f):
-    @wraps(f)
-    def wrapper(*args, **kwds):
-        start = time()
-        result = f(*args, **kwds)
-        elapsed = time() - start
-        logging.info("func %s | time %d ms" % (f.__name__, elapsed * 1000))
-        return result
-
-    return wrapper
+import importlib
+from functools import wraps
+from time import time
+
+from thirdai import data, logging
+
+
+def get_num_cpus():
+    try:
+        import multiprocessing
+
+        return multiprocessing.cpu_count()
+    except ImportError:
+        print("Could not find num_cpus, setting num_cpus to DEFAULT=1")
+        return 1
+
+
+def check_torch_installed():
+    try:
+        importlib.import_module("torch")
+    except ImportError as e:
+        raise ImportError(
+            "Distributed Bolt requires Torch Distributed as its communication backend. Please ensure that Torch is installed to enable distributed training with Bolt.t"
+        ) from e
+
+
+def timed(f):
+    @wraps(f)
+    def wrapper(*args, **kwds):
+        start = time()
+        result = f(*args, **kwds)
+        elapsed = time() - start
+        logging.info("func %s | time %d ms" % (f.__name__, elapsed * 1000))
+        return result
+
+    return wrapper
```

## thirdai/_distributed_bolt/__init__.py

 * *Ordering differences only*

```diff
@@ -1,7 +1,7 @@
-from .distributed import Communication, adds_distributed_to_bolt
-from .ray_trainer.bolt_checkpoint import BoltCheckPoint, UDTCheckPoint
-from .ray_trainer.bolt_trainer import BoltTrainer
-from .ray_trainer.train_loop_utils import prepare_model
-from .utils import get_num_cpus
-
-adds_distributed_to_bolt()
+from .distributed import Communication, adds_distributed_to_bolt
+from .ray_trainer.bolt_checkpoint import BoltCheckPoint, UDTCheckPoint
+from .ray_trainer.bolt_trainer import BoltTrainer
+from .ray_trainer.train_loop_utils import prepare_model
+from .utils import get_num_cpus
+
+adds_distributed_to_bolt()
```

## thirdai/_distributed_bolt/ray_trainer/bolt_checkpoint.py

 * *Ordering differences only*

```diff
@@ -1,102 +1,102 @@
-import os
-import tempfile
-
-from ray import train
-from ray.air.constants import MODEL_KEY
-from ray.train import Checkpoint
-from thirdai._thirdai import bolt
-
-from ..utils import timed
-
-
-class UDTCheckPoint(Checkpoint):
-    """A :py:class:`~ray.train.Checkpoint` with UDT-specific
-    functionality.
-
-    Use ``UDTCheckPoint.from_model`` to create this type of checkpoint.
-    """
-
-    @classmethod
-    @timed
-    def from_model(
-        cls,
-        model,
-        with_optimizers=True,
-    ):
-        """Create a :py:class:`~ray.train.Checkpoint` that stores a Bolt
-        model with/without optimizer states.
-
-        Args:
-            model: The UDT model to store in the checkpoint.
-
-        Returns:
-            An :py:class:`UDTCheckPoint` containing the specified ``UDT-Model``.
-
-        Examples:
-            >>> checkpoint = UDTCheckPoint.from_model(udt_model, with_optimizers=True): saving with optimizer states
-            >>> checkpoint = UDTCheckPoint.from_model(udt_model, with_optimizers=False): saving without optimizer states
-
-            >>> model = checkpoint.get_model()
-        """
-
-        temp_dir = tempfile.mkdtemp()
-        save_path = os.path.join(temp_dir, MODEL_KEY)
-        model.checkpoint(save_path) if with_optimizers else model.save(save_path)
-
-        checkpoint = cls.from_directory(temp_dir)
-
-        return checkpoint
-
-    @classmethod
-    @timed
-    def get_model(cls, checkpoint: Checkpoint):
-        """Retrieve the UDT model stored in this checkpoint."""
-        with checkpoint.as_directory() as checkpoint_path:
-            return bolt.UniversalDeepTransformer.load(
-                os.path.join(checkpoint_path, MODEL_KEY)
-            )
-
-
-class BoltCheckPoint(Checkpoint):
-    """A :py:class:`~ray.train.Checkpoint` with Bolt-specific
-    functionality.
-
-    Use ``BoltCheckpoint.from_model`` to create this type of checkpoint.
-    """
-
-    @classmethod
-    @timed
-    def from_model(
-        cls,
-        model,
-        with_optimizers=True,
-    ):
-        """Create a :py:class:`~ray.train.Checkpoint` that stores a Bolt
-        model with/without optimizer states.
-
-        Args:
-            model: The Bolt model to store in the checkpoint.
-
-        Returns:
-            An :py:class:`BoltCheckPoint` containing the specified ``Bolt-Model``.
-
-        Examples:
-            >>> checkpoint = BoltCheckPoint.from_model(bolt_model, with_optimizers=True): saving with optimizer states
-            >>> checkpoint = BoltCheckPoint.from_model(bolt_model, with_optimizers=False): saving without optimizer states
-
-            >>> model = checkpoint.get_model()
-        """
-        temp_dir = tempfile.mkdtemp()
-        save_path = os.path.join(temp_dir, MODEL_KEY)
-        model.checkpoint(save_path) if with_optimizers else model.save(save_path)
-
-        checkpoint = cls.from_directory(temp_dir)
-
-        return checkpoint
-
-    @classmethod
-    @timed
-    def get_model(self, checkpoint: Checkpoint):
-        """Retrieve the Bolt model stored in this checkpoint."""
-        with checkpoint.as_directory() as checkpoint_path:
-            return bolt.nn.Model.load(os.path.join(checkpoint_path, MODEL_KEY))
+import os
+import tempfile
+
+from ray import train
+from ray.air.constants import MODEL_KEY
+from ray.train import Checkpoint
+from thirdai._thirdai import bolt
+
+from ..utils import timed
+
+
+class UDTCheckPoint(Checkpoint):
+    """A :py:class:`~ray.train.Checkpoint` with UDT-specific
+    functionality.
+
+    Use ``UDTCheckPoint.from_model`` to create this type of checkpoint.
+    """
+
+    @classmethod
+    @timed
+    def from_model(
+        cls,
+        model,
+        with_optimizers=True,
+    ):
+        """Create a :py:class:`~ray.train.Checkpoint` that stores a Bolt
+        model with/without optimizer states.
+
+        Args:
+            model: The UDT model to store in the checkpoint.
+
+        Returns:
+            An :py:class:`UDTCheckPoint` containing the specified ``UDT-Model``.
+
+        Examples:
+            >>> checkpoint = UDTCheckPoint.from_model(udt_model, with_optimizers=True): saving with optimizer states
+            >>> checkpoint = UDTCheckPoint.from_model(udt_model, with_optimizers=False): saving without optimizer states
+
+            >>> model = checkpoint.get_model()
+        """
+
+        temp_dir = tempfile.mkdtemp()
+        save_path = os.path.join(temp_dir, MODEL_KEY)
+        model.checkpoint(save_path) if with_optimizers else model.save(save_path)
+
+        checkpoint = cls.from_directory(temp_dir)
+
+        return checkpoint
+
+    @classmethod
+    @timed
+    def get_model(cls, checkpoint: Checkpoint):
+        """Retrieve the UDT model stored in this checkpoint."""
+        with checkpoint.as_directory() as checkpoint_path:
+            return bolt.UniversalDeepTransformer.load(
+                os.path.join(checkpoint_path, MODEL_KEY)
+            )
+
+
+class BoltCheckPoint(Checkpoint):
+    """A :py:class:`~ray.train.Checkpoint` with Bolt-specific
+    functionality.
+
+    Use ``BoltCheckpoint.from_model`` to create this type of checkpoint.
+    """
+
+    @classmethod
+    @timed
+    def from_model(
+        cls,
+        model,
+        with_optimizers=True,
+    ):
+        """Create a :py:class:`~ray.train.Checkpoint` that stores a Bolt
+        model with/without optimizer states.
+
+        Args:
+            model: The Bolt model to store in the checkpoint.
+
+        Returns:
+            An :py:class:`BoltCheckPoint` containing the specified ``Bolt-Model``.
+
+        Examples:
+            >>> checkpoint = BoltCheckPoint.from_model(bolt_model, with_optimizers=True): saving with optimizer states
+            >>> checkpoint = BoltCheckPoint.from_model(bolt_model, with_optimizers=False): saving without optimizer states
+
+            >>> model = checkpoint.get_model()
+        """
+        temp_dir = tempfile.mkdtemp()
+        save_path = os.path.join(temp_dir, MODEL_KEY)
+        model.checkpoint(save_path) if with_optimizers else model.save(save_path)
+
+        checkpoint = cls.from_directory(temp_dir)
+
+        return checkpoint
+
+    @classmethod
+    @timed
+    def get_model(self, checkpoint: Checkpoint):
+        """Retrieve the Bolt model stored in this checkpoint."""
+        with checkpoint.as_directory() as checkpoint_path:
+            return bolt.nn.Model.load(os.path.join(checkpoint_path, MODEL_KEY))
```

## thirdai/_distributed_bolt/ray_trainer/bolt_trainer.py

 * *Ordering differences only*

```diff
@@ -1,83 +1,83 @@
-import os
-from typing import TYPE_CHECKING, Callable, Dict, Optional, Union
-
-from ray.train import Checkpoint, DataConfig, RunConfig, ScalingConfig
-from ray.train.data_parallel_trainer import DataParallelTrainer
-from ray.train.trainer import GenDataset
-
-if TYPE_CHECKING:
-    from ray.data.preprocessor import Preprocessor
-
-
-class BoltTrainer(DataParallelTrainer):
-    """A trainer for data parallel Bolt Model Training
-
-    Ex:
-        def train_loop_per_worker(config):
-            mnist_model = config.get('model')
-            trainer = bolt.train.DistributedTrainer(mnist_model)
-
-
-            train_y, train_y, test_x, test_y = data
-
-            epochs = 1
-            for _ in range(epochs):
-                for x, y in zip(train_x, train_y):
-                    trainer.train_on_batch(x, y, 0.001)
-
-            history = new_trainer.validate(
-                validation_data=(test_x, test_y),
-                validation_metrics=["loss", "categorical_accuracy"],
-                use_sparsity=False,
-            )
-
-            train.report(
-                history,
-                checkpoint=dist.BoltCheckPoint.from_model(trainer.model),
-            )
-
-    Args:
-
-        train_loop_per_worker: The training function to execute.
-            This can either take in no arguments or a ``config`` dict.
-        train_loop_config: Configurations to pass into
-            ``train_loop_per_worker`` if it accepts an argument.
-        backend_config: Configuration for setting up the Bolt backend. If set to
-            None, use the default configuration. This replaces the ``backend_config``
-            arg of ``DataParallelTrainer``.
-        scaling_config: Configuration for how to scale data parallel training.
-        dataset_config: Configuration for dataset ingest.
-        run_config: Configuration for the execution of the training run.
-        datasets: Any Datastreams to use for training. Use
-            the key "train" to denote which dataset is the training
-            dataset. If a ``preprocessor`` is provided and has not already been fit,
-            it will be fit on the training dataset. All datasets will be transformed
-            by the ``preprocessor`` if one is provided.
-        preprocessor: A ``ray.data.Preprocessor`` to preprocess the
-            provided datasets.
-        resume_from_checkpoint: A checkpoint to resume training from. It can be acessed in the
-            training-loop function with `train.get_checkpoint`.
-    """
-
-    def __init__(
-        self,
-        train_loop_per_worker: Union[Callable[[], None], Callable[[Dict], None]],
-        *,
-        backend_config=None,
-        train_loop_config: Optional[Dict] = None,
-        scaling_config: Optional[ScalingConfig] = None,
-        dataset_config: Optional[DataConfig] = None,
-        run_config: Optional[RunConfig] = None,
-        datasets: Optional[Dict[str, GenDataset]] = None,
-        resume_from_checkpoint: Optional[Checkpoint] = None,
-    ):
-        super(BoltTrainer, self).__init__(
-            train_loop_per_worker=train_loop_per_worker,
-            train_loop_config=train_loop_config,
-            backend_config=backend_config,
-            scaling_config=scaling_config,
-            dataset_config=dataset_config,
-            run_config=run_config,
-            datasets=datasets,
-            resume_from_checkpoint=resume_from_checkpoint,
-        )
+import os
+from typing import TYPE_CHECKING, Callable, Dict, Optional, Union
+
+from ray.train import Checkpoint, DataConfig, RunConfig, ScalingConfig
+from ray.train.data_parallel_trainer import DataParallelTrainer
+from ray.train.trainer import GenDataset
+
+if TYPE_CHECKING:
+    from ray.data.preprocessor import Preprocessor
+
+
+class BoltTrainer(DataParallelTrainer):
+    """A trainer for data parallel Bolt Model Training
+
+    Ex:
+        def train_loop_per_worker(config):
+            mnist_model = config.get('model')
+            trainer = bolt.train.DistributedTrainer(mnist_model)
+
+
+            train_y, train_y, test_x, test_y = data
+
+            epochs = 1
+            for _ in range(epochs):
+                for x, y in zip(train_x, train_y):
+                    trainer.train_on_batch(x, y, 0.001)
+
+            history = new_trainer.validate(
+                validation_data=(test_x, test_y),
+                validation_metrics=["loss", "categorical_accuracy"],
+                use_sparsity=False,
+            )
+
+            train.report(
+                history,
+                checkpoint=dist.BoltCheckPoint.from_model(trainer.model),
+            )
+
+    Args:
+
+        train_loop_per_worker: The training function to execute.
+            This can either take in no arguments or a ``config`` dict.
+        train_loop_config: Configurations to pass into
+            ``train_loop_per_worker`` if it accepts an argument.
+        backend_config: Configuration for setting up the Bolt backend. If set to
+            None, use the default configuration. This replaces the ``backend_config``
+            arg of ``DataParallelTrainer``.
+        scaling_config: Configuration for how to scale data parallel training.
+        dataset_config: Configuration for dataset ingest.
+        run_config: Configuration for the execution of the training run.
+        datasets: Any Datastreams to use for training. Use
+            the key "train" to denote which dataset is the training
+            dataset. If a ``preprocessor`` is provided and has not already been fit,
+            it will be fit on the training dataset. All datasets will be transformed
+            by the ``preprocessor`` if one is provided.
+        preprocessor: A ``ray.data.Preprocessor`` to preprocess the
+            provided datasets.
+        resume_from_checkpoint: A checkpoint to resume training from. It can be acessed in the
+            training-loop function with `train.get_checkpoint`.
+    """
+
+    def __init__(
+        self,
+        train_loop_per_worker: Union[Callable[[], None], Callable[[Dict], None]],
+        *,
+        backend_config=None,
+        train_loop_config: Optional[Dict] = None,
+        scaling_config: Optional[ScalingConfig] = None,
+        dataset_config: Optional[DataConfig] = None,
+        run_config: Optional[RunConfig] = None,
+        datasets: Optional[Dict[str, GenDataset]] = None,
+        resume_from_checkpoint: Optional[Checkpoint] = None,
+    ):
+        super(BoltTrainer, self).__init__(
+            train_loop_per_worker=train_loop_per_worker,
+            train_loop_config=train_loop_config,
+            backend_config=backend_config,
+            scaling_config=scaling_config,
+            dataset_config=dataset_config,
+            run_config=run_config,
+            datasets=datasets,
+            resume_from_checkpoint=resume_from_checkpoint,
+        )
```

## thirdai/_distributed_bolt/ray_trainer/train_loop_utils.py

 * *Ordering differences only*

```diff
@@ -1,28 +1,28 @@
-import shutil
-from typing import Dict, Optional
-
-from ray import train
-
-from ..utils import check_torch_installed, timed
-
-
-@timed
-def prepare_model(model):
-    check_torch_installed()
-
-    import torch
-    import torch.distributed as dist
-
-    if not dist.is_initialized():
-        raise RuntimeError(
-            "Torch process group must be initialized before calling this Function. Make sure you are using TorchConfig as Backend Config."
-        )
-
-    if dist.get_rank() == 0:
-        model_to_broadcast = [model]
-    else:
-        model_to_broadcast = [None]
-
-    device = torch.device("cpu")
-    dist.broadcast_object_list(model_to_broadcast, src=0, device=device)
-    return model_to_broadcast[0]
+import shutil
+from typing import Dict, Optional
+
+from ray import train
+
+from ..utils import check_torch_installed, timed
+
+
+@timed
+def prepare_model(model):
+    check_torch_installed()
+
+    import torch
+    import torch.distributed as dist
+
+    if not dist.is_initialized():
+        raise RuntimeError(
+            "Torch process group must be initialized before calling this Function. Make sure you are using TorchConfig as Backend Config."
+        )
+
+    if dist.get_rank() == 0:
+        model_to_broadcast = [model]
+    else:
+        model_to_broadcast = [None]
+
+    device = torch.device("cpu")
+    dist.broadcast_object_list(model_to_broadcast, src=0, device=device)
+    return model_to_broadcast[0]
```

## Comparing `thirdai-0.8.5.dist-info/LICENSE.txt` & `thirdai-0.8.6.dist-info/LICENSE.txt`

 * *Ordering differences only*

 * *Files 5% similar despite different names*

```diff
@@ -1,211 +1,211 @@
-1. Use of Software and License.
-1.1. License. Subject to the terms and conditions of this Agreement, ThirdAI
-grants to Evaluator a non-sublicensable, nontransferable, nonexclusive, revocable, limited
-license to access and use the software (the Software) for use solely during
-the trial period. Evaluator shall use the Software and any
-documentation, data or other information related thereto that is received from ThirdAI or its
-representatives solely for Evaluators internal use of the Software and testing and
-evaluation thereof and for no other purpose. 
-1.2. Restrictions. Evaluator shall not: (a) use the Software, any documentation or
-other information provided by ThirdAI hereunder, to create any similar software or
-documentation; (b) reproduce or modify the Software or any portion thereof, or embed the
-Software or any portion thereof into any commercial services or offerings of Evaluator; (c)
-sublicense, rent, sell, loan, lease, disclose, display, distribute, or otherwise transfer the
-Software, or any portion thereof, or use it for timesharing, rental or service bureau
-purposes, or for the benefit of a third party; (d) reverse assemble, reverse compile or
-reverse engineer the Software, or any portion thereof, or otherwise attempt to discover any
-Software source code, or underlying Confidential Information (as defined in Section 2.1
-below), or otherwise circumvent any technological measure that controls access to the
-Software; (e) copy, adapt, merge, create derivative works of, translate, localize, port or
-otherwise modify the Software; (f) publish any results of benchmark tests run on the
-Software; (g) use the Software, or allow the transfer, transmission, export or re-export of all
-or any part of the Software, in violation of any export control laws or regulations of the
-United States or any other relevant jurisdiction; (h) remove or alter any product
-identification, markings, copyright or other notices; or (i) permit any third party to engage
-in any of the foregoing proscribed acts. Evaluator agrees to ensure that there is no breach,
-compromise or violation, by Evaluator employees, consultants, agents, customers, suppliers
-or independent contractors, of such obligations and ThirdAIs and its licensors rights and
-title or interest to the Software. 
-1.3. Enterprise Agreement. Upon the successful evaluation of the Software, the
-parties shall work together in good faith to negotiate and enter into a definitive agreement
-(Definitive Agreement) for use of the Software following the Trial Period. 
-1. Confidentiality.
-2.1. Definition. Confidential Information means any information disclosed by
-either party (the Disclosing Party) to the other party (the Receiving Party) pursuant to
-this Agreement that is (a) is in written, graphic, machine readable or other tangible form
-and is marked Confidential, Proprietary or in some other manner to indicate its
-confidential nature; or (b) in the case of oral or visual disclosure is identified as confidential
-at the time of disclosure; or (c) under the circumstances should in good faith be considered
-to be confidential. Confidential Information includes, without limitation, information related
-to: research, Software plans, developments, inventions, processes, designs, markets,
-business plans, agreements with third parties, services, customers, marketing or finances of
-either party, the content or existence of any negotiations, and pricing. Notwithstanding the
-foregoing, the Software source code and all other non-public information regarding the
-Software, including information related to its underlying algorithms, shall be deemed
-Confidential Information of ThirdAI without any need for designating the same as
-confidential or proprietary.
-2.2. Obligations. Each party shall treat as confidential all Confidential Information
-of the other party, shall not use such Confidential Information except as set forth in this
-Agreement, and will not disclose such Confidential Information to any third party except as
-expressly permitted herein without the Disclosing Partys written consent. The Receiving
-Party shall use at least the same degree of care which it uses to prevent the disclosure of
-its own confidential information of like importance to prevent the disclosure of the
-Disclosing Partys Confidential Information, but in no event less than reasonable care. The
-Receiving Party shall promptly notify the Disclosing Party of any actual or suspected misuse
-or unauthorized disclosure of any of the Confidential Information. In the event of any
-termination or expiration of this Agreement, each Receiving Party will either return or, at
-the Disclosing Partys request, destroy the Confidential Information of the other Disclosing
-Party. Notwithstanding the foregoing, the obligations set forth in this Section 2 shall not
-apply with respect to any information to the extent that it is: (a) already rightfully in the
-possession of the Receiving Party without restriction prior to the first disclosure hereunder
-as shown by records or files; (b) is already or becomes generally available to the public
-after the time of disclosure other than as a result of any improper action by the Receiving
-Party; or (c) was rightfully disclosed to Receiving Party by a third party without restriction.
-The Receiving Party may make disclosures required by law or court order provided that, if
-practicable, the Receiving Party provides adequate notice and assistance to the Disclosing
-GDSVF&H\7770994.1
-Party for the purpose of enabling the Disclosing Party to prevent and/or limit the disclosure.
-2.3. Remedies. Due to the unique nature of the Confidential Information, the
-Receiving Party agrees that any breach or threatened breach of this section of this
-Agreement will cause not only financial harm to the Disclosing Party, but also irreparable
-harm for which money damages will not be an adequate remedy. Therefore, the Disclosing
-Party shall be entitled, in addition to any other legal or equitable remedies, to an injunction
-or similar equitable relief against any such breach or threatened breach without the
-necessity of posting any bond.
-3. Proprietary Rights. 
-3.1. Evaluator Data. To the extent Evaluator provides Evaluator Data (as
-defined below) in connection with the Software, Evaluator hereby grants to ThirdAI a
-worldwide, non-exclusive, royalty-free license to use, copy, access, process, reproduce,
-perform, display, modify, distribute and transmit the Evaluator Data for the sole purposes of
-providing the Software to Evaluator as set forth in this Agreement. Evaluator shall retain
-all rights, title and interests (including all proprietary and Intellectual Property Rights
-(defined below)) in and to the Evaluator Data. Evaluator Data means any data or other
-content or information provided by or on behalf of Evaluator to ThirdAI in connection with
-the Software. Evaluator, not ThirdAI, shall have sole responsibility for the accuracy, quality,
-integrity, legality, reliability, appropriateness, and intellectual property ownership or right
-to use of all Evaluator Data. Evaluator represents and warrants to ThirdAI that Evaluator
-has secured all necessary rights in the Evaluator Data as may be necessary to permit the
-access, use and distribution thereof as contemplated by this Agreement. ThirdAI is not
-responsible to Evaluator for unauthorized access to Evaluator Data or the unauthorized use
-of the Software unless such access is due to ThirdAIs gross negligence or willful
-misconduct. 
-3.2. Ownership. As between the parties, ThirdAI owns and retains all rights,
-title, and interest, including all related Intellectual Property Rights (defined below), in the
-Software, and any technology, templates, or materials used to provide the Software, or
-data derived from the Software. This Agreement does not transfer ownership rights of any
-kind in the Software, or any related materials to the Evaluator or any third party. The
-ThirdAI name, the ThirdAI logo, and the Software names associated with the Software are
-trademarks of ThirdAI or third parties, and no right or license is granted to use them.
-Evaluator may not use ThirdAIs name or trademarks without the prior written consent of
-ThirdAI. Except for the limited rights and licenses expressly granted to Evaluator hereunder,
-no other license is granted, no other use is permitted and ThirdAI (and its licensors) shall
-retain all rights, title and interests (including all proprietary and Intellectual Property Rights)
-in and to Software, and any technology, templates, materials or software used to provide
-the Software, or data derived from the Software. "Intellectual Property Rights" means
-unpatented inventions, patent applications, patents, design rights, copyrights, trademarks,
-service marks, trade names, domain name rights, mask work rights, know-how and other
-trade secret rights, and all other intellectual property rights, derivatives thereof, and forms
-of protection of a similar nature anywhere in the world.
-3.3. Feedback. Evaluator agrees that ThirdAI is free to collect, use and disclose
-aggregate measures of usage and performance, and to reuse all general knowledge,
-experience, know-how, works and technologies (including ideas, concepts, processes and
-techniques) acquired during provision of the Software hereunder, including that it could
-have acquired performing the same or similar services for another customer. Evaluator
-may provide ThirdAI with comments, suggestions, ideas, enhancement requests, feedback,
-reports, recommendations or other information or feedback concerning the Software
-(Feedback), and Evaluator hereby grants to ThirdAI a perpetual, irrevocable, non-
-exclusive, sublicensable, transferrable license to use such Feedback for any purpose,
-including all related Intellectual Property Rights. 
-4. Fees. No fees shall be payable by Evaluator for the use of the Software
-during the Trial Period. 
-5. Term and Termination. Unless earlier terminated as provided below, this
-Agreement shall commence on the Effective Date and shall continue for the Trial Period,
-unless extended upon mutual written agreement by the parties. This Agreement may be
-terminated by either party for any reason or no reason upon thirty (30) days written notice
-to the other party, or immediately upon written notice of any
-breach or threatened breach by the other party of any provision of this Agreement. Upon
-termination or expiration of this Agreement, Evaluators license to use the Software will
-automatically terminate and ThirdAI will return or delete Evaluator Data, to the extent
-provided by Evaluator during the Trial Period. Sections 1.2, 2, 3, and 7 through 9, as well as
-this sentence, shall survive any termination or expiration of this Agreement for any reason.
-The parties agree that Evaluator shall have no obligation to rent, lease, license or purchase
-the Software from ThirdAI after termination or expiration of this Agreement, nor shall
-ThirdAI have any obligation thereafter to rent, lease, license or sell the Software to
-Evaluator.
-1. Errors. In addition, Evaluator shall notify ThirdAI by telephone or electronic
-mail to the contact designated from time to time by ThirdAI upon the discovery of a
-material error or difficulty in respect of use of the Software. ThirdAI may in its sole
-discretion attempt to resolve such error, but shall be under no obligation to do so. 
-7. WARRANTY DISCLAIMER. THE PARTIES ACKNOWLEDGE THAT THE
-SOFTWARE IS PROVIDED AS IS AND MAY NOT BE FUNCTIONAL ON ANY MACHINE OR IN
-ANY ENVIRONMENT. THIRDAI MAKES NO WARRANTIES, EXPRESS OR IMPLIED, EITHER IN
-FACT OR BY OPERATION OF LAW, STATUTORY OR OTHERWISE, AND THIRDAI EXPRESSLY
-EXCLUDES AND DISCLAIMS ANY WARRANTY OF MERCHANTABILITY, FITNESS FOR A
-PARTICULAR PURPOSE, TITLE, ACCURACY, FREEDOM FROM ERRORS, FREEDOM FROM
-PROGRAMMING DEFECTS, NONINTERFERENCE AND NONINFRINGEMENT, AND ALL IMPLIED
-WARRANTIES ARISING OUT OF COURSE OF DEALING, COURSE OF PERFORMANCE AND
-USAGE OF TRADE.
-8. Limitation of Remedies and Damages. THIRDAI SHALL NOT BE
-RESPONSIBLE OR LIABLE WITH RESPECT TO ANY SUBJECT MATTER OF THIS AGREEMENT OR
-TERMS AND CONDITIONS RELATED THERETO UNDER ANY CONTRACT, NEGLIGENCE, STRICT
-LIABILITY OR OTHER THEORY (A) FOR LOSS OR INACCURACY OF DATA, OR COST OF
-PROCUREMENT OF SUBSTITUTE GOODS, SOFTWARE OR TECHNOLOGY, (B) FOR ANY
-INDIRECT, PUNITIVE, INCIDENTAL, RELIANCE, SPECIAL, EXEMPLARY OR CONSEQUENTIAL
-DAMAGES INCLUDING, BUT NOT LIMITED TO, LOSS OF REVENUES AND LOSS OF PROFITS TO
-EVALUATOR OR ANY THIRD PARTIES, OR (C) FOR ANY DIRECT DAMAGES IN EXCESS OF
-$500.00, EVEN IF THIRDAI HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
-THIRDAI SHALL NOT BE RESPONSIBLE FOR ANY MATTER BEYOND ITS REASONABLE
-CONTROL.
-9. Miscellaneous. 
-9.1. Governing Law. This Agreement shall be governed by and construed in
-accordance with, the laws of the State of Texas without regard to conflicts of law provisions
-thereof. The parties agree that any and all disputes arising out of or relating in any way to
-this Agreement shall be subject to the exclusive jurisdiction of the state or federal courts
-located in Harris County, Texas.
-9.2. Waiver. No provision of right, power or privilege under this Agreement shall
-be deemed to have been waived by any act, delay, omission or acquiescence on the part of
-any party, its agents or employees, but only by an instrument in writing signed by an
-authorized representative of each party. No waiver by any party of any breach or default of
-any provision of this Agreement by the other party shall be effective as to any other breach
-or default, whether of the same or any other provision and whether occurring prior to,
-concurrent with, or subsequent to the date of such waiver.
-9.3. Severability. If any provision of this Agreement shall be adjudged by any
-court of competent jurisdiction to be unenforceable or invalid, that provision shall be limited
-or eliminated to the minimum extent necessary so that this Agreement shall otherwise
-remain in full force and effect and enforceable.
-9.4. Entire Agreement. This Agreement, including any referenced attachments
-and/or incorporated documents, shall constitute the entire understanding between the
-parties regarding the subject matter described herein and supersedes any previous
-communications, representations or agreements whether oral or written regarding such
-subject matter.
-9.5. Modification. No change or modifications of any of the terms or conditions
-hereof shall be valid or binding on either party unless made in writing and signed by an
-authorized representative of each party making specific reference to this Agreement and
-the clause to be modified. 
-9.6. Relationship of the Parties. The parties hereto shall each be independent
-contractors in the performance of their obligations under this Agreement, and nothing
-contained herein shall be deemed to constitute either party as the agent or representative
-of the other party, or both parties as joint venturers or partners for any purpose. Each
-GDSVF&H\7770994.1
-party is solely responsible for all of its employees and agents and its labor costs and
-expenses arising in connection therewith. Except as expressly provided herein, a party
-shall have no right to exercise any control whatsoever over the activities or operations of
-the other party, or commit the other party to any obligation or course of action.
-9.7. Assignment. Evaluator may not assign or transfer any of its rights or
-obligations hereunder without the prior written consent of ThirdAI, which consent shall not
-be unreasonably withheld, and any such attempted assignment shall be void. This
-Agreement shall be binding on and inure to the benefit of the parties hereto and their
-respective successors and permitted assigns.
-9.8. Notices. All notices hereunder shall be in writing, as may be amended by 
-notice pursuant to this subsection, by (a) personal
-delivery, (b) certified or registered mail, return receipt requested, (c) overnight courier, or
-(d) confirmed electronic email; notices shall be deemed given upon receipt.
-9.9. Counterparts. This Agreement may be executed in one or more counterparts,
-each of which shall be deemed an original, but all of which taken together shall constitute
-one and the same instrument.
-9.10. Acknowledgement. EACH PARTY RECOGNIZES AND AGREES THAT THE
-WARRANTY DISCLAIMERS AND LIABILITY AND REMEDY LIMITATIONS IN THIS AGREEMENT
-ARE MATERIAL, BARGAINED FOR BASES OF THIS AGREEMENT AND THAT THEY HAVE BEEN
-TAKEN INTO ACCOUNT AND REFLECTED IN DETERMINING THE CONSIDERATION TO BE
-GIVEN BY EACH PARTY UNDER THIS AGREEMENT AND IN THE DECISION BY EACH PARTY TO
+1. Use of Software and License.
+1.1. License. Subject to the terms and conditions of this Agreement, ThirdAI
+grants to Evaluator a non-sublicensable, nontransferable, nonexclusive, revocable, limited
+license to access and use the software (the Software) for use solely during
+the trial period. Evaluator shall use the Software and any
+documentation, data or other information related thereto that is received from ThirdAI or its
+representatives solely for Evaluators internal use of the Software and testing and
+evaluation thereof and for no other purpose. 
+1.2. Restrictions. Evaluator shall not: (a) use the Software, any documentation or
+other information provided by ThirdAI hereunder, to create any similar software or
+documentation; (b) reproduce or modify the Software or any portion thereof, or embed the
+Software or any portion thereof into any commercial services or offerings of Evaluator; (c)
+sublicense, rent, sell, loan, lease, disclose, display, distribute, or otherwise transfer the
+Software, or any portion thereof, or use it for timesharing, rental or service bureau
+purposes, or for the benefit of a third party; (d) reverse assemble, reverse compile or
+reverse engineer the Software, or any portion thereof, or otherwise attempt to discover any
+Software source code, or underlying Confidential Information (as defined in Section 2.1
+below), or otherwise circumvent any technological measure that controls access to the
+Software; (e) copy, adapt, merge, create derivative works of, translate, localize, port or
+otherwise modify the Software; (f) publish any results of benchmark tests run on the
+Software; (g) use the Software, or allow the transfer, transmission, export or re-export of all
+or any part of the Software, in violation of any export control laws or regulations of the
+United States or any other relevant jurisdiction; (h) remove or alter any product
+identification, markings, copyright or other notices; or (i) permit any third party to engage
+in any of the foregoing proscribed acts. Evaluator agrees to ensure that there is no breach,
+compromise or violation, by Evaluator employees, consultants, agents, customers, suppliers
+or independent contractors, of such obligations and ThirdAIs and its licensors rights and
+title or interest to the Software. 
+1.3. Enterprise Agreement. Upon the successful evaluation of the Software, the
+parties shall work together in good faith to negotiate and enter into a definitive agreement
+(Definitive Agreement) for use of the Software following the Trial Period. 
+1. Confidentiality.
+2.1. Definition. Confidential Information means any information disclosed by
+either party (the Disclosing Party) to the other party (the Receiving Party) pursuant to
+this Agreement that is (a) is in written, graphic, machine readable or other tangible form
+and is marked Confidential, Proprietary or in some other manner to indicate its
+confidential nature; or (b) in the case of oral or visual disclosure is identified as confidential
+at the time of disclosure; or (c) under the circumstances should in good faith be considered
+to be confidential. Confidential Information includes, without limitation, information related
+to: research, Software plans, developments, inventions, processes, designs, markets,
+business plans, agreements with third parties, services, customers, marketing or finances of
+either party, the content or existence of any negotiations, and pricing. Notwithstanding the
+foregoing, the Software source code and all other non-public information regarding the
+Software, including information related to its underlying algorithms, shall be deemed
+Confidential Information of ThirdAI without any need for designating the same as
+confidential or proprietary.
+2.2. Obligations. Each party shall treat as confidential all Confidential Information
+of the other party, shall not use such Confidential Information except as set forth in this
+Agreement, and will not disclose such Confidential Information to any third party except as
+expressly permitted herein without the Disclosing Partys written consent. The Receiving
+Party shall use at least the same degree of care which it uses to prevent the disclosure of
+its own confidential information of like importance to prevent the disclosure of the
+Disclosing Partys Confidential Information, but in no event less than reasonable care. The
+Receiving Party shall promptly notify the Disclosing Party of any actual or suspected misuse
+or unauthorized disclosure of any of the Confidential Information. In the event of any
+termination or expiration of this Agreement, each Receiving Party will either return or, at
+the Disclosing Partys request, destroy the Confidential Information of the other Disclosing
+Party. Notwithstanding the foregoing, the obligations set forth in this Section 2 shall not
+apply with respect to any information to the extent that it is: (a) already rightfully in the
+possession of the Receiving Party without restriction prior to the first disclosure hereunder
+as shown by records or files; (b) is already or becomes generally available to the public
+after the time of disclosure other than as a result of any improper action by the Receiving
+Party; or (c) was rightfully disclosed to Receiving Party by a third party without restriction.
+The Receiving Party may make disclosures required by law or court order provided that, if
+practicable, the Receiving Party provides adequate notice and assistance to the Disclosing
+GDSVF&H\7770994.1
+Party for the purpose of enabling the Disclosing Party to prevent and/or limit the disclosure.
+2.3. Remedies. Due to the unique nature of the Confidential Information, the
+Receiving Party agrees that any breach or threatened breach of this section of this
+Agreement will cause not only financial harm to the Disclosing Party, but also irreparable
+harm for which money damages will not be an adequate remedy. Therefore, the Disclosing
+Party shall be entitled, in addition to any other legal or equitable remedies, to an injunction
+or similar equitable relief against any such breach or threatened breach without the
+necessity of posting any bond.
+3. Proprietary Rights. 
+3.1. Evaluator Data. To the extent Evaluator provides Evaluator Data (as
+defined below) in connection with the Software, Evaluator hereby grants to ThirdAI a
+worldwide, non-exclusive, royalty-free license to use, copy, access, process, reproduce,
+perform, display, modify, distribute and transmit the Evaluator Data for the sole purposes of
+providing the Software to Evaluator as set forth in this Agreement. Evaluator shall retain
+all rights, title and interests (including all proprietary and Intellectual Property Rights
+(defined below)) in and to the Evaluator Data. Evaluator Data means any data or other
+content or information provided by or on behalf of Evaluator to ThirdAI in connection with
+the Software. Evaluator, not ThirdAI, shall have sole responsibility for the accuracy, quality,
+integrity, legality, reliability, appropriateness, and intellectual property ownership or right
+to use of all Evaluator Data. Evaluator represents and warrants to ThirdAI that Evaluator
+has secured all necessary rights in the Evaluator Data as may be necessary to permit the
+access, use and distribution thereof as contemplated by this Agreement. ThirdAI is not
+responsible to Evaluator for unauthorized access to Evaluator Data or the unauthorized use
+of the Software unless such access is due to ThirdAIs gross negligence or willful
+misconduct. 
+3.2. Ownership. As between the parties, ThirdAI owns and retains all rights,
+title, and interest, including all related Intellectual Property Rights (defined below), in the
+Software, and any technology, templates, or materials used to provide the Software, or
+data derived from the Software. This Agreement does not transfer ownership rights of any
+kind in the Software, or any related materials to the Evaluator or any third party. The
+ThirdAI name, the ThirdAI logo, and the Software names associated with the Software are
+trademarks of ThirdAI or third parties, and no right or license is granted to use them.
+Evaluator may not use ThirdAIs name or trademarks without the prior written consent of
+ThirdAI. Except for the limited rights and licenses expressly granted to Evaluator hereunder,
+no other license is granted, no other use is permitted and ThirdAI (and its licensors) shall
+retain all rights, title and interests (including all proprietary and Intellectual Property Rights)
+in and to Software, and any technology, templates, materials or software used to provide
+the Software, or data derived from the Software. "Intellectual Property Rights" means
+unpatented inventions, patent applications, patents, design rights, copyrights, trademarks,
+service marks, trade names, domain name rights, mask work rights, know-how and other
+trade secret rights, and all other intellectual property rights, derivatives thereof, and forms
+of protection of a similar nature anywhere in the world.
+3.3. Feedback. Evaluator agrees that ThirdAI is free to collect, use and disclose
+aggregate measures of usage and performance, and to reuse all general knowledge,
+experience, know-how, works and technologies (including ideas, concepts, processes and
+techniques) acquired during provision of the Software hereunder, including that it could
+have acquired performing the same or similar services for another customer. Evaluator
+may provide ThirdAI with comments, suggestions, ideas, enhancement requests, feedback,
+reports, recommendations or other information or feedback concerning the Software
+(Feedback), and Evaluator hereby grants to ThirdAI a perpetual, irrevocable, non-
+exclusive, sublicensable, transferrable license to use such Feedback for any purpose,
+including all related Intellectual Property Rights. 
+4. Fees. No fees shall be payable by Evaluator for the use of the Software
+during the Trial Period. 
+5. Term and Termination. Unless earlier terminated as provided below, this
+Agreement shall commence on the Effective Date and shall continue for the Trial Period,
+unless extended upon mutual written agreement by the parties. This Agreement may be
+terminated by either party for any reason or no reason upon thirty (30) days written notice
+to the other party, or immediately upon written notice of any
+breach or threatened breach by the other party of any provision of this Agreement. Upon
+termination or expiration of this Agreement, Evaluators license to use the Software will
+automatically terminate and ThirdAI will return or delete Evaluator Data, to the extent
+provided by Evaluator during the Trial Period. Sections 1.2, 2, 3, and 7 through 9, as well as
+this sentence, shall survive any termination or expiration of this Agreement for any reason.
+The parties agree that Evaluator shall have no obligation to rent, lease, license or purchase
+the Software from ThirdAI after termination or expiration of this Agreement, nor shall
+ThirdAI have any obligation thereafter to rent, lease, license or sell the Software to
+Evaluator.
+1. Errors. In addition, Evaluator shall notify ThirdAI by telephone or electronic
+mail to the contact designated from time to time by ThirdAI upon the discovery of a
+material error or difficulty in respect of use of the Software. ThirdAI may in its sole
+discretion attempt to resolve such error, but shall be under no obligation to do so. 
+7. WARRANTY DISCLAIMER. THE PARTIES ACKNOWLEDGE THAT THE
+SOFTWARE IS PROVIDED AS IS AND MAY NOT BE FUNCTIONAL ON ANY MACHINE OR IN
+ANY ENVIRONMENT. THIRDAI MAKES NO WARRANTIES, EXPRESS OR IMPLIED, EITHER IN
+FACT OR BY OPERATION OF LAW, STATUTORY OR OTHERWISE, AND THIRDAI EXPRESSLY
+EXCLUDES AND DISCLAIMS ANY WARRANTY OF MERCHANTABILITY, FITNESS FOR A
+PARTICULAR PURPOSE, TITLE, ACCURACY, FREEDOM FROM ERRORS, FREEDOM FROM
+PROGRAMMING DEFECTS, NONINTERFERENCE AND NONINFRINGEMENT, AND ALL IMPLIED
+WARRANTIES ARISING OUT OF COURSE OF DEALING, COURSE OF PERFORMANCE AND
+USAGE OF TRADE.
+8. Limitation of Remedies and Damages. THIRDAI SHALL NOT BE
+RESPONSIBLE OR LIABLE WITH RESPECT TO ANY SUBJECT MATTER OF THIS AGREEMENT OR
+TERMS AND CONDITIONS RELATED THERETO UNDER ANY CONTRACT, NEGLIGENCE, STRICT
+LIABILITY OR OTHER THEORY (A) FOR LOSS OR INACCURACY OF DATA, OR COST OF
+PROCUREMENT OF SUBSTITUTE GOODS, SOFTWARE OR TECHNOLOGY, (B) FOR ANY
+INDIRECT, PUNITIVE, INCIDENTAL, RELIANCE, SPECIAL, EXEMPLARY OR CONSEQUENTIAL
+DAMAGES INCLUDING, BUT NOT LIMITED TO, LOSS OF REVENUES AND LOSS OF PROFITS TO
+EVALUATOR OR ANY THIRD PARTIES, OR (C) FOR ANY DIRECT DAMAGES IN EXCESS OF
+$500.00, EVEN IF THIRDAI HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+THIRDAI SHALL NOT BE RESPONSIBLE FOR ANY MATTER BEYOND ITS REASONABLE
+CONTROL.
+9. Miscellaneous. 
+9.1. Governing Law. This Agreement shall be governed by and construed in
+accordance with, the laws of the State of Texas without regard to conflicts of law provisions
+thereof. The parties agree that any and all disputes arising out of or relating in any way to
+this Agreement shall be subject to the exclusive jurisdiction of the state or federal courts
+located in Harris County, Texas.
+9.2. Waiver. No provision of right, power or privilege under this Agreement shall
+be deemed to have been waived by any act, delay, omission or acquiescence on the part of
+any party, its agents or employees, but only by an instrument in writing signed by an
+authorized representative of each party. No waiver by any party of any breach or default of
+any provision of this Agreement by the other party shall be effective as to any other breach
+or default, whether of the same or any other provision and whether occurring prior to,
+concurrent with, or subsequent to the date of such waiver.
+9.3. Severability. If any provision of this Agreement shall be adjudged by any
+court of competent jurisdiction to be unenforceable or invalid, that provision shall be limited
+or eliminated to the minimum extent necessary so that this Agreement shall otherwise
+remain in full force and effect and enforceable.
+9.4. Entire Agreement. This Agreement, including any referenced attachments
+and/or incorporated documents, shall constitute the entire understanding between the
+parties regarding the subject matter described herein and supersedes any previous
+communications, representations or agreements whether oral or written regarding such
+subject matter.
+9.5. Modification. No change or modifications of any of the terms or conditions
+hereof shall be valid or binding on either party unless made in writing and signed by an
+authorized representative of each party making specific reference to this Agreement and
+the clause to be modified. 
+9.6. Relationship of the Parties. The parties hereto shall each be independent
+contractors in the performance of their obligations under this Agreement, and nothing
+contained herein shall be deemed to constitute either party as the agent or representative
+of the other party, or both parties as joint venturers or partners for any purpose. Each
+GDSVF&H\7770994.1
+party is solely responsible for all of its employees and agents and its labor costs and
+expenses arising in connection therewith. Except as expressly provided herein, a party
+shall have no right to exercise any control whatsoever over the activities or operations of
+the other party, or commit the other party to any obligation or course of action.
+9.7. Assignment. Evaluator may not assign or transfer any of its rights or
+obligations hereunder without the prior written consent of ThirdAI, which consent shall not
+be unreasonably withheld, and any such attempted assignment shall be void. This
+Agreement shall be binding on and inure to the benefit of the parties hereto and their
+respective successors and permitted assigns.
+9.8. Notices. All notices hereunder shall be in writing, as may be amended by 
+notice pursuant to this subsection, by (a) personal
+delivery, (b) certified or registered mail, return receipt requested, (c) overnight courier, or
+(d) confirmed electronic email; notices shall be deemed given upon receipt.
+9.9. Counterparts. This Agreement may be executed in one or more counterparts,
+each of which shall be deemed an original, but all of which taken together shall constitute
+one and the same instrument.
+9.10. Acknowledgement. EACH PARTY RECOGNIZES AND AGREES THAT THE
+WARRANTY DISCLAIMERS AND LIABILITY AND REMEDY LIMITATIONS IN THIS AGREEMENT
+ARE MATERIAL, BARGAINED FOR BASES OF THIS AGREEMENT AND THAT THEY HAVE BEEN
+TAKEN INTO ACCOUNT AND REFLECTED IN DETERMINING THE CONSIDERATION TO BE
+GIVEN BY EACH PARTY UNDER THIS AGREEMENT AND IN THE DECISION BY EACH PARTY TO
 ENTER INTO THIS AGREEMENT.
```

## Comparing `thirdai-0.8.5.dist-info/METADATA` & `thirdai-0.8.6.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: thirdai
-Version: 0.8.5
+Version: 0.8.6
 Summary: A faster cpu machine learning library
 Author: ThirdAI
 Author-email: contact@thirdai.com
 License: proprietary
 License-File: LICENSE.txt
 Requires-Dist: numpy
 Requires-Dist: typing-extensions
```

## Comparing `thirdai-0.8.5.dist-info/RECORD` & `thirdai-0.8.6.dist-info/RECORD`

 * *Files 12% similar despite different names*

```diff
@@ -1,132 +1,133 @@
-thirdai/deployment.py,sha256=e_06MzSrth4S8Gs2dpEzfnLPaZsDcGhcJav8qfxEczY,145
-thirdai/distributed_bolt.py,sha256=MwUmwd8Q2v-ZAMtuGPBAocrQxO7ZRZZruWhlAuZzJos,139
-thirdai/embeddings.py,sha256=Afq5qcejw2NUpC_jImB5D70YLWB4ZHWROZPL7GdT44E,1312
-thirdai/hashing.py,sha256=MttqG8D0RnrPW2U-9Tf9LekMJjwnVS8I8jQAaNlucu4,136
-thirdai/licensing.py,sha256=oLmHWRqfqq6jpTTdauTyQHrY6LO_ukdTTdhgWotOPUQ,195
-thirdai/search.py,sha256=8HTl393Vt3g-OweibrR5UTYTCDCoF6Rk6WvWYRv30rM,1277
-thirdai/_download.py,sha256=16FZSiv34ZTi-u7qSM5V9pINi8Cke2qYoflLfs7YeEI,1308
-thirdai/_thirdai.cp39-win_amd64.pyd,sha256=niRtSGh5OyJT49EkobxenBcpIEOcc7LYBz1XIpBjaKM,13161472
-thirdai/__init__.py,sha256=mAka_UWHYEqlI1EY-ISpDLzDjUR0LuOTlJ7Efi4EMJI,2817
-thirdai/bolt/ner_modifications.py,sha256=tjr3IxGbdqpLM0jRhhi_39lAIR9NkJA-16Pg5FtvaGc,2340
-thirdai/bolt/seismic_modifications.py,sha256=zFgIypd8JChSOPJLxvBaxDrReeMSYnMVkowNcEo-mW4,17208
-thirdai/bolt/udt_docs.py,sha256=I8PsNGy40Mg_kkWMAz1jiGJYp6aAvdBIcuM_EDAOiRw,10707
-thirdai/bolt/udt_modifications.py,sha256=J0xsiNBgwmuYHeNJoIeJYUE-3x21-Zo6cKb7cgMr1Dc,14291
-thirdai/bolt/__init__.py,sha256=vDl_xiJp28JQe8WO8tJRtTmse7TMf4JDXZ2iAy1Du-8,583
-thirdai/data/column_map_utils.py,sha256=_Yt5TI4Y050EKUpucxaQ1QHnt-7IRzUGJED5F5MlHfA,1953
-thirdai/data/get_udt_columns.py,sha256=isYKw8MDHkGbmmnCfwbcS4d0RMaRhmbKtUKTZL2a93s,2474
-thirdai/data/type_inference.py,sha256=gzwd7eCEq6hfmzZRfm0XW99699FcsRXXYBAMTVsFLRQ,5748
-thirdai/data/__init__.py,sha256=ClztQDn5cxQRgjLEwIpep7_8EkXXTSStns3JpUlf2x0,525
-thirdai/dataset/bolt_ner_data_source.py,sha256=dIedz9pXzwMHMkpcbPvR2uBxp31ps5QNWHF1HdQdlEU,2592
-thirdai/dataset/csv_data_source.py,sha256=Dk1gPqafkSNBgvVDhXKWv-ocnPCZMe5b4SdISmVlCR0,3305
-thirdai/dataset/data_source.py,sha256=Hyuz-PcFjhEiYcUAJELweJ4y3bvWOzPdXoRb0E5CNEc,1105
-thirdai/dataset/llm_data_source.py,sha256=ge9TE2CPGnXEV-UMSUqx8syNwYHCOn0Na3ZBlHQ7yd8,2428
-thirdai/dataset/parquet_data_source.py,sha256=TNFwyX2cETN5ACnFYJazRSbEpD1gLHqjvmpXFvLUeVQ,1542
-thirdai/dataset/ray_data_source.py,sha256=TZfXnEypz8aRWoAxnQTkpaPQOo-i5imvLjoo7ubttaI,2398
-thirdai/dataset/__init__.py,sha256=WPKPAsxJk0AofKQcAw5igY2oM5_mVAJcF-S48O90nRw,404
-thirdai/demos/beir_download_utils.py,sha256=B00p4-4DQsNVqoDtCwlKwi91-ESWG3FKWJ7Gj6N40u8,8758
-thirdai/demos/download_datasets.py,sha256=fp3PecQQh0pXqCE0Bi-S6N8kf78cskKc67g2Ly6kL9k,24702
-thirdai/demos/download_tokenizer_vocabs.py,sha256=R0EQlx8DeLedQ4T5NWNZX9S6RFT-z8kKbxuOGyzIkOw,520
-thirdai/demos/__init__.py,sha256=IgkhMJRpY1kIkJ86TnqgyqLzlOEKfx-0VNCDuIG6eew,76
-thirdai/gen/questions.py,sha256=W_UZM3RHAHY9KgV-Z-qrP_5lD7wpO3oHOitOFnsFZPs,1265
-thirdai/gen/__init__.py,sha256=WdR0bbZafCuZ2pkE1FllH8gj3ePU5viS-W5leQOD2J0,26
-thirdai/neural_db/connectors.py,sha256=KRUpGFUPHhva7xoQl24W6WZCHPH1ERxqdlc-vM4SvVk,6881
-thirdai/neural_db/constraint_matcher.py,sha256=GL1jXdcpFh2sk2DFijuYp8GfWLZ5G6R5_ZG1ANruc2U,9130
-thirdai/neural_db/documents.py,sha256=UyPJ7ySvdxGXJLaVHi8s1WE5cAyhmHl5fL0bjBmHFt0,93810
-thirdai/neural_db/inverted_index.py,sha256=UnwXM30sIppy01Olnra_tA_IkSD6HtJedF6efMVFaUo,2703
-thirdai/neural_db/loggers.py,sha256=qRhnMKNz0W_i_P-lnPXCO7WI_5ZtefabYf4A21QifPc,3619
-thirdai/neural_db/neural_db.py,sha256=8S6AGyPssiYNmtYO9A4KaNrdKfI18qUpnZq1pCvKZTA,44815
-thirdai/neural_db/question_generation.py,sha256=St7DvYuMYCx6-4tGoqasEIGK6pxAJOQq94Fim3_7t9U,677
-thirdai/neural_db/savable_state.py,sha256=3BGG5bCgNtVTisXGCSxzEe-jGjw7xJsjjYw2VIMrdKY,5145
-thirdai/neural_db/sharded_documents.py,sha256=licTbOAcp2ik1y3XowZzDh34-9AeT09A7fdNWZhll-I,10446
-thirdai/neural_db/sql_helpers.py,sha256=k9IY2f90yRn5d3WgnNs9lZI-RvlKd83oeUy35dhH-eg,1718
-thirdai/neural_db/supervised_datasource.py,sha256=1mJzAVqwaWfxWcLYk7RdRxgOewARQSDYkWjtjWzf3yA,9886
-thirdai/neural_db/table.py,sha256=Fms4e3-BS_uV9LJXijSR89GkEz1lPYSaJFZK9zPln7g,8126
-thirdai/neural_db/teachers.py,sha256=H5SSOyc9eCpHU1OumYcicNsPVZdcC6Sz229slJmFJJM,959
-thirdai/neural_db/utils.py,sha256=6I-9rsPTt7Dos4m-6kEvv8E7qcsdxrbf4oprQtWD_tw,3844
-thirdai/neural_db/__init__.py,sha256=1RPvXZxeNN1lzAX1E6r0xzEldL01guW6bajYDbj_7XY,737
-thirdai/neural_db/models/finetunable_retriever.py,sha256=kzbHUtenglTyw1rjL_ZLjUnh6Dkg1OnlJILQ2EBo6iA,3418
-thirdai/neural_db/models/mach.py,sha256=ZBg6iT2GP3sIPnreHuYNa_q1lDi8B0q19pSEVmoD7zQ,27554
-thirdai/neural_db/models/mach_defaults.py,sha256=il0NX-0dmFMCPvK2EccCx_rFzSRHAX562c5d9_O77vI,1474
-thirdai/neural_db/models/mach_mixture_model.py,sha256=u0uwtMGJ_Lc_tqZ8c5nLRG-mRQjfO84w3kFn4POAIgk,28166
-thirdai/neural_db/models/models.py,sha256=POd81TS1HY1kGDA71u27Oebl1J5WWFq7sbFXknOxLgQ,575
-thirdai/neural_db/models/model_interface.py,sha256=9Yc6AVtRFN5AG3iy8LtijesYeJpQ4VCkq-NV5tPEfro,5583
-thirdai/neural_db/models/multi_mach.py,sha256=fIRK70ExUxyWesKnPbZ3Xu1D_cSowkHnE_ldqvztev8,7846
-thirdai/neural_db/models/__init__.py,sha256=vV_Fs6hpCoAgAr5cz0BvSL5xsuTLOQ91N81nIQ6o2S8,24
-thirdai/neural_db/model_bazaar/bazaar_base.py,sha256=XIjpcE61BnuUjPePRsBDMehW-F4SVTpgTxPZQ11XxDU,19936
-thirdai/neural_db/model_bazaar/bazaar_client.py,sha256=kakRZzRl3Fmhr_8HGtMF3_TrPcrYx0o7NiEtm9VlZoo,36069
-thirdai/neural_db/model_bazaar/utils.py,sha256=90k9HZoFoXB80okBZWqzcQq6hJEFhf7bUm6q6xsK4G4,4003
-thirdai/neural_db/model_bazaar/__init__.py,sha256=vYY8FhUqWKMgb0ja5tM51fGW-7oxmTtbOUhgF0aOtJQ,88
-thirdai/neural_db/parsing_utils/doc_parse.py,sha256=hZLV9FKyDCDPwD9b5anW7dwyyc4JhwJylOZtEo9-pFo,1476
-thirdai/neural_db/parsing_utils/pdf_parse.py,sha256=3-8FK23gdWpCqNll73tCC1LbeeM88irt_lTLlRV2f5Q,6055
-thirdai/neural_db/parsing_utils/sliding_pdf_parse.py,sha256=ZJ_KXgPr6ohV2rEdDXqhVTZKh9VKLcKuWXg8pSC-ZZ8,11499
-thirdai/neural_db/parsing_utils/unstructured_parse.py,sha256=Ywxud33LBtb8vdJBSdAWNtmam9-sOqJIr5ijan0aKkY,7460
-thirdai/neural_db/parsing_utils/url_parse.py,sha256=znfpaXBR33fuMgfA459iOUykb_nIqnIW-6kFSsFrQ74,4064
-thirdai/neural_db/parsing_utils/utils.py,sha256=Nmh5WqWjRxM9NvqFmjNEcbebwupLY_aDLnudEXAkBfQ,2778
-thirdai/neural_db/parsing_utils/__init__.py,sha256=0pW3zXm1yXr6c_1-PM6r8Lu7UFzU_x7sJZ1f3geF-fc,45
-thirdai/neural_db/trainer/checkpoint_config.py,sha256=lzALirPtXAXh55FIGE9ZAEnBN8JTJl9mpXX2VdKtvpo,4013
-thirdai/neural_db/trainer/training_data_manager.py,sha256=7swswE1EpWaKaRey77RBun2YBO5C-kE_vXI2mO3jLn8,7964
-thirdai/neural_db/trainer/training_progress_manager.py,sha256=8CeRRAutFtPMFM4OXhSHB7zMfvYlMjU4G6Z-Ihuj65g,11920
-thirdai/neural_db/trainer/training_progress_tracker.py,sha256=n6bH4nEJeU8Y2zjuLhDonLtuKIHJ7i9VRBLJn_VMAuA,7825
-thirdai/neural_db/trainer/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-thirdai/neural_db_v2/neural_db.py,sha256=3tYn5Mr1fzrxp4eE7Pve29EqdD7x3Qe5Z_00SV_5Aic,5460
-thirdai/neural_db_v2/__init__.py,sha256=D4ohJo1C-WVHqlNTbD39XyMU6jHSOAx_6VyJvtUlSQ0,234
-thirdai/neural_db_v2/chunk_stores/constraints.py,sha256=Rzb060BwvRU9RY11xebAsdqdlmDtF1PDGduOGBvi9o4,2171
-thirdai/neural_db_v2/chunk_stores/pandas_chunk_store.py,sha256=1aw2vjIfbxQuYcYgnbgan6of-GP5_ablwvpUux2gKKE,6128
-thirdai/neural_db_v2/chunk_stores/sqlite_chunk_store.py,sha256=mAii7Hnl9hrM-8gzeUUYjWwSU71b232UQWBj0ePBKWU,12976
-thirdai/neural_db_v2/chunk_stores/__init__.py,sha256=FiC6Nz-_t0JmRG6sX9Jkwuz3qYW5E2mGhNEUyb22Bbk,100
-thirdai/neural_db_v2/core/chunk_store.py,sha256=IqWzHu8G8XQGYdmY6_2iK4cuNSorBHLn2I0g3gMmh8o,2128
-thirdai/neural_db_v2/core/documents.py,sha256=qwrl_h78WXmf17lbmjbfmUeAtIa4VgHmvzkE03rdwJk,234
-thirdai/neural_db_v2/core/retriever.py,sha256=0hvUi1uRDcT3X8pd5cqqUAddPzAXpP-2eabfeT9WPIY,1891
-thirdai/neural_db_v2/core/supervised.py,sha256=uVxGv_Lm8QjX2pEJQgiZMCmEHHx5T40BulvJkq6tQmw,823
-thirdai/neural_db_v2/core/types.py,sha256=hByTK5mgrLeqdO_VuyG6_cAFGa970615LvrQYlSVPrU,6352
-thirdai/neural_db_v2/core/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-thirdai/neural_db_v2/documents/csv.py,sha256=YLaG8_x95nV396gqZyoMM84e9oSEFMv3L9PXruVWtC4,2478
-thirdai/neural_db_v2/documents/docx.py,sha256=pU-K9pDdrZpQcBX_mNExVIEiDmblB3yrXIVd3JHzO_s,1153
-thirdai/neural_db_v2/documents/in_memory_text.py,sha256=Da_UlRWSYYt0V3W74zceP3sCEJ1q08FH6kIvmT20DKU,1309
-thirdai/neural_db_v2/documents/pdf.py,sha256=a-IOapNIZRGOj7Pr23Lgshqs68CWdy5m2z3FHksp6nE,2304
-thirdai/neural_db_v2/documents/unstructured.py,sha256=ju64vHIA3s40JFjmjPBW9fvHuOplqDrete6PLgd6CvU,2361
-thirdai/neural_db_v2/documents/url.py,sha256=aja8ZFCsicYdooGP4rZL0xTJRXsTiB6_j97ZlfX3CdI,1417
-thirdai/neural_db_v2/documents/utils.py,sha256=yXyU_QEhD2HwznQciZMwc204yWfpX7ZAA-Qbl5Di7RY,840
-thirdai/neural_db_v2/documents/__init__.py,sha256=f9Yl3yCG_zDCDdELJfLyt0kCsTHjJEmxaM7bqtN8vM4,365
-thirdai/neural_db_v2/retrievers/finetunable_retriever.py,sha256=yH8XH-WXZkpaDbkggYHOsVQQr_4l_s9Ymj6uI0CjWnQ,2018
-thirdai/neural_db_v2/retrievers/mach.py,sha256=MezVZxIO_nlv4-3VOLlPnygoyIvM-_p1JIYpRWZYu30,6804
-thirdai/neural_db_v2/retrievers/__init__.py,sha256=JqiaXPONdAL1X79dqDfwPD7nQRFEiCvzCQl3aBD6nnI,109
-thirdai/neural_db_v2/supervised/csv_supervised.py,sha256=kPNeOZzmqBfsMe5mjojE1dG7c2l4GfM7a8QDERa_s6U,1133
-thirdai/neural_db_v2/supervised/in_memory_supervised.py,sha256=FPvB9J7YNZEQfCZrUSCkdpr9uRZoch7imatzBDbTT64,729
-thirdai/neural_db_v2/supervised/__init__.py,sha256=0X71SEqW2Wb3uK2p60_dDVQyYI1Lw5OaCPnKd3XtHUY,97
-thirdai/neural_db_v2/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-thirdai/telemetry/telemetry_daemon.py,sha256=Vx5yf-IN17TGNSe1Hy3Szqu_1kudiAl3ics-nBcdHHM,5263
-thirdai/telemetry/telemetry_start_and_stop.py,sha256=Dbw2KsFyOfjQcnrSXnwMNBLNlj62aRFcje6RfRzJJfo,5939
-thirdai/telemetry/__init__.py,sha256=cxKYgX1QWTCuYwg64_1RgBNvKL2iqtssdcc25eF4-NI,51
-thirdai/_deps/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-thirdai/_deps/ColBERT/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-thirdai/_deps/ColBERT/colbertconfig/base_config.py,sha256=_uIykIxh_rOh1R-7kKnJT6tQZ0Y65MzK8sl5XuG37Hg,2118
-thirdai/_deps/ColBERT/colbertconfig/config.py,sha256=DWl6wtKo8QNpIVK4jT2ZfdFL3VDWGuf-O4l1H42anZQ,314
-thirdai/_deps/ColBERT/colbertconfig/core_config.py,sha256=JFNb9lxpIS2UQPVWOgkUAp1i_4I7WKBcj_YeL5l8bKU,2321
-thirdai/_deps/ColBERT/colbertconfig/settings.py,sha256=7iR-B4qxyktrAozfZKqZz6kPRS-V7XSJVpqwtt-P8eY,3887
-thirdai/_deps/ColBERT/colbertconfig/__init__.py,sha256=X0Mvt4Aezar-roVLmPowuHRn8hb70Kx5ZZ0Q48A9L_w,48
-thirdai/_deps/ColBERT/colbertmodeling/base_colbert.py,sha256=U1T3bJdzGaGwUlWzzm6KmfEbgvrxBJnNuZ-aioNlA9A,1268
-thirdai/_deps/ColBERT/colbertmodeling/checkpoint.py,sha256=sG4z8HOEaJNtPx78OUtvhiAriBgB-9JEX0C48Onqoik,1418
-thirdai/_deps/ColBERT/colbertmodeling/colbert.py,sha256=KZ8NmMIIzcoNGlqFNB1_i8mdqr-3lO1YXxciGeYUToI,2277
-thirdai/_deps/ColBERT/colbertmodeling/hf_colbert.py,sha256=80OcRzbGaeu6TRRwItrFtTrxqqBEwS4a_JLLwJjvQBU,2175
-thirdai/_deps/ColBERT/colbertmodeling/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-thirdai/_deps/ColBERT/colbertmodeling/tokenization/doc_tokenization.py,sha256=wTj2V-nAAludCX7V9dwaETvnwyBAKCtsYQ6pegxK--M,2064
-thirdai/_deps/ColBERT/colbertmodeling/tokenization/query_tokenization.py,sha256=pr4oYC__9BzE-eabPPoFOlz9X9PyVtDE8232Z3Bam3s,3295
-thirdai/_deps/ColBERT/colbertmodeling/tokenization/__init__.py,sha256=7Rwy_hl_63cv9GnOParV2deBNvY1HkC6lZQm76r--wE,168
-thirdai/_deps/ColBERT/colbertutils/utils.py,sha256=TsQ71Ufd5Bu5QC1wWxQPvnSAEwjft8KLq8mUgcugf7Q,319
-thirdai/_deps/ColBERT/colbertutils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-thirdai/_distributed_bolt/distributed.py,sha256=hEMYFVuK6fME426Cb2kbv0Sz0tpeSCRQJV9MIJliius,3817
-thirdai/_distributed_bolt/utils.py,sha256=ymIc5OBP-zsmEAGt7Nw3D-2iThJR36qV0UO8Vo2e2Qk,950
-thirdai/_distributed_bolt/__init__.py,sha256=PlJqc7hzH1StDqTaayFgN0nsKtEmziFAmkb6vdjxWNo,309
-thirdai/_distributed_bolt/ray_trainer/bolt_checkpoint.py,sha256=_4QTPvNJHdiuUUKzwzWCI-lE535MyemQPSMxNXwCUAM,3248
-thirdai/_distributed_bolt/ray_trainer/bolt_trainer.py,sha256=yukFnq0lbsnxF3upYc1wxUuFBd-Xh6-es1w4mChwqlo,3450
-thirdai/_distributed_bolt/ray_trainer/train_loop_utils.py,sha256=_79EDYzO3mtbyq58QIFN6zAUOL9tgQWuZIm0SlPi6Tk,732
-thirdai/_distributed_bolt/ray_trainer/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-thirdai-0.8.5.dist-info/DELVEWHEEL,sha256=OasX8bBxYxivic4erTXsiw_OkrgUhY_et7RljZvMgqY,394
-thirdai-0.8.5.dist-info/LICENSE.txt,sha256=agSIzKgTDMipzr2SjcOEG_GTJJkwtt-4wrcVAY03gPo,16919
-thirdai-0.8.5.dist-info/METADATA,sha256=MgBUZwgmnZzh6V1dAQLMABnmQvD3k9Ne6bSvAJBatrA,7359
-thirdai-0.8.5.dist-info/RECORD,,
-thirdai-0.8.5.dist-info/top_level.txt,sha256=yBixDeDldyBUN70Yfq9rnKJYCNICw3ae7uow_BdZysA,8
-thirdai-0.8.5.dist-info/WHEEL,sha256=fVcVlLzi8CGi_Ul8vjMdn8gER25dn5GBg9E6k9z41-Y,100
-thirdai.libs/.load-order-thirdai-0.8.5,sha256=DjR-nHxdIslyHWSMiI7wCb1N5iHc-aFCMOL1DcEQXgs,54
-thirdai.libs/libomp140.x86_64-05cfcc04b0c6de37e0a3c60db4df4e20.dll,sha256=uopcjt-ef_24jRY4X0elDjJufjh27oYWnLZ2Rp1GTWY,772016
+thirdai-0.8.6.dist-info/RECORD,,
+thirdai-0.8.6.dist-info/WHEEL,sha256=oITnL8r4K9lhj-wWSHlE2XUA0J8WLXdegrKtvi_34zA,108
+thirdai-0.8.6.dist-info/top_level.txt,sha256=yBixDeDldyBUN70Yfq9rnKJYCNICw3ae7uow_BdZysA,8
+thirdai-0.8.6.dist-info/LICENSE.txt,sha256=TE8WSZup1iad99QfeTPj3_TAu3Z1qGpoB7qt2to0V2g,16709
+thirdai-0.8.6.dist-info/METADATA,sha256=g9KVSUkQGc_plko3rgPT9THzB0z3IQ5FBTHXgaD9rN0,7359
+thirdai/deployment.py,sha256=bKxfOyjcOWi_ZV9cU1bMarKY6864sxL2FmLLdFl7PO0,140
+thirdai/_thirdai.cpython-39-darwin.so,sha256=KzzYYyFQvWvUzlgwgnbhyAkJ9WNARmyqp51s4bXxrt8,10870960
+thirdai/__init__.py,sha256=052lzaFLpnhpErD9o_vgAMHyPlzokts8aAU8EErS7JI,1400
+thirdai/_download.py,sha256=uTDs7o4x--mm3q0B4FmVJnQVNoBYq_w6RJYNLtum-U4,1267
+thirdai/embeddings.py,sha256=BTlenRZIzQR6qjt0jgMuIxRSuMCeIKjroA4y39w448k,1273
+thirdai/search.py,sha256=QQKJFDo9iBMIvhJpQ_nlDGgBKXgnF4qf72j_Hl8v42s,1236
+thirdai/licensing.py,sha256=jSqJxQYMe2-65ud1HxuSvef0DB2nl4ht65LBIZv8j-0,187
+thirdai/hashing.py,sha256=Px7e7hsGGHzFYwLQQqnnP97FOP0YCa1ILYgH-b-f9Xg,131
+thirdai/distributed_bolt.py,sha256=srgoNcTMnAyTIn0V2x9C84Ef_D7gRC474XyQgO1e_UA,134
+thirdai/neural_db_v2/neural_db.py,sha256=mC4mvgtIdqFIH9YZTfCXpVge5SfcagMdQEWHRas1uEg,5373
+thirdai/neural_db_v2/__init__.py,sha256=FU84be28nsc-VKDt2aMfjzp9fDSlGfU4bsAPvV-8wmM,221
+thirdai/neural_db_v2/retrievers/__init__.py,sha256=DX9bWQioSLv1_ymO5QZRcovrjsWvkVgsW_u9vUYKgOo,145
+thirdai/neural_db_v2/retrievers/mach_ensemble.py,sha256=RS77E4IQoZC85v9zPiSai4SfeYru2Y1ELJrgUmHi118,4566
+thirdai/neural_db_v2/retrievers/finetunable_retriever.py,sha256=EAbNaTBP1T-gSptlYFiHfbYGXh2FwBn6vkuZt-1bpzo,1960
+thirdai/neural_db_v2/retrievers/mach.py,sha256=x8j36gpZ6BQQSrdQTypKNQy5u6p9nhbcepBk-QPAfkA,6813
+thirdai/neural_db_v2/core/documents.py,sha256=TyN6oB3g8R5p5--HF0_t3vNE1yHPHpr9mFm7U_1CEy0,224
+thirdai/neural_db_v2/core/retriever.py,sha256=I67ywq75gUjsauGcm4AE1v7ZV9cj5dN25n-ZGI2Q1Go,1839
+thirdai/neural_db_v2/core/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+thirdai/neural_db_v2/core/types.py,sha256=s9vtg1aUyWPKd9nKvNQ1u0zsm31kgANXfPbiiS2Ki5E,6144
+thirdai/neural_db_v2/core/supervised.py,sha256=4zKX82xnauCRfO-WU7c5-Pgi7EDFbqc81FKUVYL-GLg,796
+thirdai/neural_db_v2/core/chunk_store.py,sha256=NxFIaK0eYnBZnavVwYnFyD1AHZ5dnNuEm9ecBDakTAc,2060
+thirdai/neural_db_v2/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+thirdai/neural_db_v2/supervised/__init__.py,sha256=wr_Uxk8A0cQIYrNUwc542QDuF6EQBCQq8iwF3E3By8Y,95
+thirdai/neural_db_v2/supervised/csv_supervised.py,sha256=-SermsRPHRaZ8jkMEYQx0RG_kZYozpdz74tCNqABRUo,1093
+thirdai/neural_db_v2/supervised/in_memory_supervised.py,sha256=IXLvzDjjJd6J2dV8Y_9xuA9Giqc3IAFXR1CRkC1uLYc,706
+thirdai/neural_db_v2/chunk_stores/pandas_chunk_store.py,sha256=ebcJQ_40Uh7lndKhIuaQWyGA2BddXDw1zL8ge5YmSyQ,5948
+thirdai/neural_db_v2/chunk_stores/__init__.py,sha256=l2UqWara7FRGkXhfAGN_8CpgT7MHMwHA52eItqDfij4,98
+thirdai/neural_db_v2/chunk_stores/sqlite_chunk_store.py,sha256=F8AuqozwuzDcFS_22MYM6yi9qBB-28snvanN_QLGgWQ,12610
+thirdai/neural_db_v2/chunk_stores/constraints.py,sha256=HnEPJE8XlS2adNX2Ip37YOWchnCGHv3u2xwWDQiriJ8,2099
+thirdai/neural_db_v2/documents/docx.py,sha256=y4BXn74rnhEHUTrjTFl1tMgMbH7FargmzrK7D-xuk48,1114
+thirdai/neural_db_v2/documents/unstructured.py,sha256=pFwz4EHkc6mst--11Sij3Jlbxg9-GN4lb5gBMThvufc,2274
+thirdai/neural_db_v2/documents/in_memory_text.py,sha256=UIumgmC8w3ZE5JrynFCMngc8jYQ_-cmBRigOY0LU368,1265
+thirdai/neural_db_v2/documents/__init__.py,sha256=QY1NES3Z0UIo_qNXYtKR6pDXJaPs614oCi7rhgYS2JI,353
+thirdai/neural_db_v2/documents/pdf.py,sha256=jzCL-f37QiKxYD60nX4MAU3ZgC-ixe3O_BmWlo2pmY4,2236
+thirdai/neural_db_v2/documents/url.py,sha256=8L6IyOT0gpckQ_zEcmxFtat0tAcNeiomzysEAmleLkc,1369
+thirdai/neural_db_v2/documents/utils.py,sha256=KoKpZRGWalK4POJ3Piq9JTksN1ZP1-s_fWgNQJUZpsM,811
+thirdai/neural_db_v2/documents/csv.py,sha256=lx69eMIjmCRjlo7JNL2480gn3QJqsopVI3LBlCRBd6o,2398
+thirdai/neural_db/sql_helpers.py,sha256=NewkxcgHwb2IVAv2ayZRRSxBoC3Agoo4hzn8dkPavc0,1647
+thirdai/neural_db/connectors.py,sha256=64D5lImWjGMaTneUpc5KcynFLx4-v7gt-84BFGvplNE,6680
+thirdai/neural_db/neural_db.py,sha256=PAw0jwSjADirXRFb-Snln2b4InlU0tqAEcK562on0FI,43752
+thirdai/neural_db/supervised_datasource.py,sha256=BZ1dkMd_2Z4LZTWngCeefx6En5_I-cn8UbsIQWzCmtU,9633
+thirdai/neural_db/savable_state.py,sha256=sQen_ZRY55g1NNVRO_Q4YG_eah6icg48qA8rRrhoRyA,4988
+thirdai/neural_db/loggers.py,sha256=PWTNoWh6NbYJAJfjqIhusxooji0SdXYzq0YAwJ24RxE,3474
+thirdai/neural_db/teachers.py,sha256=yIQHF55BYVXKgVhT06IYuKz3PnXQDfGjWIC8uZFzEhY,915
+thirdai/neural_db/documents.py,sha256=YLWVMHwv2ghGv3qCXRAK5tZldZUQjBSANuLAv0b2xsQ,91379
+thirdai/neural_db/inverted_index.py,sha256=fJBx3FxG6U11GM8eRH-AeI0IYNR7urz8lYxFecNwyqA,2613
+thirdai/neural_db/__init__.py,sha256=iDGkmcqva55gqtjfBGLBfiCs87iqwtYstrgNIlale4g,708
+thirdai/neural_db/question_generation.py,sha256=5J66uS0soBTsvKVtcZRjvMUARNPfG0rE2Z1UtLTZcO8,658
+thirdai/neural_db/utils.py,sha256=L_mtOQMuX2M1iR4G7LnVefaKPzIf2SU2fIkxpF5mOv0,3711
+thirdai/neural_db/sharded_documents.py,sha256=rH0q-cODwL0ljP-xFP9NBb7cV1qoYfieINp75NMMtk0,10200
+thirdai/neural_db/table.py,sha256=EJWdHfwTyw_mjYSETm3RJovdNnHbmNyct5KXQGQ94m8,7869
+thirdai/neural_db/constraint_matcher.py,sha256=uabpfVmc14c076VZcb5HGz57FnkxScYnX9yJtgb6j6Y,8876
+thirdai/neural_db/models/models.py,sha256=TePQrKO9iaLcuZqlSH5seyVK0-VAe_OUxJ4PxzX6t_w,564
+thirdai/neural_db/models/__init__.py,sha256=a4BCgsDaLWRFC9KZGAoQHn168CvupHdQHvGTXfWg4uk,23
+thirdai/neural_db/models/mach_defaults.py,sha256=GfTDttF-BB1zCk2nrpIwI88uxJlXsZqPBH2ATQU8tLQ,1422
+thirdai/neural_db/models/finetunable_retriever.py,sha256=xKxm3AZQ5teJ6IYoPJJrGgk-40d6Wi2u8VymLUDY9zI,3313
+thirdai/neural_db/models/multi_mach.py,sha256=FQfq73X3et4AHls49rb-kcEevlPhg8R_h0z_jTfVR-M,7619
+thirdai/neural_db/models/model_interface.py,sha256=UA6nxXjp085XTcKY3w4sY5oNoF2_MELNMYl4D1pCViM,5392
+thirdai/neural_db/models/mach.py,sha256=FEhhvDtZFQGttc2Ihgcca--7ejNr9iR8FgIP6uIkiAk,26811
+thirdai/neural_db/models/mach_mixture_model.py,sha256=ezdyaN2k-Maftxv-70-9GNxVXYG3nJ1tVEmgQkJYan8,27476
+thirdai/neural_db/model_bazaar/bazaar_base.py,sha256=AhKyT-kwiqkegi633rG_AtB8q95F6BlcQRc8LaFVdW8,19358
+thirdai/neural_db/model_bazaar/bazaar_client.py,sha256=SgwFkLXrM9l9jO-U3aDTe92wlpnmgDujG2elPRO4Axc,35091
+thirdai/neural_db/model_bazaar/__init__.py,sha256=E9oax1gvOkKyoR2ogZjvC6sFQ9MARLkwBgJ-wh0Nric,86
+thirdai/neural_db/model_bazaar/utils.py,sha256=bC_CocJLGJ4KxvPPDtjjA7tvNhPhrgq90xNVBB2I_Vo,3858
+thirdai/neural_db/parsing_utils/unstructured_parse.py,sha256=wt4_--5S4dJmfUu0LHt8rjVm5HiTvabPEoQPRm1bGOI,7228
+thirdai/neural_db/parsing_utils/__init__.py,sha256=z7sk8PTJbmp13NH5jU1eqWpUE0McTcuQxg8JRMXhFEQ,44
+thirdai/neural_db/parsing_utils/pdf_parse.py,sha256=xLzgLwnB0kltIOROuNOwCzhSWMFbUnE9H1yUHkzSZ28,5866
+thirdai/neural_db/parsing_utils/url_parse.py,sha256=J4Hgsvm1YimtKRN2BlrDmRMfdWhhpU888zhyFNpKQLI,3937
+thirdai/neural_db/parsing_utils/utils.py,sha256=8c6DLFWgMyoSempSaGYW9-LSCI361EN_TmB1U9wR8UY,2678
+thirdai/neural_db/parsing_utils/sliding_pdf_parse.py,sha256=iFIj5Ht8FQw8tFnTxfkGAxj4uhPtIU7WOpyxVqp7VLE,11144
+thirdai/neural_db/parsing_utils/doc_parse.py,sha256=EN9MTdaorZ7YiNYfZrEb-pXM7ghWXWsXgPhLk_tvzYE,1430
+thirdai/neural_db/trainer/training_progress_tracker.py,sha256=1rCeXCBjsjjQMP-qjxS7uAFBlKyxyl1rgQAzwJmwRX0,7590
+thirdai/neural_db/trainer/checkpoint_config.py,sha256=nAlbz6O1NXEXFZjhJopCKfqJfd2qJwgFmtt-csiR-jI,3922
+thirdai/neural_db/trainer/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+thirdai/neural_db/trainer/training_progress_manager.py,sha256=oMXKeSSjSJX7eNZaOAZWGzmANqgiGH9lmkTwZaZYut0,11622
+thirdai/neural_db/trainer/training_data_manager.py,sha256=mDJcgQ18bsY7jru1JXG8piBwbIeV5VU6sw3I_crzRHY,7755
+thirdai/bolt/__init__.py,sha256=S_xZpcyyxyDIYOWmGthUW42ePqqykgwpI5yG3mfLPJ0,561
+thirdai/bolt/udt_docs.py,sha256=zS2ejyH4sEFp39CDSg12jkL6ibSJt-ZveaFW79WuL4Y,10517
+thirdai/bolt/ner_modifications.py,sha256=lXycA7Py4mo2hUJY-aOJE75HPvZsIyLnmTKLyYCVLZ4,2271
+thirdai/bolt/seismic_modifications.py,sha256=yVC3nuvyAIadVpx9SwaT6mShRj4ht11_EBpJgqXuXVQ,16696
+thirdai/bolt/udt_modifications.py,sha256=c2br8eR1Qb0UR7sa1ARglqt-rZgB8oNDv_sDJVfR8aI,13874
+thirdai/dataset/data_source.py,sha256=lDRv4AFB6Xh2xU3K7wKS3gbrOH20jv_l5Vc5znuhrXM,1068
+thirdai/dataset/__init__.py,sha256=KR7so0HNgMe7DYqT1r0tMM7BVtTwLf17WNC5h6QhJOg,391
+thirdai/dataset/ray_data_source.py,sha256=lHYx1iOI0q3frWbQqpnP3VNc02j_BWORpknQOaYk6xg,2340
+thirdai/dataset/bolt_ner_data_source.py,sha256=YobKSyM0_LF0_dNJCb5UKgytJBs9NAJG6Cb6hhLkc44,2515
+thirdai/dataset/csv_data_source.py,sha256=cEGwKExKp3tN9W1YULx4vWunyiC_mgX9ceiYHJGvM94,3218
+thirdai/dataset/parquet_data_source.py,sha256=30LEqy-EYlp8G0nnPAYkSHfYyjFGD8ytTCtzeuxMq-o,1500
+thirdai/dataset/llm_data_source.py,sha256=n-O39t-pZAlQ5jbwGWBahAoRstHx4EEJRbsFNw2VTxQ,2359
+thirdai/gen/questions.py,sha256=myov1237jhNIFdylGqRUpwIpOuvGwmA2ojUQVlPdqUU,1230
+thirdai/gen/__init__.py,sha256=506QSbDVrY2LAj4uFx-aMcnYssZ_FXvljkIhCXg_OqY,25
+thirdai/.dylibs/libssl.3.dylib,sha256=7fYYQJXjlJrfNfTKtkRc7Ezduef8SQtX9MQW989sY10,837536
+thirdai/.dylibs/libomp.dylib,sha256=HKjFLpYKgHOfzscoLSRKsnuZONLjGAW3oALluLNirQU,822960
+thirdai/.dylibs/libcrypto.3.dylib,sha256=Kq0izsXFKYqGecqbp-1A2vpL9p3zd01qAeszwKF8fes,4200880
+thirdai/_distributed_bolt/__init__.py,sha256=geVnxXIpgxohOKEI9VLu2WBcHsrdNwqbeaBzRCi7jtk,302
+thirdai/_distributed_bolt/distributed.py,sha256=RmSIzXV53xuPrY1B2X8JQreYNUHWF93WZomtt1-LB-c,3693
+thirdai/_distributed_bolt/utils.py,sha256=ktH7bmI_QuOo5nHOlwwc4714tLckvi_yEg_kc0xP6Cg,914
+thirdai/_distributed_bolt/ray_trainer/train_loop_utils.py,sha256=4sM70hO2sk8hv8u6qkAeLhE0PcsEN6b6gVUaP3OqTAM,704
+thirdai/_distributed_bolt/ray_trainer/bolt_checkpoint.py,sha256=qyoSqMDG_eDQGe3ameop4mRH7lKjxDAHXFTDrxSjbBE,3146
+thirdai/_distributed_bolt/ray_trainer/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+thirdai/_distributed_bolt/ray_trainer/bolt_trainer.py,sha256=dR2azVAypnAdCXgAV-BVQ0lhkmLkCNIr_olAsXYafbs,3367
+thirdai/_deps/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+thirdai/_deps/ColBERT/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+thirdai/_deps/ColBERT/colbertutils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+thirdai/_deps/ColBERT/colbertutils/utils.py,sha256=6d-wtgRB_0rowjJmBsI_ipLfzcDXeeyHSsrjfClmx2c,306
+thirdai/_deps/ColBERT/colbertconfig/config.py,sha256=VSCV3bADYIOo3aIdzes9Our3IOWYiRi-8yyFzIqEHZ8,296
+thirdai/_deps/ColBERT/colbertconfig/base_config.py,sha256=SHviKPwpuFx_YkpynLxdSklmIOEEBm8RC8y7L-ZwIbU,2047
+thirdai/_deps/ColBERT/colbertconfig/core_config.py,sha256=ADTy2I7Wpho_tnImoV-uGrPkg4t26ZeGcaLy_WbXFwI,2237
+thirdai/_deps/ColBERT/colbertconfig/__init__.py,sha256=_y3vibQjGSpTEYhPxQ76lsbkA3PoumyIQ0lodJUXYBc,46
+thirdai/_deps/ColBERT/colbertconfig/settings.py,sha256=oMIU7xRyHHSl99GR-ZjdRzRLu5JL58WvFpoiZiH2zi8,3729
+thirdai/_deps/ColBERT/colbertmodeling/checkpoint.py,sha256=gDwwo42bPZtoHbrmKu8tPYynm3HdWmYYys4fRN79Pto,1372
+thirdai/_deps/ColBERT/colbertmodeling/base_colbert.py,sha256=r0dxWX6R7zaEYPYmYurpK0R-hMnFzDwxmqMHTNYgfr4,1223
+thirdai/_deps/ColBERT/colbertmodeling/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+thirdai/_deps/ColBERT/colbertmodeling/hf_colbert.py,sha256=rY4ttXkRp2TXPVSDYA9V1a2NqYsbMCW7ojacQQ5F3qg,2111
+thirdai/_deps/ColBERT/colbertmodeling/colbert.py,sha256=Kr3LLhYwdhUlKkd5L18dGvZHvmXg0yR0xaW7p9BlH3w,2202
+thirdai/_deps/ColBERT/colbertmodeling/tokenization/doc_tokenization.py,sha256=ZZJw_n3tu4IK5rlm2CNGmKHLOmPYLKEweBXjll60qc8,1997
+thirdai/_deps/ColBERT/colbertmodeling/tokenization/query_tokenization.py,sha256=ULCPTVfxGh2eTFIBYL5eiLpWzOefHkIls6cKSqKhlMA,3187
+thirdai/_deps/ColBERT/colbertmodeling/tokenization/__init__.py,sha256=8eV8__8W3tsQjsZB-PsHyNwHnEaDcGg1aozSmFr4we4,166
+thirdai/telemetry/telemetry_start_and_stop.py,sha256=6Wsk6rJC2YqqXqKs2kycE1_q3SrYWIWFRgLDazfnQVY,5792
+thirdai/telemetry/telemetry_daemon.py,sha256=sKVnrerrcDPZ9hIyIv2Isup_iV_Gzs-Nj1KiI0akkR0,5122
+thirdai/telemetry/__init__.py,sha256=7RVS3xOMxDZGdBITzsRJ_5Jl3eExciBN0Jwqf4Sxvv4,50
+thirdai/data/type_inference.py,sha256=xk6hEcmmw6bk7-jUJXtof-JYSip9F3yLd0AZsAYOs2Q,5589
+thirdai/data/__init__.py,sha256=mjMjMqdrkrD6SBIwSU7Iw35bqCnQ2jpG2j2mXSZLpTI,507
+thirdai/data/column_map_utils.py,sha256=FoWBsur4pErVMSen1AhSVoS7KkhwhqsXMLfxQclBx5k,1905
+thirdai/data/get_udt_columns.py,sha256=IxnmMFlL-YCH0omQ3X3yJjw0w6X_KdOZGRAnEDCWcPE,2408
+thirdai/demos/download_tokenizer_vocabs.py,sha256=iOS6Kwwfk-HuXioxHpu_D5cLC6pHiiWpDzBq6xEOYrE,504
+thirdai/demos/__init__.py,sha256=LS9-lxudxPk-XDXDtZp4u5eed_4YwTVEGVu29GGhFxA,74
+thirdai/demos/download_datasets.py,sha256=YVE81n_J0QEbJG8zTdiQlPR13a0oP8k6QvpdHAbZq54,23993
+thirdai/demos/beir_download_utils.py,sha256=nRxfVA4iVKnr3pOWx_UXUtnNLiQ2lZv5wA5KDLuQ3AQ,8505
```

