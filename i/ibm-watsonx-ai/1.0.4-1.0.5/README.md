# Comparing `tmp/ibm_watsonx_ai-1.0.4.tar.gz` & `tmp/ibm_watsonx_ai-1.0.5.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "ibm_watsonx_ai-1.0.4.tar", last modified: Fri May 10 15:57:19 2024, max compression
+gzip compressed data, was "ibm_watsonx_ai-1.0.5.tar", last modified: Thu May 23 11:05:15 2024, max compression
```

## Comparing `ibm_watsonx_ai-1.0.4.tar` & `ibm_watsonx_ai-1.0.5.tar`

### file list

```diff
@@ -1,749 +1,571 @@
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.738098 ibm_watsonx_ai-1.0.4/
--rw-rw-r--   0 travis    (2000) travis    (2000)     1485 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/LICENSE.txt
--rw-rw-r--   0 travis    (2000) travis    (2000)    28837 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/MANIFEST.in
--rw-r--r--   0 travis    (2000) travis    (2000)     5543 2024-05-10 15:57:19.738098 ibm_watsonx_ai-1.0.4/PKG-INFO
--rw-rw-r--   0 travis    (2000) travis    (2000)      534 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/README.md
--rw-rw-r--   0 travis    (2000) travis    (2000)        6 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/VERSION
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:18.934098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/
--rw-rw-r--   0 travis    (2000) travis    (2000)     7171 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/Set.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      638 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/__init__.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:18.934098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/_wrappers/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/_wrappers/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    10211 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/_wrappers/requests.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    23127 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/assets.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    25125 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/client.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    29362 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/connections.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     7852 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/credentials.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:18.934098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/data_loaders/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/data_loaders/__init__.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:18.938099 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/data_loaders/datasets/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/data_loaders/datasets/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    15227 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/data_loaders/datasets/experiment.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2384 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/data_loaders/experiment.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:18.938099 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/deployment/
--rw-rw-r--   0 travis    (2000) travis    (2000)      334 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/deployment/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    30187 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/deployment/base_deployment.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)    27392 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/deployment/batch.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    12542 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/deployment/web_service.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    71150 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/deployments.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:18.938099 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/experiment/
--rw-rw-r--   0 travis    (2000) travis    (2000)      409 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/experiment/__init__.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:18.938099 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/experiment/autoai/
--rw-rw-r--   0 travis    (2000) travis    (2000)      272 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/experiment/autoai/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    40123 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/experiment/autoai/autoai.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:18.938099 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/experiment/autoai/engines/
--rw-rw-r--   0 travis    (2000) travis    (2000)      429 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/experiment/autoai/engines/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1802 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/experiment/autoai/engines/base_engine.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    66026 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/experiment/autoai/engines/service_engine.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    67208 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/experiment/autoai/engines/wml_engine.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:18.942098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/experiment/autoai/optimizers/
--rw-rw-r--   0 travis    (2000) travis    (2000)      467 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/experiment/autoai/optimizers/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1791 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/experiment/autoai/optimizers/base_auto_pipelines.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    27294 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/experiment/autoai/optimizers/local_auto_pipelines.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    41484 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/experiment/autoai/optimizers/remote_auto_pipelines.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:18.942098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/experiment/autoai/runs/
--rw-rw-r--   0 travis    (2000) travis    (2000)      460 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/experiment/autoai/runs/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    21264 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/experiment/autoai/runs/auto_pipelines_runs.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1409 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/experiment/autoai/runs/base_auto_pipelines_runs.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     8097 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/experiment/autoai/runs/local_auto_pipelines_runs.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:18.942098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/experiment/base_experiment/
--rw-rw-r--   0 travis    (2000) travis    (2000)      358 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/experiment/base_experiment/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      662 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/experiment/base_experiment/base_experiment.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:18.942098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/experiment/fm_tune/
--rw-rw-r--   0 travis    (2000) travis    (2000)      382 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/experiment/fm_tune/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    11022 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/experiment/fm_tune/tune_experiment.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     6554 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/experiment/fm_tune/tune_runs.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    17177 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/experiments.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    18791 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/export_assets.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     8099 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/factsheets.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:18.942098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/federated_learning/
--rw-rw-r--   0 travis    (2000) travis    (2000)      586 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/federated_learning/FLExceptions.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/federated_learning/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     8243 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/federated_learning/data_handler.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    15305 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/federated_learning/data_util.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:18.946098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/
--rw-rw-r--   0 travis    (2000) travis    (2000)      727 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/__init__.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:18.946098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/embeddings/
--rw-rw-r--   0 travis    (2000) travis    (2000)      499 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/embeddings/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1987 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/embeddings/base_embeddings.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    12063 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/embeddings/embeddings.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2124 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/embeddings/sentence_transformer_embeddings.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:18.946098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/extensions/
--rw-rw-r--   0 travis    (2000) travis    (2000)      272 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/extensions/__init__.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:18.946098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/extensions/langchain/
--rw-rw-r--   0 travis    (2000) travis    (2000)      301 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/extensions/langchain/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3708 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/extensions/langchain/llm.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:18.946098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/extensions/rag/
--rw-rw-r--   0 travis    (2000) travis    (2000)      380 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/extensions/rag/__init__.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:18.946098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/extensions/rag/chunker/
--rw-rw-r--   0 travis    (2000) travis    (2000)      455 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/extensions/rag/chunker/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1342 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/extensions/rag/chunker/base_chunker.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1265 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/extensions/rag/chunker/get_chunker.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4324 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/extensions/rag/chunker/langchain_chunker.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:18.946098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/extensions/rag/pattern/
--rw-rw-r--   0 travis    (2000) travis    (2000)      326 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/extensions/rag/pattern/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2823 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/extensions/rag/pattern/default_deployable_function.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    22844 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/extensions/rag/pattern/pattern.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:18.950098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/extensions/rag/utils/
--rw-rw-r--   0 travis    (2000) travis    (2000)      332 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/extensions/rag/utils/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3951 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/extensions/rag/utils/utils.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:18.950098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/extensions/rag/vector_stores/
--rw-rw-r--   0 travis    (2000) travis    (2000)      602 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/extensions/rag/vector_stores/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3401 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/extensions/rag/vector_stores/base_vector_store.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     6477 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/extensions/rag/vector_stores/langchain_vector_store_adapter.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    10833 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/extensions/rag/vector_stores/vector_store.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    11897 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/extensions/rag/vector_stores/vector_store_connector.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:18.950098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/inference/
--rw-rw-r--   0 travis    (2000) travis    (2000)      318 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/inference/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    14889 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/inference/base_model_inference.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    12624 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/inference/deployment_model_inference.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    11574 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/inference/fm_model_inference.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    19339 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/inference/model_inference.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    12212 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/model.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    29211 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/prompt_tuner.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:18.950098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/prompts/
--rw-rw-r--   0 travis    (2000) travis    (2000)      339 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/prompts/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    34441 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/prompts/prompt_template.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:18.958099 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/utils/
--rw-rw-r--   0 travis    (2000) travis    (2000)      665 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/utils/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2463 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/utils/enums.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    23406 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/utils/utils.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     9944 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models_manager.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    33462 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/functions.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:18.958099 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/helpers/
--rw-rw-r--   0 travis    (2000) travis    (2000)      555 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/helpers/__init__.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:18.962098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/helpers/connections/
--rw-rw-r--   0 travis    (2000) travis    (2000)      574 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/helpers/connections/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      549 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/helpers/connections/base_connection.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    56583 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/helpers/connections/base_data_connection.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      599 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/helpers/connections/base_location.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    80550 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/helpers/connections/connections.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    65669 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/helpers/connections/flight_service.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1014 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/helpers/connections/local.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2340 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/helpers/helpers.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     5023 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/hpo.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    29553 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/href_definitions.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    11653 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/hw_spec.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    12335 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/import_assets.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:18.962098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/__init__.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:18.962098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/__init__.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:18.962098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cloud/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cloud/__init__.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:18.962098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cloud/ibmfl/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cloud/ibmfl/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)       21 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cloud/ibmfl/_version.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:18.962098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/__init__.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:18.962098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)       21 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/_version.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:18.962098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/connection/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/connection/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2449 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/connection/route_declarations.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    17919 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/connection/websockets_connection.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:18.966098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/data/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/data/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     7961 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/data/data_handler.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    14762 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/data/data_util.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:18.966098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/model/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/model/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    25835 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/model/pytorch_fl_model.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    23131 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/model/tensorflow_fl_model.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:18.966098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/model/v2/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/model/v2/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    17962 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/model/v2/xgb_fl_model.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    17444 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/model/xgb_fl_model.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.010099 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/party/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/party/__init__.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.010099 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/party/metrics/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/party/metrics/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     5937 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/party/metrics/metrics_recorder.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    14847 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/party/party.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    12607 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/party/party_protocol_handler.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.010099 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/party/training/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/party/training/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1324 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/party/training/fedavg_local_training_handler.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    10608 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/party/training/local_training_handler.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1940 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/party/training/pfnm_local_training_handler.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.010099 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/party/training/v2/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/party/training/v2/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    18932 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/party/training/v2/xgboost_local_training_handler.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    18034 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/party/training/xgboost_local_training_handler.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.010099 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/util/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/util/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    19474 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/util/config.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.010099 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/util/pfnm/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/util/pfnm/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     9831 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/util/pfnm/core.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    12423 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/util/pfnm/matching.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3009 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/util/pfnm/utils.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.014099 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/util/xgboost/
--rw-rw-r--   0 travis    (2000) travis    (2000)      274 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/util/xgboost/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4092 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/util/xgboost/export.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     7751 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/util/xgboost/hyperparams.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      758 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/util/xgboost/utils.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.014099 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd403/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd403/__init__.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.014099 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd403/ibmfl/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd403/ibmfl/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)       21 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd403/ibmfl/_version.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.014099 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd403/ibmfl/connection/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd403/ibmfl/connection/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    17965 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd403/ibmfl/connection/websockets_connection.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.062098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd403/ibmfl/party/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd403/ibmfl/party/__init__.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.062098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd403/ibmfl/party/metrics/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd403/ibmfl/party/metrics/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     8529 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd403/ibmfl/party/metrics/metrics_recorder.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    15121 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd403/ibmfl/party/party.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    12218 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd403/ibmfl/party/party_protocol_handler.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.062098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd403/ibmfl/util/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd403/ibmfl/util/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    20561 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd403/ibmfl/util/config.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.062098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/party/
--rw-rw-r--   0 travis    (2000) travis    (2000)      271 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/party/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      563 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/party/party.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2442 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/party_env_validator.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.062098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/__init__.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.062098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/__init__.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.062098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/connection/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/connection/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2982 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/connection/route_declarations.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.066099 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      872 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/crypto_enumeration.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      705 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/crypto_exceptions.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     9731 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/crypto_library.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.066099 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/helayer/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/helayer/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    17261 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/helayer/fhe.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.102099 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/infra/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/infra/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     6533 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/infra/crypto_asym_imp_rsa.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1432 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/infra/crypto_asym_int.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3085 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/infra/crypto_cert_imp_rsa.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1104 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/infra/crypto_cert_int.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3055 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/infra/crypto_he_imp_hely.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      740 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/infra/crypto_he_int.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      878 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/infra/crypto_sym_imp_fernet.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1235 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/infra/crypto_sym_int.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.102099 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/keys_mng/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/keys_mng/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      823 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/keys_mng/crypto_key_mng_dst.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      652 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/keys_mng/crypto_key_mng_int.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    14383 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/keys_mng/crypto_keys_proto_party.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1212 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/exceptions.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.106099 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/message/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/message/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1009 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/message/message_type.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.106099 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/model/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/model/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    30886 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/model/pytorch_fl_model.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    29774 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/model/tensorflow_fl_model.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    17862 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/model/xgb_fl_model.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.106099 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/party/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/party/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    16342 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/party/party.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    13787 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/party/party_protocol_handler.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.130098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/party/training/
--rw-rw-r--   0 travis    (2000) travis    (2000)      272 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/party/training/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    10617 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/party/training/crypto_local_training_handler.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    13384 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/party/training/local_training_handler.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2168 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/party/training/pfnm_local_training_handler.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    29004 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/party/training/xgboost_local_training_handler.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.130098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/util/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/util/__init__.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.130098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/util/xgboost/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/util/xgboost/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4058 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/util/xgboost/export.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     8063 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/util/xgboost/hyperparams.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1194 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/util/xgboost/utils.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.130098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/__init__.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.130098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/__init__.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.130098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/connection/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/connection/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2982 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/connection/route_declarations.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.134098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/crypto/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/crypto/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      872 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/crypto/crypto_enumeration.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      705 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/crypto/crypto_exceptions.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     9731 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/crypto/crypto_library.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.134098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/crypto/helayer/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/crypto/helayer/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    17261 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/crypto/helayer/fhe.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.178099 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/crypto/infra/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/crypto/infra/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     6533 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/crypto/infra/crypto_asym_imp_rsa.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1432 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/crypto/infra/crypto_asym_int.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3085 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/crypto/infra/crypto_cert_imp_rsa.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1104 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/crypto/infra/crypto_cert_int.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3055 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/crypto/infra/crypto_he_imp_hely.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      740 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/crypto/infra/crypto_he_int.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      878 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/crypto/infra/crypto_sym_imp_fernet.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1235 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/crypto/infra/crypto_sym_int.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.178099 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/crypto/keys_mng/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/crypto/keys_mng/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      823 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/crypto/keys_mng/crypto_key_mng_dst.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      652 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/crypto/keys_mng/crypto_key_mng_int.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    14383 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/crypto/keys_mng/crypto_keys_proto_party.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1212 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/exceptions.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.178099 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/message/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/message/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1009 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/message/message_type.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.178099 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/model/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/model/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    30886 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/model/pytorch_fl_model.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    29774 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/model/tensorflow_fl_model.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    17862 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/model/xgb_fl_model.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.182098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/party/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/party/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    16342 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/party/party.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    13787 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/party/party_protocol_handler.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.202098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/party/training/
--rw-rw-r--   0 travis    (2000) travis    (2000)      272 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/party/training/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    10617 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/party/training/crypto_local_training_handler.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    13384 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/party/training/local_training_handler.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2168 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/party/training/pfnm_local_training_handler.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    29004 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/party/training/xgboost_local_training_handler.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.202098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/util/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/util/__init__.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.202098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/util/xgboost/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/util/xgboost/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4058 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/util/xgboost/export.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     8063 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/util/xgboost/hyperparams.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1194 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/util/xgboost/utils.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.202098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/__init__.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.202098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)       21 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/_version.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.202098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/connection/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/connection/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2927 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/connection/connection.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2450 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/connection/route_declarations.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2382 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/connection/router_handler.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    20803 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/connection/websockets_connection.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.202098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/data/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/data/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     8190 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/data/data_handler.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    15000 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/data/data_util.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      954 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/data/env_data_handler.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1980 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/data/env_spec.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2178 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/data/pandas_data_handler.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      726 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/envs.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1370 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/exceptions.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.262098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/message/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/message/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1491 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/message/json_serializer.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3731 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/message/message.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      901 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/message/message_type.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1552 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/message/pickle_serializer.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      948 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/message/serializer.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1341 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/message/serializer_factory.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      412 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/message/serializer_types.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.270099 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/model/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/model/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     9276 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/model/fl_model.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2451 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/model/model_update.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     7701 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/model/naive_bayes_fl_model.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    27487 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/model/pytorch_fl_model.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    16560 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/model/sklearn_SGD_linear_fl_model.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     6960 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/model/sklearn_fl_model.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     9187 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/model/sklearn_kmeans_fl_model.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    25212 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/model/tensorflow_fl_model.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    17962 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/model/xgb_fl_model.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.274098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/party/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/party/__init__.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.274098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/party/metrics/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/party/metrics/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     8530 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/party/metrics/metrics_recorder.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    15612 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/party/party.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    12198 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/party/party_protocol_handler.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      515 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/party/status_type.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.278098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/party/training/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/party/training/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1432 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/party/training/fedavg_local_training_handler.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    12661 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/party/training/local_training_handler.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2048 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/party/training/pfnm_local_training_handler.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    20446 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/party/training/xgboost_local_training_handler.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.278098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/util/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/util/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    20640 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/util/config.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     7885 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/util/fl_metrics.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     5484 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/util/log_config.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.282098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/util/pfnm/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/util/pfnm/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     9831 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/util/pfnm/core.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    12423 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/util/pfnm/matching.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3009 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/util/pfnm/utils.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.282098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/util/xgboost/
--rw-rw-r--   0 travis    (2000) travis    (2000)      274 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/util/xgboost/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4090 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/util/xgboost/export.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     7757 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/util/xgboost/hyperparams.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      758 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/util/xgboost/utils.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.282098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py39-tf27-pt110-sk10/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py39-tf27-pt110-sk10/__init__.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.282098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py39-tf27-pt110-sk10/ibmfl/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py39-tf27-pt110-sk10/ibmfl/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)       21 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py39-tf27-pt110-sk10/ibmfl/_version.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.286098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py39-tf27-pt110-sk10/ibmfl/model/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py39-tf27-pt110-sk10/ibmfl/model/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    28686 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py39-tf27-pt110-sk10/ibmfl/model/pytorch_fl_model.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    17964 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py39-tf27-pt110-sk10/ibmfl/model/xgb_fl_model.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.286098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py39-tf27-pt110-sk10/ibmfl/party/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py39-tf27-pt110-sk10/ibmfl/party/__init__.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.286098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py39-tf27-pt110-sk10/ibmfl/party/training/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py39-tf27-pt110-sk10/ibmfl/party/training/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    21427 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py39-tf27-pt110-sk10/ibmfl/party/training/xgboost_local_training_handler.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.286098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py39-tf27-pt110-sk10/ibmfl/util/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py39-tf27-pt110-sk10/ibmfl/util/__init__.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.286098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py39-tf27-pt110-sk10/ibmfl/util/xgboost/
--rw-rw-r--   0 travis    (2000) travis    (2000)      274 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py39-tf27-pt110-sk10/ibmfl/util/xgboost/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4221 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py39-tf27-pt110-sk10/ibmfl/util/xgboost/export.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     7751 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py39-tf27-pt110-sk10/ibmfl/util/xgboost/hyperparams.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.302098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/
--rw-rw-r--   0 travis    (2000) travis    (2000)     5756 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/TestRepoSaveLoad.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      523 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1126 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/base_constants.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3155 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/ml_api_client.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     6936 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/ml_authorization.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.310099 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepository/
--rw-rw-r--   0 travis    (2000) travis    (2000)     1339 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepository/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4144 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepository/artifact.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      623 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepository/artifact_reader.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      829 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepository/generic_archive_model_artifact.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      859 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepository/generic_archive_pipeline_artifact.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      797 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepository/hybrid_model_artifact.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    11171 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepository/meta_names.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1238 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepository/meta_props.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1125 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepository/model_artifact.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      736 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepository/pipeline_artifact.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      962 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepository/scikit_model_artifact.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      814 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepository/tensorflow_model_artifact.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      759 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepository/wml_experiment_artifact.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      741 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepository/wml_function_artifact.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      754 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepository/wml_libraries_artifact.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      753 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepository/wml_runtimes_artifact.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      800 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepository/xgboost_model_artifact.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.394098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/
--rw-rw-r--   0 travis    (2000) travis    (2000)     2456 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     6340 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/content_loaders.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1614 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/experiment_artifact.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1625 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/function_artifact.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1219 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/function_artifact_loader.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      838 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/function_artifact_reader.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      836 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/function_loader.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     6123 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/generic_archive_pipeline_model_artifact.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1329 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/generic_file_artifact_loader.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      947 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/generic_file_pipeline_model_loader.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1123 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/generic_file_reader.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4485 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/hybrid_artifact_loader.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1924 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/hybrid_pipeline_model_artifact.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      892 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/hybrid_pipeline_model_loader.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2006 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/hybrid_pipeline_model_reader.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3331 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/libraries_artifact.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1655 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/libraries_artifact_loader.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      830 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/libraries_artifact_reader.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      637 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/libraries_loader.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    19342 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/ml_repository_artifact.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      428 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/python_version.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2548 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/runtimes_artifact.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1698 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/runtimes_artifact_loader.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      835 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/runtimes_artifact_reader.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2236 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/scikit_artifact_loader.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    15122 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/scikit_pipeline_model_artifact.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1157 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/scikit_pipeline_model_loader.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2841 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/scikit_pipeline_reader.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1675 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/spark_artifact_loader.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2561 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/spark_pipeline_artifact.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      579 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/spark_pipeline_loader.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    10874 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/spark_pipeline_model_artifact.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      610 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/spark_pipeline_model_loader.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2196 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/spark_pipeline_reader.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      851 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/spark_version.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1745 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/tensorflow_artifact_loader.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     9805 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/tensorflow_pipeline_model_artifact.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1995 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/tensorflow_pipeline_model_loader.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3705 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/tensorflow_pipeline_reader.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     6113 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/version_helper.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2693 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/xgboost_model_reader.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.398099 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryclient/
--rw-rw-r--   0 travis    (2000) travis    (2000)     1320 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryclient/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2485 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryclient/content_reader.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     6393 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryclient/experiment_adapter.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    17498 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryclient/experiment_collection.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     5963 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryclient/function_adapter.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    14711 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryclient/function_collection.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2766 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryclient/libraries_adapter.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     9550 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryclient/libraries_collection.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    10656 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryclient/ml_repository_api.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4736 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryclient/ml_repository_client.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    14715 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryclient/model_adapter.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    44340 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryclient/model_collection.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2600 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryclient/runtimes_adapter.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     9728 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryclient/runtimes_collection.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2657 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryclient/wml_experiment_adapter.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    22031 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryclient/wml_experiment_collection.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.398099 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/
--rw-rw-r--   0 travis    (2000) travis    (2000)    11676 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    22137 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/api_client.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.402099 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/apis/
--rw-rw-r--   0 travis    (2000) travis    (2000)      418 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/apis/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)   258565 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/apis/repository_api.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    11443 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/apis/token_api.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     8770 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/configuration.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.706098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/
--rw-rw-r--   0 travis    (2000) travis    (2000)    11010 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2361 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/array_data_input_repository.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4520 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/array_model_metrics_output.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2993 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/array_model_output.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2407 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/array_model_version_metrics_experiments.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4538 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/array_model_version_output.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4502 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/array_pipeline_output.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3083 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/array_pipeline_version_output.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3119 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/array_training_output_experiments.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2939 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/artifact_author.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4140 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/artifact_version_metadata.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4079 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/artifact_version_repository.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3500 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/artifact_version_short_metadata.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2867 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/author_experiments.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2851 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/author_repository.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3540 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/batch_deploy_output.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3080 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/batch_deploy_output_entity.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     5834 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/batch_deploy_output_entity_execution.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3430 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/batch_deploy_output_meta.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4981 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/batch_input.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3492 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/batch_output.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2993 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/batch_output_array.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2357 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/cols_output.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3548 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/compute_configuration_experiments.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2357 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/connection.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4323 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/connection_object_source_experiments.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4323 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/connection_object_target_experiments.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4622 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/connection_object_with_name_repository.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4260 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/content_location.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3936 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/content_status.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2337 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/custom_models.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3854 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/deploy_input.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3509 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/error_bad_request_libraries_target.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4655 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/error_experiments.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3440 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/error_experiments_target.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4615 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/error_message.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4557 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/error_repository.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3397 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/error_repository_target.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3551 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/error_schema_experiments.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3499 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/error_schema_repository.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3688 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/evaluation_definition.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3966 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/evaluation_definition_experiments.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3730 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/evaluation_definition_metrics.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4060 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/evaluation_definition_repository.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4753 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/evaluation_definition_repository_metrics.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     6669 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/experiment_input.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     5829 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/experiment_input_settings.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3624 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/experiment_output.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4835 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/experiment_output_array.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2903 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/experiment_output_array_first.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2367 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/experiment_patch.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     8209 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/experiment_status_experiments.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     5724 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/framework_output_repository.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3593 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/framework_output_repository_libraries.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3581 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/framework_output_repository_runtimes.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4764 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     6739 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters_experiments.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     5018 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters_experiments_double_range.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     5018 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters_experiments_inner_double_range.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4946 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters_experiments_inner_int_range.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4262 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters_experiments_inner_values_range.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4946 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters_experiments_int_range.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2407 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters_for_status_experiments.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3808 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters_for_status_experiments_inner.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4050 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters_optimization_experiments.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4206 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters_optimization_experiments_method.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     5398 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters_optimization_experiments_method_parameters.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2367 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/input_data_schema.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3688 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/internal_input_batch.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3691 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/internal_output_batch.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2365 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/json_patch_array.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4113 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/json_patch_entity.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     5457 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/libraries_definition_input.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3611 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/libraries_definition_input_platform.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2942 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/meta_object.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3041 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/meta_object_experiments.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4898 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/meta_object_experiments_metadata.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4781 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/meta_object_functions_metadata.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     7380 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/meta_object_metadata.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3022 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/meta_object_repository.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     7694 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/meta_object_repository_metadata.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3518 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/metric_object_experiments.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2339 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/metrics_models.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     6527 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_experiment_input.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3701 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_experiment_output.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3154 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_experiment_output_array.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     8107 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_function_input.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3654 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_functions_output.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     5173 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_functions_output_array.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2971 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_functions_output_array_first.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3691 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_libraries_output.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     5173 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_libraries_output_array.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2971 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_libraries_output_array_first.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    19747 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_model_input.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3651 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_model_output.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3088 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_model_output_array.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3741 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_patch_libraries_output.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3713 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_runtime_spec_output.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     5227 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_runtime_spec_output_array.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2983 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_runtime_spec_output_array_first.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2167 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_get_presigned_url_output.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2178 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_model_size_output.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3574 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_patch_function.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2383 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_patch_libraries_input.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2387 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_patch_runtime_spec_input.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3695 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_patch_runtime_spec_output.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3710 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_upload_content_output.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3102 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_upload_content_output_entity.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4925 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_upload_content_output_metadata.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     5411 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/model_content_location.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2835 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/model_definition_models.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     9767 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/model_input.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3642 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/model_metrics.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3534 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/model_metrics_values.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3525 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/model_output.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    11672 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/model_output_entity.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3848 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/model_output_entity_pipeline_version.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3754 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/model_schemas.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3850 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/model_training_data_ref.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2355 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/model_type.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3968 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/model_version_input.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3656 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/model_version_metrics_experiments.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3616 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/model_version_output.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     8696 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/model_version_output_entity.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3606 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/model_version_output_entity_model.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3562 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/online_deploy_output.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3502 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/online_output.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3040 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/online_output_array.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2345 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/output_data_schema.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4916 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/patch_operation_experiments.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4857 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/patch_operation_functions.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4857 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/patch_operation_libraries.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4893 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/patch_operation_runtime_spec.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     5723 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/pipeline_input.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3335 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/pipeline_models.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3564 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/pipeline_output.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     6715 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/pipeline_output_entity.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2361 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/pipeline_type.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3303 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/pipeline_version_input.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3655 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/pipeline_version_output.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3935 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/pipeline_version_output_entity.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3694 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/pipeline_version_output_entity_parent.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2373 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/runtime_environment.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2843 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/runtime_models.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3493 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/runtime_output_repository.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     6179 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/runtime_spec_definition_input.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3085 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/runtime_spec_definition_input_custom_libraries.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3603 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/runtime_spec_definition_input_platform.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4498 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/runtime_spec_definition_input_public_libraries.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2953 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/runtime_spec_definition_input_repository.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2365 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/sample_scoring_input.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     5174 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/schemas.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2988 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/score_input.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2888 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/score_output.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3797 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/size_models.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2834 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/software_spec_models.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3313 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/space_models.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3715 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/spark_service.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3830 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/stream_input_internal.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     5067 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/stream_internal.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3502 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/stream_output.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3002 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/stream_output_array.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3900 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/stream_output_internal.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3494 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/tag_repository.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2911 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/token_response.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2366 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/training_data_schema.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2849 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/training_models.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3665 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/training_output_experiments.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     7857 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/training_reference_experiments.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    13762 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/training_status_experiments.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3131 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/training_status_experiments_result.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     8967 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/rest.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.710098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/util/
--rw-rw-r--   0 travis    (2000) travis    (2000)      469 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/util/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      662 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/util/base_singleton.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1115 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/util/compression_util.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      486 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/util/exceptions.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      592 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/util/file_system_ops.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      936 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/util/generic_archive_file_check.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1208 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/util/json_2_object_mapper.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3942 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/util/library_imports.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2459 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/util/spark_util.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      464 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/util/unique_id_gen.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      569 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/lifecycle.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.714099 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/messages/
--rw-rw-r--   0 travis    (2000) travis    (2000)      272 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/messages/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      390 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/messages/globalization_util.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2096 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/messages/messages.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     9557 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/messages/messages_en.json
--rw-rw-r--   0 travis    (2000) travis    (2000)    89079 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/metanames.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    37934 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/model_definition.py
--rw-rw-r--   0 travis    (2000) travis    (2000)   140580 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/models.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    12707 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/parameter_sets.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    25727 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/party_wrapper.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    23763 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/pipelines.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    18435 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/pkg_extn.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    19099 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/remote_training_system.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    35116 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/repository.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    40561 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/script.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    16118 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/service_instance.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    32617 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/shiny.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    30684 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/spaces.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    19653 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/sw_spec.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     8059 2024-05-10 15:55:06.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/task_credentials.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    44682 2024-05-10 15:55:07.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/training.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.714099 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/utils/
--rw-rw-r--   0 travis    (2000) travis    (2000)       11 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/utils/API_VERSION_PARAM
--rw-rw-r--   0 travis    (2000) travis    (2000)      361 2024-05-10 15:55:07.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/utils/__init__.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.718098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/utils/autoai/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:55:07.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/utils/autoai/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     8509 2024-05-10 15:55:07.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/utils/autoai/connection.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    12958 2024-05-10 15:55:07.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/utils/autoai/enums.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    18531 2024-05-10 15:55:07.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/utils/autoai/errors.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     5633 2024-05-10 15:55:07.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/utils/autoai/fairness.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3242 2024-05-10 15:55:07.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/utils/autoai/incremental.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3151 2024-05-10 15:55:07.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/utils/autoai/local_training_message_handler.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2089 2024-05-10 15:55:07.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/utils/autoai/progress_bar.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1677 2024-05-10 15:55:07.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/utils/autoai/training.py
--rw-rw-r--   0 travis    (2000) travis    (2000)   102128 2024-05-10 15:55:07.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/utils/autoai/utils.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3229 2024-05-10 15:55:07.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/utils/cpd_version.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.722098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/utils/deployment/
--rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-10 15:55:07.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/utils/deployment/__init__.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)     3975 2024-05-10 15:55:07.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/utils/deployment/errors.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      533 2024-05-10 15:55:07.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/utils/enums.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    27056 2024-05-10 15:55:07.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/utils/utils.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    18441 2024-05-10 15:55:07.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/volumes.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    11206 2024-05-10 15:55:07.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/wml_client_error.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    24453 2024-05-10 15:55:07.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/wml_resource.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.722098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/workspace/
--rw-rw-r--   0 travis    (2000) travis    (2000)      330 2024-05-10 15:55:07.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/workspace/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     7634 2024-05-10 15:55:07.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/workspace/workspace.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.722098 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai.egg-info/
--rw-r--r--   0 travis    (2000) travis    (2000)     5543 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai.egg-info/PKG-INFO
--rw-rw-r--   0 travis    (2000) travis    (2000)    41901 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai.egg-info/SOURCES.txt
--rw-rw-r--   0 travis    (2000) travis    (2000)        1 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai.egg-info/dependency_links.txt
--rw-rw-r--   0 travis    (2000) travis    (2000)     1042 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai.egg-info/requires.txt
--rw-rw-r--   0 travis    (2000) travis    (2000)       21 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibm_watsonx_ai.egg-info/top_level.txt
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.722098 ibm_watsonx_ai-1.0.4/ibmfl/
--rw-rw-r--   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibmfl/__init__.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.722098 ibm_watsonx_ai-1.0.4/ibmfl/data/
--rw-rw-r--   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibmfl/data/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      882 2024-05-10 15:55:07.000000 ibm_watsonx_ai-1.0.4/ibmfl/data/data_handler.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:19.722098 ibm_watsonx_ai-1.0.4/ibmfl/party/
--rw-rw-r--   0 travis    (2000) travis    (2000)        0 2024-05-10 15:57:18.000000 ibm_watsonx_ai-1.0.4/ibmfl/party/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      519 2024-05-10 15:55:07.000000 ibm_watsonx_ai-1.0.4/ibmfl/party/party.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      327 2024-05-10 15:55:07.000000 ibm_watsonx_ai-1.0.4/ibmfl/party_env_validator.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      228 2024-05-10 15:57:19.738098 ibm_watsonx_ai-1.0.4/setup.cfg
--rw-rw-r--   0 travis    (2000) travis    (2000)     4629 2024-05-10 15:55:07.000000 ibm_watsonx_ai-1.0.4/setup.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.714302 ibm_watsonx_ai-1.0.5/
+-rw-rw-r--   0 travis    (2000) travis    (2000)     1485 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/LICENSE.txt
+-rw-rw-r--   0 travis    (2000) travis    (2000)      112 2024-05-23 11:05:15.000000 ibm_watsonx_ai-1.0.5/MANIFEST.in
+-rw-r--r--   0 travis    (2000) travis    (2000)     3866 2024-05-23 11:05:15.714302 ibm_watsonx_ai-1.0.5/PKG-INFO
+-rw-rw-r--   0 travis    (2000) travis    (2000)      534 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/README.md
+-rw-rw-r--   0 travis    (2000) travis    (2000)        6 2024-05-23 11:05:15.000000 ibm_watsonx_ai-1.0.5/VERSION
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.394302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/
+-rw-rw-r--   0 travis    (2000) travis    (2000)     7171 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/Set.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      638 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/__init__.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.394302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/_wrappers/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      272 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/_wrappers/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    10211 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/_wrappers/requests.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    23127 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/assets.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    25266 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/client.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    29363 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/connections.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     7852 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/credentials.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.394302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/data_loaders/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      272 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/data_loaders/__init__.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.394302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/data_loaders/datasets/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      272 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/data_loaders/datasets/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    15227 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/data_loaders/datasets/experiment.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2384 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/data_loaders/experiment.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.394302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/deployment/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      334 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/deployment/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    30187 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/deployment/base_deployment.py
+-rwxrwxr-x   0 travis    (2000) travis    (2000)    27392 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/deployment/batch.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    12542 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/deployment/web_service.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    71182 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/deployments.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.394302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/experiment/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      409 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/experiment/__init__.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.394302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/experiment/autoai/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      272 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/experiment/autoai/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    41773 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/experiment/autoai/autoai.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.394302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/experiment/autoai/engines/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      429 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/experiment/autoai/engines/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     1799 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/experiment/autoai/engines/base_engine.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    75771 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/experiment/autoai/engines/service_engine.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    77036 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/experiment/autoai/engines/wml_engine.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.398301 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/experiment/autoai/optimizers/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      485 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/experiment/autoai/optimizers/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     1771 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/experiment/autoai/optimizers/base_auto_pipelines.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    28449 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/experiment/autoai/optimizers/local_auto_pipelines.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    42247 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/experiment/autoai/optimizers/remote_auto_pipelines.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.398301 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/experiment/autoai/runs/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      469 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/experiment/autoai/runs/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    22334 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/experiment/autoai/runs/auto_pipelines_runs.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     1403 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/experiment/autoai/runs/base_auto_pipelines_runs.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     8280 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/experiment/autoai/runs/local_auto_pipelines_runs.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.398301 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/experiment/base_experiment/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      358 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/experiment/base_experiment/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      656 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/experiment/base_experiment/base_experiment.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.398301 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/experiment/fm_tune/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      382 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/experiment/fm_tune/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    10702 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/experiment/fm_tune/tune_experiment.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     6296 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/experiment/fm_tune/tune_runs.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    17177 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/experiments.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    18792 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/export_assets.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     8100 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/factsheets.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.398301 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/federated_learning/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      588 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/federated_learning/FLExceptions.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      272 2024-05-23 11:05:15.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/federated_learning/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     8975 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/federated_learning/data_handler.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    15237 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/federated_learning/data_util.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.398301 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      728 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/__init__.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.398301 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/embeddings/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      499 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/embeddings/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2163 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/embeddings/base_embeddings.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    12154 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/embeddings/embeddings.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2094 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/embeddings/sentence_transformer_embeddings.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.398301 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/extensions/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      272 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/extensions/__init__.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.398301 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/extensions/langchain/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      301 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/extensions/langchain/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3720 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/extensions/langchain/llm.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.398301 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/extensions/rag/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      380 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/extensions/rag/__init__.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.398301 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/extensions/rag/chunker/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      469 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/extensions/rag/chunker/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     1324 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/extensions/rag/chunker/base_chunker.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     1279 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/extensions/rag/chunker/get_chunker.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     4399 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/extensions/rag/chunker/langchain_chunker.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.402302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/extensions/rag/pattern/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      326 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/extensions/rag/pattern/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3045 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/extensions/rag/pattern/default_deployable_function.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    22852 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/extensions/rag/pattern/pattern.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.402302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/extensions/rag/utils/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      332 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/extensions/rag/utils/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     4061 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/extensions/rag/utils/utils.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.402302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/extensions/rag/vector_stores/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      602 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/extensions/rag/vector_stores/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3430 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/extensions/rag/vector_stores/base_vector_store.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     6606 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/extensions/rag/vector_stores/langchain_vector_store_adapter.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    11013 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/extensions/rag/vector_stores/vector_store.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    12499 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/extensions/rag/vector_stores/vector_store_connector.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.402302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/inference/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      318 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/inference/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    14889 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/inference/base_model_inference.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    12624 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/inference/deployment_model_inference.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    11578 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/inference/fm_model_inference.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    19231 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/inference/model_inference.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    12189 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/model.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    29211 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/prompt_tuner.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.402302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/prompts/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      339 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/prompts/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    35393 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/prompts/prompt_template.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.402302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/utils/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      665 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/utils/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3638 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/utils/enums.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    23934 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/utils/utils.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    11497 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models_manager.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    33462 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/functions.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.402302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/helpers/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      589 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/helpers/__init__.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.402302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/helpers/connections/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      615 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/helpers/connections/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      543 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/helpers/connections/base_connection.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    56583 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/helpers/connections/base_data_connection.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      594 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/helpers/connections/base_location.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    80549 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/helpers/connections/connections.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    69636 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/helpers/connections/flight_service.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     1012 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/helpers/connections/local.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2341 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/helpers/helpers.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     5023 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/hpo.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    29531 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/href_definitions.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    11653 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/hw_spec.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    12335 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/import_assets.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.406302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/__init__.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.406302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      273 2024-05-23 11:05:15.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/__init__.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.406302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/party/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      271 2024-05-23 11:05:15.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/party/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      564 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/party/party.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2442 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/party_env_validator.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.406302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/
+-rw-rw-r--   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/__init__.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.406302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      268 2024-05-23 11:05:15.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)       21 2024-05-23 11:05:15.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/_version.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.406302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/connection/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      268 2024-05-23 11:05:15.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/connection/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2925 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/connection/connection.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2976 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/connection/route_declarations.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2378 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/connection/router_handler.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    20201 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/connection/websockets_connection.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.406302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      268 2024-05-23 11:05:15.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      867 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/crypto_enumeration.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      700 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/crypto_exceptions.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     9714 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/crypto_library.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.410302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/helayer/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      268 2024-05-23 11:05:15.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/helayer/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    17240 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/helayer/fhe.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.434301 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/infra/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      268 2024-05-23 11:05:15.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/infra/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     6529 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/infra/crypto_asym_imp_rsa.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     1428 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/infra/crypto_asym_int.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3081 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/infra/crypto_cert_imp_rsa.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     1100 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/infra/crypto_cert_int.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3051 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/infra/crypto_he_imp_hely.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      736 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/infra/crypto_he_int.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      874 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/infra/crypto_sym_imp_fernet.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     1231 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/infra/crypto_sym_int.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.438302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/keys_mng/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      268 2024-05-23 11:05:15.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/keys_mng/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      819 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/keys_mng/crypto_key_mng_dst.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      648 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/keys_mng/crypto_key_mng_int.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    14384 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/keys_mng/crypto_keys_proto_party.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.438302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/data/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      268 2024-05-23 11:05:15.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/data/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     8545 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/data/data_handler.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    14720 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/data/data_util.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      943 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/data/env_data_handler.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     1978 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/data/env_spec.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2173 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/data/pandas_data_handler.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      721 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/envs.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     1261 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/exceptions.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.442302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/message/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      268 2024-05-23 11:05:15.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/message/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     4588 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/message/message.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     1005 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/message/message_type.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.446302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/model/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      268 2024-05-23 11:05:15.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/model/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     9507 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/model/fl_model.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      314 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/model/model_update.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     7648 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/model/naive_bayes_fl_model.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    30874 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/model/pytorch_fl_model.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    15859 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/model/sklearn_SGD_linear_fl_model.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     6917 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/model/sklearn_fl_model.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     8896 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/model/sklearn_kmeans_fl_model.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    29765 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/model/tensorflow_fl_model.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    17890 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/model/xgb_fl_model.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.446302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/party/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      268 2024-05-23 11:05:15.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/party/__init__.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.446302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/party/metrics/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      268 2024-05-23 11:05:15.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/party/metrics/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     8525 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/party/metrics/metrics_recorder.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    16706 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/party/party.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    13679 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/party/party_protocol_handler.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      511 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/party/status_type.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.446302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/party/training/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      268 2024-05-23 11:05:15.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/party/training/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    10631 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/party/training/crypto_local_training_handler.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     1354 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/party/training/fedavg_local_training_handler.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    13380 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/party/training/local_training_handler.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2158 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/party/training/pfnm_local_training_handler.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    29002 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/party/training/xgboost_local_training_handler.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.446302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      268 2024-05-23 11:05:15.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    19463 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/config.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     7576 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/fl_metrics.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     5145 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/log_config.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.450302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/persistence/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      268 2024-05-23 11:05:15.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/persistence/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      964 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/persistence/sklearn.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.450302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/pfnm/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      268 2024-05-23 11:05:15.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/pfnm/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     9461 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/pfnm/core.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    11898 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/pfnm/matching.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3005 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/pfnm/utils.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.454302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/serialization/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      268 2024-05-23 11:05:15.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/serialization/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2086 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/serialization/pack.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      815 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/serialization/serializer.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2508 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/serialization/serializers.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.454302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/xgboost/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      268 2024-05-23 11:05:15.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/xgboost/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     4062 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/xgboost/export.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     8058 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/xgboost/hyperparams.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     1189 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/xgboost/utils.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.454302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/
+-rw-rw-r--   0 travis    (2000) travis    (2000)     5756 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/TestRepoSaveLoad.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      523 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     1126 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/base_constants.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3155 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/ml_api_client.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     6936 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/ml_authorization.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.458302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepository/
+-rw-rw-r--   0 travis    (2000) travis    (2000)     1339 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepository/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     4144 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepository/artifact.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      623 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepository/artifact_reader.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      829 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepository/generic_archive_model_artifact.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      859 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepository/generic_archive_pipeline_artifact.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      797 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepository/hybrid_model_artifact.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    11171 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepository/meta_names.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     1238 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepository/meta_props.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     1125 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepository/model_artifact.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      736 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepository/pipeline_artifact.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      962 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepository/scikit_model_artifact.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      814 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepository/tensorflow_model_artifact.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      759 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepository/wml_experiment_artifact.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      741 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepository/wml_function_artifact.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      754 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepository/wml_libraries_artifact.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      753 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepository/wml_runtimes_artifact.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      800 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepository/xgboost_model_artifact.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.522302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2456 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     6340 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/content_loaders.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     1614 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/experiment_artifact.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     1625 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/function_artifact.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     1219 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/function_artifact_loader.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      838 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/function_artifact_reader.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      836 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/function_loader.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     6123 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/generic_archive_pipeline_model_artifact.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     1329 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/generic_file_artifact_loader.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      947 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/generic_file_pipeline_model_loader.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     1123 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/generic_file_reader.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     4485 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/hybrid_artifact_loader.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     1924 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/hybrid_pipeline_model_artifact.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      892 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/hybrid_pipeline_model_loader.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2006 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/hybrid_pipeline_model_reader.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3331 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/libraries_artifact.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     1655 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/libraries_artifact_loader.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      830 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/libraries_artifact_reader.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      637 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/libraries_loader.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    19342 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/ml_repository_artifact.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      428 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/python_version.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2548 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/runtimes_artifact.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     1698 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/runtimes_artifact_loader.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      835 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/runtimes_artifact_reader.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2236 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/scikit_artifact_loader.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    15122 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/scikit_pipeline_model_artifact.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     1157 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/scikit_pipeline_model_loader.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2841 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/scikit_pipeline_reader.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     1675 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/spark_artifact_loader.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2561 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/spark_pipeline_artifact.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      579 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/spark_pipeline_loader.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    10874 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/spark_pipeline_model_artifact.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      610 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/spark_pipeline_model_loader.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2196 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/spark_pipeline_reader.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      851 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/spark_version.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     1745 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/tensorflow_artifact_loader.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     9805 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/tensorflow_pipeline_model_artifact.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     1995 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/tensorflow_pipeline_model_loader.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3705 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/tensorflow_pipeline_reader.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     6113 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/version_helper.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2693 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/xgboost_model_reader.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.526301 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryclient/
+-rw-rw-r--   0 travis    (2000) travis    (2000)     1320 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryclient/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2485 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryclient/content_reader.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     6393 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryclient/experiment_adapter.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    17498 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryclient/experiment_collection.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     5963 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryclient/function_adapter.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    14711 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryclient/function_collection.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2766 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryclient/libraries_adapter.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     9550 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryclient/libraries_collection.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    10656 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryclient/ml_repository_api.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     4736 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryclient/ml_repository_client.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    14715 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryclient/model_adapter.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    44340 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryclient/model_collection.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2600 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryclient/runtimes_adapter.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     9728 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryclient/runtimes_collection.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2657 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryclient/wml_experiment_adapter.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    22031 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryclient/wml_experiment_collection.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.530302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/
+-rw-rw-r--   0 travis    (2000) travis    (2000)    11676 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    22137 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/api_client.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.534302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/apis/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      418 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/apis/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)   258565 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/apis/repository_api.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    11443 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/apis/token_api.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     8770 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/configuration.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.678302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/
+-rw-rw-r--   0 travis    (2000) travis    (2000)    11010 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2361 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/array_data_input_repository.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     4520 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/array_model_metrics_output.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2993 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/array_model_output.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2407 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/array_model_version_metrics_experiments.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     4538 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/array_model_version_output.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     4502 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/array_pipeline_output.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3083 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/array_pipeline_version_output.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3119 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/array_training_output_experiments.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2939 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/artifact_author.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     4140 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/artifact_version_metadata.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     4079 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/artifact_version_repository.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3500 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/artifact_version_short_metadata.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2867 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/author_experiments.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2851 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/author_repository.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3540 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/batch_deploy_output.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3080 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/batch_deploy_output_entity.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     5834 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/batch_deploy_output_entity_execution.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3430 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/batch_deploy_output_meta.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     4981 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/batch_input.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3492 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/batch_output.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2993 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/batch_output_array.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2357 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/cols_output.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3548 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/compute_configuration_experiments.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2357 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/connection.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     4323 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/connection_object_source_experiments.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     4323 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/connection_object_target_experiments.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     4622 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/connection_object_with_name_repository.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     4260 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/content_location.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3936 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/content_status.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2337 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/custom_models.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3854 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/deploy_input.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3509 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/error_bad_request_libraries_target.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     4655 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/error_experiments.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3440 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/error_experiments_target.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     4615 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/error_message.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     4557 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/error_repository.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3397 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/error_repository_target.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3551 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/error_schema_experiments.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3499 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/error_schema_repository.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3688 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/evaluation_definition.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3966 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/evaluation_definition_experiments.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3730 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/evaluation_definition_metrics.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     4060 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/evaluation_definition_repository.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     4753 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/evaluation_definition_repository_metrics.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     6669 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/experiment_input.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     5829 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/experiment_input_settings.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3624 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/experiment_output.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     4835 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/experiment_output_array.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2903 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/experiment_output_array_first.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2367 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/experiment_patch.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     8209 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/experiment_status_experiments.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     5724 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/framework_output_repository.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3593 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/framework_output_repository_libraries.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3581 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/framework_output_repository_runtimes.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     4764 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     6739 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters_experiments.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     5018 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters_experiments_double_range.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     5018 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters_experiments_inner_double_range.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     4946 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters_experiments_inner_int_range.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     4262 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters_experiments_inner_values_range.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     4946 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters_experiments_int_range.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2407 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters_for_status_experiments.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3808 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters_for_status_experiments_inner.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     4050 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters_optimization_experiments.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     4206 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters_optimization_experiments_method.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     5398 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters_optimization_experiments_method_parameters.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2367 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/input_data_schema.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3688 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/internal_input_batch.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3691 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/internal_output_batch.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2365 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/json_patch_array.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     4113 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/json_patch_entity.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     5457 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/libraries_definition_input.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3611 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/libraries_definition_input_platform.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2942 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/meta_object.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3041 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/meta_object_experiments.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     4898 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/meta_object_experiments_metadata.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     4781 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/meta_object_functions_metadata.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     7380 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/meta_object_metadata.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3022 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/meta_object_repository.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     7694 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/meta_object_repository_metadata.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3518 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/metric_object_experiments.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2339 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/metrics_models.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     6527 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_experiment_input.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3701 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_experiment_output.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3154 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_experiment_output_array.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     8107 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_function_input.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3654 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_functions_output.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     5173 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_functions_output_array.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2971 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_functions_output_array_first.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3691 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_libraries_output.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     5173 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_libraries_output_array.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2971 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_libraries_output_array_first.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    19747 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_model_input.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3651 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_model_output.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3088 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_model_output_array.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3741 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_patch_libraries_output.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3713 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_runtime_spec_output.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     5227 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_runtime_spec_output_array.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2983 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_runtime_spec_output_array_first.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2167 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_get_presigned_url_output.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2178 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_model_size_output.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3574 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_patch_function.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2383 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_patch_libraries_input.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2387 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_patch_runtime_spec_input.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3695 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_patch_runtime_spec_output.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3710 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_upload_content_output.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3102 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_upload_content_output_entity.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     4925 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_upload_content_output_metadata.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     5411 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/model_content_location.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2835 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/model_definition_models.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     9767 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/model_input.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3642 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/model_metrics.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3534 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/model_metrics_values.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3525 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/model_output.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    11672 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/model_output_entity.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3848 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/model_output_entity_pipeline_version.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3754 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/model_schemas.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3850 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/model_training_data_ref.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2355 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/model_type.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3968 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/model_version_input.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3656 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/model_version_metrics_experiments.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3616 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/model_version_output.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     8696 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/model_version_output_entity.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3606 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/model_version_output_entity_model.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3562 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/online_deploy_output.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3502 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/online_output.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3040 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/online_output_array.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2345 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/output_data_schema.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     4916 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/patch_operation_experiments.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     4857 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/patch_operation_functions.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     4857 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/patch_operation_libraries.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     4893 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/patch_operation_runtime_spec.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     5723 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/pipeline_input.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3335 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/pipeline_models.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3564 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/pipeline_output.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     6715 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/pipeline_output_entity.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2361 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/pipeline_type.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3303 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/pipeline_version_input.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3655 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/pipeline_version_output.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3935 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/pipeline_version_output_entity.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3694 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/pipeline_version_output_entity_parent.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2373 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/runtime_environment.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2843 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/runtime_models.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3493 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/runtime_output_repository.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     6179 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/runtime_spec_definition_input.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3085 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/runtime_spec_definition_input_custom_libraries.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3603 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/runtime_spec_definition_input_platform.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     4498 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/runtime_spec_definition_input_public_libraries.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2953 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/runtime_spec_definition_input_repository.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2365 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/sample_scoring_input.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     5174 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/schemas.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2988 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/score_input.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2888 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/score_output.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3797 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/size_models.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2834 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/software_spec_models.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3313 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/space_models.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3715 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/spark_service.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3830 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/stream_input_internal.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     5067 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/stream_internal.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3502 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/stream_output.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3002 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/stream_output_array.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3900 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/stream_output_internal.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3494 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/tag_repository.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2911 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/token_response.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2366 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/training_data_schema.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2849 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/training_models.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3665 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/training_output_experiments.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     7857 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/training_reference_experiments.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    13762 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/training_status_experiments.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3131 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/training_status_experiments_result.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     8967 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/rest.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.682302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/util/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      469 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/util/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      662 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/util/base_singleton.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     1115 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/util/compression_util.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      486 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/util/exceptions.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      592 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/util/file_system_ops.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      936 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/util/generic_archive_file_check.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     1208 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/util/json_2_object_mapper.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3942 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/util/library_imports.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2459 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/util/spark_util.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      464 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/util/unique_id_gen.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      569 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/lifecycle.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.698302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/messages/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      272 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/messages/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      389 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/messages/globalization_util.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2095 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/messages/messages.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     9557 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/messages/messages_en.json
+-rw-rw-r--   0 travis    (2000) travis    (2000)    89233 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/metanames.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    37949 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/model_definition.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)   140150 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/models.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    12707 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/parameter_sets.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    22054 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/party_wrapper.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    23763 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/pipelines.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    18436 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/pkg_extn.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    19099 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/remote_training_system.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    35116 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/repository.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    40614 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/script.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    16118 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/service_instance.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    32617 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/shiny.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    30724 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/spaces.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    19653 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/sw_spec.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     8045 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/task_credentials.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    44686 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/training.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.702302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/utils/
+-rw-rw-r--   0 travis    (2000) travis    (2000)       11 2024-05-23 11:05:15.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/utils/API_VERSION_PARAM
+-rw-rw-r--   0 travis    (2000) travis    (2000)      361 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/utils/__init__.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.702302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/utils/autoai/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      272 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/utils/autoai/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     8072 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/utils/autoai/connection.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    13075 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/utils/autoai/enums.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    19024 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/utils/autoai/errors.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     6194 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/utils/autoai/fairness.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3577 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/utils/autoai/incremental.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3534 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/utils/autoai/local_training_message_handler.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     2199 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/utils/autoai/progress_bar.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     1673 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/utils/autoai/training.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)   110183 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/utils/autoai/utils.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3229 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/utils/cpd_version.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.702302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/utils/deployment/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      272 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/utils/deployment/__init__.py
+-rwxrwxr-x   0 travis    (2000) travis    (2000)     3975 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/utils/deployment/errors.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      534 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/utils/enums.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    26976 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/utils/utils.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    18441 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/volumes.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    11503 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/wml_client_error.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)    24397 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/wml_resource.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.702302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/workspace/
+-rw-rw-r--   0 travis    (2000) travis    (2000)      330 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/workspace/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)     7642 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/workspace/workspace.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.706302 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai.egg-info/
+-rw-r--r--   0 travis    (2000) travis    (2000)     3866 2024-05-23 11:05:15.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai.egg-info/PKG-INFO
+-rw-rw-r--   0 travis    (2000) travis    (2000)    31885 2024-05-23 11:05:15.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai.egg-info/SOURCES.txt
+-rw-rw-r--   0 travis    (2000) travis    (2000)        1 2024-05-23 11:05:15.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai.egg-info/dependency_links.txt
+-rw-rw-r--   0 travis    (2000) travis    (2000)      610 2024-05-23 11:05:15.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai.egg-info/requires.txt
+-rw-rw-r--   0 travis    (2000) travis    (2000)       21 2024-05-23 11:05:15.000000 ibm_watsonx_ai-1.0.5/ibm_watsonx_ai.egg-info/top_level.txt
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.702302 ibm_watsonx_ai-1.0.5/ibmfl/
+-rw-rw-r--   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.000000 ibm_watsonx_ai-1.0.5/ibmfl/__init__.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.702302 ibm_watsonx_ai-1.0.5/ibmfl/data/
+-rw-rw-r--   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.000000 ibm_watsonx_ai-1.0.5/ibmfl/data/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      882 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibmfl/data/data_handler.py
+drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.706302 ibm_watsonx_ai-1.0.5/ibmfl/party/
+-rw-rw-r--   0 travis    (2000) travis    (2000)        0 2024-05-23 11:05:15.000000 ibm_watsonx_ai-1.0.5/ibmfl/party/__init__.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      519 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibmfl/party/party.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      327 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/ibmfl/party_env_validator.py
+-rw-rw-r--   0 travis    (2000) travis    (2000)      228 2024-05-23 11:05:15.714302 ibm_watsonx_ai-1.0.5/setup.cfg
+-rw-rw-r--   0 travis    (2000) travis    (2000)     3670 2024-05-23 11:03:21.000000 ibm_watsonx_ai-1.0.5/setup.py
```

### Comparing `ibm_watsonx_ai-1.0.4/LICENSE.txt` & `ibm_watsonx_ai-1.0.5/LICENSE.txt`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/README.md` & `ibm_watsonx_ai-1.0.5/README.md`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/Set.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/Set.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/__init__.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/__init__.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/_wrappers/requests.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/_wrappers/requests.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/assets.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/assets.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/client.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/client.py`

 * *Files 1% similar despite different names*

```diff
@@ -264,16 +264,16 @@
                                 )
                             )
                 else:
                     self._logger.debug(
                         f"GET /ml/wml_services/version failed with status code: {response_get_wml_services.status_code}."
                     )
 
-                ##Condition for CAMS related changes to take effect (Might change)
-                if self.credentials is None:
+                # Condition for CAMS related changes to take effect (Might change)
+                if self.credentials.version is None:
                     raise WMLClientError(
                         Messages.get_message(
                             CPDVersion.supported_version_list,
                             message_id="version_not_provided",
                         )
                     )
 
@@ -407,15 +407,16 @@
         self.token = None
 
         # For cloud, service_instance.details will be set during space creation( if instance is associated ) or
         # while patching a space with an instance
 
         self.service_instance = ServiceInstance(self)
         self.volumes = Volume(self)
-        self.foundation_models = FoundationModelsManager(self)
+        if self._use_fm_ga_api:
+            self.foundation_models = FoundationModelsManager(self)
 
         if self.ICP_PLATFORM_SPACES:
             self.service_instance.details = self.service_instance.get_details()
 
         self.set = Set(self)
 
         self.spaces = Spaces(self)
@@ -504,15 +505,18 @@
     def _check_if_space_is_set(self) -> None:
         if self.default_space_id is None:
             raise WMLClientError(
                 Messages.get_message(message_id="it_is_mandatory_to_set_the_space_id")
             )
 
     def _params(
-        self, skip_space_project_chk: bool = False, skip_for_create: bool = False, skip_userfs: bool = False
+        self,
+        skip_space_project_chk: bool = False,
+        skip_for_create: bool = False,
+        skip_userfs: bool = False,
     ) -> dict:
         params = {}
         params.update({"version": self.version_param})
         if not skip_for_create:
             if self.default_space_id is not None:
                 params.update({"space_id": self.default_space_id})
             elif self.default_project_id is not None:
@@ -522,21 +526,27 @@
                 if skip_space_project_chk is False:
                     raise WMLClientError(
                         Messages.get_message(
                             message_id="it_is_mandatory_to_set_the_space_project_id"
                         )
                     )
 
-        if self.default_project_id and self.project_type == "local_git_storage" and not skip_userfs:
+        if (
+            self.default_project_id
+            and self.project_type == "local_git_storage"
+            and not skip_userfs
+        ):
             params.update({"userfs": "true"})
             if self._iam_id:
                 params.update({"iam_id": str(self._iam_id)})
 
         if (
-            not self.default_project_id or self.project_type != "local_git_storage" or skip_userfs
+            not self.default_project_id
+            or self.project_type != "local_git_storage"
+            or skip_userfs
         ) and "userfs" in params:
             del params["userfs"]
 
         return params
 
     def _get_headers(
         self,
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/connections.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/connections.py`

 * *Files 0% similar despite different names*

```diff
@@ -560,15 +560,15 @@
         .. code-block:: python
 
             client.connections.get_datasource_type_uid_by_name('cloudobjectstorage')
 
         """
         warn(
             "This method is deprecated, please use get_datasource_type_id_by_name(name)",
-            category=DeprecationWarning
+            category=DeprecationWarning,
         )
 
         return self.get_datasource_type_id_by_name(name=name)
 
     def get_datasource_type_id_by_name(self, name: str) -> str:
         """Get stored datasource types id for the given datasource type name.
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/credentials.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/credentials.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/data_loaders/datasets/experiment.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/data_loaders/datasets/experiment.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/data_loaders/experiment.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/data_loaders/experiment.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/deployment/base_deployment.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/deployment/base_deployment.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/deployment/batch.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/deployment/batch.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/deployment/web_service.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/deployment/web_service.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/deployments.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/deployments.py`

 * *Files 0% similar despite different names*

```diff
@@ -170,17 +170,17 @@
 
         if self._client.CPD_version >= 4.8 or self._client.CLOUD_PLATFORM_SPACES:
             from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes
 
             base_model_id = meta_props.get(self.ConfigurationMetaNames.BASE_MODEL_ID)
 
             if isinstance(base_model_id, ModelTypes):
-                meta_props[
-                    self.ConfigurationMetaNames.BASE_MODEL_ID
-                ] = base_model_id.value
+                meta_props[self.ConfigurationMetaNames.BASE_MODEL_ID] = (
+                    base_model_id.value
+                )
 
         metaProps = self.ConfigurationMetaNames._generate_resource_metadata(meta_props)
 
         if (
             "serving_name" in str(metaProps)
             and meta_props.get("serving_name", False)
             and "r_shiny" in str(metaProps)
@@ -790,15 +790,17 @@
             ##See if it is scoring or DecisionOptimizationJob
 
         payload = {}
 
         payload["input_data"] = score_payload
 
         if meta_props.get(self.ScoringMetaNames.SCORING_PARAMETERS) is not None:
-            payload["scoring_parameters"] = meta_props.get(self.ScoringMetaNames.SCORING_PARAMETERS)
+            payload["scoring_parameters"] = meta_props.get(
+                self.ScoringMetaNames.SCORING_PARAMETERS
+            )
 
         headers = self._client._get_headers()
 
         if transaction_id is not None:
             headers.update({"x-global-transaction-id": transaction_id})
 
         scoring_url = (
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/experiment/autoai/autoai.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/experiment/autoai/autoai.py`

 * *Files 6% similar despite different names*

```diff
@@ -4,38 +4,68 @@
 #  -----------------------------------------------------------------------------------------
 
 import copy
 from typing import List, Union
 from warnings import warn
 
 from ibm_watsonx_ai.utils.autoai.enums import (
-    TShirtSize, ClassificationAlgorithms, RegressionAlgorithms, ForecastingAlgorithms, PredictionType, Metrics, \
-    Transformers, DataConnectionTypes, PipelineTypes, PositiveLabelClass, ClassificationAlgorithmsCP4D,
-    RegressionAlgorithmsCP4D, ForecastingAlgorithmsCP4D, SamplingTypes, ImputationStrategy, ForecastingPipelineTypes,
-    TimeseriesAnomalyPredictionPipelineTypes, TimeseriesAnomalyPredictionAlgorithms)
-from ibm_watsonx_ai.utils.autoai.errors import LocalInstanceButRemoteParameter, MissingPositiveLabel, \
-    NonForecastPredictionColumnMissing, ForecastPredictionColumnsMissing, ForecastingCannotBeRunAsLocalScenario, \
-    TSNotSupported, TSADNotSupported, ImputationListNotSupported, \
-    MissingEstimatorForExistingBatchedEstimator, TimeseriesAnomalyPredictionFeatureColumnsMissing, \
-    TimeseriesAnomalyPredictionCannotBeRunAsLocalScenario, TimeseriesAnomalyPredictionUnsupportedMetric
-from ibm_watsonx_ai.utils.autoai.utils import check_dependencies_versions, \
-    validate_additional_params_for_optimizer, validate_optimizer_enum_values, \
-    translate_imputation_string_strategy_to_enum, translate_estimator_string_to_enum, \
-    translate_batched_estimator_string_to_enum
+    TShirtSize,
+    ClassificationAlgorithms,
+    RegressionAlgorithms,
+    ForecastingAlgorithms,
+    PredictionType,
+    Metrics,
+    Transformers,
+    DataConnectionTypes,
+    PipelineTypes,
+    PositiveLabelClass,
+    ClassificationAlgorithmsCP4D,
+    RegressionAlgorithmsCP4D,
+    ForecastingAlgorithmsCP4D,
+    SamplingTypes,
+    ImputationStrategy,
+    ForecastingPipelineTypes,
+    TimeseriesAnomalyPredictionPipelineTypes,
+    TimeseriesAnomalyPredictionAlgorithms,
+)
+from ibm_watsonx_ai.utils.autoai.errors import (
+    LocalInstanceButRemoteParameter,
+    MissingPositiveLabel,
+    NonForecastPredictionColumnMissing,
+    ForecastPredictionColumnsMissing,
+    ForecastingCannotBeRunAsLocalScenario,
+    TSNotSupported,
+    TSADNotSupported,
+    ImputationListNotSupported,
+    MissingEstimatorForExistingBatchedEstimator,
+    TimeseriesAnomalyPredictionFeatureColumnsMissing,
+    TimeseriesAnomalyPredictionCannotBeRunAsLocalScenario,
+    TimeseriesAnomalyPredictionUnsupportedMetric,
+)
+from ibm_watsonx_ai.utils.autoai.utils import (
+    check_dependencies_versions,
+    validate_additional_params_for_optimizer,
+    validate_optimizer_enum_values,
+    translate_imputation_string_strategy_to_enum,
+    translate_estimator_string_to_enum,
+    translate_batched_estimator_string_to_enum,
+)
 from ibm_watsonx_ai.workspace import WorkSpace
-from ibm_watsonx_ai.wml_client_error import ForbiddenActionForGitBasedProject, WMLClientError, ParamOutOfRange
+from ibm_watsonx_ai.wml_client_error import (
+    ForbiddenActionForGitBasedProject,
+    WMLClientError,
+    ParamOutOfRange,
+)
 from ibm_watsonx_ai.messages.messages import Messages
 from .engines import ServiceEngine
 from .optimizers import LocalAutoPipelines, RemoteAutoPipelines
 from .runs import AutoPipelinesRuns, LocalAutoPipelinesRuns
 from ..base_experiment.base_experiment import BaseExperiment
 
-__all__ = [
-    "AutoAI"
-]
+__all__ = ["AutoAI"]
 
 from ...credentials import Credentials
 
 
 class AutoAI(BaseExperiment):
     """AutoAI class for pipeline models optimization automation.
 
@@ -71,44 +101,52 @@
                 "iam_serviceid_crn": "...",
                 "instance_id": "...",
                 "url": "https://us-south.ml.cloud.ibm.com"
             },
             project_id="...",
             space_id="...")
     """
+
     # note: initialization of AutoAI enums as class properties
-        
+
     # note: Enums with estimators can be overwritten  in _init based on environment type (CPD or Cloud)
     ClassificationAlgorithms = ClassificationAlgorithms
     RegressionAlgorithms = RegressionAlgorithms
     ForecastingAlgorithms = ForecastingAlgorithms
     # end note
     TShirtSize = TShirtSize
     PredictionType = PredictionType
     Metrics = Metrics
     Transformers = Transformers
     DataConnectionTypes = DataConnectionTypes
     PipelineTypes = PipelineTypes
     SamplingTypes = SamplingTypes
 
-    def __init__(self,
-                 credentials: Union[Credentials, dict, 'WorkSpace'] = None,
-                 project_id: str = None,
-                 space_id: str = None,
-                 verify=None,
-                 **kwargs) -> None:
+    def __init__(
+        self,
+        credentials: Union[Credentials, dict, "WorkSpace"] = None,
+        project_id: str = None,
+        space_id: str = None,
+        verify=None,
+        **kwargs,
+    ) -> None:
         # note: as workspace is not clear enough to understand, there is a possibility to use pure
         # credentials with project and space IDs, but in addition we
         # leave a possibility to use a previous WorkSpace implementation, it could be passed as a first argument
-         # note: backward compatibility
-        if (wml_credentials:=kwargs.get('wml_credentials')) is not None:
+        # note: backward compatibility
+        if (wml_credentials := kwargs.get("wml_credentials")) is not None:
             if credentials is None:
                 credentials = wml_credentials
-            warn(("`wml_credentials` is deprecated and will be removed in future. "
-                    "Instead, please use `credentials`."), category=DeprecationWarning)
+            warn(
+                (
+                    "`wml_credentials` is deprecated and will be removed in future. "
+                    "Instead, please use `credentials`."
+                ),
+                category=DeprecationWarning,
+            )
 
         if isinstance(credentials, dict):
             credentials = Credentials.from_dict(credentials, _verify=verify)
 
         if isinstance(credentials, Credentials):
             credentials._set_env_vars_from_credentials()
 
@@ -117,32 +155,36 @@
             self._workspace = None
             self.runs = LocalAutoPipelinesRuns()
 
         else:
             if isinstance(credentials, WorkSpace):
                 self._workspace = credentials
             else:
-                self._workspace = WorkSpace(credentials=copy.copy(credentials),
-                                            project_id=project_id,
-                                            space_id=space_id,
-                                            verify=verify)
+                self._workspace = WorkSpace(
+                    credentials=copy.copy(credentials),
+                    project_id=project_id,
+                    space_id=space_id,
+                    verify=verify,
+                )
 
             self.project_id = self._workspace.project_id
             self.space_id = self._workspace.space_id
             self.runs = AutoPipelinesRuns(engine=ServiceEngine(self._workspace))
             self.runs._workspace = self._workspace
 
-        #self._block_autoai_on_git_based_project()
+        # self._block_autoai_on_git_based_project()
 
         self._init_estimator_enums()
 
         self._20_class_limit_removal_test = False
         # --- end note
 
-    def runs(self, *, filter: str) -> Union['AutoPipelinesRuns', 'LocalAutoPipelinesRuns']:
+    def runs(
+        self, *, filter: str
+    ) -> Union["AutoPipelinesRuns", "LocalAutoPipelinesRuns"]:
         """Get the historical runs but with Pipeline name filter (for remote scenario).
         Get the historical runs but with experiment name filter (for local scenario).
 
         :param filter: Pipeline name to filter the historical runs or experiment name to filter
             the local historical runs
         :type filter: str
 
@@ -159,75 +201,92 @@
             experiment.runs(filter='Test').list()
         """
 
         if self._workspace is None:
             return LocalAutoPipelinesRuns(filter=filter)
 
         else:
-            return AutoPipelinesRuns(engine=ServiceEngine(self._workspace.api_client), filter=filter)
+            return AutoPipelinesRuns(
+                engine=ServiceEngine(self._workspace.api_client), filter=filter
+            )
 
-    def optimizer(self,
-                  name: str,
-                  *,
-                  prediction_type: 'PredictionType',
-                  prediction_column: str = None,
-                  prediction_columns: List[str] = None,
-                  timestamp_column_name: str = None,
-                  scoring: 'Metrics' = None,
-                  desc: str = None,
-                  test_size: float = None,  # deprecated
-                  holdout_size: float = None,
-                  max_number_of_estimators: int = None,
-                  train_sample_rows_test_size: float = None,
-                  include_only_estimators: List[Union['ClassificationAlgorithms', 'RegressionAlgorithms',
-                                                      'ForecastingAlgorithms', 'TimeseriesAnomalyPredictionAlgorithms']] = None,
-                  daub_include_only_estimators: List[Union['ClassificationAlgorithms', 'RegressionAlgorithms']] = None,  # deprecated
-                  include_batched_ensemble_estimators: List[Union['BatchedClassificationAlgorithms',
-                                                                  'BatchedRegressionAlgorithms']] = None,
-                  backtest_num: int = None,
-                  lookback_window: int = None,
-                  forecast_window: int = None,
-                  backtest_gap_length: int = None,
-                  feature_columns: List[str] = None,
-                  pipeline_types: List[Union['ForecastingPipelineTypes', 'TimeseriesAnomalyPredictionPipelineTypes']] = None,
-                  supporting_features_at_forecast: bool = None,
-                  cognito_transform_names: List['Transformers'] = None,
-                  csv_separator: Union[List[str], str] = ',',
-                  excel_sheet: Union[str, int] = None,
-                  encoding: str = 'utf-8',
-                  positive_label: str = None,
-                  drop_duplicates: bool = True,
-                  outliers_columns: list = None,
-                  text_processing: bool = None,
-                  word2vec_feature_number: int = None,
-                  daub_give_priority_to_runtime: float = None,
-                  fairness_info: dict = None,
-                  sampling_type: 'SamplingTypes' = None,
-                  sample_size_limit: int = None,
-                  sample_rows_limit: int = None,
-                  sample_percentage_limit: float = None,
-                  n_parallel_data_connections: int = None,
-                  number_of_batch_rows: int = None,
-                  categorical_imputation_strategy: ImputationStrategy = None,
-                  numerical_imputation_strategy: ImputationStrategy = None,
-                  numerical_imputation_value: float = None,
-                  imputation_threshold: float = None,
-                  retrain_on_holdout: bool = None,
-                  categorical_columns: list = None,
-                  numerical_columns: list = None,
-                  test_data_csv_separator: Union[List[str], str] = ',',
-                  test_data_excel_sheet: str = None,
-                  test_data_encoding: str = 'utf-8',
-                  confidence_level: float = None,
-                  incremental_learning: bool = None,
-                  early_stop_enabled: bool = None,
-                  early_stop_window_size: int = None,
-                  time_ordered_data: bool = None,
-                  feature_selector_mode: str = None,
-                  **kwargs) -> Union['RemoteAutoPipelines', 'LocalAutoPipelines']:
+    def optimizer(
+        self,
+        name: str,
+        *,
+        prediction_type: "PredictionType",
+        prediction_column: str = None,
+        prediction_columns: List[str] = None,
+        timestamp_column_name: str = None,
+        scoring: "Metrics" = None,
+        desc: str = None,
+        test_size: float = None,  # deprecated
+        holdout_size: float = None,
+        max_number_of_estimators: int = None,
+        train_sample_rows_test_size: float = None,
+        include_only_estimators: List[
+            Union[
+                "ClassificationAlgorithms",
+                "RegressionAlgorithms",
+                "ForecastingAlgorithms",
+                "TimeseriesAnomalyPredictionAlgorithms",
+            ]
+        ] = None,
+        daub_include_only_estimators: List[
+            Union["ClassificationAlgorithms", "RegressionAlgorithms"]
+        ] = None,  # deprecated
+        include_batched_ensemble_estimators: List[
+            Union["BatchedClassificationAlgorithms", "BatchedRegressionAlgorithms"]
+        ] = None,
+        backtest_num: int = None,
+        lookback_window: int = None,
+        forecast_window: int = None,
+        backtest_gap_length: int = None,
+        feature_columns: List[str] = None,
+        pipeline_types: List[
+            Union[
+                "ForecastingPipelineTypes", "TimeseriesAnomalyPredictionPipelineTypes"
+            ]
+        ] = None,
+        supporting_features_at_forecast: bool = None,
+        cognito_transform_names: List["Transformers"] = None,
+        csv_separator: Union[List[str], str] = ",",
+        excel_sheet: Union[str, int] = None,
+        encoding: str = "utf-8",
+        positive_label: str = None,
+        drop_duplicates: bool = True,
+        outliers_columns: list = None,
+        text_processing: bool = None,
+        word2vec_feature_number: int = None,
+        daub_give_priority_to_runtime: float = None,
+        fairness_info: dict = None,
+        sampling_type: "SamplingTypes" = None,
+        sample_size_limit: int = None,
+        sample_rows_limit: int = None,
+        sample_percentage_limit: float = None,
+        n_parallel_data_connections: int = None,
+        number_of_batch_rows: int = None,
+        categorical_imputation_strategy: ImputationStrategy = None,
+        numerical_imputation_strategy: ImputationStrategy = None,
+        numerical_imputation_value: float = None,
+        imputation_threshold: float = None,
+        retrain_on_holdout: bool = None,
+        categorical_columns: list = None,
+        numerical_columns: list = None,
+        test_data_csv_separator: Union[List[str], str] = ",",
+        test_data_excel_sheet: str = None,
+        test_data_encoding: str = "utf-8",
+        confidence_level: float = None,
+        incremental_learning: bool = None,
+        early_stop_enabled: bool = None,
+        early_stop_window_size: int = None,
+        time_ordered_data: bool = None,
+        feature_selector_mode: str = None,
+        **kwargs,
+    ) -> Union["RemoteAutoPipelines", "LocalAutoPipelines"]:
         """
         Initialize an AutoAi optimizer.
 
         :param name: name for the AutoPipelines
         :type name: str
 
         :param prediction_type: type of the prediction
@@ -261,18 +320,18 @@
             the internal different algorithms, where only the highest ranked by model selection algorithm type is used
         :type max_number_of_estimators: int, optional
 
         :param train_sample_rows_test_size: training data sampling percentage
         :type train_sample_rows_test_size: float, optional
 
         :param daub_include_only_estimators: deprecated, use `include_only_estimators` instead
-        
-        :param include_batched_ensemble_estimators: list of batched ensemble estimators to include 
+
+        :param include_batched_ensemble_estimators: list of batched ensemble estimators to include
             in computation process, see: AutoAI.BatchedClassificationAlgorithms, AutoAI.BatchedRegressionAlgorithms
-        :type include_batched_ensemble_estimators: 
+        :type include_batched_ensemble_estimators:
             list[BatchedClassificationAlgorithms or BatchedRegressionAlgorithms], optional
 
         :param include_only_estimators: list of estimators to include in computation process, see:
             AutoAI.ClassificationAlgorithms, AutoAI.RegressionAlgorithms or AutoAI.ForecastingAlgorithms
         :type include_only_estimators: List[ClassificationAlgorithms or RegressionAlgorithms or ForecastingAlgorithms]], optional
 
         :param backtest_num: number of backtests used for forecasting prediction type, default value: 4,
@@ -426,23 +485,23 @@
         :type number_of_batch_rows: int, optional
 
         :param test_data_csv_separator: the separator, or list of separators to try for separating
             columns in a CSV user-defined holdout/test file, not used if the file_name is not a CSV file,
             default is ','
         :type test_data_csv_separator: list[str] or str, optional
 
-        :param test_data_excel_sheet: name of the excel sheet to use for user-defined holdout/test data, 
+        :param test_data_excel_sheet: name of the excel sheet to use for user-defined holdout/test data,
             only use when xlsx file is an test, dataset file, by default first sheet is used
         :type test_data_excel_sheet: str or int, optional
 
         :param test_data_encoding: encoding type for CSV user-defined holdout/test file
         :type test_data_encoding: str, optional
 
-        :param confidence_level: when the pipeline "PointwiseBoundedHoltWinters" or "PointwiseBoundedBATS" is used, 
-            the prediction interval is calculated at a given confidence_level to decide if a data record 
+        :param confidence_level: when the pipeline "PointwiseBoundedHoltWinters" or "PointwiseBoundedBATS" is used,
+            the prediction interval is calculated at a given confidence_level to decide if a data record
             is an anomaly or not, optional for timeseries anomaly prediction
         :type confidence_level: float, optional
 
         :param incremental_learning: triggers incremental learning process for supported pipelines
         :type incremental_learning: bool, optional
 
         :param early_stop_enabled: enables early stop for incremental learning process
@@ -503,76 +562,114 @@
                    prediction_type=AutoAI.PredictionType.MULTICLASS,
                    prediction_column="y",
                    scoring=AutoAI.Metrics.ROC_AUC_SCORE,
                    desc="Some description.",
                )
         """
         # note: convert `timeseries` type to PredictionType.FORECASTING:
-        if prediction_type == 'timeseries':
+        if prediction_type == "timeseries":
             prediction_type = PredictionType.FORECASTING
 
-
         if prediction_type != PredictionType.FORECASTING and retrain_on_holdout is None:
             retrain_on_holdout = True
 
         # Deprecation of excel_sheet as number:
         if isinstance(excel_sheet, int):
             warn(
-                message="Support for excel sheet as number of the sheet (int) is deprecated! Please set excel sheet with name of the sheet.")
+                message="Support for excel sheet as number of the sheet (int) is deprecated! Please set excel sheet with name of the sheet."
+            )
 
-        if prediction_type == PredictionType.TIMESERIES_ANOMALY_PREDICTION and self._workspace.api_client.ICP_PLATFORM_SPACES and \
-                self._workspace.api_client.credentials.version.startswith(('2.5', '3.0', '3.5', '4.0', '4.5', '4.6')):
+        if (
+            prediction_type == PredictionType.TIMESERIES_ANOMALY_PREDICTION
+            and self._workspace.api_client.ICP_PLATFORM_SPACES
+            and self._workspace.api_client.credentials.version.startswith(
+                ("2.5", "3.0", "3.5", "4.0", "4.5", "4.6")
+            )
+        ):
             raise TSADNotSupported()
 
-        if prediction_type in (PredictionType.FORECASTING, 'timeseries'):
-            if not numerical_imputation_strategy and type(numerical_imputation_strategy) is not list:
-                numerical_imputation_strategy = ImputationStrategy.BEST_OF_DEFAULT_IMPUTERS
-            elif not numerical_imputation_strategy and type(numerical_imputation_strategy) is list:
+        if prediction_type in (PredictionType.FORECASTING, "timeseries"):
+            if (
+                not numerical_imputation_strategy
+                and type(numerical_imputation_strategy) is not list
+            ):
+                numerical_imputation_strategy = (
+                    ImputationStrategy.BEST_OF_DEFAULT_IMPUTERS
+                )
+            elif (
+                not numerical_imputation_strategy
+                and type(numerical_imputation_strategy) is list
+            ):
                 numerical_imputation_strategy = ImputationStrategy.NO_IMPUTATION
 
             if prediction_column is not None or prediction_columns is None:
                 raise ForecastPredictionColumnsMissing()
         elif prediction_type == PredictionType.TIMESERIES_ANOMALY_PREDICTION:
-            if feature_columns is None or prediction_column is not None or prediction_columns is not None:
+            if (
+                feature_columns is None
+                or prediction_column is not None
+                or prediction_columns is not None
+            ):
                 raise TimeseriesAnomalyPredictionFeatureColumnsMissing()
             if scoring is not None and scoring not in (
-                    Metrics.F1_SCORE, Metrics.ROC_AUC_SCORE, Metrics.AVERAGE_PRECISION_SCORE, Metrics.PRECISION_SCORE,
-                    Metrics.RECALL_SCORE):
+                Metrics.F1_SCORE,
+                Metrics.ROC_AUC_SCORE,
+                Metrics.AVERAGE_PRECISION_SCORE,
+                Metrics.PRECISION_SCORE,
+                Metrics.RECALL_SCORE,
+            ):
                 raise TimeseriesAnomalyPredictionUnsupportedMetric(scoring)
         else:
             if prediction_column is None or prediction_columns is not None:
                 raise NonForecastPredictionColumnMissing(prediction_type)
 
         if test_size:
-            print('Note: Using `test_size` is deprecated. Use `holdout_size` instead.')
+            print("Note: Using `test_size` is deprecated. Use `holdout_size` instead.")
             if not holdout_size:
                 holdout_size = test_size
             test_size = None
 
         if daub_include_only_estimators:
-            print('Note: Using `daub_include_only_estimators` is deprecated. Use `include_only_estimators` instead.')
+            print(
+                "Note: Using `daub_include_only_estimators` is deprecated. Use `include_only_estimators` instead."
+            )
             if not include_only_estimators:
                 include_only_estimators = daub_include_only_estimators
             daub_include_only_estimators = None
 
-        if train_sample_rows_test_size and  self._workspace.api_client.CPD_version >= 4.6:
-            print('Note: Using `train_sample_rows_test_size` is deprecated.'
-                  'Use either `sample_rows_limit` or `sample_percentage_limit` instead.')
+        if (
+            train_sample_rows_test_size
+            and self._workspace.api_client.CPD_version >= 4.6
+        ):
+            print(
+                "Note: Using `train_sample_rows_test_size` is deprecated."
+                "Use either `sample_rows_limit` or `sample_percentage_limit` instead."
+            )
             if not sample_rows_limit and not sample_percentage_limit:
-                if type(train_sample_rows_test_size) is float and train_sample_rows_test_size <= 1:
-                    print('Value of `train_sample_rows_test_size` parameter'
-                          'will be passed as `sample_percentage_limit`.')
+                if (
+                    type(train_sample_rows_test_size) is float
+                    and train_sample_rows_test_size <= 1
+                ):
+                    print(
+                        "Value of `train_sample_rows_test_size` parameter"
+                        "will be passed as `sample_percentage_limit`."
+                    )
                     sample_percentage_limit = train_sample_rows_test_size
-                elif int(train_sample_rows_test_size) == train_sample_rows_test_size and train_sample_rows_test_size > 1:
-                    print('Value of `train_sample_rows_test_size` parameter'
-                          'will be passed as `sample_rows_limit`.')
+                elif (
+                    int(train_sample_rows_test_size) == train_sample_rows_test_size
+                    and train_sample_rows_test_size > 1
+                ):
+                    print(
+                        "Value of `train_sample_rows_test_size` parameter"
+                        "will be passed as `sample_rows_limit`."
+                    )
                     sample_rows_limit = int(train_sample_rows_test_size)
                 train_sample_rows_test_size = None
             elif sample_rows_limit or sample_percentage_limit:
-                print('Parameter `train_sample_rows_test_size` will be ignored.')
+                print("Parameter `train_sample_rows_test_size` will be ignored.")
                 train_sample_rows_test_size = None
 
         def translate_str_imputation_param(x):
             if type(x) is list and prediction_type != PredictionType.FORECASTING:
                 raise ImputationListNotSupported()
 
             if type(x) == str or (type(x) == list and type(x[0]) == str):
@@ -580,100 +677,167 @@
             else:
                 return x
 
         def translate_str_include_only_estimators_param(x):
             return [translate_estimator_string_to_enum(estimator) for estimator in x]
 
         def translate_str_include_batched_ensemble_estimators_param(x):
-            return [translate_batched_estimator_string_to_enum(estimator) for estimator in x]
+            return [
+                translate_batched_estimator_string_to_enum(estimator) for estimator in x
+            ]
 
         def translate_str_pipeline_types_param(x):
             if prediction_type == PredictionType.TIMESERIES_ANOMALY_PREDICTION:
-                return [TimeseriesAnomalyPredictionPipelineTypes(pipeline_type) for pipeline_type in x]
+                return [
+                    TimeseriesAnomalyPredictionPipelineTypes(pipeline_type)
+                    for pipeline_type in x
+                ]
             else:
                 return [ForecastingPipelineTypes(pipeline_type) for pipeline_type in x]
 
-        categorical_imputation_strategy = translate_str_imputation_param(categorical_imputation_strategy)
-        numerical_imputation_strategy = translate_str_imputation_param(numerical_imputation_strategy)
-        include_only_estimators = translate_str_include_only_estimators_param(include_only_estimators) if include_only_estimators else None
-        include_batched_ensemble_estimators = translate_str_include_batched_ensemble_estimators_param(include_batched_ensemble_estimators) if include_batched_ensemble_estimators else None
-        pipeline_types = translate_str_pipeline_types_param(pipeline_types) if pipeline_types != None else None
+        categorical_imputation_strategy = translate_str_imputation_param(
+            categorical_imputation_strategy
+        )
+        numerical_imputation_strategy = translate_str_imputation_param(
+            numerical_imputation_strategy
+        )
+        include_only_estimators = (
+            translate_str_include_only_estimators_param(include_only_estimators)
+            if include_only_estimators
+            else None
+        )
+        include_batched_ensemble_estimators = (
+            translate_str_include_batched_ensemble_estimators_param(
+                include_batched_ensemble_estimators
+            )
+            if include_batched_ensemble_estimators
+            else None
+        )
+        pipeline_types = (
+            translate_str_pipeline_types_param(pipeline_types)
+            if pipeline_types != None
+            else None
+        )
 
         if include_batched_ensemble_estimators:
             for batched_estimator in include_batched_ensemble_estimators:
                 basic_estimator_str = batched_estimator.value.split("(")[1][:-1]
-                basic_estimator = translate_estimator_string_to_enum(basic_estimator_str)
-                if include_only_estimators is not None and basic_estimator not in include_only_estimators:
-                    raise MissingEstimatorForExistingBatchedEstimator(batched_estimator, basic_estimator)
+                basic_estimator = translate_estimator_string_to_enum(
+                    basic_estimator_str
+                )
+                if (
+                    include_only_estimators is not None
+                    and basic_estimator not in include_only_estimators
+                ):
+                    raise MissingEstimatorForExistingBatchedEstimator(
+                        batched_estimator, basic_estimator
+                    )
 
         validate_optimizer_enum_values(
             prediction_type=prediction_type,
             daub_include_only_estimators=daub_include_only_estimators,
             include_only_estimators=include_only_estimators,
             include_batched_ensemble_estimators=include_batched_ensemble_estimators,
             cognito_transform_names=cognito_transform_names,
-            imputation_strategies=[x for y in list(filter(None, [categorical_imputation_strategy, numerical_imputation_strategy])) for x in (y if type(y) is list else [y])],
+            imputation_strategies=[
+                x
+                for y in list(
+                    filter(
+                        None,
+                        [
+                            categorical_imputation_strategy,
+                            numerical_imputation_strategy,
+                        ],
+                    )
+                )
+                for x in (y if type(y) is list else [y])
+            ],
             scoring=scoring,
             t_shirt_size=kwargs.get("t_shirt_size", TShirtSize.M),
-            is_cpd=self._workspace.api_client.ICP_PLATFORM_SPACES
+            is_cpd=self._workspace.api_client.ICP_PLATFORM_SPACES,
         )
 
         if daub_give_priority_to_runtime is not None:
-            if daub_give_priority_to_runtime < 0.0 or daub_give_priority_to_runtime > 5.0:
-                raise ParamOutOfRange('daub_give_priority_to_runtime', daub_give_priority_to_runtime, 0.0, 5.0)
-
-        if (prediction_type == PredictionType.BINARY and scoring in vars(PositiveLabelClass).values()
-                and positive_label is None):
-            raise MissingPositiveLabel(scoring, reason=f"\"{scoring}\" needs a \"positive_label\" "
-                                                       f"parameter to be defined when used with binary classification.")
+            if (
+                daub_give_priority_to_runtime < 0.0
+                or daub_give_priority_to_runtime > 5.0
+            ):
+                raise ParamOutOfRange(
+                    "daub_give_priority_to_runtime",
+                    daub_give_priority_to_runtime,
+                    0.0,
+                    5.0,
+                )
+
+        if (
+            prediction_type == PredictionType.BINARY
+            and scoring in vars(PositiveLabelClass).values()
+            and positive_label is None
+        ):
+            raise MissingPositiveLabel(
+                scoring,
+                reason=f'"{scoring}" needs a "positive_label" '
+                f"parameter to be defined when used with binary classification.",
+            )
 
-        if self._workspace is None and kwargs.get('t_shirt_size'):
+        if self._workspace is None and kwargs.get("t_shirt_size"):
             raise LocalInstanceButRemoteParameter(
                 "t_shirt_size",
-                reason="During LocalOptimizer initialization, \"t_shirt_size\" parameter was provided. "
-                       "\"t_shirt_size\" parameter is only applicable to the RemoteOptimizer instance."
+                reason='During LocalOptimizer initialization, "t_shirt_size" parameter was provided. '
+                '"t_shirt_size" parameter is only applicable to the RemoteOptimizer instance.',
             )
         elif self._workspace is None:
             if prediction_type == PredictionType.FORECASTING:
                 raise ForecastingCannotBeRunAsLocalScenario()
             if prediction_type == PredictionType.TIMESERIES_ANOMALY_PREDICTION:
                 raise TimeseriesAnomalyPredictionCannotBeRunAsLocalScenario()
 
             reduced_kwargs = copy.copy(kwargs)
 
-            for n in ['_force_local_scenario']:
+            for n in ["_force_local_scenario"]:
                 if n in reduced_kwargs:
                     del reduced_kwargs[n]
 
             validate_additional_params_for_optimizer(reduced_kwargs)
 
             return LocalAutoPipelines(
                 name=name,
-                prediction_type='classification' if prediction_type in ['binary', 'multiclass'] else prediction_type,
+                prediction_type=(
+                    "classification"
+                    if prediction_type in ["binary", "multiclass"]
+                    else prediction_type
+                ),
                 prediction_column=prediction_column,
                 scoring=scoring,
                 desc=desc,
                 holdout_size=holdout_size,
                 max_num_daub_ensembles=max_number_of_estimators,
                 train_sample_rows_test_size=train_sample_rows_test_size,
                 include_only_estimators=include_only_estimators,
                 include_batched_ensemble_estimators=include_batched_ensemble_estimators,
                 cognito_transform_names=cognito_transform_names,
                 positive_label=positive_label,
-                _force_local_scenario=kwargs.get('_force_local_scenario', False),
-                **reduced_kwargs
+                _force_local_scenario=kwargs.get("_force_local_scenario", False),
+                **reduced_kwargs,
             )
 
         else:
             reduced_kwargs = copy.copy(kwargs)
 
             if max_number_of_estimators is None:
-                max_number_of_estimators = reduced_kwargs.get('max_num_daub_ensembles', None)
+                max_number_of_estimators = reduced_kwargs.get(
+                    "max_num_daub_ensembles", None
+                )
 
-            for n in ['t_shirt_size', 'notebooks', 'autoai_pod_version', 'max_num_daub_ensembles']:
+            for n in [
+                "t_shirt_size",
+                "notebooks",
+                "autoai_pod_version",
+                "max_num_daub_ensembles",
+            ]:
                 if n in reduced_kwargs:
                     del reduced_kwargs[n]
 
             validate_additional_params_for_optimizer(reduced_kwargs)
 
             engine = ServiceEngine(self._workspace)
 
@@ -686,16 +850,23 @@
                 prediction_column=prediction_column,
                 prediction_columns=prediction_columns,
                 timestamp_column_name=timestamp_column_name,
                 scoring=scoring,
                 desc=desc,
                 holdout_size=holdout_size,
                 max_num_daub_ensembles=max_number_of_estimators,
-                t_shirt_size=self._workspace.restrict_pod_size(t_shirt_size=kwargs.get(
-                    't_shirt_size', TShirtSize.M if self._workspace.api_client.ICP_PLATFORM_SPACES else TShirtSize.L)
+                t_shirt_size=self._workspace.restrict_pod_size(
+                    t_shirt_size=kwargs.get(
+                        "t_shirt_size",
+                        (
+                            TShirtSize.M
+                            if self._workspace.api_client.ICP_PLATFORM_SPACES
+                            else TShirtSize.L
+                        ),
+                    )
                 ),
                 train_sample_rows_test_size=train_sample_rows_test_size,
                 include_only_estimators=include_only_estimators,
                 include_batched_ensemble_estimators=include_batched_ensemble_estimators,
                 backtest_num=backtest_num,
                 lookback_window=lookback_window,
                 forecast_window=forecast_window,
@@ -707,16 +878,16 @@
                 word2vec_feature_number=word2vec_feature_number,
                 csv_separator=csv_separator,
                 excel_sheet=excel_sheet,
                 encoding=encoding,
                 positive_label=positive_label,
                 engine=engine,
                 daub_give_priority_to_runtime=daub_give_priority_to_runtime,
-                notebooks=kwargs.get('notebooks', True),
-                autoai_pod_version=kwargs.get('autoai_pod_version', None),
+                notebooks=kwargs.get("notebooks", True),
+                autoai_pod_version=kwargs.get("autoai_pod_version", None),
                 fairness_info=fairness_info,
                 sampling_type=sampling_type,
                 sample_size_limit=sample_size_limit,
                 sample_rows_limit=sample_rows_limit,
                 sample_percentage_limit=sample_percentage_limit,
                 number_of_batch_rows=number_of_batch_rows,
                 n_parallel_data_connections=n_parallel_data_connections,
@@ -735,15 +906,15 @@
                 test_data_encoding=test_data_encoding,
                 confidence_level=confidence_level,
                 incremental_learning=incremental_learning,
                 early_stop_enabled=early_stop_enabled,
                 early_stop_window_size=early_stop_window_size,
                 time_ordered_data=time_ordered_data,
                 feature_selector_mode=feature_selector_mode,
-                **reduced_kwargs
+                **reduced_kwargs,
             )
             optimizer._workspace = self._workspace
             return optimizer
 
     def _init_estimator_enums(self):
         if self._workspace and self._workspace.api_client.ICP_PLATFORM_SPACES:
             self.ClassificationAlgorithms = ClassificationAlgorithmsCP4D
@@ -757,12 +928,15 @@
     def _block_autoai_on_git_based_project(self):
         """Raises ForbiddenActionForGitBasedProject error for AutoAI experiments on git based project.
         It can be disabled by setting environment variable ENABLE_AUTOAI to 'true'
         """
         from os import environ
 
         if self._workspace:
-            if getattr(self._workspace.api_client, 'project_type', None) == 'local_git_storage' \
-                    and environ.get('ENABLE_AUTOAI', 'false').lower() == 'false':
-                raise ForbiddenActionForGitBasedProject(reason="Creating AutoAI experiment is not supported for git based project.")
-
-
+            if (
+                getattr(self._workspace.api_client, "project_type", None)
+                == "local_git_storage"
+                and environ.get("ENABLE_AUTOAI", "false").lower() == "false"
+            ):
+                raise ForbiddenActionForGitBasedProject(
+                    reason="Creating AutoAI experiment is not supported for git based project."
+                )
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/experiment/autoai/engines/base_engine.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/experiment/autoai/engines/base_engine.py`

 * *Files 7% similar despite different names*

```diff
@@ -7,57 +7,57 @@
 from typing import List, TYPE_CHECKING
 
 if TYPE_CHECKING:
     from pandas import DataFrame
     from sklearn.pipeline import Pipeline
     from ibm_watsonx_ai.helpers.connections import DataConnection
 
-__all__ = [
-    "BaseEngine"
-]
+__all__ = ["BaseEngine"]
 
 
 class BaseEngine(ABC):
     """Base abstract class for Engines."""
 
     @abstractmethod
     def get_params(self) -> dict:
         """Fetch configuration parameters"""
         pass
 
     @abstractmethod
-    def fit(self,
-            training_data_reference: List['DataConnection'],
-            training_results_reference: 'DataConnection',
-            background_mode: bool = True) -> dict:
+    def fit(
+        self,
+        training_data_reference: List["DataConnection"],
+        training_results_reference: "DataConnection",
+        background_mode: bool = True,
+    ) -> dict:
         """Schedule a fit/run/training."""
         pass
 
     @abstractmethod
     def get_run_status(self) -> str:
         """Fetch status of a training."""
         pass
 
     @abstractmethod
     def get_run_details(self) -> dict:
         """Fetch training details"""
         pass
 
     @abstractmethod
-    def summary(self) -> 'DataFrame':
+    def summary(self) -> "DataFrame":
         """Fetch all pipelines results"""
         pass
 
     @abstractmethod
     def get_pipeline_details(self, pipeline_name: str = None) -> dict:
         """Fetch details of particular pipeline"""
         pass
 
     @abstractmethod
-    def get_pipeline(self, pipeline_name: str, local_path: str = '.') -> 'Pipeline':
+    def get_pipeline(self, pipeline_name: str, local_path: str = ".") -> "Pipeline":
         """Download and load computed pipeline"""
         pass
 
     @abstractmethod
-    def get_best_pipeline(self, local_path: str = '.') -> 'Pipeline':
+    def get_best_pipeline(self, local_path: str = ".") -> "Pipeline":
         """Download and load the best pipeline"""
         pass
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/experiment/autoai/optimizers/base_auto_pipelines.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/experiment/autoai/optimizers/base_auto_pipelines.py`

 * *Files 19% similar despite different names*

```diff
@@ -8,50 +8,50 @@
 
 if TYPE_CHECKING:
     from pandas import DataFrame
     from ibm_watsonx_ai.utils.autoai.enums import PipelineTypes
     from sklearn.pipeline import Pipeline
     from numpy import ndarray
 
-__all__ = [
-    "BaseAutoPipelines"
-]
+__all__ = ["BaseAutoPipelines"]
 
 
 class BaseAutoPipelines:
     """Base abstract class for Pipeline Optimizers."""
 
     @abstractmethod
     def get_params(self) -> dict:
         """Get configuration parameters of AutoPipelines"""
         pass
 
     @abstractmethod
-    def fit(self, *args, **kwargs) -> 'Pipeline':
+    def fit(self, *args, **kwargs) -> "Pipeline":
         """Run fit job."""
         pass
 
     @abstractmethod
-    def summary(self) -> 'DataFrame':
+    def summary(self) -> "DataFrame":
         """List all computed pipelines."""
         pass
 
     @abstractmethod
     def get_pipeline_details(self, pipeline_name: str = None) -> dict:
         """Get details of computed pipeline. Details like pipeline steps."""
         pass
 
     @abstractmethod
-    def get_pipeline(self, pipeline_name: str, astype: 'PipelineTypes') -> Union['Pipeline', 'TrainablePipeline']:
+    def get_pipeline(
+        self, pipeline_name: str, astype: "PipelineTypes"
+    ) -> Union["Pipeline", "TrainablePipeline"]:
         """Get particular computed Pipeline"""
         pass
 
-    def get_pipeline_notebook(self,
-                     pipeline_name: str = None,
-                     persist: 'bool' = False) -> Union['Pipeline', 'TrainablePipeline']:
+    def get_pipeline_notebook(
+        self, pipeline_name: str = None, persist: "bool" = False
+    ) -> Union["Pipeline", "TrainablePipeline"]:
         """Get particular computed pipeline notebook"""
         pass
 
     @abstractmethod
-    def predict(self, X: Union['DataFrame', 'ndarray']) -> 'ndarray':
+    def predict(self, X: Union["DataFrame", "ndarray"]) -> "ndarray":
         """Use predict on top of the computed pipeline."""
         pass
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/experiment/autoai/optimizers/local_auto_pipelines.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/experiment/autoai/optimizers/local_auto_pipelines.py`

 * *Files 8% similar despite different names*

```diff
@@ -12,34 +12,46 @@
 from typing import Union, List, Tuple, TYPE_CHECKING
 from warnings import filterwarnings
 
 from numpy import ndarray
 from pandas import DataFrame
 from pandas import Series
 from ibm_watsonx_ai.utils.autoai.enums import (
-    PredictionType, Directions, MetricsToDirections, PipelineTypes)
+    PredictionType,
+    Directions,
+    MetricsToDirections,
+    PipelineTypes,
+)
 from ibm_watsonx_ai.utils.autoai.errors import FitNeeded
-from ibm_watsonx_ai.utils.autoai.local_training_message_handler import LocalTrainingMessageHandler
+from ibm_watsonx_ai.utils.autoai.local_training_message_handler import (
+    LocalTrainingMessageHandler,
+)
 from ibm_watsonx_ai.utils.autoai.utils import (
-    try_import_lale, create_summary, download_experiment_details_from_file, prepare_model_location_path,
-     try_import_joblib)
+    try_import_lale,
+    create_summary,
+    download_experiment_details_from_file,
+    prepare_model_location_path,
+    try_import_joblib,
+)
 from ibm_watsonx_ai.wml_client_error import WMLClientError
 
 from .base_auto_pipelines import BaseAutoPipelines
 
 if TYPE_CHECKING:
     from ibm_watsonx_ai.utils.autoai.enums import (
-        Metrics, ClassificationAlgorithms, RegressionAlgorithms, Transformers)
+        Metrics,
+        ClassificationAlgorithms,
+        RegressionAlgorithms,
+        Transformers,
+    )
     from ibm_watsonx_ai.helpers import DataConnection
     from sklearn.pipeline import Pipeline
     from ibm_boto3 import resource
 
-__all__ = [
-    "LocalAutoPipelines"
-]
+__all__ = ["LocalAutoPipelines"]
 DATE_FORMAT = "%Y-%m-%dT%H:%M:%SZ"
 
 
 class LocalAutoPipelines(BaseAutoPipelines):
     """LocalAutoPipelines class for pipeline operation automation.
 
     :param name: name for the AutoPipelines
@@ -83,93 +95,104 @@
     :param _result_client: internal argument to auto-gen notebooks
     :type _result_client: client or resource, optional
 
     :param _force_local_scenario: internal argument to force local scenario enablement
     :type _force_local_scenario: bool, optional
     """
 
-    def __init__(self,
-                 name: str,
-                 prediction_type: 'PredictionType',
-                 prediction_column: str,
-                 scoring: 'Metrics',
-                 desc: str = None,
-                 holdout_size: float = 0.1,
-                 max_num_daub_ensembles: int = None,
-                 train_sample_rows_test_size: float = 1.,
-                 include_only_estimators: List[Union['ClassificationAlgorithms', 'RegressionAlgorithms']] = None,
-                 cognito_transform_names: List['Transformers'] = None,
-                 positive_label: str = None,
-                 _data_clients: List[Tuple['DataConnection', 'resource']] = None,
-                 _result_client: Tuple['DataConnection', 'resource'] = None,
-                 _force_local_scenario: bool = False,
-                 **_additional_params):
+    def __init__(
+        self,
+        name: str,
+        prediction_type: "PredictionType",
+        prediction_column: str,
+        scoring: "Metrics",
+        desc: str = None,
+        holdout_size: float = 0.1,
+        max_num_daub_ensembles: int = None,
+        train_sample_rows_test_size: float = 1.0,
+        include_only_estimators: List[
+            Union["ClassificationAlgorithms", "RegressionAlgorithms"]
+        ] = None,
+        cognito_transform_names: List["Transformers"] = None,
+        positive_label: str = None,
+        _data_clients: List[Tuple["DataConnection", "resource"]] = None,
+        _result_client: Tuple["DataConnection", "resource"] = None,
+        _force_local_scenario: bool = False,
+        **_additional_params,
+    ):
 
         self._force_local_scenario = _force_local_scenario
         self._training_data_reference = None
         self._training_result_reference = None
         self._additional_params = _additional_params
 
         # note: Local scenario should be implemented in the future (ai4ml needed locally)
         if _data_clients is None and _result_client is None:
             if not self._force_local_scenario:
                 raise NotImplementedError("Local scenario not yet implemented.")
 
             import logging
+
             # Disable printing to suppress warnings from ai4ml
             with redirect_stdout(open(os.devnull, "w")):
                 try:
-                    from ai4ml.joint_optimizers.prep_daub_cog_opt import PrepDaubCogOptEstimator
+                    from ai4ml.joint_optimizers.prep_daub_cog_opt import (
+                        PrepDaubCogOptEstimator,
+                    )
                     from ai4ml.utils.ai4ml_status import StatusMessageHandler
 
                 except ModuleNotFoundError:
-                    raise ModuleNotFoundError("To be able to use a Local Optimizer version, you need to have "
-                                              "a full ai4ml installed locally.")
+                    raise ModuleNotFoundError(
+                        "To be able to use a Local Optimizer version, you need to have "
+                        "a full ai4ml installed locally."
+                    )
 
             # note: ai4ml uses a default root handler, we need to recreate it to be able to log into the file
             for handler in logging.root.handlers[:]:
                 logging.root.removeHandler(handler)
 
-            logging.basicConfig(filename='local_auto_pipelines.log',
-                                filemode='w',
-                                format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
-                                datefmt=DATE_FORMAT,
-                                level=logging.DEBUG)
+            logging.basicConfig(
+                filename="local_auto_pipelines.log",
+                filemode="w",
+                format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
+                datefmt=DATE_FORMAT,
+                level=logging.DEBUG,
+            )
             # -- end note
 
             self.params = {
-                'name': name,
-                'desc': desc if desc else '',
-                'prediction_type': prediction_type,
-                'prediction_column': prediction_column,
-                'scoring': scoring,
-                'holdout_size': holdout_size,
-                'max_num_daub_ensembles': max_num_daub_ensembles,
-                'train_sample_rows_test_size': train_sample_rows_test_size,
-                'include_only_estimators': include_only_estimators,
-                'cognito_transform_names': cognito_transform_names,
-                'positive_label': positive_label
+                "name": name,
+                "desc": desc if desc else "",
+                "prediction_type": prediction_type,
+                "prediction_column": prediction_column,
+                "scoring": scoring,
+                "holdout_size": holdout_size,
+                "max_num_daub_ensembles": max_num_daub_ensembles,
+                "train_sample_rows_test_size": train_sample_rows_test_size,
+                "include_only_estimators": include_only_estimators,
+                "cognito_transform_names": cognito_transform_names,
+                "positive_label": positive_label,
             }
             self.best_pipeline = None
             self._pdcoe = None
             self._computed_pipelines_details = None
             self.logger = logging.getLogger()
 
         # note: this is the auto-gen notebook local scenario implementation
         else:
             self._data_clients = _data_clients
             self._result_client = _result_client
             self.params = {
-                'name': name,
-                'desc': desc if desc else '',
-                'prediction_type': prediction_type,
-                'prediction_column': prediction_column,
-                'scoring': scoring,
-                'holdout_size': holdout_size,
-                'max_num_daub_ensembles': max_num_daub_ensembles
+                "name": name,
+                "desc": desc if desc else "",
+                "prediction_type": prediction_type,
+                "prediction_column": prediction_column,
+                "scoring": scoring,
+                "holdout_size": holdout_size,
+                "max_num_daub_ensembles": max_num_daub_ensembles,
             }
 
     def get_params(self) -> dict:
         """Get configuration parameters of AutoPipelines.
 
         :return: AutoPipelines parameters
         :rtype: dict
@@ -200,15 +223,15 @@
             #                                     "LogisticRegressionEstimator",
             #                                     "RandomForestClassifierEstimator",
             #                                     "XGBClassifierEstimator"]
             # }
         """
         return self.params
 
-    def fit(self, X: 'DataFrame', y: 'Series') -> 'Pipeline':
+    def fit(self, X: "DataFrame", y: "Series") -> "Pipeline":
         """Run a training process of AutoAI locally.
 
         :param X: training dataset
         :type X: pandas.DataFrame
 
         :param y: target values
         :type y: pandas.Series
@@ -226,23 +249,27 @@
 
             fitted_best_model = local_optimizer.fit(X=test_data_x, y=test_data_y)
         """
         if not self._force_local_scenario:
             raise NotImplementedError("Local scenario not yet implemented.")
 
         if not isinstance(X, DataFrame) or not isinstance(y, Series):
-            raise TypeError("\"X\" should be of type pandas.DataFrame and \"y\" should be of type pandas.Series.")
+            raise TypeError(
+                '"X" should be of type pandas.DataFrame and "y" should be of type pandas.Series.'
+            )
 
         self._pdcoe = self._train(train_x=X, train_y=y)
         self.best_pipeline = self._pdcoe.best_pipeline
-        self._computed_pipelines_details = self._pdcoe.status_msg_handler.status_dict['ml_metrics']['global_output']
+        self._computed_pipelines_details = self._pdcoe.status_msg_handler.status_dict[
+            "ml_metrics"
+        ]["global_output"]
 
         return self._pdcoe.best_pipeline
 
-    def get_holdout_data(self) -> Tuple['DataFrame', 'ndarray']:
+    def get_holdout_data(self) -> Tuple["DataFrame", "ndarray"]:
         """Provide holdout part of the training dataset (X and y) to the user.
 
         :return: X, y
         :rtype: tuple[DataFrame, ndarray]
 
         **Example**
 
@@ -254,22 +281,24 @@
 
             holdout_data = local_optimizer.get_holdout_data()
         """
         if not self._force_local_scenario:
             raise NotImplementedError("Local scenario not yet implemented.")
 
         if self._pdcoe is None:
-            raise FitNeeded(reason="To list computed pipelines parameters, "
-                                   "first schedule a fit job by using a fit() method.")
+            raise FitNeeded(
+                reason="To list computed pipelines parameters, "
+                "first schedule a fit job by using a fit() method."
+            )
 
         columns = self._pdcoe.column_headers_list_Xholdout
 
         return DataFrame(self._pdcoe.X_holdout, columns=columns), self._pdcoe.y_holdout
 
-    def summary(self) -> 'DataFrame':
+    def summary(self) -> "DataFrame":
         """Prints AutoPipelineOptimizer Pipelines details (autoai trained pipelines).
 
         :return: Pandas DataFrame with computed pipelines and ML metrics
         :rtype: pandas.DataFrame
 
         **Example**
 
@@ -285,50 +314,69 @@
             #                training_normalized_gini_coefficient  ...  training_f1
             # Pipeline Name                                        ...
             # Pipeline_3                                 0.359173  ...     0.449197
             # Pipeline_4                                 0.359173  ...     0.449197
             # Pipeline_1                                 0.358124  ...     0.449057
             # Pipeline_2                                 0.358124  ...     0.449057
         """
-        if hasattr(self, '_result_client'):
+        if hasattr(self, "_result_client"):
             details = download_experiment_details_from_file(self._result_client)
 
-            return create_summary(details=details, scoring=self.params['scoring'])
+            return create_summary(details=details, scoring=self.params["scoring"])
 
         # note: pure local scenario
         else:
             if not self._force_local_scenario:
                 raise NotImplementedError("Local scenario not yet implemented.")
 
-            score_names = [f"training_{name}" for name in
-                           self._computed_pipelines_details['Pipeline0']['Score']['training']['scores'].keys()]
-            columns = (['Pipeline Name', 'Number of enhancements'] + score_names)
+            score_names = [
+                f"training_{name}"
+                for name in self._computed_pipelines_details["Pipeline0"]["Score"][
+                    "training"
+                ]["scores"].keys()
+            ]
+            columns = ["Pipeline Name", "Number of enhancements"] + score_names
             values = []
 
             for name, pipeline in self._computed_pipelines_details.items():
                 pipeline_name = f"Pipeline_{name.split('P')[-1]}"
-                num_enhancements = len(pipeline['CompositionSteps']) - 5
-                scores = [score for score in pipeline['Score']['training']['scores'].values()]
+                num_enhancements = len(pipeline["CompositionSteps"]) - 5
+                scores = [
+                    score for score in pipeline["Score"]["training"]["scores"].values()
+                ]
                 values.append([pipeline_name, num_enhancements] + scores)
 
             pipelines = DataFrame(data=values, columns=columns)
-            pipelines.drop_duplicates(subset="Pipeline Name", keep='first', inplace=True)
-            pipelines.set_index('Pipeline Name', inplace=True)
-
-            if (MetricsToDirections[self._pdcoe.scorer_for_ranking.upper()].value ==
-                    Directions.ASCENDING):
-                return pipelines.sort_values(by=[f"training_{self._pdcoe.scorer_for_ranking}"], ascending=False).rename(
+            pipelines.drop_duplicates(
+                subset="Pipeline Name", keep="first", inplace=True
+            )
+            pipelines.set_index("Pipeline Name", inplace=True)
+
+            if (
+                MetricsToDirections[self._pdcoe.scorer_for_ranking.upper()].value
+                == Directions.ASCENDING
+            ):
+                return pipelines.sort_values(
+                    by=[f"training_{self._pdcoe.scorer_for_ranking}"], ascending=False
+                ).rename(
                     {
-                        f"training_{self._pdcoe.scorer_for_ranking}": f"training_{self._pdcoe.scorer_for_ranking}_(optimized)"},
-                    axis='columns')
+                        f"training_{self._pdcoe.scorer_for_ranking}": f"training_{self._pdcoe.scorer_for_ranking}_(optimized)"
+                    },
+                    axis="columns",
+                )
 
             else:
-                return pipelines.sort_values(by=[f"training_{self._pdcoe.scorer_for_ranking}"]).rename({
-                    f"training_{self._pdcoe.scorer_for_ranking}": f"training_{self._pdcoe.scorer_for_ranking}_(optimized)"},
-                    axis='columns')
+                return pipelines.sort_values(
+                    by=[f"training_{self._pdcoe.scorer_for_ranking}"]
+                ).rename(
+                    {
+                        f"training_{self._pdcoe.scorer_for_ranking}": f"training_{self._pdcoe.scorer_for_ranking}_(optimized)"
+                    },
+                    axis="columns",
+                )
         # --- end note
 
     def get_pipeline_details(self, pipeline_name: str = None) -> dict:
         """Fetch specific pipeline details, eg. steps etc.
 
         :param pipeline_name: pipeline name eg. Pipeline_1, if not specified, best pipeline parameters will be fetched
         :type pipeline_name: str, optional
@@ -342,61 +390,75 @@
 
             from ibm_watsonx_ai.experiment import AutoAI
             experiment = AutoAI()
             local_optimizer = experiment.optimizer()
 
             pipeline_details = local_optimizer.get_pipeline_details(pipeline_name="Pipeline_1")
         """
-        if hasattr(self, '_result_client'):
+        if hasattr(self, "_result_client"):
             details = download_experiment_details_from_file(self._result_client)
 
             if pipeline_name is None:
                 pipeline_name = self.summary().index[0]
 
             pipeline_parameters = {
                 "composition_steps": [],
                 "pipeline_nodes": [],
             }
 
-            for pipeline in details['entity']['status'].get('metrics', []):
-                if (pipeline['context']['phase'] == 'global_output' and
-                        pipeline['context']['intermediate_model']['name'].split('P')[-1] == pipeline_name.split('_')[
-                            -1]):
-                    pipeline_parameters['composition_steps'] = pipeline['context']['intermediate_model'][
-                        'composition_steps']
-                    pipeline_parameters['pipeline_nodes'] = pipeline['context']['intermediate_model']['pipeline_nodes']
+            for pipeline in details["entity"]["status"].get("metrics", []):
+                if (
+                    pipeline["context"]["phase"] == "global_output"
+                    and pipeline["context"]["intermediate_model"]["name"].split("P")[-1]
+                    == pipeline_name.split("_")[-1]
+                ):
+                    pipeline_parameters["composition_steps"] = pipeline["context"][
+                        "intermediate_model"
+                    ]["composition_steps"]
+                    pipeline_parameters["pipeline_nodes"] = pipeline["context"][
+                        "intermediate_model"
+                    ]["pipeline_nodes"]
 
             return pipeline_parameters
 
         else:
             if not self._force_local_scenario:
                 raise NotImplementedError("Local scenario not yet implemented.")
 
             if self._pdcoe is None:
-                raise FitNeeded(reason="To list computed pipelines parameters, "
-                                       "first schedule a fit job by using a fit() method.")
+                raise FitNeeded(
+                    reason="To list computed pipelines parameters, "
+                    "first schedule a fit job by using a fit() method."
+                )
 
             if pipeline_name is None:
                 pipeline_name = self.summary().index[0]
 
-            pipeline_name = pipeline_name.replace('_', '')
+            pipeline_name = pipeline_name.replace("_", "")
 
             pipeline_parameters = {
-                "composition_steps": self._computed_pipelines_details[pipeline_name]['CompositionSteps'].values(),
-                "pipeline_nodes": [node['op'] for node in
-                                   self._computed_pipelines_details[pipeline_name]['Params']['pipeline'][
-                                       'nodes'].values()],
+                "composition_steps": self._computed_pipelines_details[pipeline_name][
+                    "CompositionSteps"
+                ].values(),
+                "pipeline_nodes": [
+                    node["op"]
+                    for node in self._computed_pipelines_details[pipeline_name][
+                        "Params"
+                    ]["pipeline"]["nodes"].values()
+                ],
             }
 
             return pipeline_parameters
 
-    def get_pipeline(self,
-                     pipeline_name: str = None,
-                     astype: 'PipelineTypes' = PipelineTypes.LALE,
-                     persist: 'bool' = False) -> Union['Pipeline', 'TrainablePipeline']:
+    def get_pipeline(
+        self,
+        pipeline_name: str = None,
+        astype: "PipelineTypes" = PipelineTypes.LALE,
+        persist: "bool" = False,
+    ) -> Union["Pipeline", "TrainablePipeline"]:
         """Get specified computed pipeline.
 
         :param pipeline_name: pipeline name, if you want to see the pipelines names, please use summary() method,
             if this parameter is None, the best pipeline will be fetched
         : type pipeline_name: str, optional
 
         :param astype: type of returned pipeline model, if not specified, lale type is chosen
@@ -424,45 +486,55 @@
             type(pipeline_3)
 
             # Result:
             # <class 'sklearn.pipeline.Pipeline'>
 
         """
         # note: try to download and load pipeline model from COS (auto-gen notebook scenario)
-        if hasattr(self, '_result_client'):
-            from ibm_watsonx_ai.utils.autoai.utils import create_model_download_link, remove_file
+        if hasattr(self, "_result_client"):
+            from ibm_watsonx_ai.utils.autoai.utils import (
+                create_model_download_link,
+                remove_file,
+            )
+
             joblib = try_import_joblib()
-            filename = 'pipeline_model_auto-gen_notebook.pickle'
+            filename = "pipeline_model_auto-gen_notebook.pickle"
 
             try:
                 if pipeline_name is not None:
                     key = self._result_client[0].location._model_location
 
                 else:
                     best_pipeline_name = self.summary().index[0]
-                    best_pipeline_name = f"Pipeline{int(best_pipeline_name.split('Pipeline_')[-1]) - 1}"
-                    path = prepare_model_location_path(model_path=self._result_client[0].location._model_location)
-                    key = f'{path}{best_pipeline_name}/model.pickle'
+                    best_pipeline_name = (
+                        f"Pipeline{int(best_pipeline_name.split('Pipeline_')[-1]) - 1}"
+                    )
+                    path = prepare_model_location_path(
+                        model_path=self._result_client[0].location._model_location
+                    )
+                    key = f"{path}{best_pipeline_name}/model.pickle"
 
                 self._result_client[1].meta.client.download_file(
                     Bucket=self._result_client[0].location.bucket,
                     Filename=filename,
-                    Key=key)
+                    Key=key,
+                )
 
             except Exception as cos_access_exception:
                 raise ConnectionError(
                     f"Unable to access data object in cloud object storage with credentials supplied. "
-                    f"Error: {cos_access_exception}")
+                    f"Error: {cos_access_exception}"
+                )
 
             # Disable printing to suppress warning from ai4ml
             with redirect_stdout(open(os.devnull, "w")):
                 pipeline_model = joblib.load(filename)
 
             # note: show download link in the notebook and save file or delete it after memory load
-            path = os.path.join(os.path.abspath('.'), filename)
+            path = os.path.join(os.path.abspath("."), filename)
             if persist:
                 create_model_download_link(path)
                 print(f"Local path to downloaded model: {path}")
 
             else:
                 remove_file(filename)
             # --- end note
@@ -470,43 +542,50 @@
 
         # note: normal local scenario
         else:
             if not self._force_local_scenario:
                 raise NotImplementedError("Local scenario not yet implemented.")
 
             if self._pdcoe is None:
-                raise FitNeeded(reason="To get computed pipeline, "
-                                       "first schedule a fit job by using a fit() method.")
+                raise FitNeeded(
+                    reason="To get computed pipeline, "
+                    "first schedule a fit job by using a fit() method."
+                )
 
             if pipeline_name is None:
                 pipeline_model = self.best_pipeline
 
             else:
-                pipeline_name = pipeline_name.replace('_', '')
-                pipeline_model = self._computed_pipelines_details[pipeline_name]['Model']
+                pipeline_name = pipeline_name.replace("_", "")
+                pipeline_model = self._computed_pipelines_details[pipeline_name][
+                    "Model"
+                ]
         # --- end note
 
         if astype == PipelineTypes.SKLEARN:
             return pipeline_model
 
         elif astype == PipelineTypes.LALE:
             try_import_lale()
             from lale.helpers import import_from_sklearn_pipeline
+
             return import_from_sklearn_pipeline(pipeline_model)
 
         else:
-            raise ValueError('Incorrect value of \'astype\'. '
-                             'Should be either PipelineTypes.SKLEARN or PipelineTypes.LALE')
-
-    def get_pipeline_notebook(self,
-                     pipeline_name: str = None,
-                     persist: 'bool' = False) -> Union['Pipeline', 'TrainablePipeline']:
+            raise ValueError(
+                "Incorrect value of 'astype'. "
+                "Should be either PipelineTypes.SKLEARN or PipelineTypes.LALE"
+            )
+
+    def get_pipeline_notebook(
+        self, pipeline_name: str = None, persist: "bool" = False
+    ) -> Union["Pipeline", "TrainablePipeline"]:
         raise WMLClientError("Not supported in local scenario.")
 
-    def predict(self, X: Union['DataFrame', 'ndarray']) -> 'ndarray':
+    def predict(self, X: Union["DataFrame", "ndarray"]) -> "ndarray":
         """Predict method called on top of the best computed pipeline.
 
         :param X: test data for prediction
         :type X: numpy.ndarray or pandas.DataFrame
 
         :return: model predictions
         :rtype: numpy.ndarray
@@ -517,48 +596,58 @@
 
             from ibm_watsonx_ai.experiment import AutoAI
             experiment = AutoAI()
             local_optimizer = experiment.optimizer()
 
             predictions = local_optimizer.predict(X=test_data)
         """
-        if hasattr(self, '_result_client'):
+        if hasattr(self, "_result_client"):
             if isinstance(X, DataFrame) or isinstance(X, ndarray):
                 best_pipeline = self.get_pipeline(astype=PipelineTypes.SKLEARN)
                 return best_pipeline.predict(X if isinstance(X, ndarray) else X.values)
 
             else:
-                raise TypeError("X should be either of type pandas.DataFrame or numpy.ndarray")
+                raise TypeError(
+                    "X should be either of type pandas.DataFrame or numpy.ndarray"
+                )
 
         else:
             if not self._force_local_scenario:
                 raise NotImplementedError("Local scenario not yet implemented.")
 
             if isinstance(X, DataFrame) or isinstance(X, ndarray):
                 if self.best_pipeline:
-                    return self.best_pipeline.predict(X if isinstance(X, ndarray) else X.values)
+                    return self.best_pipeline.predict(
+                        X if isinstance(X, ndarray) else X.values
+                    )
                 else:
-                    raise FitNeeded("To list computed pipelines parameters, "
-                                    "first schedule a fit job by using a fit() method.")
+                    raise FitNeeded(
+                        "To list computed pipelines parameters, "
+                        "first schedule a fit job by using a fit() method."
+                    )
             else:
-                raise TypeError("X should be either of type pandas.DataFrame or numpy.ndarray")
+                raise TypeError(
+                    "X should be either of type pandas.DataFrame or numpy.ndarray"
+                )
 
-    def get_data_connections(self) -> List['DataConnection']:
+    def get_data_connections(self) -> List["DataConnection"]:
         """Provides list of DataConnections with training data that user specified.
 
         :return: list of DataConnections with populated optimizer parameters
         :rtype: list[DataConnection]
         """
-        if hasattr(self, '_data_clients'):
+        if hasattr(self, "_data_clients"):
             return self._training_data_reference
 
         else:
             raise NotImplementedError("Local scenario not yet implemented.")
 
-    def _train(self, train_x: 'DataFrame', train_y: 'Series') -> 'PrepDaubCogOptEstimator':
+    def _train(
+        self, train_x: "DataFrame", train_y: "Series"
+    ) -> "PrepDaubCogOptEstimator":
         """Prepare and run PDCOE optimizer/estimator.
 
         :param train_x: training dataset
         :type train_x: pandas.DataFrame
 
         :param train_y: target values
         :type train_y: pandas.Series
@@ -571,56 +660,74 @@
             from ai4ml.joint_optimizers.prep_daub_cog_opt import PrepDaubCogOptEstimator
             from ai4ml.utils.ai4ml_status import StatusMessageHandler
 
         filterwarnings("ignore")
         message_handler_with_progress_bar = LocalTrainingMessageHandler()
         train_id = str(uuid.uuid4())
 
-        self.logger.debug(f"train_id: {train_id} --- Preparing started at: {strftime(DATE_FORMAT, gmtime())}")
+        self.logger.debug(
+            f"train_id: {train_id} --- Preparing started at: {strftime(DATE_FORMAT, gmtime())}"
+        )
 
         pdcoe_signature = signature(PrepDaubCogOptEstimator)
 
         # note: prepare estimator parameters
         estimator_parameters = {
-            'learning_type': self.params['prediction_type'],
-            'run_cognito_flag': True,
-            'show_status_flag': True,
-            'status_msg_handler': StatusMessageHandler(
-                job_id=train_id, handle_func=message_handler_with_progress_bar.on_training_message),
-            'compute_feature_importances_flag': self.params.get('compute_feature_importances_flag', True),
+            "learning_type": self.params["prediction_type"],
+            "run_cognito_flag": True,
+            "show_status_flag": True,
+            "status_msg_handler": StatusMessageHandler(
+                job_id=train_id,
+                handle_func=message_handler_with_progress_bar.on_training_message,
+            ),
+            "compute_feature_importances_flag": self.params.get(
+                "compute_feature_importances_flag", True
+            ),
             # TODO: expose this parameter to the user
-            'compute_feature_importances_options': ['pipeline'],
-            'compute_pipeline_notebooks_flag': False,
-            'scorer_for_ranking': self.params['scoring'],
-            'cognito_transform_names': self.params.get('cognito_transform_names')
+            "compute_feature_importances_options": ["pipeline"],
+            "compute_pipeline_notebooks_flag": False,
+            "scorer_for_ranking": self.params["scoring"],
+            "cognito_transform_names": self.params.get("cognito_transform_names"),
         }
 
-        if self.params['max_num_daub_ensembles'] is not None:
-            estimator_parameters['max_num_daub_ensembles'] = self.params['max_num_daub_ensembles']
+        if self.params["max_num_daub_ensembles"] is not None:
+            estimator_parameters["max_num_daub_ensembles"] = self.params[
+                "max_num_daub_ensembles"
+            ]
 
         # note: only pass positive label when scoring is binary
-        if self.params['positive_label'] and self.params['scoring'] == PredictionType.BINARY:
-            estimator_parameters['positive_class'] = self.params['positive_label']
+        if (
+            self.params["positive_label"]
+            and self.params["scoring"] == PredictionType.BINARY
+        ):
+            estimator_parameters["positive_class"] = self.params["positive_label"]
         # --- end note
 
-        if pdcoe_signature.parameters.get('target_label_name') is not None:
-            estimator_parameters['target_label_name'] = self.params['prediction_column']
+        if pdcoe_signature.parameters.get("target_label_name") is not None:
+            estimator_parameters["target_label_name"] = self.params["prediction_column"]
 
-        if 'CPU' in os.environ:
+        if "CPU" in os.environ:
             try:
-                self.logger.debug(f"train_id: {train_id} --- Using {os.environ.get('CPU', 1)} CPUs")
+                self.logger.debug(
+                    f"train_id: {train_id} --- Using {os.environ.get('CPU', 1)} CPUs"
+                )
                 # TODO: expose this parameter to the user
-                estimator_parameters['cpus_available'] = int(self.params.get('cpus_available',
-                                                                             os.environ.get('CPU', 1)))
+                estimator_parameters["cpus_available"] = int(
+                    self.params.get("cpus_available", os.environ.get("CPU", 1))
+                )
             except Exception as e:
                 self.logger.error(f"Fail setting CPUs ({e}) {traceback.format_exc()}")
         # --- end note
 
-        pdcoe = PrepDaubCogOptEstimator(**estimator_parameters, **self._additional_params)
-        self.logger.debug(f"{train_id} --- Training started at: {strftime(DATE_FORMAT, gmtime())}")
+        pdcoe = PrepDaubCogOptEstimator(
+            **estimator_parameters, **self._additional_params
+        )
+        self.logger.debug(
+            f"{train_id} --- Training started at: {strftime(DATE_FORMAT, gmtime())}"
+        )
 
         # Disable printing to suppress warnings from ai4ml
         with redirect_stdout(open(os.devnull, "w")):
             pdcoe.fit(train_x, train_y.values)
 
         if message_handler_with_progress_bar.progress_bar is not None:
             message_handler_with_progress_bar.progress_bar.last_update()
@@ -628,9 +735,11 @@
 
         else:
             message_handler_with_progress_bar.progress_bar_2.last_update()
             message_handler_with_progress_bar.progress_bar_2.close()
             message_handler_with_progress_bar.progress_bar_1.last_update()
             message_handler_with_progress_bar.progress_bar_1.close()
 
-        self.logger.debug(f"{train_id} --- End training at: {strftime(DATE_FORMAT, gmtime())}")
+        self.logger.debug(
+            f"{train_id} --- End training at: {strftime(DATE_FORMAT, gmtime())}"
+        )
         return pdcoe
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/experiment/autoai/optimizers/remote_auto_pipelines.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/experiment/autoai/optimizers/remote_auto_pipelines.py`

 * *Files 10% similar despite different names*

```diff
@@ -10,39 +10,61 @@
 from warnings import warn
 from contextlib import redirect_stdout
 
 from numpy import ndarray
 from pandas import DataFrame
 
 from ibm_watsonx_ai.helpers.connections import (
-    DataConnection, S3Location, FSLocation, AssetLocation, ContainerLocation, DatabaseLocation)
+    DataConnection,
+    S3Location,
+    FSLocation,
+    AssetLocation,
+    ContainerLocation,
+    DatabaseLocation,
+)
 from ibm_watsonx_ai.utils.autoai.enums import (
-    RunStateTypes, PipelineTypes, TShirtSize, ClassificationAlgorithms, RegressionAlgorithms, DataConnectionTypes,
-    ForecastingPipelineTypes, PredictionType, TimeseriesAnomalyPredictionPipelineTypes)
+    RunStateTypes,
+    PipelineTypes,
+    TShirtSize,
+    ClassificationAlgorithms,
+    RegressionAlgorithms,
+    DataConnectionTypes,
+    ForecastingPipelineTypes,
+    PredictionType,
+    TimeseriesAnomalyPredictionPipelineTypes,
+)
 from ibm_watsonx_ai.utils.autoai.errors import (
-    FitNotCompleted, MissingDataPreprocessingStep, DataSourceSizeNotSupported,
-    TrainingDataSourceIsNotFile, NoneDataConnection, PipelineNotLoaded,
-    ForecastingUnsupportedOperation, LibraryNotCompatible, InvalidDataAsset, TestDataNotPresent,
-    FutureExogenousFeaturesNotSupported)
+    FitNotCompleted,
+    MissingDataPreprocessingStep,
+    DataSourceSizeNotSupported,
+    TrainingDataSourceIsNotFile,
+    NoneDataConnection,
+    PipelineNotLoaded,
+    ForecastingUnsupportedOperation,
+    LibraryNotCompatible,
+    InvalidDataAsset,
+    TestDataNotPresent,
+    FutureExogenousFeaturesNotSupported,
+)
 from ibm_watsonx_ai.utils.autoai.utils import try_import_lale, all_logging_disabled
-from ibm_watsonx_ai.utils.autoai.connection import validate_source_data_connections, \
-    validate_results_data_connection
+from ibm_watsonx_ai.utils.autoai.connection import (
+    validate_source_data_connections,
+    validate_results_data_connection,
+)
 from ibm_watsonx_ai.utils import DisableWarningsLogger, WMLClientError
 from ibm_watsonx_ai.messages.messages import Messages
 from .base_auto_pipelines import BaseAutoPipelines
 from ibm_watsonx_ai.experiment.autoai.engines import ServiceEngine
 
 if TYPE_CHECKING:
     from ibm_watsonx_ai.experiment.autoai.engines import WMLEngine
     from ibm_watsonx_ai.utils.autoai.enums import Metrics, PredictionType, Transformers
     from sklearn.pipeline import Pipeline
 
-__all__ = [
-    "RemoteAutoPipelines"
-]
+__all__ = ["RemoteAutoPipelines"]
 
 
 class RemoteAutoPipelines(BaseAutoPipelines):
     """RemoteAutoPipelines class for pipeline operation automation on Service.
 
     :param name: name for the AutoPipelines
     :type name: str
@@ -111,147 +133,159 @@
                 are selected in place of insignificant features. The model is evaluated again. If there is still drop
                 in accuracy all features are used.
                 The "on" mode removes all insignificant features (0.0. importance). Feature selector is applied during
                 cognito phase (applicable to pipelines with feature engineering stage).
     :type feature_selector_mode: str, optional
     """
 
-    def __init__(self,
-                 name: str,
-                 prediction_type: 'PredictionType',
-                 prediction_column: str,
-                 prediction_columns: List[str],
-                 timestamp_column_name: str,
-                 engine: Union['WMLEngine', 'ServiceEngine'],
-                 scoring: 'Metrics' = None,
-                 desc: str = None,
-                 holdout_size: float = None,
-                 max_num_daub_ensembles: int = None,
-                 t_shirt_size: 'TShirtSize' = TShirtSize.M,
-                 train_sample_rows_test_size: float = None,
-                 include_only_estimators: List[Union['ClassificationAlgorithms', 'RegressionAlgorithms']] = None,
-                 include_batched_ensemble_estimators: List[Union['BatchedClassificationAlgorithms',
-                                                                 'BatchedRegressionAlgorithms']] = None,
-                 backtest_num: int = None,
-                 lookback_window: int = None,
-                 forecast_window: int = None,
-                 backtest_gap_length: int = None,
-                 cognito_transform_names: List['Transformers'] = None,
-                 csv_separator: Union[List[str], str] = ',',
-                 excel_sheet: Union[str, int] = None,
-                 encoding: str = 'utf-8',
-                 positive_label: str = None,
-                 drop_duplicates: bool = True,
-                 outliers_columns: list = None,
-                 text_processing: bool = True,
-                 word2vec_feature_number: int = None,
-                 daub_give_priority_to_runtime: float = None,
-                 notebooks=False,
-                 autoai_pod_version=None,
-                 text_columns_names=None,
-                 n_parallel_data_connections=None,
-                 test_data_csv_separator: Union[List[str], str] = ',',
-                 test_data_excel_sheet: Union[str, int] = None,
-                 test_data_encoding: str = 'utf-8',
-                 sampling_type=None,
-                 sample_size_limit=None,
-                 sample_rows_limit=None,
-                 sample_percentage_limit=None,
-                 number_of_batch_rows=None,
-                 categorical_imputation_strategy=None,
-                 numerical_imputation_strategy=None,
-                 numerical_imputation_value=None,
-                 imputation_threshold=None,
-                 fairness_info: dict = None,
-                 retrain_on_holdout: bool = True,
-                 feature_columns: List[str] = None,
-                 pipeline_types: List[Union['ForecastingPipelineTypes', 'TimeseriesAnomalyPredictionPipelineTypes']] = None,
-                 supporting_features_at_forecast: bool = None,
-                 categorical_columns: list = None,
-                 numerical_columns: list = None,
-                 confidence_level: float = None,
-                 incremental_learning: bool = None,
-                 early_stop_enabled: bool = None,
-                 early_stop_window_size: int = None,
-                 time_ordered_data: bool = None,
-                 feature_selector_mode: str = None,
-                 **kwargs):
+    def __init__(
+        self,
+        name: str,
+        prediction_type: "PredictionType",
+        prediction_column: str,
+        prediction_columns: List[str],
+        timestamp_column_name: str,
+        engine: Union["WMLEngine", "ServiceEngine"],
+        scoring: "Metrics" = None,
+        desc: str = None,
+        holdout_size: float = None,
+        max_num_daub_ensembles: int = None,
+        t_shirt_size: "TShirtSize" = TShirtSize.M,
+        train_sample_rows_test_size: float = None,
+        include_only_estimators: List[
+            Union["ClassificationAlgorithms", "RegressionAlgorithms"]
+        ] = None,
+        include_batched_ensemble_estimators: List[
+            Union["BatchedClassificationAlgorithms", "BatchedRegressionAlgorithms"]
+        ] = None,
+        backtest_num: int = None,
+        lookback_window: int = None,
+        forecast_window: int = None,
+        backtest_gap_length: int = None,
+        cognito_transform_names: List["Transformers"] = None,
+        csv_separator: Union[List[str], str] = ",",
+        excel_sheet: Union[str, int] = None,
+        encoding: str = "utf-8",
+        positive_label: str = None,
+        drop_duplicates: bool = True,
+        outliers_columns: list = None,
+        text_processing: bool = True,
+        word2vec_feature_number: int = None,
+        daub_give_priority_to_runtime: float = None,
+        notebooks=False,
+        autoai_pod_version=None,
+        text_columns_names=None,
+        n_parallel_data_connections=None,
+        test_data_csv_separator: Union[List[str], str] = ",",
+        test_data_excel_sheet: Union[str, int] = None,
+        test_data_encoding: str = "utf-8",
+        sampling_type=None,
+        sample_size_limit=None,
+        sample_rows_limit=None,
+        sample_percentage_limit=None,
+        number_of_batch_rows=None,
+        categorical_imputation_strategy=None,
+        numerical_imputation_strategy=None,
+        numerical_imputation_value=None,
+        imputation_threshold=None,
+        fairness_info: dict = None,
+        retrain_on_holdout: bool = True,
+        feature_columns: List[str] = None,
+        pipeline_types: List[
+            Union[
+                "ForecastingPipelineTypes", "TimeseriesAnomalyPredictionPipelineTypes"
+            ]
+        ] = None,
+        supporting_features_at_forecast: bool = None,
+        categorical_columns: list = None,
+        numerical_columns: list = None,
+        confidence_level: float = None,
+        incremental_learning: bool = None,
+        early_stop_enabled: bool = None,
+        early_stop_window_size: int = None,
+        time_ordered_data: bool = None,
+        feature_selector_mode: str = None,
+        **kwargs,
+    ):
 
         # Deprecation of excel_sheet as number:
         if isinstance(excel_sheet, int) or isinstance(test_data_excel_sheet, int):
             warn(
-                message="Support for excel sheet as number of the sheet (int) is deprecated! Please set excel sheet with name of the sheet.")
+                message="Support for excel sheet as number of the sheet (int) is deprecated! Please set excel sheet with name of the sheet."
+            )
 
         self.params = {
-            'name': name,
-            'desc': desc if desc else '',
-            'prediction_type': prediction_type if prediction_type != 'timeseries' else 'forecasting',
-            'prediction_column': prediction_column,
-            'prediction_columns': prediction_columns,
-            'timestamp_column_name': timestamp_column_name,
-            'scoring': scoring,
-            'holdout_size': holdout_size,
-            'max_num_daub_ensembles': max_num_daub_ensembles,
-            't_shirt_size': t_shirt_size,
-            'train_sample_rows_test_size': train_sample_rows_test_size,
-            'include_only_estimators': include_only_estimators,
-            'include_batched_ensemble_estimators': include_batched_ensemble_estimators,
-            'backtest_num': backtest_num,
-            'lookback_window': lookback_window,
-            'forecast_window': forecast_window,
-            'backtest_gap_length': backtest_gap_length,
-            'cognito_transform_names': cognito_transform_names,
-            'csv_separator': csv_separator,
-            'excel_sheet': excel_sheet,
-            'encoding': encoding,
-            'positive_label': positive_label,
-            'drop_duplicates': drop_duplicates,
-            'outliers_columns': outliers_columns,
-            'notebooks': notebooks,
-            'autoai_pod_version': autoai_pod_version,
-            'text_processing': text_processing,
-            'word2vec_feature_number': word2vec_feature_number,
-            'daub_give_priority_to_runtime': daub_give_priority_to_runtime,
-            'text_columns_names': text_columns_names,
-            'sampling_type': sampling_type,
-            'sample_size_limit': sample_size_limit,
-            'sample_rows_limit': sample_rows_limit,
-            'sample_percentage_limit': sample_percentage_limit,
-            'number_of_batch_rows': number_of_batch_rows,
-            'n_parallel_data_connections': n_parallel_data_connections,
-            'test_data_csv_separator': test_data_csv_separator,
-            'test_data_excel_sheet': test_data_excel_sheet,
-            'test_data_encoding': test_data_encoding,
-            'categorical_imputation_strategy': categorical_imputation_strategy,
-            'numerical_imputation_strategy': numerical_imputation_strategy,
-            'numerical_imputation_value': numerical_imputation_value,
-            'imputation_threshold': imputation_threshold,
-            'retrain_on_holdout': retrain_on_holdout,
-            'feature_columns': feature_columns,
-            'pipeline_types': pipeline_types,
-            'supporting_features_at_forecast': supporting_features_at_forecast,
-            'numerical_columns': numerical_columns,
-            'categorical_columns': categorical_columns,
-            'confidence_level': confidence_level,
-            'incremental_learning': incremental_learning,
-            'early_stop_enabled': early_stop_enabled,
-            'early_stop_window_size': early_stop_window_size,
-            'time_ordered_data': time_ordered_data,
-            'feature_selector_mode': feature_selector_mode,
+            "name": name,
+            "desc": desc if desc else "",
+            "prediction_type": (
+                prediction_type if prediction_type != "timeseries" else "forecasting"
+            ),
+            "prediction_column": prediction_column,
+            "prediction_columns": prediction_columns,
+            "timestamp_column_name": timestamp_column_name,
+            "scoring": scoring,
+            "holdout_size": holdout_size,
+            "max_num_daub_ensembles": max_num_daub_ensembles,
+            "t_shirt_size": t_shirt_size,
+            "train_sample_rows_test_size": train_sample_rows_test_size,
+            "include_only_estimators": include_only_estimators,
+            "include_batched_ensemble_estimators": include_batched_ensemble_estimators,
+            "backtest_num": backtest_num,
+            "lookback_window": lookback_window,
+            "forecast_window": forecast_window,
+            "backtest_gap_length": backtest_gap_length,
+            "cognito_transform_names": cognito_transform_names,
+            "csv_separator": csv_separator,
+            "excel_sheet": excel_sheet,
+            "encoding": encoding,
+            "positive_label": positive_label,
+            "drop_duplicates": drop_duplicates,
+            "outliers_columns": outliers_columns,
+            "notebooks": notebooks,
+            "autoai_pod_version": autoai_pod_version,
+            "text_processing": text_processing,
+            "word2vec_feature_number": word2vec_feature_number,
+            "daub_give_priority_to_runtime": daub_give_priority_to_runtime,
+            "text_columns_names": text_columns_names,
+            "sampling_type": sampling_type,
+            "sample_size_limit": sample_size_limit,
+            "sample_rows_limit": sample_rows_limit,
+            "sample_percentage_limit": sample_percentage_limit,
+            "number_of_batch_rows": number_of_batch_rows,
+            "n_parallel_data_connections": n_parallel_data_connections,
+            "test_data_csv_separator": test_data_csv_separator,
+            "test_data_excel_sheet": test_data_excel_sheet,
+            "test_data_encoding": test_data_encoding,
+            "categorical_imputation_strategy": categorical_imputation_strategy,
+            "numerical_imputation_strategy": numerical_imputation_strategy,
+            "numerical_imputation_value": numerical_imputation_value,
+            "imputation_threshold": imputation_threshold,
+            "retrain_on_holdout": retrain_on_holdout,
+            "feature_columns": feature_columns,
+            "pipeline_types": pipeline_types,
+            "supporting_features_at_forecast": supporting_features_at_forecast,
+            "numerical_columns": numerical_columns,
+            "categorical_columns": categorical_columns,
+            "confidence_level": confidence_level,
+            "incremental_learning": incremental_learning,
+            "early_stop_enabled": early_stop_enabled,
+            "early_stop_window_size": early_stop_window_size,
+            "time_ordered_data": time_ordered_data,
+            "feature_selector_mode": feature_selector_mode,
         }
 
         if fairness_info:
-            self.params['fairness_info'] = fairness_info
+            self.params["fairness_info"] = fairness_info
 
-        self._engine: Union['WMLEngine', 'ServiceEngine'] = engine
+        self._engine: Union["WMLEngine", "ServiceEngine"] = engine
         self._engine.initiate_remote_resources(params=self.params, **kwargs)
         self.best_pipeline = None
         self._workspace = None
 
-    def _get_engine(self) -> Union['WMLEngine', 'ServiceEngine']:
+    def _get_engine(self) -> Union["WMLEngine", "ServiceEngine"]:
         """Return Engine for development purposes."""
         return self._engine
 
     ####################################################
     #   WML Pipeline Part / Parameters for AUtoAI POD  #
     ####################################################
     def get_params(self) -> dict:
@@ -286,30 +320,32 @@
             #                                "LGBMClassifierEstimator",
             #                                "LogisticRegressionEstimator",
             #                                "RandomForestClassifierEstimator",
             #                                "XGBClassifierEstimator"]
             # }
         """
         _params = self._engine.get_params().copy()
-        del _params['autoai_pod_version']
-        del _params['notebooks']
+        del _params["autoai_pod_version"]
+        del _params["notebooks"]
 
         return _params
 
     ###########################################################
     #   WML Training Part / Parameters for AUtoAI Experiment  #
     ###########################################################
-    def fit(self,
-            train_data: 'DataFrame' = None,
-            *,
-            training_data_reference: List['DataConnection'] = None,
-            training_results_reference: 'DataConnection' = None,
-            background_mode=False,
-            test_data_references: List['DataConnection'] = None,
-            training_data_references: List['DataConnection'] = None) -> dict:
+    def fit(
+        self,
+        train_data: "DataFrame" = None,
+        *,
+        training_data_reference: List["DataConnection"] = None,
+        training_results_reference: "DataConnection" = None,
+        background_mode=False,
+        test_data_references: List["DataConnection"] = None,
+        training_data_references: List["DataConnection"] = None,
+    ) -> dict:
         """Run a training process on Service of autoai on top of the training data referenced by DataConnection.
 
         :param training_data_reference: data storage connection details to inform where training data is stored,
             deprecated parameter, use `training_data_references` instead
 
         :param training_data_references: data storage connection details to inform where training data is stored,
             new version of `training_data_reference`
@@ -355,157 +391,219 @@
                 ),
                 background_mode=False)
         """
         if training_data_references is not None:
             training_data_reference = training_data_references
 
         if training_data_reference is None or not training_data_reference:
-            raise NoneDataConnection('training_data_references')
+            raise NoneDataConnection("training_data_references")
 
         for conn in training_data_reference:
-            if self._workspace.api_client.project_type == 'local_git_storage':
-                conn.location.userfs = 'true'
+            if self._workspace.api_client.project_type == "local_git_storage":
+                conn.location.userfs = "true"
             conn.set_client(self._workspace.api_client)
             # TODO: remove S3 implementation
             if conn.type == DataConnectionTypes.S3:
                 conn._validate_cos_resource()
 
-        training_data_reference = [new_conn for conn in training_data_reference for new_conn in
-                                   conn._subdivide_connection()]
+        training_data_reference = [
+            new_conn
+            for conn in training_data_reference
+            for new_conn in conn._subdivide_connection()
+        ]
 
         if isinstance(test_data_references, list):
             for conn in test_data_references:
                 # Update test data ref with client object, experiment parameters
                 if isinstance(conn, DataConnection):
-                    if self._workspace.api_client.project_type == 'local_git_storage':
-                        conn.location.userfs = 'true'
+                    if self._workspace.api_client.project_type == "local_git_storage":
+                        conn.location.userfs = "true"
                     conn.set_client(self._workspace.api_client)
                     # TODO: remove S3 implementation
                     if conn.type == DataConnectionTypes.S3:
                         conn._validate_cos_resource()
 
                     conn.auto_pipeline_params = self._engine._auto_pipelines_parameters
 
         # note: update each training data connection with pipeline parameters for holdout split recreation
         for data_connection in training_data_reference:
-            data_connection.auto_pipeline_params = self._engine._auto_pipelines_parameters
+            data_connection.auto_pipeline_params = (
+                self._engine._auto_pipelines_parameters
+            )
 
         if isinstance(train_data, DataFrame):
-            if training_data_reference[0].type == 'container':
-                training_data_reference[0].write(data=train_data,
-                                                remote_name=training_data_reference[0].location.path)
-            elif training_data_reference[0].type == 'connection_asset':
+            if training_data_reference[0].type == "container":
+                training_data_reference[0].write(
+                    data=train_data,
+                    remote_name=training_data_reference[0].location.path,
+                )
+            elif training_data_reference[0].type == "connection_asset":
                 try:
                     # Cloud
-                    training_data_reference[0].write(data=train_data,
-                                                    remote_name=training_data_reference[0].location.file_name)
+                    training_data_reference[0].write(
+                        data=train_data,
+                        remote_name=training_data_reference[0].location.file_name,
+                    )
                 except AttributeError as e:
                     # CPD
-                    raise WMLClientError("Writing `train_data` to the DataConnection object passed in .fit() function is not supported. Please upload the data first and then run training again, without `train_data` parameter.") from e
-        
+                    raise WMLClientError(
+                        "Writing `train_data` to the DataConnection object passed in .fit() function is not supported. Please upload the data first and then run training again, without `train_data` parameter."
+                    ) from e
+
         elif train_data is None:
             pass
 
         else:
             raise TypeError("train_data should be of type pandas.DataFrame")
 
         # self._validate_training_data_size(training_data_reference)
 
-        training_data_reference = validate_source_data_connections(training_data_reference, workspace=self._workspace,
-                                                                   deployment=False)
+        training_data_reference = validate_source_data_connections(
+            training_data_reference, workspace=self._workspace, deployment=False
+        )
 
         # note: for FSLocation we are creating asset and changing location to AssetLocation
         # so href is not set properly, setter on api_client will resolve that issue
         for conn in training_data_reference:
             conn.set_client(self._workspace.api_client)
         # --- end note
 
-        training_results_reference = self.determine_result_reference(training_results_reference,
-                                                                     training_data_reference,
-                                                                     "default_autoai_out")
-
-        run_params = self._engine.fit(training_data_reference=training_data_reference,
-                                      training_results_reference=training_results_reference,
-                                      background_mode=background_mode,
-                                      test_data_references=test_data_references)
+        training_results_reference = self.determine_result_reference(
+            training_results_reference, training_data_reference, "default_autoai_out"
+        )
+
+        run_params = self._engine.fit(
+            training_data_reference=training_data_reference,
+            training_results_reference=training_results_reference,
+            background_mode=background_mode,
+            test_data_references=test_data_references,
+        )
 
         for conn in training_data_reference:
-            metrics = run_params['entity']['status'].get('metrics', [])
-            if metrics and metrics[-1]['context'].get('fairness'):
-                conn.auto_pipeline_params['fairness_info'] = metrics[-1]['context']['fairness'].get('info')
+            metrics = run_params["entity"]["status"].get("metrics", [])
+            if metrics and metrics[-1]["context"].get("fairness"):
+                conn.auto_pipeline_params["fairness_info"] = metrics[-1]["context"][
+                    "fairness"
+                ].get("info")
 
         return run_params
 
-    def determine_result_reference(self, results_reference, data_references, result_path):
+    def determine_result_reference(
+        self, results_reference, data_references, result_path
+    ):
         # note: if user did not provide results storage information, use default ones
         if results_reference is None:
-            if isinstance(data_references[0].location, S3Location) and not self._workspace.api_client.ICP_PLATFORM_SPACES:
+            if (
+                isinstance(data_references[0].location, S3Location)
+                and not self._workspace.api_client.ICP_PLATFORM_SPACES
+            ):
                 results_reference = DataConnection(
                     connection=data_references[0].connection,
-                    location=S3Location(bucket=data_references[0].location.bucket,
-                                        path=".")
+                    location=S3Location(
+                        bucket=data_references[0].location.bucket, path="."
+                    ),
                 )
 
-            elif isinstance(data_references[0].location, AssetLocation) and not self._workspace.api_client.ICP_PLATFORM_SPACES:
-                connection_id = data_references[0].location._get_connection_id(self._workspace.api_client)
+            elif (
+                isinstance(data_references[0].location, AssetLocation)
+                and not self._workspace.api_client.ICP_PLATFORM_SPACES
+            ):
+                connection_id = data_references[0].location._get_connection_id(
+                    self._workspace.api_client
+                )
 
                 if connection_id is not None:
                     results_reference = DataConnection(
                         connection_asset_id=connection_id,
                         location=S3Location(
-                            bucket=data_references[0].location._get_bucket(self._workspace.api_client),
-                            path=result_path)
+                            bucket=data_references[0].location._get_bucket(
+                                self._workspace.api_client
+                            ),
+                            path=result_path,
+                        ),
                     )
 
                 else:  # set container output location when default DAta Asset is as a train ref
                     results_reference = DataConnection(
-                        location=ContainerLocation(path=result_path))
+                        location=ContainerLocation(path=result_path)
+                    )
+
+            elif (
+                isinstance(data_references[0].location, ContainerLocation)
+                and not self._workspace.api_client.ICP_PLATFORM_SPACES
+            ):
+                results_reference = DataConnection(
+                    location=ContainerLocation(path=result_path)
+                )
 
-            elif isinstance(data_references[0].location,
-                            ContainerLocation) and not self._workspace.api_client.ICP_PLATFORM_SPACES:
-                results_reference = DataConnection(location=ContainerLocation(path=result_path))
-
-            elif isinstance(data_references[0].location,
-                            DatabaseLocation) and not self._workspace.api_client.ICP_PLATFORM_SPACES:
-                results_reference = DataConnection(location=ContainerLocation(path=result_path))
+            elif (
+                isinstance(data_references[0].location, DatabaseLocation)
+                and not self._workspace.api_client.ICP_PLATFORM_SPACES
+            ):
+                results_reference = DataConnection(
+                    location=ContainerLocation(path=result_path)
+                )
 
             else:
                 location = FSLocation()
-                client = self._engine._api_client if isinstance(self._engine, ServiceEngine) else self._engine._wml_client
+                client = (
+                    self._engine._api_client
+                    if isinstance(self._engine, ServiceEngine)
+                    else self._engine._wml_client
+                )
                 if self._workspace.api_client.default_project_id is None:
-                    
-                    location.path = location.path.format(option='spaces',
-                                                            id=client.default_space_id)
+
+                    location.path = location.path.format(
+                        option="spaces", id=client.default_space_id
+                    )
 
                 else:
-                    location.path = location.path.format(option='projects',
-                                                            id=client.default_project_id)
-                results_reference = DataConnection(
-                    connection=None,
-                    location=location
-                )
+                    location.path = location.path.format(
+                        option="projects", id=client.default_project_id
+                    )
+                results_reference = DataConnection(connection=None, location=location)
         elif getattr(results_reference, "type", False) == "fs":
-            client = self._engine._api_client if isinstance(self._engine, ServiceEngine) else self._engine._wml_client
+            client = (
+                self._engine._api_client
+                if isinstance(self._engine, ServiceEngine)
+                else self._engine._wml_client
+            )
             if self._workspace.api_client.default_project_id is None:
-                results_reference.location.path = results_reference.location.path.format(option='spaces',
-                                                                                         id=client.default_space_id)
+                results_reference.location.path = (
+                    results_reference.location.path.format(
+                        option="spaces", id=client.default_space_id
+                    )
+                )
             else:
-                results_reference.location.path = results_reference.location.path.format(option='projects',
-                                                                                         id=client.default_project_id)
+                results_reference.location.path = (
+                    results_reference.location.path.format(
+                        option="projects", id=client.default_project_id
+                    )
+                )
         # -- end note
         if isinstance(results_reference.location, AssetLocation):
-            if results_reference.location._get_connection_id(self._workspace.api_client) is None:
+            if (
+                results_reference.location._get_connection_id(
+                    self._workspace.api_client
+                )
+                is None
+            ):
                 raise InvalidDataAsset(
-                    reason="Please specify Data Asset pointing to connection e.g. COS as an output.")
+                    reason="Please specify Data Asset pointing to connection e.g. COS as an output."
+                )
 
         # note: results can be stored only on FS or COS
-        if not isinstance(results_reference.location,
-                          (S3Location, FSLocation, AssetLocation, ContainerLocation)):
-            raise TypeError('Unsupported results location type. Results referance can be stored'
-                            ' only on S3Location or FSLocation or AssetLocation.')
+        if not isinstance(
+            results_reference.location,
+            (S3Location, FSLocation, AssetLocation, ContainerLocation),
+        ):
+            raise TypeError(
+                "Unsupported results location type. Results referance can be stored"
+                " only on S3Location or FSLocation or AssetLocation."
+            )
         # -- end
 
         return results_reference
 
     #####################
     #   Run operations  #
     #####################
@@ -552,15 +650,17 @@
     def cancel_run(self) -> None:
         """Cancels an AutoAI run."""
         self._engine.cancel_run()
 
     #################################
     #   Pipeline models operations  #
     #################################
-    def summary(self, scoring: str = None, sort_by_holdout_score: bool = True) -> 'DataFrame':
+    def summary(
+        self, scoring: str = None, sort_by_holdout_score: bool = True
+    ) -> "DataFrame":
         """Print AutoPipelineOptimizer Pipelines details (autoai trained pipelines).
 
         :param scoring: scoring metric which user wants to use to sort pipelines by,
             when not provided use optimized one
         :type scoring: string, optional
 
         :param sort_by_holdout_score: indicates if we want to sort pipelines by holdout metric or by training one,
@@ -615,18 +715,20 @@
             #     'composition_steps': ['TrainingDataset_full_4521_16', 'Split_TrainingHoldout',
             #                           'TrainingDataset_full_4068_16', 'Preprocessor_default', 'DAUB'],
             #     'pipeline_nodes': ['PreprocessingTransformer', 'GradientBoostingClassifierEstimator']
             # }
         """
         return self._engine.get_pipeline_details(pipeline_name=pipeline_name)
 
-    def get_pipeline(self,
-                     pipeline_name: str = None,
-                     astype: 'PipelineTypes' = PipelineTypes.LALE,
-                     persist: 'bool' = False) -> Union['Pipeline', 'TrainablePipeline']:
+    def get_pipeline(
+        self,
+        pipeline_name: str = None,
+        astype: "PipelineTypes" = PipelineTypes.LALE,
+        persist: "bool" = False,
+    ) -> Union["Pipeline", "TrainablePipeline"]:
         """Download specified pipeline from Service.
 
         :param pipeline_name: pipeline name, if you want to see the pipelines names, please use summary() method,
             if this parameter is None, the best pipeline will be fetched
         :type pipeline_name: str, optional
 
         :param astype: type of returned pipeline model, if not specified, lale type is chosen
@@ -655,46 +757,57 @@
             # <class 'sklearn.pipeline.Pipeline'>
             pipeline_4 = remote_optimizer.get_pipeline(pipeline_name='Pipeline_1', persist=True)
             # Selected pipeline stored under: "absolute_local_path_to_model/model.pickle"
 
         """
         try:
             if pipeline_name is None:
-                pipeline_model, check_lale = self._engine.get_best_pipeline(persist=persist)
+                pipeline_model, check_lale = self._engine.get_best_pipeline(
+                    persist=persist
+                )
 
             else:
-                pipeline_model, check_lale = self._engine.get_pipeline(pipeline_name=pipeline_name, persist=persist)
+                pipeline_model, check_lale = self._engine.get_pipeline(
+                    pipeline_name=pipeline_name, persist=persist
+                )
 
         except ForecastingUnsupportedOperation as e:
             raise e
 
         except LibraryNotCompatible as e:
             raise e
 
         except Exception as e:
-            raise PipelineNotLoaded(pipeline_name if pipeline_name is not None else 'best pipeline',
-                                    reason=f"Pipeline with such a name probably does not exist. "
-                                           f"Please make sure you specify correct pipeline name. Error: {e}")
+            raise PipelineNotLoaded(
+                pipeline_name if pipeline_name is not None else "best pipeline",
+                reason=f"Pipeline with such a name probably does not exist. "
+                f"Please make sure you specify correct pipeline name. Error: {e}",
+            )
 
         if astype == PipelineTypes.SKLEARN:
             return pipeline_model
 
         elif astype == PipelineTypes.LALE:
             if check_lale:
                 try_import_lale()
             from lale.helpers import import_from_sklearn_pipeline
+
             return import_from_sklearn_pipeline(pipeline_model)
         else:
-            raise ValueError('Incorrect value of \'astype\'. '
-                             'Should be either PipelineTypes.SKLEARN or PipelineTypes.LALE')
-
-    def get_pipeline_notebook(self,
-                              pipeline_name: str = None,
-                              filename: str = None,
-                              insert_to_cell: bool = False) -> str:
+            raise ValueError(
+                "Incorrect value of 'astype'. "
+                "Should be either PipelineTypes.SKLEARN or PipelineTypes.LALE"
+            )
+
+    def get_pipeline_notebook(
+        self,
+        pipeline_name: str = None,
+        filename: str = None,
+        insert_to_cell: bool = False,
+    ) -> str:
         """Download specified pipeline notebook from Service.
 
         :param pipeline_name: pipeline name, if you want to see the pipelines names, please use summary() method,
             if this parameter is None, the best pipeline will be fetched
         :type pipeline_name: str, optional
 
         :param filename: filename under which the pipeline notebook will be saved
@@ -721,53 +834,63 @@
         """
         if pipeline_name is None:
             pipeline_name = self._engine.summary().index[0]
 
         path = self._engine.get_pipeline_notebook(pipeline_name, filename=filename)
 
         if insert_to_cell:
-            with open(path, 'r') as f:
+            with open(path, "r") as f:
                 content = json.loads(f.read())
 
             def translate_cell_to_str(cell):
-                if cell['cell_type'] == 'code':
-                    return ''.join(cell['source'])
-                elif cell['cell_type'] == 'markdown':
-                    return '\n'.join(["# " + l for l in ''.join(cell['source']).split('\n')])
+                if cell["cell_type"] == "code":
+                    return "".join(cell["source"])
+                elif cell["cell_type"] == "markdown":
+                    return "\n".join(
+                        ["# " + l for l in "".join(cell["source"]).split("\n")]
+                    )
                 else:
-                    return ''
+                    return ""
 
-            result = '\n\n'.join([translate_cell_to_str(c) for c in content['cells']])
+            result = "\n\n".join([translate_cell_to_str(c) for c in content["cells"]])
 
             import sys
 
             def get_optimizer_var_name():
                 for name, module in sys.modules.items():
                     for varname, obj in module.__dict__.items():
                         if obj is self:
                             return varname
 
-            if '\'PUT_YOUR_APIKEY_HERE\'' in result:
+            if "'PUT_YOUR_APIKEY_HERE'" in result:
                 try:
                     optimizer_var_name = get_optimizer_var_name()
-                    result = result.replace('\'PUT_YOUR_APIKEY_HERE\'', f'{optimizer_var_name}._workspace.credentials.api_key')
+                    result = result.replace(
+                        "'PUT_YOUR_APIKEY_HERE'",
+                        f"{optimizer_var_name}._workspace.credentials.api_key",
+                    )
 
                 except:
                     pass
 
             import IPython.core
+
             ipython = IPython.core.getipython.get_ipython()
             comment = "# generated by get_pipeline_notebook(insert_to_cell=True) from previous cell\n\n"
             ipython.set_next_input(comment + result, replace=False)
 
         return path
 
     # note: predict on top of the best computed pipeline, best pipeline is downloaded for the first time
-    def predict(self, X: Union['DataFrame', 'ndarray']=None, observations: Union['DataFrame', 'ndarray']=None,
-                supporting_features: Union['DataFrame', 'ndarray']=None) -> 'ndarray':
+    def predict(
+        self,
+        X: Union["DataFrame", "ndarray"] = None,
+        observations: Union["DataFrame", "ndarray"] = None,
+        supporting_features: Union["DataFrame", "ndarray"] = None,
+    ) -> "ndarray":
         """Predict method called on top of the best fetched pipeline.
 
         :param X: test data for prediction
         :type X: numpy.ndarray or pandas.DataFrame
 
         :param observations: new observations of forecasting data that were used to train AutoAI model,
             supported only for forecasting pipelines
@@ -775,102 +898,143 @@
 
         :param supporting_features: future values of exogenous features, supported only for forecasting pipelines
         :type supporting_features: numpy.ndarray or pandas.DataFrame
 
         :return: model predictions
         :rtype: numpy.ndarray
         """
-        is_forecasting = self.params.get('prediction_type') in (PredictionType.FORECASTING, 'timeseries',
-                                                                PredictionType.TIMESERIES_ANOMALY_PREDICTION)
+        is_forecasting = self.params.get("prediction_type") in (
+            PredictionType.FORECASTING,
+            "timeseries",
+            PredictionType.TIMESERIES_ANOMALY_PREDICTION,
+        )
 
         if not is_forecasting and X is None:
             raise ValueError("X parameter is required in predict method.")
         if is_forecasting:
             if X is not None and observations is not None:
-                raise ValueError("For forecasting the  parameter X should not be used. Please use observations instead of X.")
+                raise ValueError(
+                    "For forecasting the  parameter X should not be used. Please use observations instead of X."
+                )
             else:
                 X = observations if observations is not None else X
 
         if self.best_pipeline is None:
             # note: automatically download the best computed pipeline
             if self.get_run_status() == RunStateTypes.COMPLETED:
                 self.best_pipeline, _ = self._engine.get_best_pipeline()
             else:
-                raise FitNotCompleted(self._engine._current_run_id,
-                                      reason="Please check the run status with run_status() method.")
+                raise FitNotCompleted(
+                    self._engine._current_run_id,
+                    reason="Please check the run status with run_status() method.",
+                )
             # --- end note
 
         additional_params = {}
         if supporting_features is not None:
             if is_forecasting:
                 additional_params.update(
-                    {'supporting_features': supporting_features if isinstance(supporting_features,
-                                                                           ndarray) else supporting_features.values})
+                    {
+                        "supporting_features": (
+                            supporting_features
+                            if isinstance(supporting_features, ndarray)
+                            else supporting_features.values
+                        )
+                    }
+                )
             else:
-                raise FutureExogenousFeaturesNotSupported(self.params.get('prediction_type'))
+                raise FutureExogenousFeaturesNotSupported(
+                    self.params.get("prediction_type")
+                )
 
         if isinstance(X, DataFrame) or isinstance(X, ndarray):
-            return self.best_pipeline.predict(X if isinstance(X, ndarray) else X.values, **additional_params)
+            return self.best_pipeline.predict(
+                X if isinstance(X, ndarray) else X.values, **additional_params
+            )
         elif X is None and is_forecasting:
             return self.best_pipeline.predict()
         else:
-            raise TypeError("X should be either of type pandas.DataFrame or numpy.ndarray")
+            raise TypeError(
+                "X should be either of type pandas.DataFrame or numpy.ndarray"
+            )
 
     # --- end note
 
-    def get_data_connections(self) -> List['DataConnection']:
+    def get_data_connections(self) -> List["DataConnection"]:
         """Create DataConnection objects for further user usage
             (eg. to handle data storage connection or to recreate autoai holdout split).
 
         :return: list of DataConnection with populated optimizer parameters
         :rtype: list[DataConnection]
         """
 
         optimizer_parameters = self.get_params()
         run_details = self.get_run_details(include_metrics=True, _internal=True)
 
-        user_holdout_exists = True if run_details['entity'].get('test_data_references') else False
+        user_holdout_exists = (
+            True if run_details["entity"].get("test_data_references") else False
+        )
 
-        training_data_references = run_details['entity']['training_data_references']
+        training_data_references = run_details["entity"]["training_data_references"]
 
         data_connections = [
-            DataConnection._from_dict(_dict=data_connection) for data_connection in training_data_references]
-
-        for data_connection in data_connections:  # note: populate DataConnections with optimizer params
+            DataConnection._from_dict(_dict=data_connection)
+            for data_connection in training_data_references
+        ]
+
+        for (
+            data_connection
+        ) in data_connections:  # note: populate DataConnections with optimizer params
             data_connection.auto_pipeline_params = deepcopy(optimizer_parameters)
-            data_connection.set_client(self._engine._api_client if isinstance(self._engine, ServiceEngine) else self._engine._wml_client)
+            data_connection.set_client(
+                self._engine._api_client
+                if isinstance(self._engine, ServiceEngine)
+                else self._engine._wml_client
+            )
             data_connection._run_id = self._engine._current_run_id
             data_connection._user_holdout_exists = user_holdout_exists
 
-            metrics = run_details['entity']['status'].get('metrics', [])
-            if metrics and metrics[-1]['context'].get('fairness'):
-                data_connection.auto_pipeline_params['fairness_info'] = metrics[-1]['context']['fairness'].get('info')
+            metrics = run_details["entity"]["status"].get("metrics", [])
+            if metrics and metrics[-1]["context"].get("fairness"):
+                data_connection.auto_pipeline_params["fairness_info"] = metrics[-1][
+                    "context"
+                ]["fairness"].get("info")
 
         return data_connections
 
-    def get_test_data_connections(self) -> List['DataConnection']:
+    def get_test_data_connections(self) -> List["DataConnection"]:
         """Create DataConnection objects for further user usage (To recreate autoai holdout that user specified).
 
         :return: list of DataConnection with populated optimizer parameters
         :rtype: list[DataConnection]
         """
         optimizer_parameters = self.get_params()
         run_details = self.get_run_details(_internal=True)
 
-        if not run_details['entity'].get('test_data_references'):
-            raise TestDataNotPresent(reason="User specified test data was not present in this experiment. "
-                                            "Try to use 'with_holdout_split' parameter for original "
-                                            "training_data_references to retrieve test data.")
+        if not run_details["entity"].get("test_data_references"):
+            raise TestDataNotPresent(
+                reason="User specified test data was not present in this experiment. "
+                "Try to use 'with_holdout_split' parameter for original "
+                "training_data_references to retrieve test data."
+            )
 
-        test_data_references = run_details['entity']['test_data_references']
+        test_data_references = run_details["entity"]["test_data_references"]
 
         data_connections = [
-            DataConnection._from_dict(_dict=data_connection) for data_connection in test_data_references]
-
-        for data_connection in data_connections:  # note: populate DataConnections with optimizer params
+            DataConnection._from_dict(_dict=data_connection)
+            for data_connection in test_data_references
+        ]
+
+        for (
+            data_connection
+        ) in data_connections:  # note: populate DataConnections with optimizer params
             data_connection.auto_pipeline_params = deepcopy(optimizer_parameters)
-            data_connection.set_client(self._engine._api_client if isinstance(self._engine, ServiceEngine) else self._engine._wml_client)
+            data_connection.set_client(
+                self._engine._api_client
+                if isinstance(self._engine, ServiceEngine)
+                else self._engine._wml_client
+            )
             data_connection._run_id = self._engine._current_run_id
             data_connection._test_data = True
             data_connection._user_holdout_exists = True
 
         return data_connections
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/experiment/autoai/runs/base_auto_pipelines_runs.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/experiment/autoai/runs/base_auto_pipelines_runs.py`

 * *Files 3% similar despite different names*

```diff
@@ -6,24 +6,22 @@
 from abc import abstractmethod
 from typing import TYPE_CHECKING, List
 
 if TYPE_CHECKING:
     from pandas import DataFrame
     from ibm_watsonx_ai.helpers import DataConnection
 
-__all__ = [
-    "BaseAutoPipelinesRuns"
-]
+__all__ = ["BaseAutoPipelinesRuns"]
 
 
 class BaseAutoPipelinesRuns:
     """Base abstract class for Pipeline Optimizers Runs."""
 
     @abstractmethod
-    def list(self) -> 'DataFrame':
+    def list(self) -> "DataFrame":
         """Lists historical runs/fits with status."""
         pass
 
     @abstractmethod
     def get_params(self, run_id: str = None) -> dict:
         """Get executed optimizers configs parameters based on the run_id."""
         pass
@@ -35,10 +33,10 @@
 
     @abstractmethod
     def get_optimizer(self, run_id: str):
         """Create instance of AutoPipelinesRuns with all computed pipelines computed by AutoAi on Service."""
         pass
 
     @abstractmethod
-    def get_data_connections(self, run_id: str) -> List['DataConnection']:
+    def get_data_connections(self, run_id: str) -> List["DataConnection"]:
         """Create DataConnection objects for further user usage"""
         pass
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/experiment/autoai/runs/local_auto_pipelines_runs.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/experiment/autoai/runs/local_auto_pipelines_runs.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,22 +1,22 @@
 #  -----------------------------------------------------------------------------------------
 #  (C) Copyright IBM Corp. 2023-2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
-__all__ = [
-    "LocalAutoPipelinesRuns"
-]
+__all__ = ["LocalAutoPipelinesRuns"]
 
 from typing import List, Dict, Union, TYPE_CHECKING, Optional
 
 from pandas import DataFrame
 import warnings
 
-from ibm_watsonx_ai.experiment.autoai.optimizers.local_auto_pipelines import LocalAutoPipelines
+from ibm_watsonx_ai.experiment.autoai.optimizers.local_auto_pipelines import (
+    LocalAutoPipelines,
+)
 from ibm_watsonx_ai.helpers import DataConnection
 from ibm_watsonx_ai.utils.autoai.utils import prepare_cos_client
 from .base_auto_pipelines_runs import BaseAutoPipelinesRuns
 
 
 if TYPE_CHECKING:
     from ..optimizers import RemoteAutoPipelines
@@ -31,32 +31,35 @@
     """
 
     def __init__(self, filter: str = None) -> None:
         self.experiment_name = filter
         self.training_data_reference = None
         self.training_result_reference = None
 
-    def __call__(self, *, filter: str) -> 'LocalAutoPipelinesRuns':
+    def __call__(self, *, filter: str) -> "LocalAutoPipelinesRuns":
         raise NotImplementedError("Not yet implemented in the local scenario.")
 
-    def list(self) -> 'DataFrame':
+    def list(self) -> "DataFrame":
         raise NotImplementedError("Not yet implemented in the local scenario.")
 
     def get_params(self, run_id: str = None) -> dict:
         raise NotImplementedError("Not yet implemented in the local scenario.")
 
     def get_run_details(self, run_id: str = None) -> dict:
         raise NotImplementedError("Not yet implemented in the local scenario.")
 
-    def get_optimizer(self,
-                      run_id: Optional[str] = None,
-                      metadata: Dict[str, Union[List['DataConnection'],  'DataConnection', str, int]] = None,
-                      api_client: 'APIClient' = None,
-                      **kwargs
-                      ) -> Union['LocalAutoPipelines', 'RemoteAutoPipelines']:
+    def get_optimizer(
+        self,
+        run_id: Optional[str] = None,
+        metadata: Dict[
+            str, Union[List["DataConnection"], "DataConnection", str, int]
+        ] = None,
+        api_client: "APIClient" = None,
+        **kwargs,
+    ) -> Union["LocalAutoPipelines", "RemoteAutoPipelines"]:
         """Get historical optimizer from historical experiment.
 
         :param run_id: ID of the local historical experiment run (option not yet available)
         :type run_id: str, optional
 
         :param metadata: option to pass information about COS data reference
         :type metadata: dict, optional
@@ -90,84 +93,103 @@
                            training_status="./75eec2e0-2600-4b7e-bcf2-ea54f2471400/9236e3ab-25e2-4daa-86a8-fd009d4e1e7d/training-status.json",
                        )
                    )
                )
             optimizer = AutoAI().runs.get_optimizer(metadata)
         """
         # note: backward compatibility
-        if (wml_client:=kwargs.get('wml_client')) is not None:
+        if (wml_client := kwargs.get("wml_client")) is not None:
             if api_client is None:
                 api_client = wml_client
-            warnings.warn(("`wml_client` is deprecated and will be removed in future. "
-                  "Instead, please use `api_client`."), category=DeprecationWarning)
-        
+            warnings.warn(
+                (
+                    "`wml_client` is deprecated and will be removed in future. "
+                    "Instead, please use `api_client`."
+                ),
+                category=DeprecationWarning,
+            )
+
         # --- end note
         if run_id is not None:
-            raise NotImplementedError("run_id option is not yet implemented in the local scenario.")
+            raise NotImplementedError(
+                "run_id option is not yet implemented in the local scenario."
+            )
 
         else:
-            training_result_reference = metadata.get('training_result_reference')
+            training_result_reference = metadata.get("training_result_reference")
 
             # note: cloud auto-gen notebook scenario
             # note: save training connection to be able to further provide this data via get_data_connections
-            self.training_data_reference: List['DataConnection'] = metadata.get('training_data_references',
-                                                                                metadata.get('training_data_reference'))
-            self.training_result_reference: 'DataConnection' = training_result_reference
+            self.training_data_reference: List["DataConnection"] = metadata.get(
+                "training_data_references", metadata.get("training_data_reference")
+            )
+            self.training_result_reference: "DataConnection" = training_result_reference
 
             # note: fill experiment parameters to be able to recreate holdout split
             for data in self.training_data_reference:
                 data._fill_experiment_parameters(
-                    prediction_type=metadata.get('prediction_type'),
-                    prediction_column=metadata.get('prediction_column'),
-                    holdout_size=metadata.get('holdout_size'),
-                    csv_separator=metadata.get('csv_separator', ','),
-                    excel_sheet=metadata.get('excel_sheet', 0),
-                    encoding=metadata.get('encoding', 'utf-8')
+                    prediction_type=metadata.get("prediction_type"),
+                    prediction_column=metadata.get("prediction_column"),
+                    holdout_size=metadata.get("holdout_size"),
+                    csv_separator=metadata.get("csv_separator", ","),
+                    excel_sheet=metadata.get("excel_sheet", 0),
+                    encoding=metadata.get("encoding", "utf-8"),
                 )
 
             self.training_result_reference._fill_experiment_parameters(
-                prediction_type=metadata.get('prediction_type'),
-                prediction_column=metadata.get('prediction_column'),
-                holdout_size=metadata.get('holdout_size'),
-                csv_separator=metadata.get('csv_separator', ','),
-                excel_sheet=metadata.get('excel_sheet', 0),
-                encoding=metadata.get('encoding', 'utf-8')
+                prediction_type=metadata.get("prediction_type"),
+                prediction_column=metadata.get("prediction_column"),
+                holdout_size=metadata.get("holdout_size"),
+                csv_separator=metadata.get("csv_separator", ","),
+                excel_sheet=metadata.get("excel_sheet", 0),
+                encoding=metadata.get("encoding", "utf-8"),
             )
             # --- end note
 
             # Note: We need to fetch credentials when 'container' is the type
-            if (hasattr(self.training_result_reference, 'type') and
-                (self.training_result_reference.type == 'container' or
-                self.training_result_reference.type == 'data_asset' or
-                    self.training_result_reference.type == 'connection_asset') and api_client is not None):
+            if (
+                hasattr(self.training_result_reference, "type")
+                and (
+                    self.training_result_reference.type == "container"
+                    or self.training_result_reference.type == "data_asset"
+                    or self.training_result_reference.type == "connection_asset"
+                )
+                and api_client is not None
+            ):
                 self.training_result_reference.set_client(api_client)
 
             for data_ref in self.training_data_reference:
-                if hasattr(data_ref, 'type') and (data_ref.type == 'container' or
-                                                    data_ref.type == 'connection_asset' or
-                                                    data_ref.type == 'data_asset') and api_client is not None:
+                if (
+                    hasattr(data_ref, "type")
+                    and (
+                        data_ref.type == "container"
+                        or data_ref.type == "connection_asset"
+                        or data_ref.type == "data_asset"
+                    )
+                    and api_client is not None
+                ):
                     data_ref.set_client(api_client)
             # --- end note
 
             data_clients, result_client = prepare_cos_client(
                 training_data_references=self.training_data_reference,
-                training_result_reference=self.training_result_reference
+                training_result_reference=self.training_result_reference,
             )
 
             optimizer = LocalAutoPipelines(
-                name='Auto-gen notebook from COS',
-                prediction_type=metadata.get('prediction_type'),
-                prediction_column=metadata.get('prediction_column'),
-                scoring=metadata.get('scoring'),
-                holdout_size=metadata.get('holdout_size'),
-                max_num_daub_ensembles=metadata.get('max_number_of_estimators'),
+                name="Auto-gen notebook from COS",
+                prediction_type=metadata.get("prediction_type"),
+                prediction_column=metadata.get("prediction_column"),
+                scoring=metadata.get("scoring"),
+                holdout_size=metadata.get("holdout_size"),
+                max_num_daub_ensembles=metadata.get("max_number_of_estimators"),
                 _data_clients=data_clients,
-                _result_client=result_client
+                _result_client=result_client,
             )
             optimizer._training_data_reference = self.training_data_reference
             optimizer._training_result_reference = self.training_result_reference
 
             return optimizer
             # --- end note
 
-    def get_data_connections(self, run_id: str) -> List['DataConnection']:
+    def get_data_connections(self, run_id: str) -> List["DataConnection"]:
         raise NotImplementedError("Not yet implemented in the local scenario.")
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/experiment/base_experiment/base_experiment.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/experiment/base_experiment/base_experiment.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,17 +1,15 @@
 #  -----------------------------------------------------------------------------------------
 #  (C) Copyright IBM Corp. 2023-2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
 from abc import ABC, abstractmethod
 
-__all__ = [
-    "BaseExperiment"
-]
+__all__ = ["BaseExperiment"]
 
 
 class BaseExperiment(ABC):
     """Base abstract class for Experiment."""
 
     @abstractmethod
     def runs(self, *, filter: str):
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/experiment/fm_tune/tune_experiment.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/experiment/fm_tune/tune_experiment.py`

 * *Files 4% similar despite different names*

```diff
@@ -4,15 +4,19 @@
 #  -----------------------------------------------------------------------------------------
 from __future__ import annotations
 from enum import Enum
 
 from typing import TYPE_CHECKING
 
 from ibm_watsonx_ai.foundation_models.prompt_tuner import PromptTuner
-from ibm_watsonx_ai.foundation_models.utils.enums import PromptTuningTypes, PromptTuningInitMethods, TuneExperimentTasks
+from ibm_watsonx_ai.foundation_models.utils.enums import (
+    PromptTuningTypes,
+    PromptTuningInitMethods,
+    TuneExperimentTasks,
+)
 from ibm_watsonx_ai.foundation_models.utils.utils import _check_model_state
 from ibm_watsonx_ai.experiment.base_experiment import BaseExperiment
 from ibm_watsonx_ai.wml_client_error import WMLClientError
 from ibm_watsonx_ai import APIClient
 
 from .tune_runs import TuneRuns
 
@@ -48,37 +52,42 @@
         from ibm_watsonx_ai.experiment import TuneExperiment
 
         experiment = TuneExperiment(
             credentials=Credentials(...),
             project_id="...",
             space_id="...")
     """
-    def __init__(self,
-                 credentials: Credentials | dict[str, str],
-                 project_id: str | None = None,
-                 space_id: str | None = None,
-                 verify: str | bool | None = None) -> None:
+
+    def __init__(
+        self,
+        credentials: Credentials | dict[str, str],
+        project_id: str | None = None,
+        space_id: str | None = None,
+        verify: str | bool | None = None,
+    ) -> None:
 
         self.client = APIClient(credentials, verify=verify)
         if not self.client.CLOUD_PLATFORM_SPACES and self.client.CPD_version < 4.8:
             raise WMLClientError(error_msg="Operation is unsupported for this release.")
 
         if project_id:
             self.client.set.default_project(project_id)
         else:
             self.client.set.default_space(space_id)
 
         self.PromptTuningTypes = PromptTuningTypes
         self.PromptTuningInitMethods = PromptTuningInitMethods
 
-        self.Tasks = self._get_tasks_enum()  # Note: Dynamically create enum with supported ENUM Tasks
+        self.Tasks = (
+            self._get_tasks_enum()
+        )  # Note: Dynamically create enum with supported ENUM Tasks
 
         self.runs = TuneRuns(client=self.client)
 
-    def runs(self, *, filter: str) -> 'TuneRuns':
+    def runs(self, *, filter: str) -> "TuneRuns":
         """Get the historical tuning runs but with name filter.
 
         :param filter: filter, user can choose which runs to fetch specifying tuning name
         :type filter: str
 
         **Examples**
 
@@ -87,31 +96,33 @@
             from ibm_watsonx_ai.experiment import TuneExperiment
 
             experiment = TuneExperiment(...)
             experiment.runs(filter='prompt tuning name').list()
         """
         return TuneRuns(client=self.client, filter=filter)
 
-    def prompt_tuner(self,
-                     name: str,  # Note: Rest API does not require name,
-                     task_id: str,
-                     description: str = None,
-                     base_model: str = None,
-                     accumulate_steps: int = None,
-                     batch_size: int = None,
-                     init_method: str = None,
-                     init_text: str = None,
-                     learning_rate: float = None,
-                     max_input_tokens: int = None,
-                     max_output_tokens: int = None,
-                     num_epochs: int = None,
-                     verbalizer: str = None,
-                     tuning_type: str = None,
-                     auto_update_model: bool = True,
-                     group_by_name: bool = False) -> PromptTuner:
+    def prompt_tuner(
+        self,
+        name: str,  # Note: Rest API does not require name,
+        task_id: str,
+        description: str = None,
+        base_model: str = None,
+        accumulate_steps: int = None,
+        batch_size: int = None,
+        init_method: str = None,
+        init_text: str = None,
+        learning_rate: float = None,
+        max_input_tokens: int = None,
+        max_output_tokens: int = None,
+        num_epochs: int = None,
+        verbalizer: str = None,
+        tuning_type: str = None,
+        auto_update_model: bool = True,
+        group_by_name: bool = False,
+    ) -> PromptTuner:
         """Initialize a PromptTuner module.
 
         :param name: name for the PromptTuner
         :type name: str
 
         :param task_id: task that is targeted for this model. Example: `experiment.Tasks.CLASSIFICATION`
 
@@ -129,23 +140,23 @@
 
         :param description: description
         :type description: str, optional
 
         :param base_model: model id of the base model for this prompt tuning. Example: google/flan-t5-xl
         :type base_model: str, optional
 
-        :param accumulate_steps: Number of steps to be used for gradient accumulation. Gradient accumulation 
-            refers to a method of collecting gradient for configured number of steps instead of updating 
-            the model variables at every step and then applying the update to model variables. 
-            This can be used as a tool to overcome smaller batch size limitation. 
+        :param accumulate_steps: Number of steps to be used for gradient accumulation. Gradient accumulation
+            refers to a method of collecting gradient for configured number of steps instead of updating
+            the model variables at every step and then applying the update to model variables.
+            This can be used as a tool to overcome smaller batch size limitation.
             Often also referred in conjunction with "effective batch size". Possible values: 1  value  128,
             default value: 16
         :type accumulate_steps: int, optional
 
-        :param batch_size: The batch size is a number of samples processed before the model is updated. 
+        :param batch_size: The batch size is a number of samples processed before the model is updated.
             Possible values: 1  value  16, default value: 16
         :type batch_size: int, optional
 
         :param init_method: `text` method requires `init_text` to be set. Allowable values: [`random`, `text`],
             default value: `random`
         :type init_method: str, optional
 
@@ -164,20 +175,20 @@
             default value: 128
         :type max_output_tokens: int, optional
 
         :param num_epochs: number of epochs to tune the prompt vectors, this affects the quality of the trained model.
             Possible values: 1  value  50, default value: 20
         :type num_epochs: int, optional
 
-        :param verbalizer: verbalizer template to be used for formatting data at train and inference time. 
-            This template may use brackets to indicate where fields from the data model must be rendered. 
+        :param verbalizer: verbalizer template to be used for formatting data at train and inference time.
+            This template may use brackets to indicate where fields from the data model must be rendered.
             The default value is "{{input}}" which means use the raw text, default value: `Input: {{input}} Output:`
         :type verbalizer: str, optional
 
-        :param tuning_type: type of Peft (Parameter-Efficient Fine-Tuning) config to build. 
+        :param tuning_type: type of Peft (Parameter-Efficient Fine-Tuning) config to build.
             Allowable values: [`experiment.PromptTuningTypes.PT`], default value: `experiment.PromptTuningTypes.PT`
         :type tuning_type: str, optional
 
         :param auto_update_model: define if model should be automatically updated, default value: `True`
         :type auto_update_model: bool, optional
 
         :param group_by_name: define if tunings should be grouped by name, default value: `False`
@@ -203,46 +214,64 @@
                 num_epochs=6,
                 tuning_type=experiment.PromptTuningTypes.PT,
                 verbalizer="Extract the satisfaction from the comment. Return simple '1' for satisfied customer or '0' for unsatisfied. Input: {{input}} Output: ",
                 auto_update_model=True)
         """
 
         task_id, base_model = [
-            enum_possible_field.value if isinstance(enum_possible_field, Enum) else enum_possible_field for
-            enum_possible_field in (task_id, base_model)]
-
-        prompt_tuning_supported_models = [model_spec['model_id'] for model_spec in
-                                          self.client.foundation_models.get_model_specs_with_prompt_tuning_support().get('resources', [])]
+            (
+                enum_possible_field.value
+                if isinstance(enum_possible_field, Enum)
+                else enum_possible_field
+            )
+            for enum_possible_field in (task_id, base_model)
+        ]
+
+        prompt_tuning_supported_models = [
+            model_spec["model_id"]
+            for model_spec in self.client.foundation_models.get_model_specs_with_prompt_tuning_support().get(
+                "resources", []
+            )
+        ]
         if base_model not in prompt_tuning_supported_models:
-            raise WMLClientError(f"Base model '{base_model}' is not supported. Supported models: {prompt_tuning_supported_models}")
+            raise WMLClientError(
+                f"Base model '{base_model}' is not supported. Supported models: {prompt_tuning_supported_models}"
+            )
 
         # check if model is in constricted mode
         _check_model_state(self.client, base_model)
 
-        prompt_tuner = PromptTuner(name=name,
-                                   task_id=task_id,
-                                   description=description,
-                                   base_model=base_model,
-                                   accumulate_steps=accumulate_steps,
-                                   batch_size=batch_size,
-                                   init_method=init_method,
-                                   init_text=init_text,
-                                   learning_rate=learning_rate,
-                                   max_input_tokens=max_input_tokens,
-                                   max_output_tokens=max_output_tokens,
-                                   num_epochs=num_epochs,
-                                   tuning_type=tuning_type,
-                                   verbalizer=verbalizer,
-                                   auto_update_model=auto_update_model,
-                                   group_by_name=group_by_name)
+        prompt_tuner = PromptTuner(
+            name=name,
+            task_id=task_id,
+            description=description,
+            base_model=base_model,
+            accumulate_steps=accumulate_steps,
+            batch_size=batch_size,
+            init_method=init_method,
+            init_text=init_text,
+            learning_rate=learning_rate,
+            max_input_tokens=max_input_tokens,
+            max_output_tokens=max_output_tokens,
+            num_epochs=num_epochs,
+            tuning_type=tuning_type,
+            verbalizer=verbalizer,
+            auto_update_model=auto_update_model,
+            group_by_name=group_by_name,
+        )
 
         prompt_tuner._client = self.client
 
         return prompt_tuner
 
     def _get_tasks_enum(self):
         try:
-            from ibm_watsonx_ai.foundation_models.utils.utils import get_all_supported_tasks_dict
-            return Enum(value='TuneExperimentTasks',
-                        names=get_all_supported_tasks_dict(url=self.client.credentials.url))
+            from ibm_watsonx_ai.foundation_models.utils.utils import (
+                get_all_supported_tasks_dict,
+            )
+
+            return Enum(
+                value="TuneExperimentTasks",
+                names=get_all_supported_tasks_dict(url=self.client.credentials.url),
+            )
         except Exception:
             return TuneExperimentTasks
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/experiment/fm_tune/tune_runs.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/experiment/fm_tune/tune_runs.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,15 +1,13 @@
 #  -----------------------------------------------------------------------------------------
 #  (C) Copyright IBM Corp. 2023-2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
-__all__ = [
-    "TuneRuns"
-]
+__all__ = ["TuneRuns"]
 
 from pandas import DataFrame
 
 from ibm_watsonx_ai.foundation_models.prompt_tuner import PromptTuner
 from ibm_watsonx_ai.wml_client_error import WMLClientError
 from ibm_watsonx_ai import APIClient
 
@@ -23,26 +21,28 @@
     :param filter: filter, user can choose which runs to fetch specifying tuning name
     :type filter: str, optional
 
     :param limit: int number of records to be returned
     :type limit: int
     """
 
-    def __init__(self, client: 'APIClient', filter: str = None, limit: int = 50) -> None:
+    def __init__(
+        self, client: "APIClient", filter: str = None, limit: int = 50
+    ) -> None:
 
         self.client = client
         self.tuning_name = filter
         self.limit = limit
 
-    def __call__(self, *, filter: str = None, limit: int = 50) -> 'TuneRuns':
+    def __call__(self, *, filter: str = None, limit: int = 50) -> "TuneRuns":
         self.tuning_name = filter
         self.limit = limit
         return self
 
-    def list(self) -> 'DataFrame':
+    def list(self) -> "DataFrame":
         """Lists historical runs with status. If user has a lot of runs stored in the service,
         it may take long time to fetch all the information. If there is no limit set,
         get last 50 records.
 
         :return: Pandas DataFrame with runs IDs and state
         :rtype: pandas.DataFrame
 
@@ -52,36 +52,40 @@
 
             from ibm_watsonx_ai.experiment import TuneExperiment
 
             experiment = TuneExperiment(...)
             df = experiment.runs.list()
         """
 
-        runs_details = self.client.training.get_details(get_all=True if self.tuning_name else False,
-                                                        limit=None if self.tuning_name else self.limit,
-                                                        training_type='prompt_tuning',
-                                                        _internal=True)
+        runs_details = self.client.training.get_details(
+            get_all=True if self.tuning_name else False,
+            limit=None if self.tuning_name else self.limit,
+            training_type="prompt_tuning",
+            _internal=True,
+        )
 
-        columns = ['timestamp', 'run_id', 'state', 'prompt tuning name']
+        columns = ["timestamp", "run_id", "state", "prompt tuning name"]
 
         records = []
-        for run in runs_details['resources']:
+        for run in runs_details["resources"]:
             if len(records) >= self.limit:
                 break
 
-            if {'entity', 'metadata'}.issubset(run.keys()):
+            if {"entity", "metadata"}.issubset(run.keys()):
 
-                timestamp = run['metadata'].get('modified_at')
-                run_id = run['metadata'].get('id', run['metadata'].get('guid'))
-                state = run['entity'].get('status', {}).get('state')
-                tuning_name = run['entity'].get('name', 'Unknown')
+                timestamp = run["metadata"].get("modified_at")
+                run_id = run["metadata"].get("id", run["metadata"].get("guid"))
+                state = run["entity"].get("status", {}).get("state")
+                tuning_name = run["entity"].get("name", "Unknown")
 
                 record = [timestamp, run_id, state, tuning_name]
 
-                if self.tuning_name is None or (self.tuning_name and self.tuning_name == tuning_name):
+                if self.tuning_name is None or (
+                    self.tuning_name and self.tuning_name == tuning_name
+                ):
                     records.append(record)
 
         runs = DataFrame(data=records, columns=columns)
         return runs.sort_values(by=["timestamp"], ascending=False)
 
     def get_tuner(self, run_id: str) -> PromptTuner:
         """Create instance of PromptTuner based on tuning run with specific run_id.
@@ -100,43 +104,49 @@
 
             experiment = TuneExperiment(credentials, ...)
             historical_tuner = experiment.runs.get_tuner(run_id='02bab973-ae83-4283-9d73-87b9fd462d35')
         """
         # note: normal scenario
 
         if not isinstance(run_id, str):
-            raise WMLClientError(f"Provided run_id type was {type(run_id)} (should be a string)")
+            raise WMLClientError(
+                f"Provided run_id type was {type(run_id)} (should be a string)"
+            )
 
-        entity = self.client.training.get_details(run_id).get('entity')
+        entity = self.client.training.get_details(run_id).get("entity")
         if not entity:
             raise WMLClientError("Provided run_id was invalid")
 
-        tuning_params = entity['prompt_tuning']
+        tuning_params = entity["prompt_tuning"]
 
-        prompt_tuner = PromptTuner(name=entity.get('name'),
-                                   task_id=tuning_params.get('task_id'),
-                                   description=entity.get('description'),
-                                   base_model=tuning_params.get('base_model', {}).get('name'),
-                                   accumulate_steps=tuning_params.get('accumulate_steps'),
-                                   batch_size=tuning_params.get('batch_size'),
-                                   init_method=tuning_params.get('init_method'),
-                                   init_text=tuning_params.get('init_text'),
-                                   learning_rate=tuning_params.get('learning_rate'),
-                                   max_input_tokens=tuning_params.get('max_input_tokens'),
-                                   max_output_tokens=tuning_params.get('max_output_tokens'),
-                                   num_epochs=tuning_params.get('num_epochs'),
-                                   tuning_type=tuning_params.get('tuning_type'),
-                                   verbalizer=tuning_params.get('verbalizer'),
-                                   auto_update_model=entity.get('auto_update_model'))
+        prompt_tuner = PromptTuner(
+            name=entity.get("name"),
+            task_id=tuning_params.get("task_id"),
+            description=entity.get("description"),
+            base_model=tuning_params.get("base_model", {}).get("name"),
+            accumulate_steps=tuning_params.get("accumulate_steps"),
+            batch_size=tuning_params.get("batch_size"),
+            init_method=tuning_params.get("init_method"),
+            init_text=tuning_params.get("init_text"),
+            learning_rate=tuning_params.get("learning_rate"),
+            max_input_tokens=tuning_params.get("max_input_tokens"),
+            max_output_tokens=tuning_params.get("max_output_tokens"),
+            num_epochs=tuning_params.get("num_epochs"),
+            tuning_type=tuning_params.get("tuning_type"),
+            verbalizer=tuning_params.get("verbalizer"),
+            auto_update_model=entity.get("auto_update_model"),
+        )
 
         prompt_tuner.id = run_id
         prompt_tuner._client = self.client
         return prompt_tuner
 
-    def get_run_details(self, run_id: str = None, include_metrics: bool = False) -> dict:
+    def get_run_details(
+        self, run_id: str = None, include_metrics: bool = False
+    ) -> dict:
         """Get run details. If run_id is not supplied, last run will be taken.
 
         :param run_id: ID of the run
         :type run_id: str, optional
 
         :param include_metrics: indicates to include metrics in the training details output
         :type include_metrics: bool, optional
@@ -151,18 +161,22 @@
             from ibm_watsonx_ai.experiment import TuneExperiment
             experiment = TuneExperiment(credentials, ...)
 
             experiment.runs.get_run_details(run_id='02bab973-ae83-4283-9d73-87b9fd462d35')
             experiment.runs.get_run_details()
         """
         if run_id is None:
-            details = self.client.training.get_details(limit=1, training_type='prompt_tuning', _internal=True).get('resources')[0]
+            details = self.client.training.get_details(
+                limit=1, training_type="prompt_tuning", _internal=True
+            ).get("resources")[0]
         else:
-            details = self.client.training.get_details(training_id=run_id, _internal=True)
+            details = self.client.training.get_details(
+                training_id=run_id, _internal=True
+            )
 
         if include_metrics:
             return details
 
-        if details['entity']['status'].get('metrics', False):
-            del details['entity']['status']['metrics']
+        if details["entity"]["status"].get("metrics", False):
+            del details["entity"]["status"]["metrics"]
 
         return details
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/experiments.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/experiments.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/export_assets.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/export_assets.py`

 * *Files 0% similar despite different names*

```diff
@@ -14,14 +14,15 @@
 
 if TYPE_CHECKING:
     from ibm_watsonx_ai import APIClient
     import pandas
 
 _DEFAULT_LIST_LENGTH = 50
 
+
 class Export(WMLResource):
     def __init__(self, client: APIClient) -> None:
         WMLResource.__init__(self, __name__, client)
 
         self._client = client
         self.ConfigurationMetaNames = ExportMetaNames()
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/factsheets.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/factsheets.py`

 * *Files 0% similar despite different names*

```diff
@@ -11,14 +11,15 @@
 from ibm_watsonx_ai.wml_client_error import WMLClientError, WrongMetaProps
 from ibm_watsonx_ai.wml_resource import WMLResource
 from .metanames import FactsheetsMetaNames
 
 if TYPE_CHECKING:
     from ibm_watsonx_ai import APIClient
 
+
 class Factsheets(WMLResource):
     """Link WML Model to Model Entry."""
 
     cloud_platform_spaces: bool = False
     icp_platform_spaces: bool = False
 
     def __init__(self, client: APIClient) -> None:
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/federated_learning/FLExceptions.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/federated_learning/FLExceptions.py`

 * *Files 2% similar despite different names*

```diff
@@ -4,9 +4,10 @@
 #  -----------------------------------------------------------------------------------------
 
 #  -----------------------------------------------------------------------------------------
 #  (C) Copyright IBM Corp. 20212022.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
+
 class FLException(Exception):
-    pass
+    pass
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/federated_learning/data_handler.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/federated_learning/data_handler.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,19 +1,28 @@
 #  -----------------------------------------------------------------------------------------
 #  (C) Copyright IBM Corp. 2023-2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
-import logging
 import abc
+import logging
+
 import numpy
 
-from ibm_watsonx_ai.federated_learning.data_util import get_min, get_max, get_mean,\
-    get_var, get_std, get_quantile, get_normalizer, get_standardscaler, \
-    get_minmaxscaler
+from ibm_watsonx_ai.federated_learning.data_util import (
+    get_min,
+    get_max,
+    get_mean,
+    get_var,
+    get_std,
+    get_quantile,
+    get_normalizer,
+    get_standardscaler,
+    get_minmaxscaler,
+)
 from ibm_watsonx_ai.federated_learning.FLExceptions import FLException
 
 logger = logging.getLogger(__name__)
 
 
 class DataHandler(abc.ABC):
     """
@@ -34,27 +43,52 @@
         """
         Access the local dataset and return the training and testing dataset
         as a tuple.
 
         :param kwargs:
         :return: `tuple`. (training_set, testing_set)
         """
-        raise NotImplemented
+        raise NotImplementedError
+
+    def get_train_counts(self):
+        """
+        Returns the training sample size.
+
+        :return: The training sample size
+        :rtype: `int`
+        """
+        if self.x_train is not None and isinstance(self.x_train, numpy.ndarray):
+            return self.x_train.shape[0]
+        else:
+            logger.info(
+                "The attribute x_train is None or is not of numpy.ndarray format!"
+                "Trying to access training counts assuming data is loaded \
+                as DataGenerator type."
+            )
+            try:
+                counts = len(self.training_generator.filenames)
+                assert isinstance(counts, int)
+                return counts
+            except:
+                raise FLException(
+                    "Error ocurred during accessing the training sample size."
+                )
 
     def get_val_data(self, **kwargs):
         """
         Access the local dataset and return the validation dataset
         as a tuple.
         :param kwargs:
         :return: `tuple`. (validation_set)
         """
         pass
 
-    def get_statistics_of_training_data(self, sample_data_schema,
-                                        lst_stats_name, **kwargs):
+    def get_statistics_of_training_data(
+        self, sample_data_schema, lst_stats_name, **kwargs
+    ):
         """
         Return the corresponding statistics, which is specified by the
         provided list of statistics names, of the local training dataset.
 
         :param sample_data_schema: Provided data with only feature values. \
         Assuming the dataset has shape (num_samples, num_features).
         :type sample_data_schema: `np.array`
@@ -66,56 +100,63 @@
         :return: The requested statistics based on the local dataset.
         :rtype: `dict`
         """
         if sample_data_schema is None and self.x_train is None:
             raise FLException("No data is provided!")
         elif sample_data_schema is None and self.x_train is not None:
             sample_data_schema = self.x_train
-            logger.warning('No dataset is provided, use the data schema from '
-                           'training dataset(x_train) as the default one.')
+            logger.warning(
+                "No dataset is provided, use the data schema from "
+                "training dataset(x_train) as the default one."
+            )
 
         if type(lst_stats_name) is not list:
-            raise FLException("list of requested statistics badly form. "
-                              "It should of type list, but it is instead "
-                              "of type " + str(type(lst_stats_name)))
+            raise FLException(
+                "list of requested statistics badly form. "
+                "It should of type list, but it is instead "
+                "of type " + str(type(lst_stats_name))
+            )
         list_stats = {}
         if not isinstance(sample_data_schema, numpy.ndarray):
-            raise FLException("Expecting the local dataset to be of type "
-                              "numpy.ndarray, instead it is of type " +
-                              str(type(sample_data_schema)))
+            raise FLException(
+                "Expecting the local dataset to be of type "
+                "numpy.ndarray, instead it is of type " + str(type(sample_data_schema))
+            )
 
         while len(lst_stats_name) != 0:
             tmp_stat_name = lst_stats_name.pop()
             if type(tmp_stat_name) is not str:
-                logger.warning("Skipping the current requested statistics "
-                               "name. It should be of type string, "
-                               "but now it is instead of type" +
-                               str(type(tmp_stat_name)))
-            elif tmp_stat_name == 'min' or tmp_stat_name == 'minimum':
+                logger.warning(
+                    "Skipping the current requested statistics "
+                    "name. It should be of type string, "
+                    "but now it is instead of type" + str(type(tmp_stat_name))
+                )
+            elif tmp_stat_name == "min" or tmp_stat_name == "minimum":
                 list_stats[tmp_stat_name] = get_min(sample_data_schema)
-            elif tmp_stat_name == 'max' or tmp_stat_name == 'maximum':
+            elif tmp_stat_name == "max" or tmp_stat_name == "maximum":
                 list_stats[tmp_stat_name] = get_max(sample_data_schema)
-            elif tmp_stat_name == 'mean':
+            elif tmp_stat_name == "mean":
                 list_stats[tmp_stat_name] = get_mean(sample_data_schema)
-            elif tmp_stat_name == 'var' or tmp_stat_name == 'variance':
+            elif tmp_stat_name == "var" or tmp_stat_name == "variance":
                 list_stats[tmp_stat_name] = get_var(sample_data_schema)
-            elif tmp_stat_name == 'std' or \
-                    tmp_stat_name == 'standard deviation':
+            elif tmp_stat_name == "std" or tmp_stat_name == "standard deviation":
                 list_stats[tmp_stat_name] = get_std(sample_data_schema)
-            elif tmp_stat_name == 'quantile':
-                if kwargs and 'q' in kwargs:
+            elif tmp_stat_name == "quantile":
+                if kwargs and "q" in kwargs:
                     list_stats[tmp_stat_name] = get_quantile(
-                        sample_data_schema,
-                        percentage=kwargs['q'])
+                        sample_data_schema, percentage=kwargs["q"]
+                    )
                 else:
-                    raise FLException('Cannot compute quantile, '
-                                      'missing quantile requirement.')
+                    raise FLException(
+                        "Cannot compute quantile, " "missing quantile requirement."
+                    )
             else:
-                logger.warning("Current required statistics "
-                               "is not supported. Skipping...")
+                logger.warning(
+                    "Current required statistics " "is not supported. Skipping..."
+                )
 
         return list_stats
 
     def get_preprocessor(self, sample_data_schema, preprocessor_name, **kwargs):
         """
         Set the data preprocessor of the data handler class as the requested
         type of preprocessor. The supported preprocessors
@@ -135,48 +176,54 @@
         :type kwargs: `dict`
         :return: None
         """
         if sample_data_schema is None and self.x_train is None:
             raise FLException("No data is provided!")
         elif sample_data_schema is None and self.x_train is not None:
             sample_data_schema = self.x_train
-            logger.warning('No dataset is provided, use the data schema from '
-                           'training dataset(x_train) as the default one.')
+            logger.warning(
+                "No dataset is provided, use the data schema from "
+                "training dataset(x_train) as the default one."
+            )
 
         if type(preprocessor_name) is not str:
-            raise FLException("Expecting the requested preprocessor "
-                              "to be of type string, instead it is of type" +
-                              str(type(preprocessor_name)))
+            raise FLException(
+                "Expecting the requested preprocessor "
+                "to be of type string, instead it is of type"
+                + str(type(preprocessor_name))
+            )
 
         if not isinstance(sample_data_schema, numpy.ndarray):
-            raise FLException("Expecting the local dataset to be of type "
-                              "numpy.ndarray, instead it is of type " +
-                              str(type(sample_data_schema)))
-
-        if preprocessor_name == 'normalizer' or \
-                preprocessor_name == 'normalization':
-            if 'norm' in kwargs:
-                self.preprocessor = get_normalizer(sample_data_schema,
-                                                   norm=kwargs['norm'])
+            raise FLException(
+                "Expecting the local dataset to be of type "
+                "numpy.ndarray, instead it is of type " + str(type(sample_data_schema))
+            )
+
+        if preprocessor_name == "normalizer" or preprocessor_name == "normalization":
+            if "norm" in kwargs:
+                self.preprocessor = get_normalizer(
+                    sample_data_schema, norm=kwargs["norm"]
+                )
             else:
                 self.preprocessor = get_normalizer(sample_data_schema)
-        elif preprocessor_name == 'standardscaler' or \
-                preprocessor_name == 'standardization':
+        elif (
+            preprocessor_name == "standardscaler"
+            or preprocessor_name == "standardization"
+        ):
             mean_val = None
             std = None
-            if 'mean' in kwargs:
-                mean_val = kwargs['mean']
-            if 'scale' in kwargs:
-                std = kwargs['scale']
-            self.preprocessor = get_standardscaler(sample_data_schema,
-                                                   mean_val=mean_val,
-                                                   std=std)
-        elif preprocessor_name == 'minmaxscaler':
-            if 'feature_range' in kwargs:
+            if "mean" in kwargs:
+                mean_val = kwargs["mean"]
+            if "scale" in kwargs:
+                std = kwargs["scale"]
+            self.preprocessor = get_standardscaler(
+                sample_data_schema, mean_val=mean_val, std=std
+            )
+        elif preprocessor_name == "minmaxscaler":
+            if "feature_range" in kwargs:
                 self.preprocessor = get_minmaxscaler(
-                    sample_data_schema,
-                    feature_range=kwargs['feature_range'])
+                    sample_data_schema, feature_range=kwargs["feature_range"]
+                )
             else:
                 self.preprocessor = get_minmaxscaler(sample_data_schema)
         else:
-            logger.warning("Required preprocessor is not supported. "
-                           "Skipping...")
+            logger.warning("Required preprocessor is not supported. " "Skipping...")
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/federated_learning/data_util.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/data/data_util.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,24 +1,20 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
 """
 Module providing utility functions helpful for preproccessing data
 """
-#  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 20212022.
-#  https://opensource.org/licenses/BSD-3-Clause
-#  -----------------------------------------------------------------------------------------
+import logging
 
 import numpy as np
-import logging
 
-from ibm_watsonx_ai.federated_learning.FLExceptions import FLException
+from ibmfl.exceptions import FLException
 
 logger = logging.getLogger(__name__)
 
 # TODO get dp stats
 
 
 def get_min(data, **kwargs):
@@ -36,16 +32,15 @@
     :return: A vector of shape (1, num_features) stores the minimum value \
     of each feature across all samples.
     :rtype: `np.array` of `float`
     """
     try:
         min_vec = np.min(data, axis=0)
     except Exception as ex:
-        raise FLException('Error occurred when calculating '
-                          'the minimum value. ' + str(ex))
+        raise FLException("Error occurred when calculating " "the minimum value. " + str(ex))
     return min_vec
 
 
 def get_max(data, **kwargs):
     """
     Assuming the dataset is loaded as type `np.array`, and has shape
      (num_samples, num_features).
@@ -60,16 +55,15 @@
     :return: A vector of shape (1, num_features) stores the maximum value \
     of each feature across all samples.
     :rtype: `np.array` of `float`
     """
     try:
         max_vec = np.max(data, axis=0)
     except Exception as ex:
-        raise FLException('Error occurred when calculating '
-                          'the maximum value. ' + str(ex))
+        raise FLException("Error occurred when calculating " "the maximum value. " + str(ex))
     return max_vec
 
 
 def get_mean(data, **kwargs):
     """
     Assuming the dataset is loaded as type `np.array`, and has shape
      (num_samples, num_features).
@@ -84,16 +78,15 @@
     :return: A vector of shape (1, num_features) stores the maximum value \
     of each feature across all samples.
     :rtype: `np.array` of `float`
     """
     try:
         var_vec = np.var(data, axis=0)
     except Exception as ex:
-        raise FLException('Error occurred when calculating '
-                          'the mean value. ' + str(ex))
+        raise FLException("Error occurred when calculating " "the mean value. " + str(ex))
     return var_vec
 
 
 def get_var(data, **kwargs):
     """
     Assuming the dataset is loaded as type `np.array`, and has shape
      (num_samples, num_features).
@@ -108,16 +101,15 @@
     :return: A vector of shape (1, num_features) stores the variance
     of each feature across all samples.
     :rtype: `np.array` of `float`
     """
     try:
         var_vec = np.var(data, axis=0)
     except Exception as ex:
-        raise FLException('Error occurred when calculating '
-                          'the variance. ' + str(ex))
+        raise FLException("Error occurred when calculating " "the variance. " + str(ex))
     return var_vec
 
 
 def get_std(data, **kwargs):
     """
     Assuming the dataset is loaded as type `np.array`, and has shape
      (num_samples, num_features).
@@ -132,16 +124,15 @@
     :return: A vector of shape (1, num_features) stores the
     standard deviation of each feature across all samples.
     :rtype: `np.array` of `float`
     """
     try:
         std_vec = np.std(data, axis=0)
     except Exception as ex:
-        raise FLException('Error occurred when calculating '
-                          'the standard deviation. ' + str(ex))
+        raise FLException("Error occurred when calculating " "the standard deviation. " + str(ex))
     return std_vec
 
 
 def get_quantile(data, percentage, **kwargs):
     """
     Assuming the dataset is loaded as type `np.array`, and has shape
      (num_samples, num_features).
@@ -159,20 +150,19 @@
     :return: A vector of shape (1, num_features) stores the
     standard deviation of each feature across all samples.
     :rtype: `np.array` of `float`
     """
     try:
         quantile_vec = np.quantile(data, q=percentage, axis=0)
     except Exception as ex:
-        raise FLException('Error occurred when calculating '
-                          'the quantile. ' + str(ex))
+        raise FLException("Error occurred when calculating " "the quantile. " + str(ex))
     return quantile_vec
 
 
-def get_normalizer(data, norm='l2'):
+def get_normalizer(data, norm="l2"):
     """
     Obtain the normalizer that perform the normalization preprocessing
     technique across all features via sklearn.preprocessing.normalizer API.
     A normalizer will scale a dataset w.r.t. features to unit norm.
 
     :param data: Provided dataset, assume each row is a data sample and \
     each column is one feature.
@@ -182,22 +172,22 @@
     :type norm: `str`
     :return: The normalizer preprocessor that can be applied to perform \
     normalizing preprocessing step for the party's local dataset \
     via `transform` method.
     :rtype: `sklearn.preprocessing.data.Normalizer`
     """
     from sklearn import preprocessing
+
     try:
         normalizer = preprocessing.Normalizer(norm=norm).fit(data)
 
         # test the normalizer
         normalizer.transform(data)
     except Exception as ex:
-        raise FLException('Error occurred when obtaining '
-                          'the normalizer. ' + str(ex))
+        raise FLException("Error occurred when obtaining " "the normalizer. " + str(ex))
     return normalizer
 
 
 def get_standardscaler(data, mean_val=None, std=None):
     """
     Obtain the StandardScaler that perform the standardization preprocessing
     technique with provided mean and standard deviation values
@@ -218,29 +208,27 @@
     :type std: `np.ndarray`
     :return: The standard scaler preprocessor that can be applied to perform \
     standardization preprocessing step for the party's local dataset \
     via `transform` method.
     :rtype: 'sklearn.preprocessing.data.StandardScaler'
     """
     from sklearn import preprocessing
+
     try:
         scaler = preprocessing.StandardScaler().fit(data)
 
         # set scaler with correct mean_val and std values
         if mean_val is not None:
-            logger.info("Set mean_val value of the StandardScaler "
-                        "as the provided mean...")
+            logger.info("Set mean_val value of the StandardScaler " "as the provided mean...")
             scaler.mean_ = mean_val
         if std is not None:
-            logger.info("Set standard deviation of the StandardScaler "
-                        "as the provided standard deviation...")
+            logger.info("Set standard deviation of the StandardScaler " "as the provided standard deviation...")
             scaler.scale_ = std
     except Exception as ex:
-        raise FLException('Error occurred when obtaining '
-                          'the standardscaler. ' + str(ex))
+        raise FLException("Error occurred when obtaining " "the standardscaler. " + str(ex))
 
     return scaler
 
 
 def get_minmaxscaler(data, feature_range=(0, 1)):
     """
     Obtain a MinMaxScaler that perform the MinMaxScale preprocessing technique
@@ -259,20 +247,19 @@
     :type feature_range: tuple (min, max), default=(0, 1)
     :return: The minmaxscaler preprocessor that can be applied to perform \
     minmax scaling preprocessing step for the party's local dataset \
     via `transform` method.
     :rtype: `sklearn.preprocessing.data.MinMaxScaler`
     """
     from sklearn import preprocessing
+
     try:
-        scaler = preprocessing.MinMaxScaler(feature_range=feature_range).\
-            fit(data)
+        scaler = preprocessing.MinMaxScaler(feature_range=feature_range).fit(data)
     except Exception as ex:
-        raise FLException('Error occurred when obtaining '
-                          'the minmaxcaler. ' + str(ex))
+        raise FLException("Error occurred when obtaining " "the minmaxcaler. " + str(ex))
     return scaler
 
 
 def get_reweighing_weights(data, sensitive_attribute, columns):
     """
     Calculates reweighing weights for points, assuming:
     * privileged group has sensitive attribute value = 1, unprivileged group is 0
@@ -294,56 +281,57 @@
 
     (features, labels) = data
 
     training_dataset = pd.DataFrame(data=features)
     class_values = labels.tolist()
 
     training_dataset.columns = columns
-    training_dataset['class'] = class_values
+    training_dataset["class"] = class_values
 
     nrows = training_dataset.shape[0]
 
-    priv = sum(training_dataset[sensitive_attribute])/nrows
-    unpriv = nrows - sum(training_dataset[sensitive_attribute])/nrows
-    pos = sum(training_dataset['class'])/nrows
-    neg = nrows - sum(training_dataset['class'])/nrows
+    priv = sum(training_dataset[sensitive_attribute]) / nrows
+    unpriv = nrows - sum(training_dataset[sensitive_attribute]) / nrows
+    pos = sum(training_dataset["class"]) / nrows
+    neg = nrows - sum(training_dataset["class"]) / nrows
 
     tmp_unp_train_data = training_dataset[training_dataset[sensitive_attribute] == 0]
     tmp_p_train_data = training_dataset[training_dataset[sensitive_attribute] == 1]
 
     if len(tmp_unp_train_data) > 0:
-        unpriv_neg = tmp_unp_train_data['class'].value_counts()[0]/nrows
-        if sum(tmp_unp_train_data['class']) > 0:
-            unpriv_pos = tmp_unp_train_data['class'].value_counts()[1]/nrows
+        unpriv_neg = tmp_unp_train_data["class"].value_counts()[0] / nrows
+        if sum(tmp_unp_train_data["class"]) > 0:
+            unpriv_pos = tmp_unp_train_data["class"].value_counts()[1] / nrows
         else:
             unpriv_pos = 0
     else:
         unpriv_neg = 0
     if len(tmp_p_train_data) > 0:
-        priv_neg = tmp_p_train_data['class'].value_counts()[0]/nrows
-        if sum(tmp_p_train_data['class']) > 0:
-            priv_pos = tmp_p_train_data['class'].value_counts()[1]/nrows
+        priv_neg = tmp_p_train_data["class"].value_counts()[0] / nrows
+        if sum(tmp_p_train_data["class"]) > 0:
+            priv_pos = tmp_p_train_data["class"].value_counts()[1] / nrows
         else:
             priv_pos = 0
     else:
         priv_pos = 0
 
     weight = []
     for index, row in training_dataset.iterrows():
-        if row[sensitive_attribute] == 0 and row['class'] == 0:
+        if row[sensitive_attribute] == 0 and row["class"] == 0:
             weight.append(unpriv * neg / unpriv_neg)
-        elif row[sensitive_attribute] == 0 and row['class'] == 1:
+        elif row[sensitive_attribute] == 0 and row["class"] == 1:
             weight.append(unpriv * pos / unpriv_pos)
-        elif row[sensitive_attribute] == 1 and row['class'] == 0:
+        elif row[sensitive_attribute] == 1 and row["class"] == 0:
             weight.append(priv * neg / priv_neg)
-        elif row[sensitive_attribute] == 1 and row['class'] == 1:
+        elif row[sensitive_attribute] == 1 and row["class"] == 1:
             weight.append(priv * pos / priv_pos)
 
     return np.array(weight)
 
+
 def get_hist_counts(data, sensitive_attribute, columns, eps):
     """
     Calculates noisy counts for reweighing with differential privacy (epsilon set in
     datahandler), assuming:
     * privileged group has sensitive attribute value = 1, unprivileged group is 0
     * positive class has value = 1, negative class is 0
     weight = P_expected(sensitive_attribute & class)/P_observed(sensitive_attribute & class)
@@ -367,39 +355,39 @@
     (features, labels) = data
     counts = {}
 
     training_dataset = pd.DataFrame(data=features)
     class_values = labels.tolist()
 
     training_dataset.columns = columns
-    training_dataset['class'] = class_values
+    training_dataset["class"] = class_values
 
-    data_whole = training_dataset[[sensitive_attribute, 'class']]
+    data_whole = training_dataset[[sensitive_attribute, "class"]]
     data_whole_0 = data_whole[data_whole[sensitive_attribute] == 0]
     data_whole_0_t = data_whole_0.T
 
     data_whole_0_t = np.array(data_whole_0_t)
     dp_hist_0, dp_bins_0 = dp.histogram(data_whole_0_t[1], bins=2, epsilon=eps)
-    counts['unp_neg'] = dp_hist_0[0]
-    counts['unp_pos'] = dp_hist_0[1]
+    counts["unp_neg"] = dp_hist_0[0]
+    counts["unp_pos"] = dp_hist_0[1]
 
     data_whole_1 = data_whole[data_whole[sensitive_attribute] == 1]
     data_whole_1_t = data_whole_1.T
 
     data_whole_1_t = np.array(data_whole_1_t)
     dp_hist_1, dp_bins_1 = dp.histogram(data_whole_1_t[1], bins=2, epsilon=eps)
-    counts['p_neg'] = dp_hist_1[0]
-    counts['p_pos'] = dp_hist_1[1]
+    counts["p_neg"] = dp_hist_1[0]
+    counts["p_pos"] = dp_hist_1[1]
 
     tmp_unp_train_data = training_dataset[training_dataset[sensitive_attribute] == 0]
     tmp_p_train_data = training_dataset[training_dataset[sensitive_attribute] == 1]
-    unpriv_neg = tmp_unp_train_data['class'].value_counts()[0]
-    unpriv_pos = tmp_unp_train_data['class'].value_counts()[1]
-    priv_neg = tmp_p_train_data['class'].value_counts()[0]
-    priv_pos = tmp_p_train_data['class'].value_counts()[1]
-
-    counts['unpriv_neg'] = unpriv_neg
-    counts['unpriv_pos'] = unpriv_pos
-    counts['priv_neg'] = priv_neg
-    counts['priv_pos'] = priv_pos
+    unpriv_neg = tmp_unp_train_data["class"].value_counts()[0]
+    unpriv_pos = tmp_unp_train_data["class"].value_counts()[1]
+    priv_neg = tmp_p_train_data["class"].value_counts()[0]
+    priv_pos = tmp_p_train_data["class"].value_counts()[1]
+
+    counts["unpriv_neg"] = unpriv_neg
+    counts["unpriv_pos"] = unpriv_pos
+    counts["priv_neg"] = priv_neg
+    counts["priv_pos"] = priv_pos
 
     return counts
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/__init__.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/__init__.py`

 * *Ordering differences only*

 * *Files 1% similar despite different names*

```diff
@@ -10,8 +10,8 @@
     get_model_lifecycle,
     get_supported_tasks,
     get_model_specs_with_prompt_tuning_support,
     get_custom_model_specs,
     get_embedding_model_specs,
 )
 from ibm_watsonx_ai.foundation_models.inference.model_inference import ModelInference
-from ibm_watsonx_ai.foundation_models.embeddings import Embeddings
+from ibm_watsonx_ai.foundation_models.embeddings import Embeddings
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/embeddings/base_embeddings.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/embeddings/base_embeddings.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 #  -----------------------------------------------------------------------------------------
 #  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
 from __future__ import annotations
+import warnings
 import copy
 import importlib
 from abc import ABC, abstractmethod
 
 
 class BaseEmbeddings(ABC):
     """Langchain-like embedding function interface."""
@@ -50,10 +51,12 @@
                         cls = getattr(module, class_type)
                     except AttributeError:
                         raise AttributeError(
                             f"Module: {module} has no attribute {class_type}"
                         )
 
                     if cls:
-                        return cls(**data)
+                        with warnings.catch_warnings(record=True):
+                            warnings.simplefilter("ignore", category=DeprecationWarning)
+                            return cls(**data)
 
         return None
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/embeddings/embeddings.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/embeddings/embeddings.py`

 * *Files 2% similar despite different names*

```diff
@@ -6,18 +6,22 @@
 
 from __future__ import annotations
 import time
 import os
 from typing import TypeAlias, TYPE_CHECKING
 from concurrent.futures import ThreadPoolExecutor
 from functools import reduce
+from enum import Enum
 
-from ibm_watsonx_ai.wml_client_error import WMLClientError, InvalidMultipleArguments
+from ibm_watsonx_ai.wml_client_error import (
+    WMLClientError,
+    InvalidMultipleArguments,
+    ParamOutOfRange,
+)
 from .base_embeddings import BaseEmbeddings
-from ibm_watsonx_ai.foundation_models.utils.enums import EmbeddingTypes
 from ibm_watsonx_ai.wml_resource import WMLResource
 import ibm_watsonx_ai._wrappers.requests as requests
 
 if TYPE_CHECKING:
     from ibm_watsonx_ai import APIClient
     from ibm_watsonx_ai import Credentials
 
@@ -25,15 +29,15 @@
 ParamsType: TypeAlias = dict[str, str | dict[str, str]]
 PayloadType: TypeAlias = dict[str, str | list[str] | ParamsType]
 
 
 __all__ = ["Embeddings"]
 
 # Do not change below, required by service
-_MAX_INPUTS_LENGTH = 20
+_MAX_INPUTS_LENGTH = 1000
 
 
 class Embeddings(BaseEmbeddings, WMLResource):
     """Instantiate the embeddings service.
 
     :param model_id: the type of model to use
     :type model_id: str, optional
@@ -101,25 +105,26 @@
         params: ParamsType | None = None,
         credentials: Credentials | dict[str, str] | None = None,
         project_id: str | None = None,
         space_id: str | None = None,
         api_client: APIClient | None = None,
         verify: bool | str | None = None,
     ) -> None:
-        if isinstance(model_id, EmbeddingTypes):
+        if isinstance(model_id, Enum):
             self.model_id = model_id.value
         else:
             self.model_id = model_id
 
         self.params = params
 
         Embeddings._validate_type(params, "params", dict, False)
 
         if credentials:
             from ibm_watsonx_ai import APIClient
+
             self._client = APIClient(credentials, verify=verify)
         elif api_client:
             self._client = api_client
         else:
             raise InvalidMultipleArguments(
                 params_names_list=["credentials", "api_client"],
                 reason="None of the arguments were provided.",
@@ -155,14 +160,19 @@
         :param concurrency_limit: number of requests that will be sent in parallel, max is 10, defaults to 10
         :type concurrency_limit: int, optional
         :return: scoring results containing generated embeddings vectors
         :rtype: dict
         """
         self._validate_type(inputs, "inputs", list, True)
 
+        if concurrency_limit > 10 or concurrency_limit < 1:
+            raise ParamOutOfRange(
+                param_name="concurrency_limit", value=concurrency_limit, min=1, max=10
+            )
+
         if isinstance(inputs, list) and len(inputs) > _MAX_INPUTS_LENGTH:
             generated_responses = []
             inputs_splited = [
                 inputs[i : i + _MAX_INPUTS_LENGTH]
                 for i in range(0, len(inputs), _MAX_INPUTS_LENGTH)
             ]
             if len(inputs_splited) <= concurrency_limit:
@@ -195,18 +205,17 @@
                 return left_copy
 
             return reduce(
                 reduce_response, generated_responses[1:], generated_responses[0]
             )
 
         else:
-            # Refactor when APIClient will be ported
-            from ibm_watsonx_ai.href_definitions import HrefDefinitions
-
-            generate_url = HrefDefinitions(client=self._client).get_fm_embeddings_href()
+            generate_url = (
+                self._client.service_instance._href_definitions.get_fm_embeddings_href()
+            )
             return self._generate(generate_url, inputs, params)
 
     def embed_documents(
         self,
         texts: list[str],
         params: ParamsType | None = None,
         concurrency_limit: int = 10,
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/embeddings/sentence_transformer_embeddings.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/embeddings/sentence_transformer_embeddings.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,14 +1,15 @@
 #  -----------------------------------------------------------------------------------------
 #  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
 
 from ibm_watsonx_ai.foundation_models.embeddings.base_embeddings import BaseEmbeddings
+from ibm_watsonx_ai.wml_client_error import MissingExtension
 
 
 class SentenceTransformerEmbeddings(BaseEmbeddings):
     """Embedding that utilizes sentence transformer, compatibile with ``RAGPattern``.
 
     Requires sentence_transformers to be installed by pip.
 
@@ -25,17 +26,15 @@
     def __init__(
         self, model_name: str, model_params: dict = None, encode_params: dict = None
     ) -> None:
         super().__init__()
         try:
             from sentence_transformers import SentenceTransformer
         except ImportError:
-            raise ImportError(
-                "Could not import sentence_transformers: Please install sentence_transformers extension."
-            )
+            raise MissingExtension("sentence_transformers")
         self.model_name = model_name
         self.model_params = model_params or {}
         self.encode_params = encode_params or {}
         self.model = SentenceTransformer(self.model_name, **self.model_params)
 
     def embed_documents(self, texts: list[str]) -> list[list[float]]:
         return self.model.encode(texts, **self.encode_params).tolist()
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/extensions/langchain/llm.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/extensions/langchain/llm.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,19 +1,20 @@
 #  -----------------------------------------------------------------------------------------
 #  (C) Copyright IBM Corp. 2023-2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 import logging
 from typing import Any, List, Mapping, Optional
+from ibm_watsonx_ai.wml_client_error import MissingExtension
 
 try:
     from langchain.llms.base import LLM
     from langchain.llms.utils import enforce_stop_tokens
 except ImportError:
-    raise ImportError("Could not import langchain: Please install langchain extension.")
+    raise MissingExtension("langchain")
 from ibm_watsonx_ai.foundation_models import Model, ModelInference
 from ibm_watsonx_ai.foundation_models.utils.utils import (
     _raise_watsonxllm_deprecation_warning,
 )
 
 logger = logging.getLogger(__name__)
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/extensions/rag/chunker/base_chunker.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/extensions/rag/chunker/base_chunker.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,44 +1,44 @@
 #  -----------------------------------------------------------------------------------------
 #  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
-from typing import Sequence, Any
+from typing import Sequence, Any, TypeVar
 from abc import ABC, abstractmethod
 
-from langchain_core.documents import Document
-
 
 __all__ = [
     "BaseChunker",
+    "ChunkType",
 ]
 
 
+ChunkType = TypeVar("ChunkType")
+
+
 class BaseChunker(ABC):
     """
     Class responsible for handling operations of splitting documents
     within the RAG application.
     """
 
     @abstractmethod
-    def split_documents(
-        self, documents: Sequence[str | Document]
-    ) -> list[str | Document]:
+    def split_documents(self, documents: Sequence[ChunkType]) -> list[ChunkType]:
         """
         Split series of documents into smaller parts based on
         the provided chunker settings.
 
         :param documents: sequence of elements that contain context in the format of text
-        :type: Sequence[str | Document]
+        :type: Sequence[ChunkType]
 
         :return: list of documents splitter into smaller ones, having less content
-        :rtype: list[str | Document]
+        :rtype: list[ChunkType]
         """
 
     @abstractmethod
     def to_dict(self) -> dict[str, Any]:
-        """Return dict that can be used to recreate instance of the LCChunker."""
+        """Return dict that can be used to recreate instance of the BaseChunker."""
 
     @abstractmethod
-    def from_dict(self, d: dict[str, Any]) -> 'BaseChunker':
+    def from_dict(self, d: dict[str, Any]) -> "BaseChunker":
         """Create instance from the dictionary"""
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/extensions/rag/chunker/get_chunker.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/extensions/rag/chunker/get_chunker.py`

 * *Files 9% similar despite different names*

```diff
@@ -2,15 +2,15 @@
 #  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
 from typing import Literal, Any
 
 from .base_chunker import BaseChunker
-from .langchain_chunker import LCChunker
+from .langchain_chunker import LangchainChunker
 
 
 _supported_providers = ["langchain"]
 
 
 def get_chunker(
     provider: Literal["langchain"], settings: dict[str, Any] = None
@@ -27,15 +27,15 @@
     :return: instance of BaseChunker that can split user's documents or text
     :rtype: BaseChunker
     """
     settings = settings or {}
 
     match provider:
         case "langchain":
-            chunker = LCChunker(**settings)
+            chunker = LangchainChunker(**settings)
 
         case _:
             raise ValueError(
                 "{} provider is not supported! Use one of {}.".format(
                     provider, _supported_providers
                 )
             )
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/extensions/rag/chunker/langchain_chunker.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/extensions/rag/chunker/langchain_chunker.py`

 * *Files 11% similar despite different names*

```diff
@@ -2,25 +2,24 @@
 #  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
 from typing import Literal, Sequence, Any
 
 from langchain.text_splitter import TextSplitter
-from langchain_core.documents import Document
 
-from .base_chunker import BaseChunker
+from .base_chunker import BaseChunker, ChunkType
 
 
 __all__ = [
-    "LCChunker",
+    "LangchainChunker",
 ]
 
 
-class LCChunker(BaseChunker):
+class LangchainChunker(BaseChunker):
     """
     Wrapper for langchain TextSplitter.
 
     :param method: describes type of the TextSplitter as the main instance performing chunking
     :type method: Literal["recursive", "character", "token"]
 
     :param chunk_size: maximum size of a single chunk that is returned
@@ -51,15 +50,15 @@
         self.chunk_size = chunk_size
         self.chunk_overlap = chunk_overlap
         self.encoding_name = encoding_name
         self.model_name = model_name
         self.separators = kwargs.pop("separators", ["\n\n", "(?<=\. )", "\n", " ", ""])
         self._text_splitter = self._get_text_splitter()
 
-    def __eq__(self, other: 'LCChunker') -> bool:
+    def __eq__(self, other: "LangchainChunker") -> bool:
         return self.to_dict() == other.to_dict()
 
     def _get_text_splitter(self) -> TextSplitter:
         """Create instance of TextSplitter based on the settings."""
 
         match self.method:
             case "recursive":
@@ -97,33 +96,39 @@
                     )
                 )
 
         return text_splitter
 
     def to_dict(self) -> dict[str, Any]:
         """
-        Return dict that can be used to recreate instance of the LCChunker.
+        Return dict that can be used to recreate instance of the LangchainChunker.
         """
-        params = ("method", "chunk_size", "chunk_overlap", "encoding_name", "model_name")
+        params = (
+            "method",
+            "chunk_size",
+            "chunk_overlap",
+            "encoding_name",
+            "model_name",
+        )
 
         ret = {k: v for k, v in self.__dict__.items() if k in params}
 
         return ret
 
     @classmethod
-    def from_dict(cls, d: dict[str, Any]) -> 'LCChunker':
+    def from_dict(cls, d: dict[str, Any]) -> "LangchainChunker":
         """Create instance from the dictionary"""
 
         return cls(**d)
 
-    def split_documents(self, documents: Sequence[Document]) -> list[Document]:
+    def split_documents(self, documents: Sequence[ChunkType]) -> list[ChunkType]:
         """
         Split series of documents into smaller parts based on the provided
         chunker settings.
 
         :param documents: sequence of elements that contain context in the format of text
-        :type documents: Sequence[Document]
+        :type documents: Sequence[ChunkType]
 
         :return: list of documents splitter into smaller ones, having less content
-        :rtype: list[Document]
+        :rtype: list[ChunkType]
         """
         return self._text_splitter.split_documents(documents)
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/extensions/rag/pattern/default_deployable_function.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/extensions/rag/pattern/default_deployable_function.py`

 * *Files 20% similar despite different names*

```diff
@@ -26,55 +26,60 @@
                     ['answer 2', [ {'page_content': 'page content 2',
                                     'metadata':     'metadata 2'} ]]
                 ]
             }
         ]
     }
     """
-    from ibm_watsonx_ai import APIClient
+    from ibm_watsonx_ai import APIClient, Credentials
     from ibm_watsonx_ai.foundation_models import ModelInference
     from ibm_watsonx_ai.foundation_models.extensions.rag import VectorStore
     from ibm_watsonx_ai.metanames import RAGPatternParamsMetaNames
 
-    client = APIClient(params["credentials"])
+    client = APIClient(Credentials.from_dict(params["credentials"]))
     client.set.default_space(params["space_id"])
 
     vector_store = VectorStore.from_dict(client=client, data=params["vector_store"])
     prompt = params["prompt"]
     model = ModelInference(api_client=client, **params["model"])
     rag_params = params["rag_params"]
 
     def score(payload):
-        result = {
-            "predictions": [{"fields": ["answer", "reference_documents"], "values": []}]
-        }
+        result = {"predictions": [{"fields": ["answer", "reference_documents"]}]}
 
+        all_prompts = []
+        all_retrieved_docs = []
         for question in payload[client.deployments.ScoringMetaNames.INPUT_DATA][0][
             "values"
         ]:
             num_retrieved_docs = rag_params.get(
                 RAGPatternParamsMetaNames.NUM_RETRIEVED_DOCS
             )
             retrieved_docs = vector_store.search(query=question, k=num_retrieved_docs)
+            all_retrieved_docs.append(retrieved_docs)
             reference_documents = [doc.page_content for doc in retrieved_docs]
 
             prompt_variables = {
                 "question": question,
                 "reference_documents": "\n".join(reference_documents),
             }
             prompt_input_text = prompt.format(**prompt_variables)
+            all_prompts.append(prompt_input_text)
 
-            answer = model.generate_text(prompt=prompt_input_text)
+        answers = model.generate_text(prompt=all_prompts)
 
-            result["predictions"][0]["values"].append(
+        predictions = [
+            [
+                answer,
                 [
-                    answer,
-                    [
-                        {"page_content": doc.page_content, "metadata": doc.metadata}
-                        for doc in retrieved_docs
-                    ],
-                ]
-            )
+                    {"page_content": doc.page_content, "metadata": doc.metadata}
+                    for doc in retrieved_docs
+                ],
+            ]
+            for answer, retrieved_docs in zip(answers, all_retrieved_docs)
+        ]
+
+        result["predictions"][0]["values"] = predictions
 
         return result
 
     return score
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/extensions/rag/pattern/pattern.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/extensions/rag/pattern/pattern.py`

 * *Files 1% similar despite different names*

```diff
@@ -426,17 +426,17 @@
         defaults: tuple | list = args_spec.defaults or []
         args = args_spec.args or []
 
         if len(args) > 0 and args[-1] == "params":
             default_deployable_params = {
                 "credentials": self._credentials.to_dict(),
                 "space_id": self.space_id,
-                "vector_store": self.vector_store.to_dict()
-                if self.vector_store
-                else None,
+                "vector_store": (
+                    self.vector_store.to_dict() if self.vector_store else None
+                ),
                 "prompt": self.prompt_text,
                 "model": self.model.get_identifying_params() if self.model else None,
                 "rag_params": self.rag_params,
             }
 
             if provided_deployable_params := defaults[-1]:
                 default_deployable_params.update(provided_deployable_params)
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/extensions/rag/utils/utils.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/extensions/rag/utils/utils.py`

 * *Files 3% similar despite different names*

```diff
@@ -16,15 +16,17 @@
 
 logger = logging.getLogger(__name__)
 
 
 # Verbose display in notebooks
 
 
-def verbose_search(question: str, documents: list[Document] | list[tuple[Document, float]]) -> None:
+def verbose_search(
+    question: str, documents: list[Document] | list[tuple[Document, float]]
+) -> None:
     """Display a table with found documents.
 
     :param question: question/query used for search
     :type question: str
     :param documents: list of documents found with question or list of tuples (if search was done with scores)
     :type documents: list[langchain_core.documents.Document] | list[Document, float]
     :raises ImportError: if it is notebook environment but IPython is not found
@@ -59,16 +61,19 @@
 
             # Parsing rows and adding them to the DataFrame
             for doc in documents:
                 row = {"page_content": doc.page_content}
                 row.update(doc.metadata)
                 # Adding score (if provided)
                 if scores:
-                    row['score'] = scores.pop(0)
-                df = pd.concat([df, pd.DataFrame({key: [value] for key, value in row.items()})], ignore_index=True)
+                    row["score"] = scores.pop(0)
+                df = pd.concat(
+                    [df, pd.DataFrame({key: [value] for key, value in row.items()})],
+                    ignore_index=True,
+                )
 
             display(df)
         else:
             display(Markdown("No documents were found."))
     else:
         if len(documents) > 0:
             if scores:
@@ -79,14 +84,15 @@
                     logger.info(f"{i} |  {d.page_content}   | {d.metadata}")
         else:
             logger.info("No documents were found.")
 
 
 # SSL Certificates
 
+
 def is_valid_certificate(cert_string: str) -> bool:
     try:
         ssl.PEM_cert_to_DER_cert(cert_string)
         return True
     except Exception:
         return False
 
@@ -98,17 +104,21 @@
         try:
             cert_decoded = base64.b64decode(cert).decode()
             if is_valid_certificate(cert_decoded):
                 return cert_decoded
             else:
                 raise ValueError("Not a valid SSL certificate.")
         except Exception as e:
-            raise ValueError(f"Error occured when trying to get the SSL certificate: {e}")
+            raise ValueError(
+                f"Error occured when trying to get the SSL certificate: {e}"
+            )
 
 
 def save_ssl_certificate_as_file(ssl_certificate_content: str, file_path: str) -> str:
     ssl_certificate_content = get_ssl_certificate(ssl_certificate_content)
-    with open(file_path, 'w') as file:
+    with open(file_path, "w") as file:
         file.write(ssl_certificate_content)
 
-    logger.info(f"SSL certificate was found and written to {file_path}. It will be used for the connection for the VectorStore.")
+    logger.info(
+        f"SSL certificate was found and written to {file_path}. It will be used for the connection for the VectorStore."
+    )
     return file_path
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/extensions/rag/vector_stores/__init__.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/extensions/rag/vector_stores/__init__.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/extensions/rag/vector_stores/base_vector_store.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/extensions/rag/vector_stores/base_vector_store.py`

 * *Files 3% similar despite different names*

```diff
@@ -35,33 +35,38 @@
         :param content: unstructured list of data to be added
         :type content: list[str] | list[dict] | list
 
         :return: list of ids
         :rtype: list[str]
         """
         pass
-    
+
     @abstractmethod
     async def add_documents_async(
         self, content: list[str] | list[dict] | list, **kwargs: Any
     ) -> list[str]:
         """Add document to the RAG's vector store asynchronously.
         List must contain either strings, dicts with a required field ``content`` of str type or langchain ``Document``.
 
         :param content: unstructured list of data to be added
         :type content: list[str] | list[dict] | list
 
         :return: list of ids
         :rtype: list[str]
         """
         pass
-    
+
     @abstractmethod
     def search(
-        self, query: str, k: int, include_scores: bool = False, verbose: bool = False, **kwargs: Any
+        self,
+        query: str,
+        k: int,
+        include_scores: bool = False,
+        verbose: bool = False,
+        **kwargs: Any,
     ) -> list:
         """Get documents that would fit the query.
 
         :param query: question asked by a user
         :type query: str
 
         :param k: max number of similar documents
@@ -73,15 +78,15 @@
         :param verbose: print formated response to the output, defaults to False
         :type verbose: bool, optional
 
         :return: list of found documents
         :rtype: list
         """
         pass
-    
+
     @abstractmethod
     def delete(self, ids: list[str], **kwargs: Any) -> None:
         """Delete documents with provided ids.
 
         :param ids: IDs of documents to delete
         :type ids: list[str]
         """
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/extensions/rag/vector_stores/langchain_vector_store_adapter.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/extensions/rag/vector_stores/langchain_vector_store_adapter.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,20 +1,21 @@
 #  -----------------------------------------------------------------------------------------
 #  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
+from ibm_watsonx_ai.wml_client_error import MissingExtension
 
 try:
     from langchain_core.vectorstores import VectorStore as LangchainVectorStore
     from langchain_core.vectorstores import (
         VectorStoreRetriever as LangchainVectorStoreRetriever,
     )
     from langchain_core.documents import Document
 except ImportError:
-    raise ImportError("Could not import langchain: Please install langchain extension.")
+    raise MissingExtension("langchain")
 
 import hashlib
 from typing import Coroutine, Any
 import logging
 
 from ibm_watsonx_ai.foundation_models.embeddings import BaseEmbeddings
 from ibm_watsonx_ai.foundation_models.extensions.rag.utils.utils import verbose_search
@@ -61,21 +62,24 @@
     ) -> Coroutine[Any, Any, list[str]]:
         ids, docs = self._process_documents(content)
         return await self._langchain_vector_store.aadd_documents(
             docs, ids=ids, **kwargs
         )
 
     def search(
-        self, query: str, k: int, include_scores: bool = False, verbose: bool = False, **kwargs: Any
+        self,
+        query: str,
+        k: int,
+        include_scores: bool = False,
+        verbose: bool = False,
+        **kwargs: Any,
     ) -> list:
         if include_scores:
-            result = (
-                self._langchain_vector_store.similarity_search_with_score(
-                    query, k=k, **kwargs
-                )
+            result = self._langchain_vector_store.similarity_search_with_score(
+                query, k=k, **kwargs
             )
         else:
             result = self._langchain_vector_store.similarity_search(
                 query, k=k, **kwargs
             )
 
         if verbose:
@@ -101,23 +105,25 @@
 
         :return: lists with IDs and docs
         :rtype: tuple[list[str], list[langchain_core.documents.Document]
         """
         docs = self._as_langchain_documents(content)
         if docs:
             # Take only unique ID document. Get two lists, one with ids, one with documents
-            return tuple(map(
-                list,
-                zip(
-                    *{
-                        hashlib.sha256(doc.page_content.encode()).hexdigest(): doc
-                        for doc in docs
-                    }.items()
-                ),
-            ))
+            return tuple(
+                map(
+                    list,
+                    zip(
+                        *{
+                            hashlib.sha256(doc.page_content.encode()).hexdigest(): doc
+                            for doc in docs
+                        }.items()
+                    ),
+                )
+            )
         else:
             return [], []
 
     def _as_langchain_documents(
         self, content: list[str] | list[dict] | list
     ) -> list[Document]:
         """Creates a langchain ``Document`` list from list of potentially unstructured data.
@@ -135,15 +141,17 @@
                 result.append(Document(page_content=doc))
             elif isinstance(doc, dict):
                 content_str: str | None = doc.get("content", None)
                 metadata = doc.get("metadata", {})
 
                 if content_str:
                     if isinstance(metadata, dict):
-                        result.append(Document(page_content=content_str, metadata=metadata))
+                        result.append(
+                            Document(page_content=content_str, metadata=metadata)
+                        )
                     else:
                         logger.warning(
                             f"Document: {doc} is incorrect. Metadata needs to be given with 'metadata' attribute and it needs to be a serializable dict. Skipping."
                         )
                         continue
                 else:
                     logger.warning(
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/extensions/rag/vector_stores/vector_store.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/extensions/rag/vector_stores/vector_store.py`

 * *Files 3% similar despite different names*

```diff
@@ -3,14 +3,15 @@
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 from __future__ import annotations
 import contextlib
 from typing import Any, Coroutine
 import logging
 import copy
+from warnings import warn
 
 from ibm_watsonx_ai.client import APIClient
 from ibm_watsonx_ai.foundation_models.embeddings import BaseEmbeddings
 from ibm_watsonx_ai.foundation_models.extensions.rag.vector_stores.base_vector_store import (
     BaseVectorStore,
 )
 from ibm_watsonx_ai.foundation_models.extensions.rag.vector_stores.langchain_vector_store_adapter import (
@@ -63,18 +64,18 @@
     You might use custom embeddings for adding and searching documents.
 
     .. code-block:: python
         from ibm_watsonx_ai import APIClient
         from ibm_watsonx_ai.foundation_models.extensions.rag import VectorStore
         from ibm_watsonx_ai.foundation_models.embeddings import SentenceTransformerEmbeddings
 
-        client = APIClient(credentials)
+        api_client = APIClient(credentials)
 
         vector_store = VectorStore(
-                client,
+                api_client,
                 connection_id='***',
                 params={
                     'index_name': 'my_test_index',
                 },
                 embeddings=SentenceTransformerEmbeddings('sentence-transformers/all-MiniLM-L6-v2')
             )
 
@@ -90,18 +91,18 @@
         Optionally, like in langchain, it is possible to use cloud id and api key parameters to connect to Elastic Cloud.
         The ``params`` argument can be used as direct params to langchain's ``ElasticsearchStore`` constructor.
 
     .. code-block:: python
         from ibm_watsonx_ai import APIClient
         from ibm_watsonx_ai.foundation_models.extensions.rag import VectorStore
 
-        client = APIClient(credentials)
+        api_client = APIClient(credentials)
 
         vector_store = VectorStore(
-                client,
+                api_client,
                 params={
                     'index_name': 'my_test_index',
                     'model_id': ".elser_model_2_linux-x86_64",
                     'cloud_id': '***',
                     'api_key': '***'
                 },
             )
@@ -113,24 +114,32 @@
         ])
 
         vector_store.search('one', k=1)
     """
 
     def __init__(
         self,
-        client: APIClient | None = None,
+        api_client: APIClient | None = None,
         connection_id: str | None = None,
         langchain_vector_store: LangchainVectorStore | None = None,
         data_source_type: VectorStoreDataSourceType | None = None,
         params: dict | None = None,
         embeddings: BaseEmbeddings | None = None,
+        **kwargs,
     ) -> None:
         super().__init__()
 
-        self._client: APIClient = client
+        self._client: APIClient = api_client
+        if "client" in kwargs and self._client is None:
+            warn(
+                "Parameter `client` is deprecated. Use `api_client` instead.",
+                DeprecationWarning,
+            )
+            self._client = kwargs["client"]
+
         self._vector_store: BaseVectorStore = None
         self._connection_id: str = connection_id
         self._params: dict = params or {}
         self._data_source_type: VectorStoreDataSourceType = data_source_type
         self._embeddings: BaseEmbeddings = embeddings
 
         if self._connection_id:
@@ -150,23 +159,22 @@
                         langchain_vector_store
                     )
                 )
             else:
                 raise TypeError("Langchain vector store was of incorrect type.")
         elif self._data_source_type:
             logger.info("Connecting by manually set data source type.")
-            self._vector_store = VectorStoreConnector(properties={'embeddings': self._embeddings, **self._params}).get_from_type(self._data_source_type)
+            self._vector_store = VectorStoreConnector(
+                properties={"embeddings": self._embeddings, **self._params}
+            ).get_from_type(self._data_source_type)
         else:
             raise TypeError(
                 "To establish connection, please provide 'connection_id', 'langchain_vector_store' or 'data_source_type'."
             )
 
-        if not self._embeddings:
-            logger.warning("Embeddings were not set up in the constructor.\nData ingestion and retrieval could be done on vector store side or might not work correctly.")
-
     def _get_connection_type(self, connection_details: dict[str, list | dict]) -> str:
         """Determine connection type from connection details by comparing it to the available list of data source types.
 
         :param connection_details: dict containing connection details
         :type connection_details: dict[str, list]
 
         :raises KeyError: if connection data source is invalid
@@ -198,15 +206,17 @@
         datasouce_type = self._get_connection_type(connection_data)
         properties = connection_data["entity"]["properties"]
 
         param_dict = {**properties, **self._params}
 
         logger.info(f"Initializing vector store of type: {datasouce_type}")
 
-        self._vector_store = VectorStoreConnector({'embeddings': self._embeddings, **param_dict}).get_from_type(
+        self._vector_store = VectorStoreConnector(
+            {"embeddings": self._embeddings, **param_dict}
+        ).get_from_type(
             datasouce_type  # type: ignore[arg-type]
         )
 
         logger.info("Success. Vector store initialized correctly.")
 
         return datasouce_type
 
@@ -218,18 +228,19 @@
         """
         params = copy.deepcopy(self._params)
 
         return {
             "connection_id": self._connection_id,
             "data_source_type": str(self._data_source_type),
             "params": params,
-            "embeddings":
+            "embeddings": (
                 self._embeddings.to_dict()
                 if isinstance(self._embeddings, BaseEmbeddings)
                 else {}
+            ),
         }
 
     @classmethod
     def from_dict(
         cls, client: APIClient | None = None, data: dict | None = None
     ) -> VectorStore:
         """Creates ``VectorStore`` using only primitive data type dict.
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/extensions/rag/vector_stores/vector_store_connector.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/extensions/rag/vector_stores/vector_store_connector.py`

 * *Files 4% similar despite different names*

```diff
@@ -16,14 +16,15 @@
     LangchainVectorStoreAdapter,
 )
 from ibm_watsonx_ai.foundation_models.extensions.rag.utils.utils import (
     save_ssl_certificate_as_file,
 )
 
 from langchain_core.vectorstores import VectorStore as LangchainVectorStore
+from ibm_watsonx_ai.wml_client_error import MissingExtension
 
 logger = logging.getLogger(__name__)
 
 
 class VectorStoreDataSourceType(str, Enum):
     ELASTICSEARCH = "elasticsearch"
     CHROMA = "chroma"
@@ -54,15 +55,17 @@
         def deepcopy_if_possible(obj):
             try:
                 return copy.deepcopy(obj)
             except Exception:
                 return obj
 
         self.properties: dict = (
-            {key: deepcopy_if_possible(value) for key, value in properties.items()} if isinstance(properties, dict) else {}
+            {key: deepcopy_if_possible(value) for key, value in properties.items()}
+            if isinstance(properties, dict)
+            else {}
         )
 
     @staticmethod
     def get_type_from_langchain_vector_store(
         langchain_vector_store: Any,
     ) -> VectorStoreDataSourceType:
         """Returns ``DataSourceType`` for concrete langchain ``VectorStore`` class.
@@ -101,17 +104,17 @@
             case VectorStoreDataSourceType.CHROMA:
                 return self.get_chroma()
             case VectorStoreDataSourceType.MILVUS:
                 return self.get_milvus()
             case _:
                 raise TypeError("Data source type not supported.")
 
-    def get_langchain_adapter(              # type: ignore[return]
+    def get_langchain_adapter(  # type: ignore[return]
         self, langchain_vector_store: Any
-    ) -> LangchainVectorStoreAdapter | None:  
+    ) -> LangchainVectorStoreAdapter | None:
         """Creates adapter for concrete vector store from langchain.
 
         :param langchain_vector_store: object that is subclass of Langchain VectorStore
         :type langchain_vector_store: Any
 
         :raises ImportError: langchain required
         :return: proper adapter for the vector store
@@ -121,159 +124,185 @@
         if isinstance(langchain_vector_store, LangchainVectorStore):
             return LangchainVectorStoreAdapter(vector_store=langchain_vector_store)
 
     def get_chroma(self) -> LangchainVectorStoreAdapter:
         """Creates Chroma in-memory vector store.
 
         :raises ImportError: langchain required
-        :return: vector store adapter for langchain's Chroma
+        :return: vector store adapter for LangChain's Chroma
         :rtype: LangchainVectorStoreAdapter
         """
-        from langchain_community.vectorstores.chroma import Chroma
+        try:
+            from langchain_chroma import Chroma
+        except ImportError:
+            raise MissingExtension("langchain_chroma")
 
         parsed_params = self.properties
 
         # Set embedding from params
-        parsed_params['embedding_function'] = parsed_params.pop("embeddings", None)
+        parsed_params["embedding_function"] = parsed_params.pop("embeddings", None)
 
-        if parsed_params['embedding_function'] is None:
+        if parsed_params["embedding_function"] is None:
             raise ValueError("Embedding function is required for Chroma.")
 
         return LangchainVectorStoreAdapter(Chroma(**parsed_params))
 
     def get_milvus(self) -> LangchainVectorStoreAdapter:
         """Creates Milvus vector store.
 
         :raises ImportError: langchain required
-        :return: vector store adapter for langchain's Milvus
+        :return: vector store adapter for LangChain's Milvus
         :rtype: LangchainVectorStoreAdapter
         """
-        from langchain_community.vectorstores.milvus import Milvus
+        try:
+            from langchain_community.vectorstores.milvus import Milvus
+        except ImportError:
+            raise MissingExtension("langchain_community")
 
         parsed_params = self.properties
         parsed_params.pop("ssl", "")
 
         # Prepare connection_args (if not present)
-        if 'connection_args' not in parsed_params:
-            parsed_params['connection_args'] = {}
+        if "connection_args" not in parsed_params:
+            parsed_params["connection_args"] = {}
 
         # Get SSL certificate saved to file
-        if 'ssl_certificate' in parsed_params:
-            parsed_params['connection_args']['ca_pem_path'] = save_ssl_certificate_as_file(parsed_params.pop('ssl_certificate'), "milvus_ca_ssl.crt")
-            parsed_params['connection_args']['secure'] = True
+        if "ssl_certificate" in parsed_params:
+            parsed_params["connection_args"]["ca_pem_path"] = (
+                save_ssl_certificate_as_file(
+                    parsed_params.pop("ssl_certificate"), "milvus_ca_ssl.crt"
+                )
+            )
+            parsed_params["connection_args"]["secure"] = True
 
         # Connection 'username' is 'user' in Milvus
-        if 'username' in parsed_params:
-            parsed_params['user'] = parsed_params.pop('username')
+        if "username" in parsed_params:
+            parsed_params["user"] = parsed_params.pop("username")
 
         # Connection 'database' is 'db_name' in Milvus
-        if 'database' in parsed_params:
-            parsed_params['db_name'] = parsed_params.pop('database')
+        if "database" in parsed_params:
+            parsed_params["db_name"] = parsed_params.pop("database")
 
         # Move each param that was in parsed_params to connection_args if we expect it here
-        for param in ['uri', 'host', 'port', 'user', 'password', 'db_name', 'secure', 'client_key_path',
-                      'client_pem_path', 'ca_pem_path', 'server_pem_path', 'server_name']:
+        for param in [
+            "uri",
+            "host",
+            "port",
+            "user",
+            "password",
+            "db_name",
+            "secure",
+            "client_key_path",
+            "client_pem_path",
+            "ca_pem_path",
+            "server_pem_path",
+            "server_name",
+        ]:
 
             if param in parsed_params.keys():
-                parsed_params['connection_args'][param] = parsed_params.pop(param)
+                parsed_params["connection_args"][param] = parsed_params.pop(param)
 
-        parsed_params['embedding_function'] = parsed_params.pop("embeddings", None)
+        parsed_params["embedding_function"] = parsed_params.pop("embeddings", None)
 
         return LangchainVectorStoreAdapter(Milvus(**parsed_params))
 
     def get_elasticsearch(self) -> LangchainVectorStoreAdapter:
         """Creates Elasticsearch vector store.
 
         :raises ImportError: langchain required
-        :return: vector store adapter for langchain's Elasticsearch
+        :return: vector store adapter for LangChain's Elasticsearch
         :rtype: LangchainVectorStoreAdapter
         """
-        from langchain_elasticsearch import (
-            ElasticsearchStore,
-            SparseRetrievalStrategy,
-            ApproxRetrievalStrategy,
-            ExactRetrievalStrategy,
-        )
+        try:
+            from langchain_elasticsearch import (
+                ElasticsearchStore,
+                SparseRetrievalStrategy,
+                ApproxRetrievalStrategy,
+                ExactRetrievalStrategy,
+            )
 
-        from langchain_elasticsearch.vectorstores import BaseRetrievalStrategy
+            from langchain_elasticsearch.vectorstores import BaseRetrievalStrategy
+        except ImportError:
+            raise MissingExtension("langchain_elasticsearch")
 
         parsed_params = self.properties
 
         # Always use empty es_params if not provided
         parsed_params["es_params"] = self.properties.pop("es_params", {})
 
         # Drop unnecessary stuff from connection asset if they are present
         parsed_params.pop("auth_method", None)
         parsed_params.pop("use_anonymous_access", None)
 
         # Parse ES connection data - select proper connection type
         # Connecting by 'url': username/password or api_key
-        if 'url' in parsed_params:
+        if "url" in parsed_params:
             # Get URL of ES instance
             parsed_params["es_url"] = parsed_params.pop("url")
 
             # Detect credentials given in connection asset
-            if 'username' in parsed_params and 'password' in parsed_params:
+            if "username" in parsed_params and "password" in parsed_params:
                 # Connect by username and password extracted from connection
                 parsed_params["es_user"] = parsed_params.pop("username")
                 parsed_params["es_password"] = parsed_params.pop("password")
-                parsed_params.pop('api_key', None)
-            elif 'api_key' in parsed_params:
+                parsed_params.pop("api_key", None)
+            elif "api_key" in parsed_params:
                 # Connect by api key
                 parsed_params["es_api_key"] = parsed_params.pop("api_key")
 
                 parsed_params.pop("username", None)
                 parsed_params.pop("password", None)
             else:
                 raise ValueError(
                     """To connect to given hostname ['url'] provide
                                 either ['username', 'password'] or ['api_key'].
                                 Make sure those fields are present in connection details or parameters given
-                                upon VectorStore initialization. """)
-        elif 'es_url' in parsed_params:
-            if 'es_user' in parsed_params and 'es_password' in parsed_params:
+                                upon VectorStore initialization. """
+                )
+        elif "es_url" in parsed_params:
+            if "es_user" in parsed_params and "es_password" in parsed_params:
                 pass
-            elif 'es_api_key' in parsed_params:
+            elif "es_api_key" in parsed_params:
                 pass
             else:
                 raise ValueError(
                     """To connect to given hostname ['es_url'] provide
                                 either ['es_user', 'es_password'] or ['es_api_key'].
                                 Make sure those fields are present in parameters given
                                 upon VectorStore initialization. """
                 )
         # Connecting by '(es_)cloud_id' to Elasticsearch cloud
-        elif 'cloud_id' in parsed_params and \
-             'api_key' in parsed_params:
-            parsed_params['es_cloud_id'] = parsed_params.pop('cloud_id', None)
-            parsed_params['es_api_key'] = parsed_params.pop('api_key', None)
-        elif 'es_cloud_id' in parsed_params and \
-             'es_api_key' in parsed_params:
+        elif "cloud_id" in parsed_params and "api_key" in parsed_params:
+            parsed_params["es_cloud_id"] = parsed_params.pop("cloud_id", None)
+            parsed_params["es_api_key"] = parsed_params.pop("api_key", None)
+        elif "es_cloud_id" in parsed_params and "es_api_key" in parsed_params:
             pass
         else:
             raise ValueError(
                 """Connection data was not sufficent. Either provide:
                              - ['url', 'username', 'password'],
                              - ['url', 'api_key'],
                              - ['cloud_id', 'api_key']
                              or
                              - ['es_url', 'es_user', 'es_password'],
                              - ['es_url', 'es_api_key'],
                              - ['es_cloud_id', 'es_api_key'],
                              in your connection asset or in params for VectorStore."""
             )
 
-        if 'index_name' not in parsed_params:
+        if "index_name" not in parsed_params:
             raise ValueError("Provide 'index_name' in params.")
 
         # Parse SSL certificate
         ssl_certificate_content = parsed_params.pop("ssl_certificate", None)
 
         if ssl_certificate_content:
-            parsed_params['es_params']['ca_certs'] = save_ssl_certificate_as_file(ssl_certificate_content, "es_ca_ssl.crt")
+            parsed_params["es_params"]["ca_certs"] = save_ssl_certificate_as_file(
+                ssl_certificate_content, "es_ca_ssl.crt"
+            )
 
         # Determine retrieval strategy type from parameters
         if "strategy" not in parsed_params or not isinstance(
             parsed_params["strategy"], BaseRetrievalStrategy
         ):
             if "model_id" in parsed_params:
                 parsed_params["strategy"] = SparseRetrievalStrategy(
@@ -283,10 +312,10 @@
                 parsed_params["strategy"] = ApproxRetrievalStrategy(
                     parsed_params.pop("query_model_id")
                 )
             else:
                 parsed_params["strategy"] = ExactRetrievalStrategy()
 
         # Set embedding from params
-        parsed_params['embedding'] = parsed_params.pop("embeddings", None)
+        parsed_params["embedding"] = parsed_params.pop("embeddings", None)
 
         return LangchainVectorStoreAdapter(ElasticsearchStore(**parsed_params))
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/inference/base_model_inference.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/inference/base_model_inference.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/inference/deployment_model_inference.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/inference/deployment_model_inference.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/inference/fm_model_inference.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/inference/fm_model_inference.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,19 +1,20 @@
 #  -----------------------------------------------------------------------------------------
 #  (C) Copyright IBM Corp. 2023-2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
 from __future__ import annotations
+from enum import Enum
 from typing import Generator, cast, TYPE_CHECKING
 
 __all__ = ["FMModelInference"]
 
 from ibm_watsonx_ai.wml_client_error import WMLClientError
-from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes, DecodingMethods
+from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods
 from ibm_watsonx_ai.foundation_models.utils.utils import _check_model_state
 from ibm_watsonx_ai.metanames import GenTextParamsMetaNames
 from ibm_watsonx_ai.messages.messages import Messages
 from .base_model_inference import BaseModelInference
 
 if TYPE_CHECKING:
     from ibm_watsonx_ai import APIClient
@@ -27,15 +28,15 @@
         *,
         model_id: str,
         api_client: APIClient,
         params: dict | None = None,
         validate: bool = True,
     ):
         self.model_id = model_id
-        if isinstance(self.model_id, ModelTypes):
+        if isinstance(self.model_id, Enum):
             self.model_id = self.model_id.value
 
         self.params = params
         FMModelInference._validate_type(params, "params", dict, False)
 
         self._client = api_client
         if validate:
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/inference/model_inference.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/inference/model_inference.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,21 +1,22 @@
 #  -----------------------------------------------------------------------------------------
 #  (C) Copyright IBM Corp. 2023-2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 from __future__ import annotations
 
 from typing import TYPE_CHECKING, Generator, cast
+from enum import Enum
 
 from ibm_watsonx_ai.wml_client_error import (
     WMLClientError,
     ParamOutOfRange,
     InvalidMultipleArguments,
+    MissingExtension,
 )
-from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes
 from .base_model_inference import BaseModelInference
 from .fm_model_inference import FMModelInference
 from .deployment_model_inference import DeploymentModelInference
 
 if TYPE_CHECKING:
     from ibm_watsonx_ai import APIClient, Credentials
     from langchain_ibm import WatsonxLLM
@@ -118,15 +119,15 @@
         project_id: str | None = None,
         space_id: str | None = None,
         verify: bool | str | None = None,
         api_client: APIClient | None = None,
         validate: bool = True,
     ) -> None:
         self.model_id = model_id
-        if isinstance(self.model_id, ModelTypes):
+        if isinstance(self.model_id, Enum):
             self.model_id = self.model_id.value
 
         self.deployment_id = deployment_id
 
         if self.model_id and self.deployment_id:
             raise InvalidMultipleArguments(
                 params_names_list=["model_id", "deployment_id"],
@@ -269,15 +270,15 @@
             guardrails_pii_params=guardrails_pii_params,
             concurrency_limit=concurrency_limit,
             async_mode=async_mode,
         )
 
     def generate_text(
         self,
-        prompt: str | None = None,
+        prompt: str | list | None = None,
         params: dict | None = None,
         raw_response: bool = False,
         guardrails: bool = False,
         guardrails_hap_params: dict | None = None,
         guardrails_pii_params: dict | None = None,
         concurrency_limit: int = 10,
     ) -> str | dict:
@@ -490,15 +491,13 @@
             llm_chain = LLMChain(llm=deployed_model.to_langchain(), prompt=PromptTemplate.from_template(prompt_template))
             llm_chain('sunflower')
 
         """
         try:
             from langchain_ibm import WatsonxLLM
         except ImportError:
-            raise ImportError(
-                "Could not import langchain_ibm: Please install `langchain_ibm` extension."
-            )
+            raise MissingExtension("langchain_ibm")
         return WatsonxLLM(watsonx_model=self)
 
     def get_identifying_params(self) -> dict:
         """Represent Model Inference's setup in dictionary"""
         return self._inference.get_identifying_params()
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/model.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/model.py`

 * *Files 2% similar despite different names*

```diff
@@ -3,14 +3,15 @@
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 from __future__ import annotations
 
 from typing import TYPE_CHECKING, Generator
 
 from ibm_watsonx_ai.foundation_models.inference import ModelInference
+from ibm_watsonx_ai.wml_client_error import MissingExtension
 
 if TYPE_CHECKING:
     from langchain_ibm import WatsonxLLM
     from ibm_watsonx_ai import Credentials
 
 
 class Model(ModelInference):
@@ -140,17 +141,16 @@
             llm_chain = LLMChain(llm=flan_ul2_model.to_langchain(), prompt=PromptTemplate.from_template(prompt_template))
             llm_chain('sunflower')
 
         """
         try:
             from langchain_ibm import WatsonxLLM
         except ImportError:
-            raise ImportError(
-                "Could not import langchain_ibm: Please install `langchain_ibm` extension."
-            )
+            raise MissingExtension("langchain_ibm")
+
         return WatsonxLLM(watsonx_model=self)
 
     def generate(
         self,
         prompt: str | list | None = None,
         params: dict | None = None,
         guardrails: bool = False,
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/prompt_tuner.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/prompt_tuner.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/prompts/prompt_template.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/prompts/prompt_template.py`

 * *Files 2% similar despite different names*

```diff
@@ -2,14 +2,15 @@
 #  (C) Copyright IBM Corp. 2023-2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
 from __future__ import annotations
 
 from typing import TYPE_CHECKING, cast, Any
+from enum import Enum
 
 if TYPE_CHECKING:
     import langchain
     from langchain.prompts import PromptTemplate as LcPromptTemplate
 from dataclasses import dataclass
 from datetime import datetime
 
@@ -137,15 +138,15 @@
     ) -> None:
         self.name = name
         self._prompt_id: str | None = None
         self._created_at: int | None = None
         self._lock: PromptTemplateLock | None = None
         self._is_template: bool | None = None
         self.model_id: ModelTypes | str | None = model_id
-        if isinstance(self.model_id, ModelTypes):
+        if isinstance(self.model_id, Enum):
             self.model_id = self.model_id.value
         self.model_params = (
             model_params.copy() if model_params is not None else model_params
         )
         self.task_ids = task_ids.copy() if task_ids is not None else task_ids
         self.template_version = template_version
         self.description = description
@@ -283,14 +284,38 @@
                                          input_variables=['object'],
                                          examples=[['What is the Stock Market?',
                                                     'A stock market is a place where investors buy and sell shares of publicly traded companies.']])
 
         stored_prompt_template = prompt_mgr.store_prompt(prompt_template)
         print(stored_prompt_template.prompt_id)   # id of prompt template asset
 
+    .. note::
+        Here's an example of how you can pass variables to your deployed prompt template.
+
+        .. code-block:: python
+
+            from ibm_watsonx_ai.metanames import GenTextParamsMetaNames
+
+            meta_props = {
+                client.deployments.ConfigurationMetaNames.NAME: "SAMPLE DEPLOYMENT PROMPT TEMPLATE",
+                client.deployments.ConfigurationMetaNames.ONLINE: {},
+                client.deployments.ConfigurationMetaNames.BASE_MODEL_ID: ModelTypes.GRANITE_13B_CHAT_V2
+                }
+
+            deployment_details = client.deployments.create(stored_prompt_template.prompt_id, meta_props)
+
+            client.deployments.generate_text(
+                deployment_id=deployment_details["metadata"]["id"],
+                params={
+                    GenTextParamsMetaNames.PROMPT_VARIABLES: {
+                        "object": "brain"
+                    }
+                }
+            )
+
     """
 
     def __init__(
         self,
         credentials: Credentials | dict | None = None,
         *,
         project_id: str | None = None,
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/utils/__init__.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/utils/__init__.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models/utils/utils.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models/utils/utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,38 +1,62 @@
 #  -----------------------------------------------------------------------------------------
 #  (C) Copyright IBM Corp. 2023-2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 from __future__ import annotations
 
 import warnings
+from enum import Enum
 from string import Formatter
-from typing import TYPE_CHECKING, Any, Sequence, cast, Mapping
+from typing import TYPE_CHECKING, Any, Sequence, cast, Mapping, Generator
 from dataclasses import dataclass, KW_ONLY, asdict
 from json import loads as json_loads
 from warnings import warn
+from pprint import pprint
 
 from ibm_watsonx_ai._wrappers import requests
 from ibm_watsonx_ai.helpers import DataConnection
 from ibm_watsonx_ai.messages.messages import Messages
 from ibm_watsonx_ai.wml_client_error import (
     WMLClientError,
     InvalidMultipleArguments,
     InvalidValue,
 )
 from ibm_watsonx_ai.utils import next_resource_generator
 from ibm_watsonx_ai.utils.autoai.utils import load_file_from_file_system_nonautoai
-from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes
 from ibm_watsonx_ai.utils.autoai.enums import DataConnectionTypes
 from ibm_watsonx_ai.lifecycle import SpecStates
 
 if TYPE_CHECKING:
     from ibm_watsonx_ai import APIClient
 
 
+class StrEnum(str, Enum):
+    """
+    External class created for the needs of auto-generated enums
+
+    UseCase of StrEnum:
+    When we call print function on StrEnum attribute we are getting value ot them instead of Enum object
+
+    Example of StrEnum
+    TestEnum.Enum1 == "enum1" --> True
+
+    Example of Enum
+    TestEnum.Enum1 == "enum1 --> False
+    """
+
+    def __str__(self) -> str:
+        return self.value
+
+    @classmethod
+    def show(cls) -> None:
+        models_dict = {model.name: model.value for model in cls}
+        pprint(models_dict)
+
+
 @dataclass
 class PromptTuningParams:
     base_model: dict
     _: KW_ONLY
     accumulate_steps: int | None = None
     batch_size: int | None = None
     init_method: str | None = None
@@ -106,15 +130,15 @@
     warn(
         "`get_model_specs()` function is deprecated from 1.0, please use `client.foundation_models.get_model_specs()` function instead.",
         category=DeprecationWarning,
     )
 
     try:
         if model_id:
-            if isinstance(model_id, ModelTypes):
+            if isinstance(model_id, Enum):
                 model_id = model_id.value
             try:
                 return [
                     res
                     for res in _get_foundation_models_spec(
                         f"{url}/ml/v1/foundation_model_specs",
                         "Get available foundation models",
@@ -507,17 +531,17 @@
 
 
 def get_all_supported_tasks_dict(
     url: str = "https://us-south.ml.cloud.ibm.com",
 ) -> dict:
     tasks_dict = dict()
     for task_spec in get_supported_tasks(url).get("resources", []):
-        tasks_dict[
-            task_spec["label"].replace("-", "_").replace(" ", "_").upper()
-        ] = task_spec["task_id"]
+        tasks_dict[task_spec["label"].replace("-", "_").replace(" ", "_").upper()] = (
+            task_spec["task_id"]
+        )
     return tasks_dict
 
 
 def load_request_json(
     run_id: str,
     api_client: APIClient,
     run_params: dict[str, Any] | None = None,
@@ -614,28 +638,24 @@
     ) -> None:
         """Check for unused args."""
         extra_args = set(kwargs).difference(used_args)
         if extra_args:
             raise KeyError(extra_args)
 
 
-class HAPDetectionWarning(UserWarning):
-    ...
+class HAPDetectionWarning(UserWarning): ...
 
 
-class PIIDetectionWarning(UserWarning):
-    ...
+class PIIDetectionWarning(UserWarning): ...
 
 
-class LifecycleWarning(UserWarning):
-    ...
+class LifecycleWarning(UserWarning): ...
 
 
-class WatsonxLLMDeprecationWarning(UserWarning):
-    ...
+class WatsonxLLMDeprecationWarning(UserWarning): ...
 
 
 def _raise_watsonxllm_deprecation_warning() -> None:
     warnings.warn(
         "ibm_watsonx_ai.foundation_models.extensions.langchain.WatsonxLLM"
         " is deprecated and will not be supported in the future. "
         "Please import from langchain-ibm instead.\n"
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/foundation_models_manager.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/foundation_models_manager.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,58 +1,73 @@
 #  -----------------------------------------------------------------------------------------
 #  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
 from __future__ import annotations
+from enum import Enum
+from functools import cached_property
 
-from typing import TYPE_CHECKING, Generator
+from typing import TYPE_CHECKING, Generator, Literal
 import warnings
 
 from ibm_watsonx_ai.wml_resource import WMLResource
-from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes
 from ibm_watsonx_ai.messages.messages import Messages
 from ibm_watsonx_ai.wml_client_error import WMLClientError
+from ibm_watsonx_ai.foundation_models.utils.utils import StrEnum
 
 if TYPE_CHECKING:
     from ibm_watsonx_ai import APIClient
 
 
 class FoundationModelsManager(WMLResource):
     def __init__(self, client: APIClient):
         WMLResource.__init__(self, __name__, client)
         self._client = client
 
+    @cached_property
+    def TextModels(self):
+        return StrEnum("TextModels", self._get_model_dict("base"))
+
+    @cached_property
+    def EmbeddingModels(self):
+        return StrEnum("EmbeddingModels", self._get_model_dict("embedding"))
+
+    @cached_property
+    def PromptTunableModels(self):
+        return StrEnum("PromptTunableModels", self._get_model_dict("prompt_tuning"))
+
     def _get_spec(
         self,
         url: str,
         operation_name: str,
         error_msg_id: str,
         model_id: str | None = None,
         limit: int | None = 50,
         filters: str | None = None,
         asynchronous: bool = False,
         get_all: bool = False,
     ) -> dict | None:
-        params = self._client._params(skip_userfs=True)
+        params = self._client._params(skip_userfs=True, skip_space_project_chk=True)
         if filters:
             params.update({"filters": filters})
 
         try:
             if model_id:
                 result = self._get_with_or_without_limit(
                     url,
                     limit=None,
                     op_name=operation_name,
                     query_params=params,
                     _all=True,
                     _async=False,
+                    skip_space_project_chk=True,
                 )
 
-                if isinstance(model_id, ModelTypes):
+                if isinstance(model_id, Enum):
                     model_id = model_id.value
 
                 model_res = [
                     res for res in result["resources"] if res["model_id"] == model_id
                 ]
 
                 if len(model_res) > 0:
@@ -63,14 +78,15 @@
                 return self._get_with_or_without_limit(
                     url=url,
                     limit=limit,
                     op_name=operation_name,
                     query_params=params,
                     _async=asynchronous,
                     _all=get_all,
+                    skip_space_project_chk=True,
                 )
         except WMLClientError as e:
             raise WMLClientError(
                 Messages.get_message(
                     self._client.credentials.url,
                     message_id=error_msg_id,
                 ),
@@ -112,14 +128,15 @@
             # GET MODEL SPECS BY MODEL_ID
             client.foundation_models.get_model_specs(model_id="google/flan-ul2")
         """
         return self._get_spec(
             url=self._client.service_instance._href_definitions.get_fm_specifications_href(),
             operation_name="Get available foundation models",
             error_msg_id="fm_prompt_tuning_no_model_specs",
+            filters="function_text_generation",
             model_id=model_id,
             limit=limit,
             asynchronous=asynchronous,
             get_all=get_all,
         )
 
     def get_custom_model_specs(
@@ -277,7 +294,32 @@
 
             client.foundation_models.get_model_lifecycle(
                 model_id="ibm/granite-13b-instruct-v2"
                 )
         """
         model_spec = self.get_model_specs(model_id)
         return model_spec.get("lifecycle") if model_spec is not None else None
+
+    def _get_model_dict(
+        self, model_type: Literal["base", "embedding", "prompt_tuning"]
+    ) -> dict:
+        """
+        Operation to retrieve the dictionary of models to Enum
+
+        :param model_type: Type of model function
+        :type model_type: Literal["base", "embedding", "prompt_tuning"]
+
+        :return: dict of models to Enum
+        :rtype: dict
+        """
+        function_dict = {
+            "base": self.get_model_specs,
+            "embedding": self.get_embeddings_model_specs,
+            "prompt_tuning": self.get_model_specs_with_prompt_tuning_support,
+        }
+        model_specs_dict = {}
+        for model_spec in function_dict[model_type]()["resources"]:
+            if "model_id" in model_spec:
+                model_specs_dict[
+                    model_spec["model_id"].split("/")[-1].replace("-", "_").upper()
+                ] = model_spec["model_id"]
+        return model_specs_dict
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/functions.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/functions.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/helpers/connections/base_connection.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/helpers/connections/base_connection.py`

 * *Files 4% similar despite different names*

```diff
@@ -2,17 +2,15 @@
 #  (C) Copyright IBM Corp. 2023-2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
 from abc import ABC
 from copy import deepcopy
 
-__all__ = [
-    "BaseConnection"
-]
+__all__ = ["BaseConnection"]
 
 
 class BaseConnection(ABC):
     """Base class for storage Connections."""
 
     def to_dict(self) -> dict:
         """Get a json dictionary representing this model."""
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/helpers/connections/base_data_connection.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/helpers/connections/base_data_connection.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/helpers/connections/base_location.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/helpers/connections/base_location.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,20 +1,19 @@
 #  -----------------------------------------------------------------------------------------
 #  (C) Copyright IBM Corp. 2023-2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
 from abc import ABC
 
-__all__ = [
-    "BaseLocation"
-]
+__all__ = ["BaseLocation"]
 
 
 class BaseLocation(ABC):
     """Base class for storage Location."""
+
     def to_dict(self) -> dict:
         """Get a json dictionary representing this model."""
         return vars(self)
 
-    def _get_file_size(self, **kwargs) -> 'int':
+    def _get_file_size(self, **kwargs) -> "int":
         raise NotImplementedError
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/helpers/connections/connections.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/helpers/connections/connections.py`

 * *Files 0% similar despite different names*

```diff
@@ -1749,15 +1749,14 @@
                 os.stat(path=self.path).st_size if os.path.isfile(path=self.path) else 0
             )
             # -- end note
         return size
 
 
 class AssetLocation(BaseLocation):
-
     def __init__(self, asset_id: str) -> None:
         self.href = None
         self._initial_asset_id = asset_id
         self.__api_client = None
 
         self.id = asset_id
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/helpers/connections/flight_service.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/helpers/connections/flight_service.py`

 * *Files 9% similar despite different names*

```diff
@@ -16,18 +16,26 @@
 from contextlib import nullcontext
 from functools import partial
 from typing import List, Optional, Iterable, Generator
 
 from ibm_watsonx_ai.utils.autoai.errors import InvalidSamplingType, CorruptedData
 from ibm_watsonx_ai.utils.autoai.enums import PredictionType, SamplingTypes
 from ibm_watsonx_ai.wml_client_error import (
-    DataStreamError, WrongLocationProperty, WrongFileLocation, SpaceIDandProjectIDCannotBeNone, EmptyDataSource)
-from ibm_watsonx_ai.utils.utils import is_lib_installed, prepare_interaction_props_for_cos
+    DataStreamError,
+    WrongLocationProperty,
+    WrongFileLocation,
+    SpaceIDandProjectIDCannotBeNone,
+    EmptyDataSource,
+)
+from ibm_watsonx_ai.utils.utils import (
+    is_lib_installed,
+    prepare_interaction_props_for_cos,
+)
 
-is_lib_installed(lib_name='pyarrow', install=True)
+is_lib_installed(lib_name="pyarrow", install=True)
 import pyarrow as pa
 from pyarrow import flight
 from pyarrow.lib import ArrowException
 from warnings import warn
 from math import ceil
 
 
@@ -116,87 +124,104 @@
 
     :param total_percentage_limit: upper limit for overall data that should be downloaded in percent of all dataset,
         must be a float number between 0 and 1, if more than one of: `total_size_limit`, `total_nrows_limit`,
         `total_percentage_limit` are set, then data are limited to the lower threshold
     :type total_percentage_limit: float, optional
     """
 
-    def __init__(self,
-                 headers: dict,
-                 sampling_type: str,
-                 label: str,
-                 learning_type: str,
-                 params: dict,
-                 project_id: Optional[str] = None,
-                 space_id: Optional[str] = None,
-                 asset_id: Optional[str] = None,
-                 connection_id: Optional[str] = None,
-                 data_location: Optional[dict] = None,
-                 enable_subsampling: Optional[bool] = False,
-                 callback: Optional['Callback'] = None,
-                 data_batch_size_limit: Optional[int] = 1073741824,  # 1GB in Bytes 
-                 logical_batch_size_limit: Optional[int] = None,
-                 flight_parameters: dict = None,
-                 extra_interaction_properties: dict = None,
-                 fallback_to_one_connection: Optional[bool] = True,
-                 number_of_batch_rows: int = None,
-                 stop_after_first_batch: bool = False,
-                 return_subsampling_stats: bool = False,
-                 total_size_limit=1073741824,  # 1GB in Bytes
-                 total_nrows_limit=None,
-                 total_percentage_limit=1.0,
-                 **kwargs
-                 ) -> None:
+    def __init__(
+        self,
+        headers: dict,
+        sampling_type: str,
+        label: str,
+        learning_type: str,
+        params: dict,
+        project_id: Optional[str] = None,
+        space_id: Optional[str] = None,
+        asset_id: Optional[str] = None,
+        connection_id: Optional[str] = None,
+        data_location: Optional[dict] = None,
+        enable_subsampling: Optional[bool] = False,
+        callback: Optional["Callback"] = None,
+        data_batch_size_limit: Optional[int] = 1073741824,  # 1GB in Bytes
+        logical_batch_size_limit: Optional[int] = None,
+        flight_parameters: dict = None,
+        extra_interaction_properties: dict = None,
+        fallback_to_one_connection: Optional[bool] = True,
+        number_of_batch_rows: int = None,
+        stop_after_first_batch: bool = False,
+        return_subsampling_stats: bool = False,
+        total_size_limit=1073741824,  # 1GB in Bytes
+        total_nrows_limit=None,
+        total_percentage_limit=1.0,
+        **kwargs,
+    ) -> None:
 
         if project_id is None and space_id is None:
             raise SpaceIDandProjectIDCannotBeNone(
-                reason="'space_id' and 'project_id' are None. Please set one of them.")
+                reason="'space_id' and 'project_id' are None. Please set one of them."
+            )
 
         self.headers = headers  # Service authorization headers
 
         self.number_of_batch_rows = number_of_batch_rows
         self.stop_after_first_batch = stop_after_first_batch
 
-        # backward compatibility: 
-        if kwargs.get('data_size_limit'):
-            warn("The parameters data_size_limit in FlightConnection is deprecated. Use total_size_limit instead.")
-            self.total_size_limit = kwargs.get('data_size_limit')
+        # backward compatibility:
+        if kwargs.get("data_size_limit"):
+            warn(
+                "The parameters data_size_limit in FlightConnection is deprecated. Use total_size_limit instead."
+            )
+            self.total_size_limit = kwargs.get("data_size_limit")
 
         # Note: Upper bound limitation for data in memory
-        self.data_batch_size_limit = data_batch_size_limit  # size of normal or subsampled batch in RAM (Bytes)
+        self.data_batch_size_limit = (
+            data_batch_size_limit  # size of normal or subsampled batch in RAM (Bytes)
+        )
 
         self.total_size_limit = total_size_limit
         self.total_nrows_limit = total_nrows_limit
         self.yielded_nrows = 0
         if self.total_size_limit:
-            self.data_batch_size_limit = min(self.total_size_limit, self.data_batch_size_limit)
+            self.data_batch_size_limit = min(
+                self.total_size_limit, self.data_batch_size_limit
+            )
 
         if self.total_nrows_limit:
             if self.number_of_batch_rows:
-                self.number_of_batch_rows = min(self.total_nrows_limit, self.number_of_batch_rows)
-            elif self.total_nrows_limit < DEFAULT_BATCH_SIZE_FLIGHT_COMMAND:  # 10000 is default_batch_size in Flight Service command
+                self.number_of_batch_rows = min(
+                    self.total_nrows_limit, self.number_of_batch_rows
+                )
+            elif (
+                self.total_nrows_limit < DEFAULT_BATCH_SIZE_FLIGHT_COMMAND
+            ):  # 10000 is default_batch_size in Flight Service command
                 self.number_of_batch_rows = self.total_nrows_limit
 
-        if not isinstance(total_percentage_limit, float) or \
-                (isinstance(total_percentage_limit, float) and (
-                        total_percentage_limit <= 0.0 or total_percentage_limit > 1.0)):
-            raise ValueError("Invalid `total_percentage_limit` parameter's value. "
-                             "The `total_percentage_limit` need to be float between 0.0 and 1.0.")
+        if not isinstance(total_percentage_limit, float) or (
+            isinstance(total_percentage_limit, float)
+            and (total_percentage_limit <= 0.0 or total_percentage_limit > 1.0)
+        ):
+            raise ValueError(
+                "Invalid `total_percentage_limit` parameter's value. "
+                "The `total_percentage_limit` need to be float between 0.0 and 1.0."
+            )
         else:
             self.total_percentage_limit = total_percentage_limit
 
         # --- end note
 
         # callback is used in the backend to send status messages
         self.callback = callback if callback is not None else FakeCallback()
 
         # Handle logical_batch_size_limit parameter set up
         if logical_batch_size_limit is None:
             # Set the logical batch size regarding sample size limit on given hardware (tshirt size)
-            self.logical_batch_size_limit = 2 * self.data_batch_size_limit  # two times the size of larger not subsampled batch in RAM (Bytes)
+            self.logical_batch_size_limit = (
+                2 * self.data_batch_size_limit
+            )  # two times the size of larger not subsampled batch in RAM (Bytes)
         else:
             self.logical_batch_size_limit = logical_batch_size_limit
 
         # Note: Variables from AutoAI training
         self.sampling_type = sampling_type
         self.label = label
         self.learning_type = learning_type
@@ -210,96 +235,120 @@
 
         self.data_location = data_location
 
         # Note: control and store variables of flight reading mechanism
         self.lock_read = threading.Lock()
         self.stop_reading = False
         self.row_size = 0
-        self.threads_exceptions: List['str'] = []
+        self.threads_exceptions: List["str"] = []
         self.q = queue.Queue()
         # a threading.Condition() to notify of q or
         # stop_reading changes
         self.read_status_change = threading.Condition()
 
-        self.subsampled_data: 'pd.DataFrame' = pd.DataFrame()
-        self.data: 'pd.DataFrame' = pd.DataFrame()
+        self.subsampled_data: "pd.DataFrame" = pd.DataFrame()
+        self.data: "pd.DataFrame" = pd.DataFrame()
         self.enable_subsampling = enable_subsampling
         self.return_subsampling_stats = return_subsampling_stats
-        self.total_size = 0  # total size of downloaded data in Bytes (only in single thread)
-        self.downloaded_data_size = 0  # total size of downloaded data in Bytes (every case)
-        self.downloaded_data_nrows = 0  # total number  of downloaded data in rows (every case)
+        self.total_size = (
+            0  # total size of downloaded data in Bytes (only in single thread)
+        )
+        self.downloaded_data_size = (
+            0  # total size of downloaded data in Bytes (every case)
+        )
+        self.downloaded_data_nrows = (
+            0  # total number  of downloaded data in rows (every case)
+        )
 
         self.batch_queue = []
 
-        self.flight_parameters = flight_parameters if flight_parameters is not None else {}
-        self._api_client = (kwargs.get('_api_client') or kwargs.get('_wml_client'))
+        self.flight_parameters = (
+            flight_parameters if flight_parameters is not None else {}
+        )
+        self._api_client = kwargs.get("_api_client") or kwargs.get("_wml_client")
 
         # user can define how many parallel connections initiate to database
-        self.max_flight_batch_number = self.params.get('n_parallel_data_connections', DEFAULT_PARTITIONS_NUM)
-        if 'num_partitions' in self.flight_parameters:
-            self.max_flight_batch_number = self.flight_parameters['num_partitions']
+        self.max_flight_batch_number = self.params.get(
+            "n_parallel_data_connections", DEFAULT_PARTITIONS_NUM
+        )
+        if "num_partitions" in self.flight_parameters:
+            self.max_flight_batch_number = self.flight_parameters["num_partitions"]
         # --- end note
 
         self.fallback_to_one_connection = fallback_to_one_connection
 
         self.read_binary = False
         self.write_binary = False
 
-        self._infer_as_varchar = 'false'  # by default set infer_as_varchar to false in flight command. If None - the infer_as_varchar parameter won't be send.
+        self._infer_as_varchar = "false"  # by default set infer_as_varchar to false in flight command. If None - the infer_as_varchar parameter won't be send.
 
         additional_connection_args = {}
-        if os.environ.get('TLS_ROOT_CERTS_PATH'):
-            additional_connection_args['tls_root_certs'] = os.environ.get('TLS_ROOT_CERTS_PATH')
+        if os.environ.get("TLS_ROOT_CERTS_PATH"):
+            additional_connection_args["tls_root_certs"] = os.environ.get(
+                "TLS_ROOT_CERTS_PATH"
+            )
 
         self.extra_interaction_properties = extra_interaction_properties
 
         self.flight_location = None
         self.flight_port = None
 
         self._set_default_flight_location()
 
         self.flight_client = flight.FlightClient(
             location=f"grpc+tls://{self.flight_location}:{self.flight_port}",
             disable_server_verification=True,
             override_hostname=self.flight_location,
-            **additional_connection_args
+            **additional_connection_args,
         )
 
         self.empty_data_threads = set()
         # note: client as property and setter for dynamic href creation for AssetLocation
 
     @property
     def _wml_client(self):
         # note: backward compatibility
-        warn(("`_wml_client` is deprecated and will be removed in future. "
-                "Instead, please use `_api_client`."), category=DeprecationWarning)
+        warn(
+            (
+                "`_wml_client` is deprecated and will be removed in future. "
+                "Instead, please use `_api_client`."
+            ),
+            category=DeprecationWarning,
+        )
         # --- end note
         return self._api_client
-    
+
     @_wml_client.setter
     def _wml_client(self, var):
         # note: backward compatibility
-        warn(("`_wml_client` is deprecated and will be removed in future. "
-                "Instead, please use `_api_client`."), category=DeprecationWarning)
+        warn(
+            (
+                "`_wml_client` is deprecated and will be removed in future. "
+                "Instead, please use `_api_client`."
+            ),
+            category=DeprecationWarning,
+        )
         # --- end note
         self._api_client = var
-    
+
     @property
     def infer_as_varchar(self):
         return self._infer_as_varchar
 
     @infer_as_varchar.setter
     def infer_as_varchar(self, var):
         if var is None:
             self._infer_as_varchar = var
-        elif var in ('true', True, 'false', False):
+        elif var in ("true", True, "false", False):
             self._infer_as_varchar = str(var).lower()
         else:
-            raise ValueError("FlightConnection.infer_as_varchar property received invalid value."
-                             "A valid value is one of (None, 'true', 'false')")
+            raise ValueError(
+                "FlightConnection.infer_as_varchar property received invalid value."
+                "A valid value is one of (None, 'true', 'false')"
+            )
 
     def _q_put_nowait(self, item):
         # we are not interested in q size increase, so no Condition waiting
         self.q.put_nowait(item)
 
     def _q_get(self, **kwargs):
         # return item in q, and notify interest threads that q is changing
@@ -322,95 +371,132 @@
         # to notify waiting threads
         with self.read_status_change:
             self.stop_reading = value
             self.read_status_change.notify_all()
 
     def _set_default_flight_location(self) -> None:
         """Try to set default flight location and port from WS."""
-        if not os.environ.get(
-                'FLIGHT_SERVICE_LOCATION') and self._api_client and self._api_client.CLOUD_PLATFORM_SPACES:
+        if (
+            not os.environ.get("FLIGHT_SERVICE_LOCATION")
+            and self._api_client
+            and self._api_client.CLOUD_PLATFORM_SPACES
+        ):
             try:
-                flight_location = self._api_client.PLATFORM_URLS_MAP[self._api_client.credentials.url].replace(
-                    'https://', '')
+                flight_location = self._api_client.PLATFORM_URLS_MAP[
+                    self._api_client.credentials.url
+                ].replace("https://", "")
             except Exception as e:
-                if self._api_client.credentials.url in self._api_client.PLATFORM_URLS_MAP.values():
-                    flight_location = self._api_client.credentials.url.replace('https://', '')
+                if (
+                    self._api_client.credentials.url
+                    in self._api_client.PLATFORM_URLS_MAP.values()
+                ):
+                    flight_location = self._api_client.credentials.url.replace(
+                        "https://", ""
+                    )
                 else:
                     raise e
             flight_port = 443
         else:
-            host = os.environ.get('ASSET_API_SERVICE_HOST', os.environ.get('CATALOG_API_SERVICE_HOST'))
+            host = os.environ.get(
+                "ASSET_API_SERVICE_HOST", os.environ.get("CATALOG_API_SERVICE_HOST")
+            )
+
+            if host is None or "api." not in host:
+                default_service_url = os.environ.get(
+                    "RUNTIME_FLIGHT_SERVICE_URL", "grpc+tls://wdp-connect-flight:443"
+                )
+                default_service_url = default_service_url.split("//")[-1]
+                flight_location = os.environ.get("FLIGHT_SERVICE_LOCATION")
+                flight_port = os.environ.get("FLIGHT_SERVICE_PORT")
 
-            if host is None or 'api.' not in host:
-                default_service_url = os.environ.get('RUNTIME_FLIGHT_SERVICE_URL', 'grpc+tls://wdp-connect-flight:443')
-                default_service_url = default_service_url.split('//')[-1]
-                flight_location = os.environ.get('FLIGHT_SERVICE_LOCATION')
-                flight_port = os.environ.get('FLIGHT_SERVICE_PORT')
+                if flight_location is None or flight_location == "":
+                    flight_location = default_service_url.split(":")[0]
 
-                if flight_location is None or flight_location == '':
-                    flight_location = default_service_url.split(':')[0]
-
-                if flight_port is None or flight_port == '':
-                    flight_port = default_service_url.split(':')[-1]
+                if flight_port is None or flight_port == "":
+                    flight_port = default_service_url.split(":")[-1]
 
             else:
                 flight_location = host
-                flight_port = '443'
+                flight_port = "443"
 
         self.flight_location = flight_location
         self.flight_port = flight_port
 
         logger.debug(f"Flight location: {self.flight_location}")
         logger.debug(f"Flight port: {self.flight_port}")
 
-    def authenticate(self) -> 'flight.ClientAuthHandler':
+    def authenticate(self) -> "flight.ClientAuthHandler":
         """Create an authenticator object for Flight Service."""
 
         class TokenClientAuthHandler(flight.ClientAuthHandler):
             """Authenticator implementation from pyarrow flight."""
 
             def __init__(self, token, _type: str, impersonate: bool = False):
                 super().__init__()
                 if impersonate:
-                    self.token = bytes(f'{token}', 'utf-8')
+                    self.token = bytes(f"{token}", "utf-8")
                 else:
-                    self.token = bytes(f'{_type} ' + token, 'utf-8')
+                    self.token = bytes(f"{_type} " + token, "utf-8")
 
             def authenticate(self, outgoing, incoming):
                 outgoing.write(self.token)
                 self.token = incoming.read()
 
             def get_token(self):
                 logger.debug(f"Flight service get_token() {self.token}")
                 return self.token
 
-        if 'Bearer' in self.headers.get('Authorization', ''):
+        if "Bearer" in self.headers.get("Authorization", ""):
             if "impersonate" in self.headers:
-                authorization_header = self.headers.get('Authorization', 'Bearer  ')
-                impersonate_header = self.headers.get('impersonate')
-                auth_json_str = json.dumps(dict(authorization=authorization_header, impersonate=impersonate_header))
-                return TokenClientAuthHandler(token=auth_json_str, _type='json_string', impersonate=True)
+                authorization_header = self.headers.get("Authorization", "Bearer  ")
+                impersonate_header = self.headers.get("impersonate")
+                auth_json_str = json.dumps(
+                    dict(
+                        authorization=authorization_header,
+                        impersonate=impersonate_header,
+                    )
+                )
+                return TokenClientAuthHandler(
+                    token=auth_json_str, _type="json_string", impersonate=True
+                )
             else:
-                return TokenClientAuthHandler(token=self.headers.get('Authorization', 'Bearer  ').split('Bearer ')[-1],
-                                              _type='Bearer')
+                return TokenClientAuthHandler(
+                    token=self.headers.get("Authorization", "Bearer  ").split(
+                        "Bearer "
+                    )[-1],
+                    _type="Bearer",
+                )
 
-        elif 'Basic' in self.headers.get('Authorization', ''):
+        elif "Basic" in self.headers.get("Authorization", ""):
             if "impersonate" in self.headers:
-                authorization_header = self.headers.get('Authorization', 'Basic  ')
-                impersonate_header = self.headers.get('impersonate')
-                auth_json_str = json.dumps(dict(authorization=authorization_header, impersonate=impersonate_header))
-                return TokenClientAuthHandler(token=auth_json_str, _type='json_string', impersonate=True)
+                authorization_header = self.headers.get("Authorization", "Basic  ")
+                impersonate_header = self.headers.get("impersonate")
+                auth_json_str = json.dumps(
+                    dict(
+                        authorization=authorization_header,
+                        impersonate=impersonate_header,
+                    )
+                )
+                return TokenClientAuthHandler(
+                    token=auth_json_str, _type="json_string", impersonate=True
+                )
             else:
-                return TokenClientAuthHandler(token=self.headers.get('Authorization', 'Basic  ').split('Basic ')[-1],
-                                              _type='Basic')
+                return TokenClientAuthHandler(
+                    token=self.headers.get("Authorization", "Basic  ").split("Basic ")[
+                        -1
+                    ],
+                    _type="Basic",
+                )
 
         else:
-            return TokenClientAuthHandler(token=self.headers.get('Authorization'), _type='Bearer')
+            return TokenClientAuthHandler(
+                token=self.headers.get("Authorization"), _type="Bearer"
+            )
 
-    def get_endpoints(self) -> Iterable[List['flight.FlightEndpoint']]:
+    def get_endpoints(self) -> Iterable[List["flight.FlightEndpoint"]]:
         """Listing all available Flight Service endpoints (one endpoint corresponds to one batch)"""
 
         max_auth_waiting_time = 180
         real_waiting_time = 0
         count_auth_retries_power = 4
         retry_authentication = True
 
@@ -421,34 +507,39 @@
             except Exception as e:
                 if "failed to connect to all addresses" in str(e):
                     retry_authentication = True  # wait up to 180 seconds - Flight Service can be restarting issue #30564
                 else:
                     retry_authentication = False
 
                 if retry_authentication and real_waiting_time < max_auth_waiting_time:
-                    logger.debug(f"Cannot connect to Flight Service in {real_waiting_time}s,"
-                                 f" attempting to retry the authentication. ")
+                    logger.debug(
+                        f"Cannot connect to Flight Service in {real_waiting_time}s,"
+                        f" attempting to retry the authentication. "
+                    )
                     waiting_time = 2**count_auth_retries_power
                     real_waiting_time += waiting_time
                     time.sleep(waiting_time)
                     count_auth_retries_power += 1
                 else:
                     # suggest CPD users to check the Flight variables
-                    if hasattr(self._api_client, 'ICP_PLATFORM_SPACES'):
+                    if hasattr(self._api_client, "ICP_PLATFORM_SPACES"):
                         if self._api_client.ICP_PLATFORM_SPACES:
                             raise ConnectionError(
                                 f"Cannot connect to the Flight service. Please make sure you set correct "
                                 f"FLIGHT_SERVICE_LOCATION and FLIGHT_SERVICE_PORT environmental variables.\n"
                                 f"If you are trying to connect to FIPS-enabled cluster, "
                                 f"please set the following as environment variable and try again:\n"
                                 f"GRPC_SSL_CIPHER_SUITES="
                                 f"ECDHE-RSA-AES128-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384 "
-                                f"Error: {e}")
+                                f"Error: {e}"
+                            )
                     else:
-                        raise ConnectionError(f"Cannot connect to the Flight service. Error: {e}")
+                        raise ConnectionError(
+                            f"Cannot connect to the Flight service. Error: {e}"
+                        )
 
         count = 0
         retry = True
         while retry:
             try:
                 retry = False
                 count += 1
@@ -457,61 +548,67 @@
                         flight.FlightDescriptor.for_command(source_command)
                     )
 
                     yield info.endpoints
 
             except flight.FlightInternalError as e:
                 logger.debug(f"Caught FlightInternalError in get_endpoints: {str(e)}")
-                if 'CDICO2034E' in str(e):
-                    if 'The property [infer_as_varchar] is not supported.' in str(e):
+                if "CDICO2034E" in str(e):
+                    if "The property [infer_as_varchar] is not supported." in str(e):
                         # Don't send infer_as_varchar in flight command and try again.
                         self.infer_as_varchar = None
                         retry = True
-                    elif 'The property [quote_character]' in str(e):
+                    elif "The property [quote_character]" in str(e):
                         # Don't send quote_character in flight command if it is not yet supported and try again.
-                        self.params['quote_character'] = None
+                        self.params["quote_character"] = None
                         retry = True
                     else:
                         raise WrongLocationProperty(reason=str(e))
 
                     if count > 1:
                         logger.debug(f"Reached the max retry times ${count}")
                         raise e
-                elif any(err_code in str(e)
-                         for err_code in ['CDICO2016E', 'CDICO2026E', 'CDICO2027E']):
+                elif any(
+                    err_code in str(e)
+                    for err_code in ["CDICO2016E", "CDICO2026E", "CDICO2027E"]
+                ):
                     retry = True
                     if count < 3:
-                        logger.debug(f"Retry to get the endpoints of flight service at the ${count} time")
+                        logger.debug(
+                            f"Retry to get the endpoints of flight service at the ${count} time"
+                        )
                         time.sleep(3)
                     else:
                         logger.debug(f"Reached the max retry times ${count}")
                         raise e
-                elif 'CDICO2015E' in str(e):
+                elif "CDICO2015E" in str(e):
                     raise WrongFileLocation(reason=str(e))
-                elif 'CDICO9999E' in str(e):
+                elif "CDICO9999E" in str(e):
                     raise WrongLocationProperty(reason=str(e))
                 else:
                     raise e
             except pa.lib.ArrowInvalid as e:
                 logger.debug(f"Caught ArrowInvalid in get_endpoints: {str(e)}")
-                if any(err_msg in str(e)
-                         for err_msg in ['No asset found with the id', 'Asset with ID']):
+                if any(
+                    err_msg in str(e)
+                    for err_msg in ["No asset found with the id", "Asset with ID"]
+                ):
                     retry = True
                     if count < 3:
-                        logger.debug(f"Retry to get the endpoints of flight service at the ${count} time")
+                        logger.debug(
+                            f"Retry to get the endpoints of flight service at the ${count} time"
+                        )
                         time.sleep(3)
                     else:
                         logger.debug(f"Reached the max retry times ${count}")
                         raise e
                 else:
                     raise e
 
-    def _get_data(self,
-                  thread_number: int,
-                  endpoint: 'flight.FlightEndpoint') -> None:
+    def _get_data(self, thread_number: int, endpoint: "flight.FlightEndpoint") -> None:
         """Read data from Flight Service (only one batch).
 
         :param thread_number: specific number of the downloading thread
         :type thread_number: int
 
         :param endpoint: flight endpoint
         :type endpoint: flight.FlightEndpoint
@@ -521,38 +618,46 @@
         """
         try:
             reader = self.flight_client.do_get(endpoint.ticket)
             mb_counter = 0
 
             while True:
                 mb_counter += 1
-                data, row_size = self._read_chunk(thread_number=thread_number, reader=reader, mb_counter=mb_counter)
+                data, row_size = self._read_chunk(
+                    thread_number=thread_number, reader=reader, mb_counter=mb_counter
+                )
 
                 if row_size != 0:  # append batches only when we have data
                     if not self.stop_reading:
                         self._chunk_queue_check(data=data, thread_number=thread_number)
 
                         if self.enable_subsampling:
                             # put all data into further subsampling
-                            logger.debug(f"GD {thread_number}: putting mini batch to the queue...")
+                            logger.debug(
+                                f"GD {thread_number}: putting mini batch to the queue..."
+                            )
                             self._q_put_nowait((thread_number, data))
                             logger.debug(f"GD {thread_number}: mini batch already put.")
 
                         else:
                             with self.lock_read:
                                 self.total_size = self.total_size + row_size * len(data)
 
                                 # note: what to do when we have total size nearly under the limit
                                 if self.total_size <= self.data_batch_size_limit:
-                                    upper_row_limit = (self.data_batch_size_limit - self.total_size) // row_size
+                                    upper_row_limit = (
+                                        self.data_batch_size_limit - self.total_size
+                                    ) // row_size
                                     data = data.iloc[:upper_row_limit]
                                     self._q_put_nowait((thread_number, data))
                                 # --- end note
                                 else:
-                                    self._q_put_nowait((thread_number, 0))  # finish this thread
+                                    self._q_put_nowait(
+                                        (thread_number, 0)
+                                    )  # finish this thread
                                     self._set_stop_reading(True)
 
                     else:
                         break
 
             logger.debug(f"GD {thread_number}: Finishing thread work...")
             self._q_put_nowait((thread_number, 0))  # finish this thread
@@ -564,34 +669,38 @@
 
         except Exception as e:
             logger.debug(f"GD {thread_number}: Some error occurred. Error: {e}")
             self._q_put_nowait((thread_number, 0))
             self.threads_exceptions.append(str(e))
 
     @staticmethod
-    def _cast_columns_to_float64_and_bool(data: 'pd.DataFrame') -> 'pd.DataFrame':
+    def _cast_columns_to_float64_and_bool(data: "pd.DataFrame") -> "pd.DataFrame":
         def check_bool_value(value: str):
             if value.lower() == "true":
                 return True
             elif value.lower() == "false":
                 return False
             else:
                 return value
 
         """Flight Service will cast decfloat types to strings, we need to cast them to correct types."""
         for i, (col_name, col_type) in enumerate(zip(data.dtypes.index, data.dtypes)):
-            if col_type == 'object':
+            if col_type == "object":
                 try:
-                    data[col_name] = data[col_name].astype('float64')
-                except ValueError:  # ignore when column cannot be cast to other type than string
+                    data[col_name] = data[col_name].astype("float64")
+                except (
+                    ValueError
+                ):  # ignore when column cannot be cast to other type than string
                     # logger.debug(f"Column '{col_name}' cannot be cast to float64 as it has normal strings inside")
                     try:
                         data[col_name] = data[col_name].apply(check_bool_value)
                     except ValueError:  # ignore when column cannot be cast to bool
-                        logger.debug(f"Column '{col_name}' cannot be cast to bool as it has normal strings inside")
+                        logger.debug(
+                            f"Column '{col_name}' cannot be cast to bool as it has normal strings inside"
+                        )
                     except Exception as e:
                         logger.debug(f"Casting data column '{col_name}' error: {e}")
                 except Exception as e:
                     logger.debug(f"Casting data column '{col_name}' error: {e}")
 
         return data
 
@@ -599,21 +708,27 @@
         """Process data columns and log data size"""
         logger.debug(f"Process data: Casting string data to numerical columns")
         data = self._cast_columns_to_float64_and_bool(data)
 
         logger.debug(f"BATCH SIZE: {sys.getsizeof(data)} Bytes")
         return data
 
-    def _read_chunk(self, thread_number: int, reader: 'flight.FlightStreamReader', mb_counter: int):
+    def _read_chunk(
+        self, thread_number: int, reader: "flight.FlightStreamReader", mb_counter: int
+    ):
         """Provides unified reading method for flight chunks."""
-        logger.debug(f"RC {thread_number}: Waiting for next mini batch from Flight Service...")
+        logger.debug(
+            f"RC {thread_number}: Waiting for next mini batch from Flight Service..."
+        )
         try:
             # Flight Service could split one batch into several chunks to have better performance
             mini_batch, metadata = reader.read_chunk()
-            logger.debug(f"RC {thread_number}: Mini batch received from Flight Service.")
+            logger.debug(
+                f"RC {thread_number}: Mini batch received from Flight Service."
+            )
             data = pa.Table.from_batches(batches=[mini_batch]).to_pandas()
         except StopIteration as stop_iteration_error:
             if self.row_size == 0:
                 self.empty_data_threads.add(thread_number)
             raise stop_iteration_error
 
         if len(data) == 1:
@@ -626,60 +741,70 @@
         else:  # len(data) == 0
             row_size = 0
 
         if self.row_size == 0:
             self.row_size = row_size
 
         mini_batch_size = sys.getsizeof(data)
-        logger.debug(f"RC {thread_number}: downloading mini_batch: {mb_counter} / shape: {data.shape} "
-                     f"/ size: {mini_batch_size}")
+        logger.debug(
+            f"RC {thread_number}: downloading mini_batch: {mb_counter} / shape: {data.shape} "
+            f"/ size: {mini_batch_size}"
+        )
 
         with self.lock_read:
             self.downloaded_data_size += mini_batch_size
             self.downloaded_data_nrows += len(data)
 
         if mb_counter % 10 == 0:
-            logger.debug(f"RC {thread_number}: Total downloaded data size: "
-                         f"{self.downloaded_data_size / 1024 / 1024}MB, rows: {self.downloaded_data_nrows}")
+            logger.debug(
+                f"RC {thread_number}: Total downloaded data size: "
+                f"{self.downloaded_data_size / 1024 / 1024}MB, rows: {self.downloaded_data_nrows}"
+            )
 
         return data, row_size
 
-    def _chunk_queue_check(self, data: 'pd.DataFrame', thread_number: int):
+    def _chunk_queue_check(self, data: "pd.DataFrame", thread_number: int):
         # note: check if queue is not too large, if it is, wait 3 sec and check again
         while True:
-            q_valid_size = self.q.qsize()-len(self.empty_data_threads)
+            q_valid_size = self.q.qsize() - len(self.empty_data_threads)
             if self.number_of_batch_rows is not None:
                 data_rows = len(data) * q_valid_size
                 if data_rows > self.number_of_batch_rows and not self.stop_reading:
-                    logger.debug(f"QC {thread_number}: Waiting 3 sec as data queue is too large.")
+                    logger.debug(
+                        f"QC {thread_number}: Waiting 3 sec as data queue is too large."
+                    )
                     logger.debug(
                         f"QC {thread_number}: Data queue size: {data_rows} rows, max rows per batch: "
-                        f"{self.number_of_batch_rows}")
+                        f"{self.number_of_batch_rows}"
+                    )
                     with self.read_status_change:
                         self.read_status_change.wait(timeout=3)
                     continue
                 else:
                     break
             else:
                 data_size = sys.getsizeof(data) * q_valid_size
                 if data_size >= self.data_batch_size_limit and not self.stop_reading:
-                    logger.debug(f"QC {thread_number}: Waiting 3 sec as data queue is too large.")
+                    logger.debug(
+                        f"QC {thread_number}: Waiting 3 sec as data queue is too large."
+                    )
                     logger.debug(
                         f"QC {thread_number}: Data queue size: {data_size / 1024 / 1024}MB, available memory: "
-                        f"{self.data_batch_size_limit / 1024 / 1024}MB")
+                        f"{self.data_batch_size_limit / 1024 / 1024}MB"
+                    )
                     with self.read_status_change:
                         self.read_status_change.wait(timeout=3)
                     continue
                 else:
                     break
         # --- end note
 
-    def _get_all_data_in_batches(self,
-                                 thread_number: int,
-                                 endpoint: 'flight.FlightEndpoint') -> None:
+    def _get_all_data_in_batches(
+        self, thread_number: int, endpoint: "flight.FlightEndpoint"
+    ) -> None:
         """Read data from Flight Service.
 
         :param thread_number: specific number of the downloading thread
         :type thread_number: int
 
         :param endpoint: flight endpoint
         :type endpoint: flight.FlightEndpoint
@@ -689,19 +814,25 @@
         """
         try:
             reader = self.flight_client.do_get(endpoint.ticket)
             mb_counter = 0
 
             while True:
                 mb_counter += 1
-                data, row_size = self._read_chunk(thread_number=thread_number, reader=reader, mb_counter=mb_counter)
-
-                if row_size != 0 and not self.stop_reading:  # append batches only when we have data
+                data, row_size = self._read_chunk(
+                    thread_number=thread_number, reader=reader, mb_counter=mb_counter
+                )
+
+                if (
+                    row_size != 0 and not self.stop_reading
+                ):  # append batches only when we have data
                     self._chunk_queue_check(data=data, thread_number=thread_number)
-                    logger.debug(f"RT {thread_number}: putting mini batch to the queue...")
+                    logger.debug(
+                        f"RT {thread_number}: putting mini batch to the queue..."
+                    )
                     self._q_put_nowait((thread_number, data))
                     logger.debug(f"RT {thread_number}: mini batch already put.")
 
                 else:
                     break
 
             logger.debug(f"RT {thread_number}: Finishing thread work...")
@@ -745,108 +876,140 @@
         self.callback.status_message("Starting reading training data in batches...")
         if self.enable_subsampling:
             for data in self.read():
                 self.yielded_nrows += len(data)
                 if self.threads_exceptions and data.empty:
                     raise DataStreamError(reason=str(self.threads_exceptions))
                 if self.return_subsampling_stats:
-                    data_stats = {"data_size": self.downloaded_data_size,
-                                  "data_nrows": self.downloaded_data_nrows,
-                                  "data_batch_size": sys.getsizeof(data),
-                                  "data_batch_nrows": len(data),
-                                  "no_batches": ceil(self.downloaded_data_nrows / len(data)) if len(data) > 0 else 0}
+                    data_stats = {
+                        "data_size": self.downloaded_data_size,
+                        "data_nrows": self.downloaded_data_nrows,
+                        "data_batch_size": sys.getsizeof(data),
+                        "data_batch_nrows": len(data),
+                        "no_batches": (
+                            ceil(self.downloaded_data_nrows / len(data))
+                            if len(data) > 0
+                            else 0
+                        ),
+                    }
                     yield data, data_stats
                 else:
                     yield data
                 # rerurn stats here
 
             self.enable_subsampling = False
 
         sequences = []
         for n, endpoints in enumerate(self.get_endpoints()):
-            self.max_flight_batch_number = len(endpoints)  # when flight does not want to open more endpoints
+            self.max_flight_batch_number = len(
+                endpoints
+            )  # when flight does not want to open more endpoints
             threads = []
             sequences.append(threads)
 
             for i, endpoint in enumerate(endpoints):
-                reading_thread = threading.Thread(target=self._get_all_data_in_batches, args=(i, endpoint))
+                reading_thread = threading.Thread(
+                    target=self._get_all_data_in_batches, args=(i, endpoint)
+                )
                 threads.append(reading_thread)
-                logger.debug(f"IR: Starting batch reading thread: {i}, sequence: {n}...")
+                logger.debug(
+                    f"IR: Starting batch reading thread: {i}, sequence: {n}..."
+                )
                 reading_thread.start()
 
-        for n, batch in enumerate(self.create_logical_batch(timeout=60 * 60, _type='data')):
+        for n, batch in enumerate(
+            self.create_logical_batch(timeout=60 * 60, _type="data")
+        ):
             logger.debug(f"IR: Logical batch number {n} received.")
             logger.debug(f"IR: Passing batch to upper layer.")
             self._check_for_breaking_exceptions()
             self.yielded_nrows += len(batch)
             if (n == 0 and self.stop_after_first_batch) or (
-                    self.total_nrows_limit and self.total_nrows_limit - self.yielded_nrows <= 0):
+                self.total_nrows_limit
+                and self.total_nrows_limit - self.yielded_nrows <= 0
+            ):
                 self._set_stop_reading(True)
 
             if len(batch) == 0:
                 break
 
             if self.return_subsampling_stats:
-                yield batch, {"data_batch_size": sys.getsizeof(batch),
-                              "data_batch_nrows": len(batch)}
+                yield batch, {
+                    "data_batch_size": sys.getsizeof(batch),
+                    "data_batch_nrows": len(batch),
+                }
             else:
                 yield batch
 
             logger.debug(f"IR: We have control back, creating next batch...")
             if (n == 0 and self.stop_after_first_batch) or (
-                    self.total_nrows_limit and self.total_nrows_limit - self.yielded_nrows <= 0):
+                self.total_nrows_limit
+                and self.total_nrows_limit - self.yielded_nrows <= 0
+            ):
                 break
 
-    def read(self) -> 'pd.DataFrame':
+    def read(self) -> "pd.DataFrame":
         """Fetch the data from Flight Service. Fetching is done in batches.
         There is an upper top limit of data size to be fetched configured to 1 GB.
 
         :return: fetched data
         :rtype: pandas.DataFrame
         """
         self.callback.status_message("Starting reading training data...")
         sequences = []
 
         def start_reading_threads():
             # Note: endpoints are created by Flight Service based on number of partitions configured
             # one endpoint serves multiple mini batches of the data
             try:
                 if self.enable_subsampling:
-                    subsampling_thread = threading.Thread(target=self._get_sampling_strategy(), args=(self.label,))
+                    subsampling_thread = threading.Thread(
+                        target=self._get_sampling_strategy(), args=(self.label,)
+                    )
                     subsampling_thread.start()
 
                 else:
                     normal_read_thread = threading.Thread(target=self.normal_read)
                     normal_read_thread.start()
 
                 for n, endpoints in enumerate(self.get_endpoints()):
-                    self.max_flight_batch_number = len(endpoints)  # when flight does not want to open more endpoints
+                    self.max_flight_batch_number = len(
+                        endpoints
+                    )  # when flight does not want to open more endpoints
                     threads = []
                     sequences.append(threads)
 
                     for i, endpoint in enumerate(endpoints):
-                        reading_thread = threading.Thread(target=self._get_data, args=(i, endpoint))
+                        reading_thread = threading.Thread(
+                            target=self._get_data, args=(i, endpoint)
+                        )
                         threads.append(reading_thread)
-                        logger.debug(f"R: Starting batch reading thread: {i}, sequence: {n}...")
+                        logger.debug(
+                            f"R: Starting batch reading thread: {i}, sequence: {n}..."
+                        )
 
                         reading_thread.start()
 
             except Exception as e:
                 self.row_size = -1  # further we raise an error to finish thread
                 self._set_stop_reading(True)
                 raise e
             finally:
 
                 for n, sequence in enumerate(sequences):
                     for i, thread in enumerate(sequence):
-                        logger.debug(f"R: Joining batch reading thread {i}, sequence: {n}...")
+                        logger.debug(
+                            f"R: Joining batch reading thread {i}, sequence: {n}..."
+                        )
 
                         thread.join()
 
-                self._q_put_nowait((-1, -1))  # send close queue message (stop reading logical batches)
+                self._q_put_nowait(
+                    (-1, -1)
+                )  # send close queue message (stop reading logical batches)
                 try:
                     if self.enable_subsampling:
                         subsampling_thread.join()
 
                     else:
                         normal_read_thread.join()
                 except:
@@ -855,33 +1018,37 @@
         try:
             start_reading_threads()
 
             self._check_for_breaking_exceptions()
 
             if self.enable_subsampling:
                 if self.row_size > 0:
-                    self.subsampled_data = self._sample_data_to_percentage(self.total_percentage_limit,
-                                                                           self.subsampled_data,
-                                                                           self.downloaded_data_nrows)
+                    self.subsampled_data = self._sample_data_to_percentage(
+                        self.total_percentage_limit,
+                        self.subsampled_data,
+                        self.downloaded_data_nrows,
+                    )
                 self.subsampled_data = self._process_data(self.subsampled_data)
                 self.yielded_nrows += len(self.subsampled_data)
                 yield self.subsampled_data
 
             else:
                 self.data = self._process_data(self.data)
                 self.yielded_nrows += len(self.data)
                 yield self.data
 
         except ValueError as e:
             if not self.fallback_to_one_connection:
                 raise e
 
             timeout = 60 * 10
-            self.callback.status_message(f"Parallel data connection problem, "
-                                         f"fallback to 1 connection with {timeout} timeout.")
+            self.callback.status_message(
+                f"Parallel data connection problem, "
+                f"fallback to 1 connection with {timeout} timeout."
+            )
             logger.debug(f"Data merging error: {e}")
             logger.debug(f"Fallback to 1 connection. Timeout: {timeout}s")
             time.sleep(timeout)
 
             self.max_flight_batch_number = 1
             sequences = []
             self._q_reset()
@@ -907,18 +1074,20 @@
 
     def _get_max_rows(self, _type: str) -> int:
         if self.number_of_batch_rows is not None:
             return self.number_of_batch_rows
 
         while True:
             if self.row_size == -1:
-                raise TypeError("Data could not be read. Try to use a binary read mode.")
+                raise TypeError(
+                    "Data could not be read. Try to use a binary read mode."
+                )
 
             if self.row_size != 0:
-                if _type == 'data':
+                if _type == "data":
                     max_rows = self.data_batch_size_limit // self.row_size
 
                     if self.total_size_limit is not None:
                         # Note: convert total_size_limit to total_nrows_limit, later only total_nrows_limit will be used.
                         max_rows_total = self.total_size_limit // self.row_size
                         if self.total_nrows_limit:
                             if max_rows_total < self.total_nrows_limit:
@@ -933,60 +1102,68 @@
 
                 else:
                     return self.logical_batch_size_limit // self.row_size
 
             else:
                 self._check_for_breaking_exceptions()
                 if self.threads_exceptions:
-                    logger.debug(f"R: Partitions Thread Errors: {self.threads_exceptions}")
+                    logger.debug(
+                        f"R: Partitions Thread Errors: {self.threads_exceptions}"
+                    )
                     raise DataStreamError(reason=str(self.threads_exceptions))
                 time.sleep(1)
 
     def regression_random_sampling(self, label_column: str) -> None:
         """Start collecting sampled data (random sample for regression problem)"""
         logger.debug(f"Starting regression random sampling.")
-        max_rows = self._get_max_rows(_type='data')
+        max_rows = self._get_max_rows(_type="data")
         downloaded_data_size = 0
         data_batch_size = []
 
         for data in self.create_logical_batch():
             data = self.data_dropna(data, label_column)
             data_size = sys.getsizeof(data)
             downloaded_data_size += data_size
             data_batch_size.append(data_size)
-            self.callback.status_message(f"Downloaded data size: {downloaded_data_size // 1024 // 1024} MB")
+            self.callback.status_message(
+                f"Downloaded data size: {downloaded_data_size // 1024 // 1024} MB"
+            )
             logger.debug(f"Logical batch size: {data_size}")
 
             # join previous sampled batch with new one
             if self.subsampled_data is not None:
                 data = pd.concat([self.subsampled_data, data])
 
             if len(data) <= max_rows:
                 self.subsampled_data = data
 
             else:
                 self.subsampled_data = data.sample(n=max_rows, random_state=0, axis=0)
 
             self.data_batch_size = data_batch_size[0]
-            logger.debug(f"Subsampled batch size: {sys.getsizeof(self.subsampled_data)}")
+            logger.debug(
+                f"Subsampled batch size: {sys.getsizeof(self.subsampled_data)}"
+            )
 
     def stratified_subsampling(self, label_column: str) -> None:
         """Start collecting sampled data (stratified sample for classification problem)"""
         logger.debug(f"Starting classification stratified sampling.")
-        max_rows = self._get_max_rows(_type='data')
+        max_rows = self._get_max_rows(_type="data")
         downloaded_data_size = 0
         data_batch_size = []
 
         for data in self.create_logical_batch():
             data = self.data_dropna(data, label_column)
 
             data_size = sys.getsizeof(data)
             downloaded_data_size += data_size
             data_batch_size.append(data_size)
-            self.callback.status_message(f"Downloaded data size: {downloaded_data_size // 1024 // 1024} MB")
+            self.callback.status_message(
+                f"Downloaded data size: {downloaded_data_size // 1024 // 1024} MB"
+            )
             logger.debug(f"Logical batch size: {data_size}")
 
             stats = data[label_column].value_counts()
             indexes = stats[stats == 1].index.values
             for i in indexes:
                 logger.debug(f"Unique value in label column: {i}")
                 data = data[data[label_column] != i]
@@ -995,58 +1172,70 @@
             if self.subsampled_data is not None:
                 data = pd.concat([self.subsampled_data, data])
 
             if len(data) <= max_rows:
                 self.subsampled_data = data
 
             else:
-                self.subsampled_data = data.groupby(label_column, group_keys=False).apply(lambda x: x.sample(frac=max_rows / len(data)))
-
-            logger.debug(f"Subsampled batch size: {sys.getsizeof(self.subsampled_data)}")
+                self.subsampled_data = data.groupby(
+                    label_column, group_keys=False
+                ).apply(lambda x: x.sample(frac=max_rows / len(data)))
+
+            logger.debug(
+                f"Subsampled batch size: {sys.getsizeof(self.subsampled_data)}"
+            )
         self.data_batch_size = data_batch_size[0]
 
     def truncate_sampling(self, label_column: str) -> None:
         """Start collecting sampled data (truncate sample for forecasting problem)"""
         logger.debug(f"Starting forecasting truncate sampling.")
-        max_rows = self._get_max_rows(_type='data')
+        max_rows = self._get_max_rows(_type="data")
         downloaded_data_size = 0
         data_batch_size = []
 
         for data in self.create_logical_batch():
             data_size = sys.getsizeof(data)
             downloaded_data_size += data_size
             data_batch_size.append(data_size)
-            self.callback.status_message(f"Downloaded data size: {downloaded_data_size // 1024 // 1024} MB")
+            self.callback.status_message(
+                f"Downloaded data size: {downloaded_data_size // 1024 // 1024} MB"
+            )
             logger.debug(f"Logical batch size: {data_size}")
 
             # join previous sampled batch with new one
             if self.subsampled_data is not None:
                 data = pd.concat([self.subsampled_data, data])
 
             if len(data) <= max_rows:
                 self.subsampled_data = data
 
             else:
-                timestamp_column = self.params.get('timestamp_column') or self.params.get('timestamp_column_name')
+                timestamp_column = self.params.get(
+                    "timestamp_column"
+                ) or self.params.get("timestamp_column_name")
                 if timestamp_column is not None:
                     data = data.sort_values(timestamp_column)
                 self.subsampled_data = data.tail(max_rows)
 
-            logger.debug(f"Subsampled batch size: {sys.getsizeof(self.subsampled_data)}")
+            logger.debug(
+                f"Subsampled batch size: {sys.getsizeof(self.subsampled_data)}"
+            )
         self.data_batch_size = data_batch_size[0]
 
     def normal_read(self):
         """Start collecting all the data when user do not want to subsample.
         This should be limited to the max data size 1GB, see limitation implementation in self._read_data().
         """
         for data in self.create_logical_batch():
             self.data = data
             break
 
-    def create_logical_batch(self, timeout: int = 60 * 10, _type: str = 'logical') -> 'pd.DataFrame':
+    def create_logical_batch(
+        self, timeout: int = 60 * 10, _type: str = "logical"
+    ) -> "pd.DataFrame":
         """Create a logical batch for sampling, logical batch is larger ~2GB.
         If used with normal read, it will return all of the collected data, max 1GB based on a limitation.
         """
         max_rows = self._get_max_rows(_type=_type)
         logical_batch = []
         mini_batch_counter = 0
         threads_finished = 0
@@ -1060,20 +1249,23 @@
                         yield self._process_data(data)
                     break
 
                 logger.debug("LB: Waiting for mini batch to appear in the queue...")
                 thread_number, data = self._q_get(timeout=timeout)  # wait max 10 min
 
                 self.q.task_done()
-                logger.debug(f"LB: Mini batch received and taken from the queue. Batch from thread: {thread_number}")
+                logger.debug(
+                    f"LB: Mini batch received and taken from the queue. Batch from thread: {thread_number}"
+                )
 
                 # when the last thread finish reading (finish sequence (-1, -1))
                 if isinstance(data, int) and thread_number == -1 and data == -1:
                     logger.debug(
-                        f"LB: Received the final batch. Batch from thread: {thread_number}")
+                        f"LB: Received the final batch. Batch from thread: {thread_number}"
+                    )
                     self._set_stop_reading(True)
                     if logical_batch:
                         data = pd.concat(logical_batch)  # flush last data
                         yield self._process_data(data)
                     break
 
                 # when not the last thread finished reading, continue
@@ -1082,20 +1274,22 @@
                     continue
 
                 mini_batch_counter += 1
 
                 logger.debug(f"LB: Mini batch number: {mini_batch_counter}")
 
             except queue.Empty:
-                if data is None or isinstance(data,
-                                              int) or data.empty:  # raise the error only if any data were not downloaded.
+                if (
+                    data is None or isinstance(data, int) or data.empty
+                ):  # raise the error only if any data were not downloaded.
                     threads_finished += 1
                     raise DataStreamError(
-                        reason=f'LB: No data were downloaded in the thread due to timeout on '
-                               f'waiting for data batch: max {timeout / 60} minutes, data: {data}')
+                        reason=f"LB: No data were downloaded in the thread due to timeout on "
+                        f"waiting for data batch: max {timeout / 60} minutes, data: {data}"
+                    )
 
             logical_batch.append(data.iloc[:max_rows])
             not_used_data = data.iloc[max_rows:]
             max_rows = max_rows - len(logical_batch[-1])
 
             if max_rows > 0:
                 continue
@@ -1105,39 +1299,40 @@
                 data = pd.concat(logical_batch)  # flush data
                 yield self._process_data(data)
 
                 # note: check if the not_used_data contains enough data to produce next logical batch:
                 while len(not_used_data) > self._get_max_rows(_type=_type):
                     max_rows = self._get_max_rows(_type=_type)
                     logger.debug("LB: Yielding logical batch!")
-                    data = not_used_data.iloc[:max_rows] # flush data
+                    data = not_used_data.iloc[:max_rows]  # flush data
                     not_used_data = not_used_data.iloc[max_rows:]
                     yield self._process_data(data)
                 # end note
                 logical_batch = [not_used_data]
 
                 if self.total_nrows_limit and self.yielded_nrows:
-                    max_rows = min(self._get_max_rows(_type=_type) - len(logical_batch[-1]),
-                                   self.total_nrows_limit - self.yielded_nrows)
+                    max_rows = min(
+                        self._get_max_rows(_type=_type) - len(logical_batch[-1]),
+                        self.total_nrows_limit - self.yielded_nrows,
+                    )
                     if max_rows < 0:  # finish if limit has been reached
                         self._set_stop_reading(True)
                         break
                 else:
                     max_rows = self._get_max_rows(_type=_type) - len(logical_batch[-1])
 
-
     def _select_source_command(self, infer_schema: bool = False) -> List[str]:
         """Based on a data source type, select appropriate commands for flight service configuration."""
 
         infer_schema = infer_schema
         if self.read_binary:
             infer_schema = False
 
         if self.write_binary:
-            command = {"interaction_properties": {'write_mode': "write_raw"}}
+            command = {"interaction_properties": {"write_mode": "write_raw"}}
 
         else:
             # limitation for number of rows passed within one mini batch
             # from flight service (can be between 100 and 10000)
             # need to test performance with big data
             default_batch_size = DEFAULT_BATCH_SIZE_FLIGHT_COMMAND
             # when reading binary batches, default row is one value of 32k.
@@ -1145,256 +1340,326 @@
             # batches of 300MB being read and kept in memory as 10k chunks
             # of 32k. This clutters the memory and is more work for GC.
             # Defaulting to 1000 when reading binary to decrease footprint.
             if self.read_binary:
                 default_batch_size = DEFAULT_BATCH_SIZE_FLIGHT_COMMAND_BINARY_READ
             # when dataset is small we received empty dfs, it is ok (4 is optimum for larger than 1 GB)
             command = {
-                "num_partitions": self.max_flight_batch_number if self.max_flight_batch_number is not None else DEFAULT_PARTITIONS_NUM,
+                "num_partitions": (
+                    self.max_flight_batch_number
+                    if self.max_flight_batch_number is not None
+                    else DEFAULT_PARTITIONS_NUM
+                ),
                 "batch_size": default_batch_size,
-                "interaction_properties": {}
+                "interaction_properties": {},
             }
 
         if self.extra_interaction_properties is not None:
             command["interaction_properties"].update(self.extra_interaction_properties)
 
         # note: if number_of_batch_rows is bigger that 10 000 then set the 10 000 as batch_size in command. #
         # to specify bigger batch_size use flight_parameters parameter - Flight limitation is 100 000(but sometimes it's to much).
-        if self.number_of_batch_rows is not None and self.number_of_batch_rows < default_batch_size:
-            command['batch_size'] = self.number_of_batch_rows
+        if (
+            self.number_of_batch_rows is not None
+            and self.number_of_batch_rows < default_batch_size
+        ):
+            command["batch_size"] = self.number_of_batch_rows
 
         command.update(self.flight_parameters)
 
         if self.space_id is not None:
-            command['space_id'] = self.space_id
+            command["space_id"] = self.space_id
 
         elif self.project_id is not None:
-            command['project_id'] = self.project_id
+            command["project_id"] = self.project_id
 
         if self.read_binary:
-            command['interaction_properties'].update({'read_mode': "read_raw"})
+            command["interaction_properties"].update({"read_mode": "read_raw"})
 
         if self.asset_id:
-            command['asset_id'] = self.asset_id
+            command["asset_id"] = self.asset_id
             ## TODO: Remove commented code below - issue https://github.ibm.com/NGP-TWC/ml-planning/issues/28663
             # if infer_schema:
             #     command['interaction_properties'].update({'infer_schema': "true"})
 
         elif self.connection_id:
-            command['asset_id'] = self.connection_id
+            command["asset_id"] = self.connection_id
 
-        if 'bucket' in self.data_location['location']:
-            if self.data_location.get('location', {}).get('path', None) is None \
-                    and 'file_name' in self.data_location.get('location', ''):
-                self.data_location['location']['path'] = self.data_location['location']['file_name']
-            command['interaction_properties'].update(prepare_interaction_props_for_cos(
-                self.params, self.data_location['location']['path']))
-
-            if 'file_format' in command['interaction_properties'] and ('write_mode' in command[
-                'interaction_properties'] or self.read_binary):
-                del command['interaction_properties']['file_format']
-
-            if 'sheet_name' in command['interaction_properties'] and ('write_mode' in command[
-                'interaction_properties'] or self.read_binary):
-                del command['interaction_properties']['sheet_name']
-
-            if 'encoding' in command['interaction_properties'] and ('write_mode' in command[
-                'interaction_properties'] or self.read_binary):
-                del command['interaction_properties']['encoding']
-
-            if 'quote_character' in command['interaction_properties'] and ('write_mode' in command[
-                'interaction_properties'] or self.read_binary):
-                del command['interaction_properties']['quote_character']
+        if "bucket" in self.data_location["location"]:
+            if self.data_location.get("location", {}).get(
+                "path", None
+            ) is None and "file_name" in self.data_location.get("location", ""):
+                self.data_location["location"]["path"] = self.data_location["location"][
+                    "file_name"
+                ]
+            command["interaction_properties"].update(
+                prepare_interaction_props_for_cos(
+                    self.params, self.data_location["location"]["path"]
+                )
+            )
+
+            if "file_format" in command["interaction_properties"] and (
+                "write_mode" in command["interaction_properties"] or self.read_binary
+            ):
+                del command["interaction_properties"]["file_format"]
+
+            if "sheet_name" in command["interaction_properties"] and (
+                "write_mode" in command["interaction_properties"] or self.read_binary
+            ):
+                del command["interaction_properties"]["sheet_name"]
+
+            if "encoding" in command["interaction_properties"] and (
+                "write_mode" in command["interaction_properties"] or self.read_binary
+            ):
+                del command["interaction_properties"]["encoding"]
+
+            if "quote_character" in command["interaction_properties"] and (
+                "write_mode" in command["interaction_properties"] or self.read_binary
+            ):
+                del command["interaction_properties"]["quote_character"]
 
             if infer_schema:
-                command['interaction_properties']['infer_schema'] = "true"
-            command['interaction_properties']['file_name'] = self.data_location['location']['path']
-            command['interaction_properties']['bucket'] = self.data_location['location']['bucket']
-
-        elif 'schema_name' in self.data_location['location']:
-            command['interaction_properties']['schema_name'] = self.data_location['location']['schema_name']
-            command['interaction_properties']['table_name'] = self.data_location['location']['table_name']
-            if self.data_location['location'].get('catalog_name', None):
-                command['interaction_properties']['catalog_name'] = self.data_location['location']['catalog_name']
+                command["interaction_properties"]["infer_schema"] = "true"
+            command["interaction_properties"]["file_name"] = self.data_location[
+                "location"
+            ]["path"]
+            command["interaction_properties"]["bucket"] = self.data_location[
+                "location"
+            ]["bucket"]
+
+        elif "schema_name" in self.data_location["location"]:
+            command["interaction_properties"]["schema_name"] = self.data_location[
+                "location"
+            ]["schema_name"]
+            command["interaction_properties"]["table_name"] = self.data_location[
+                "location"
+            ]["table_name"]
+            if self.data_location["location"].get("catalog_name", None):
+                command["interaction_properties"]["catalog_name"] = self.data_location[
+                    "location"
+                ]["catalog_name"]
 
         elif not self.asset_id:
-            command['interaction_properties'].update(self.data_location['location'])
+            command["interaction_properties"].update(self.data_location["location"])
             if infer_schema:
-                command['interaction_properties']['infer_schema'] = "true"
+                command["interaction_properties"]["infer_schema"] = "true"
 
         # Property 'infer_as_varchar` needs to be false, when 'infer_schema' is true
-        if command['interaction_properties'].get('infer_schema',
-                                                 "false") == "true" and self.infer_as_varchar is not None:
-            command['interaction_properties'].update({'infer_as_varchar': self.infer_as_varchar})
+        if (
+            command["interaction_properties"].get("infer_schema", "false") == "true"
+            and self.infer_as_varchar is not None
+        ):
+            command["interaction_properties"].update(
+                {"infer_as_varchar": self.infer_as_varchar}
+            )
 
         # Git based project assets property
-        if self.data_location is not None and str(self.data_location['location'].get('userfs', 'false')).lower() == 'true':
-            command.update({'userfs': True})
+        if (
+            self.data_location is not None
+            and str(self.data_location["location"].get("userfs", "false")).lower()
+            == "true"
+        ):
+            command.update({"userfs": True})
+
+        if "path" in command["interaction_properties"]:
+            command["interaction_properties"]["file_name"] = command[
+                "interaction_properties"
+            ]["path"]
+            del command["interaction_properties"]["path"]
 
-        if 'path' in command['interaction_properties']:
-            command['interaction_properties']['file_name'] = command['interaction_properties']['path']
-            del command['interaction_properties']['path']
-
-        if 'connection_properties' not in command:
+        if "connection_properties" not in command:
             logger.debug(f"Command: {command}")
 
         return [json.dumps(command)]
 
     def read_binary_data(self, read_to_file: str | None = None) -> Generator | list:
         """Try to read data from flight service using the 'read_raw' parameter. This will allow to fetch binary data.
         Binary read should be used for small data, like json files, zip files etc. not for the big datasets as
         each data batch is joined to the previous one in-memory.
         """
         self.read_binary = True
 
-        if self.flight_parameters.get('num_partitions') is None:
-            self.flight_parameters['num_partitions'] = 1
+        if self.flight_parameters.get("num_partitions") is None:
+            self.flight_parameters["num_partitions"] = 1
             self.max_flight_batch_number = 1
 
-        cm = open(read_to_file, 'wb') if read_to_file else nullcontext()
+        cm = open(read_to_file, "wb") if read_to_file else nullcontext()
         with cm as sink:
             binary_data_array = []
             for n, endpoints in enumerate(self.get_endpoints()):
                 for i, endpoint in enumerate(endpoints):
                     reader = self.flight_client.do_get(endpoint.ticket)
                     try:
                         while True:
                             mini_batch, metadata = reader.read_chunk()
                             if read_to_file:
-                                sink.write(b''.join(mini_batch.columns[0].tolist()))
+                                sink.write(b"".join(mini_batch.columns[0].tolist()))
                             else:
                                 binary_data_array.extend(mini_batch.columns[0].tolist())
                     except StopIteration:
                         pass
         if read_to_file:
             return [read_to_file]
         else:
-            binary_data_container = b''.join(binary_data_array)
+            binary_data_container = b"".join(binary_data_array)
             yield binary_data_container
 
     def write_binary_data(self, file_path: str) -> None:
         """Write data in 16MB binary data blocks. 16MB upper limit is set by the Flight Service.
         The writer will open the source local file and will stream one batch of 16MB to the Flight.
         Only 16MB of data is loaded into the memory at a time.
 
         :param file_path: path to the source file
         :type file_path: str
         """
         self.write_binary = True
-        schema = pa.schema([
-            ('content', pa.binary())
-        ])
+        schema = pa.schema([("content", pa.binary())])
         commands = self._select_source_command(infer_schema=False)
 
         self.flight_client.authenticate(self.authenticate())
-        writer, reader = self.flight_client.do_put(flight.FlightDescriptor.for_command(commands[0]), schema)
+        writer, reader = self.flight_client.do_put(
+            flight.FlightDescriptor.for_command(commands[0]), schema
+        )
 
         with writer:
             batch_max_size = 16770000  # almost 16MB
 
-            with open(file_path, 'rb') as file:
-                for block in iter(partial(file.read, batch_max_size), b''):
-                    writer.write_batch(pa.record_batch([pa.array([block], type=pa.binary())], schema=schema))
+            with open(file_path, "rb") as file:
+                for block in iter(partial(file.read, batch_max_size), b""):
+                    writer.write_batch(
+                        pa.record_batch(
+                            [pa.array([block], type=pa.binary())], schema=schema
+                        )
+                    )
                     self.flight_client.wait_for_available()
 
-    def write_data(self, data: 'pd.DataFrame'):
+    def write_data(self, data: "pd.DataFrame"):
         """Write data from pandas DataFrame. The limit is 16MB dataframe as this is the upper batch size limit.
         Upper layer should fallback to use binary write.
         """
         schema = pa.Schema.from_pandas(data)
         commands = self._select_source_command(infer_schema=False)
 
         self.flight_client.authenticate(self.authenticate())
-        writer, reader = self.flight_client.do_put(flight.FlightDescriptor.for_command(commands[0]), schema)
+        writer, reader = self.flight_client.do_put(
+            flight.FlightDescriptor.for_command(commands[0]), schema
+        )
 
         with writer:
             writer.write_table(pa.Table.from_pandas(data))
             self.flight_client.wait_for_available()
 
         return writer, reader
 
-    def get_batch_writer(self, schema: 'pa.Schema') -> 'FlightStreamWriter':
+    def get_batch_writer(self, schema: "pa.Schema") -> "FlightStreamWriter":
         """Prepare FlightStreamWriter and return it."""
         commands = self._select_source_command(infer_schema=False)
 
         self.flight_client.authenticate(self.authenticate())
-        writer, reader = self.flight_client.do_put(flight.FlightDescriptor.for_command(commands[0]), schema)
+        writer, reader = self.flight_client.do_put(
+            flight.FlightDescriptor.for_command(commands[0]), schema
+        )
         return writer
 
     def _check_for_breaking_exceptions(self):
         # Log every partition thread error (should be the flight service info included)
         if self.threads_exceptions:
             logger.debug(f"IR/R: Partitions Thread Errors: {self.threads_exceptions}")
 
             for msg in self.threads_exceptions:
                 if "Data could not be read" in msg:
                     raise TypeError(msg)
-                if 'Internal error occurred' in msg:
+                if "Internal error occurred" in msg:
                     raise TypeError(msg)
                 if "Unknown error: Wrapping" in msg:
                     raise CorruptedData(msg)
 
         # Note: check if any data were downloaded:
         if len(self.empty_data_threads) == self.max_flight_batch_number:
             raise EmptyDataSource()
 
     def _get_sampling_strategy(self):
         """Return sampling strategy for given sampling and learning type."""
         random_sampling_pred_types = (
-            PredictionType.REGRESSION, PredictionType.CLASSIFICATION, PredictionType.BINARY, PredictionType.MULTICLASS
+            PredictionType.REGRESSION,
+            PredictionType.CLASSIFICATION,
+            PredictionType.BINARY,
+            PredictionType.MULTICLASS,
         )
         stratified_sampling_pred_types = (
-            PredictionType.CLASSIFICATION, PredictionType.BINARY, PredictionType.MULTICLASS
+            PredictionType.CLASSIFICATION,
+            PredictionType.BINARY,
+            PredictionType.MULTICLASS,
         )
         truncate_sampling_pred_types = (
-            PredictionType.FORECASTING, PredictionType.TIMESERIES_ANOMALY_PREDICTION
+            PredictionType.FORECASTING,
+            PredictionType.TIMESERIES_ANOMALY_PREDICTION,
         )
 
-        if self.sampling_type == SamplingTypes.RANDOM and self.learning_type in random_sampling_pred_types:
+        if (
+            self.sampling_type == SamplingTypes.RANDOM
+            and self.learning_type in random_sampling_pred_types
+        ):
             return self.regression_random_sampling
-        elif self.sampling_type == SamplingTypes.STRATIFIED and self.learning_type in stratified_sampling_pred_types:
+        elif (
+            self.sampling_type == SamplingTypes.STRATIFIED
+            and self.learning_type in stratified_sampling_pred_types
+        ):
             return self.stratified_subsampling
-        elif self.sampling_type == SamplingTypes.LAST_VALUES and self.learning_type in truncate_sampling_pred_types:
+        elif (
+            self.sampling_type == SamplingTypes.LAST_VALUES
+            and self.learning_type in truncate_sampling_pred_types
+        ):
             return self.truncate_sampling
         else:
             raise InvalidSamplingType(self.sampling_type, self.learning_type)
 
     def _sample_data_to_percentage(self, percentage, data, full_data_nrows):
         if self.enable_subsampling:
             if percentage < 1.0 and percentage > 0.0:
                 nrows = int(percentage * full_data_nrows)
-                logger.debug(f"Running sampling to prercentage of data: "
-                             f"{percentage} from {full_data_nrows} rows gives {nrows} of data rows")
+                logger.debug(
+                    f"Running sampling to prercentage of data: "
+                    f"{percentage} from {full_data_nrows} rows gives {nrows} of data rows"
+                )
                 if len(data) < nrows:
                     return data  # no sampling needed
 
                 if self.sampling_type == SamplingTypes.RANDOM:
                     data = data.sample(n=nrows, random_state=0, axis=0)
                 elif self.sampling_type == SamplingTypes.STRATIFIED:
-                    data = data.groupby(self.label, group_keys=False).apply(lambda x: x.sample(frac=nrows/len(data)))
+                    data = data.groupby(self.label, group_keys=False).apply(
+                        lambda x: x.sample(frac=nrows / len(data))
+                    )
 
                 elif self.sampling_type == SamplingTypes.LAST_VALUES:
-                    timestamp_column = self.params.get('timestamp_column') or self.params.get('timestamp_column_name')
+                    timestamp_column = self.params.get(
+                        "timestamp_column"
+                    ) or self.params.get("timestamp_column_name")
                     if timestamp_column is not None:
                         data = data.sort_values(timestamp_column)
                     data = data.tail(nrows)
                 else:
                     raise NotImplementedError(
-                        f"Sampling type: {self.sampling_type} does not supports additional sampling to percentage of data.")
+                        f"Sampling type: {self.sampling_type} does not supports additional sampling to percentage of data."
+                    )
                 return data
             elif percentage == 1.0:
                 return data
             else:
-                raise ValueError('The `percentage` need to be float between 0.0 and 1.0.')
+                raise ValueError(
+                    "The `percentage` need to be float between 0.0 and 1.0."
+                )
         else:
             return data
 
-
     def data_dropna(self, data, label_column):
         # simple preprocess before sampling
         data_with_nans_len = len(data)
         data.dropna(inplace=True, subset=[label_column])
         removed_rows = data_with_nans_len - len(data)
         if removed_rows > 0:
-            self.downloaded_data_nrows = self.downloaded_data_nrows - removed_rows  # fix for no_batches calculation #31307
+            self.downloaded_data_nrows = (
+                self.downloaded_data_nrows - removed_rows
+            )  # fix for no_batches calculation #31307
 
         return data
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/helpers/connections/local.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/helpers/connections/local.py`

 * *Files 1% similar despite different names*

```diff
@@ -5,18 +5,19 @@
 
 
 import sys
 import pandas as pd
 
 
 class LocalBatchReader:
-    """LocalBatchReader is designed to """
+    """LocalBatchReader is designed to"""
+
     def __init__(self, file_path: str, batch_size: int = 1073741824 // 10):
         self.file_path = file_path
-        self.batch_size = batch_size    # default 100 MB
+        self.batch_size = batch_size  # default 100 MB
         self.row_size = 0
 
         self._determine_row_size()
 
     def _determine_row_size(self) -> None:
         data_row = pd.read_csv(self.file_path, chunksize=1)
         self.row_size = sys.getsizeof(data_row)
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/helpers/helpers.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/helpers/helpers.py`

 * *Files 4% similar despite different names*

```diff
@@ -42,15 +42,15 @@
     """
     config = ConfigParser()
     config.read(config_path)
 
     return json.loads(config.get(env_name, credentials_name))
 
 
-def pipeline_to_script(pipeline) -> Union['str', 'HTML']:
+def pipeline_to_script(pipeline) -> Union["str", "HTML"]:
     """Create a python script based on a passed pipeline model. (Python code representation of pipeline model)
 
     :param pipeline: pipeline model to be written as script
     :type pipeline: Pipeline or TrainedPipeline
 
     :return: information about script location
     :rtype: str or html
@@ -62,22 +62,23 @@
         pipeline_to_script(pipeline=best_pipeline)
     """
     from lale.helpers import import_from_sklearn_pipeline
     from sklearn.pipeline import Pipeline
     from ibm_watsonx_ai.utils.autoai.utils import is_ipython
     from ibm_watsonx_ai.utils import create_download_link
     import os
+
     script_name = "pipeline_script.py"
 
     if isinstance(pipeline, Pipeline):
         pipeline = import_from_sklearn_pipeline(pipeline)
 
     script = pipeline.pretty_print()
 
-    with open(script_name, 'w') as f:
+    with open(script_name, "w") as f:
         f.write(script)
 
     script_location = f"{os.path.abspath('.')}/{script_name}"
 
     if is_ipython():
         return create_download_link(script_location)
     else:
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/hpo.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/hpo.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/href_definitions.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/href_definitions.py`

 * *Files 0% similar despite different names*

```diff
@@ -388,17 +388,15 @@
 
     def get_platform_spaces_members_href(self, spaces_id: str) -> str:
         return SPACES_MEMBERS_HREF_PATTERN.format(
             self._get_platform_url_if_exists(), spaces_id
         )
 
     def get_v4_instance_id_href(self, instance_id: str) -> str:
-        return V4_INSTANCE_ID_HREF_PATTERN.format(
-            self._credentials.url, instance_id
-        )
+        return V4_INSTANCE_ID_HREF_PATTERN.format(self._credentials.url, instance_id)
 
     def get_async_deployment_job_href(self) -> str:
         return DEPLOYMENT_JOB_HREF_PATTERN.format(self._credentials.url + self.prepend)
 
     def get_async_deployment_jobs_href(self, job_id: str) -> str:
         return DEPLOYMENT_JOBS_HREF_PATTERN.format(
             self._credentials.url + self.prepend, job_id
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/hw_spec.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/hw_spec.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/import_assets.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/import_assets.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/connection/route_declarations.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/connection/route_declarations.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,19 +1,20 @@
-"""
-Create routes for the server handler
-"""
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
-import logging
+"""
+Create routes for the server handler
+"""
 import json
-from ibmfl.message.message_type import MessageType
+import logging
+
 from ibmfl.message.message import Message
+from ibmfl.message.message_type import MessageType
 
 logger = logging.getLogger(__name__)
 
 
 def default_end_point(message):
     """
     All Requests whose request type is not determined are routed to this method
@@ -28,51 +29,56 @@
     :type kwargs: `dict`
 
     """
     # more detailed logging should be done on request message
 
     logging.info(message.__dict__)
 
-    message.set_data({'status': 'error'})
+    message.set_data({"status": "error"})
 
     return message
 
 
 def get_aggregator_router(router, agg_proto_handler):
     """
     Route for register party
 
     :param router: Router object
     :type router: `Router`
     :param agg_proto_handler: ProtoHandler object
     :type agg_proto_handler: `ProtoHandler`
     :return: None
     """
-    router.add_routes({
-        '{}'.format(MessageType.REGISTER.value):
-        agg_proto_handler.register_party,
-
-        '{}'.format(MessageType.TRAIN.value):
-        agg_proto_handler.process_model_update_requests,
-
-        'default': default_end_point
-    })
+    router.add_routes(
+        {
+            "{}".format(MessageType.REGISTER.value): agg_proto_handler.register_party,
+            "{}".format(MessageType.TRAIN.value): agg_proto_handler.process_model_update_requests,
+            "default": default_end_point,
+        }
+    )
 
 
 def get_party_router(router, party_proto_handler):
     """
     Route for register party
 
     :param router: Router object
     :type router: `Router`
     :param party_proto_handler: ProtoHandler object
     :type party_proto_handler: `ProtoHandler`
     :return: None
     """
-    router.add_routes({
-        '{}'.format(MessageType.TRAIN.value): party_proto_handler.handle_async_request,
-        '{}'.format(MessageType.SAVE_MODEL.value): party_proto_handler.handle_request,
-        '{}'.format(MessageType.EVAL_MODEL.value): party_proto_handler.handle_request,
-        '{}'.format(MessageType.SYNC_MODEL.value): party_proto_handler.handle_request,
-        '{}'.format(MessageType.STOP.value): party_proto_handler.handle_request,
-        'default': default_end_point
-    })
+    router.add_routes(
+        {
+            "{}".format(MessageType.TRAIN.value): party_proto_handler.handle_async_request,
+            "{}".format(MessageType.SAVE_MODEL.value): party_proto_handler.handle_request,
+            "{}".format(MessageType.EVAL_MODEL.value): party_proto_handler.handle_request,
+            "{}".format(MessageType.SYNC_MODEL.value): party_proto_handler.handle_request,
+            "{}".format(MessageType.STOP.value): party_proto_handler.handle_request,
+            "{}".format(MessageType.PREPROCESS.value): party_proto_handler.handle_prepreocess_request,
+            "{}".format(MessageType.REQUEST_CERT.value): party_proto_handler.handle_request,
+            "{}".format(MessageType.GENERATE_KEYS.value): party_proto_handler.handle_request,
+            "{}".format(MessageType.DISTRIBUTE_KEYS.value): party_proto_handler.handle_request,
+            "{}".format(MessageType.SET_KEYS.value): party_proto_handler.handle_request,
+            "default": default_end_point,
+        }
+    )
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/connection/websockets_connection.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/connection/websockets_connection.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,468 +1,476 @@
-"""Connection class which uses flask and request libraries to create a server
-client combo
-"""
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
-import logging
-import socket
-
-import ibm_watsonx_ai._wrappers.requests as requests
 import asyncio
+import concurrent.futures._base
+import logging
 import signal
-import random
-import websockets
-import pathlib
-from websockets import exceptions
-import string
-import argparse
+import ssl
 import threading
 import time
-import ssl
-from _collections import deque
-import sys
-import traceback
+from collections import deque
 
-from ibmfl.connection.connection import ConnectionStatus
-from ibmfl.connection.connection import FLConnection, FLSender, FLReceiver
+import websockets
+
+from ibmfl.connection.connection import FLConnection, FLReceiver, FLSender
+from ibmfl.message.message import Message, ResponseMessage
 from ibmfl.message.message_type import MessageType
-from ibmfl.message.message import Message
-from ibmfl.message.serializer_types import SerializerTypes
-from ibmfl.message.serializer_factory import SerializerFactory
 
 logger = logging.getLogger(__name__)
 
+# This variable MUST be set to False in production. It can be set to True only in local testing environment.
+TEST_MODE = False
 
-# Maintained only on the party side
-# Used to signal to websockets that there is a message ready to be sent to the aggregator
-pSendEvt = threading.Event()
+# TODO: remove  self.temp_rts_id = '4a666f4a-87c0-475d-bfb2-312d186163b4'
 
-# Used to signal that there is a message that was received from the aggregator
+party_rbuffer = deque([])
+party_sbuffer = deque([])
 pRecvEvt = threading.Event()
+pRecvEvt.clear()
+pSendEvt = threading.Event()
+pSendEvt.clear()
 
 
+class SWSConnection(FLConnection):
+    DEFAULT_HOST = "127.0.0.1"
+    DEFAULT_PORT = 8765
+    TEMP_rts_id = "4a666f4a-87c0-475d-bfb2-312d186163b4"
+    DEFAULT_CONFIG = {"ip": DEFAULT_HOST, "port": DEFAULT_PORT, "rts_id": TEMP_rts_id}
+    rts_id = None
 
-# Used to store messages to be sent to aggregator
-party_sbuffer = deque([])
-# Used to store messages received from aggregator
-party_rbuffer = deque([])
+    def __init__(self, config):
+        self.stopped = False  # flag has been exposed in generated script.
+        self.settings = self.process_config(config)
+        self.rts_id = self.settings.get("id")
+        logger.debug("METHOD = SWSConnection.init")
+        logger.debug(self.settings)
 
-pRouter = None
+    def initialize(self, **kwargs):
+        pass
 
+    def initialize_receiver(self, **kwargs):
+        self.router = kwargs.get("router")
+        self.settings["verify"] = kwargs.get("verify") if kwargs.get("verify") is not None else True
+        self.settings["token"] = kwargs.get("token")
+        # TODO: Check if sender is intialized, otherwise ?
+        self.receiver = SWSReceiver(self.router, self.sender, self.rts_id, self.settings)
+        self.receiver.initialize()
+        self.receiver.stoppedStateObj = self
 
-class WSConnection(FLConnection):
+    def initialize_sender(self, **kwargs):
+        self.sender = SWSSender(self.rts_id, self.settings)
+        self.sender.initialize()
 
-    def __init__(self, config):
-        """
-        Initializes the connection object
-        :param config: dictionary of configuration provided to connection
-        :type config: `dict`
-        """
-
-        pSendEvt.clear()
-        pRecvEvt.clear()
-        # comes from the config file
-        self.started = False
-        self.stopped = False
-        self.settings = config
-        self.receiver = None
-        self.sender = None
-        self.loop1 = None
-        self.loop2 = None
-        self.stop1 = None
-        self.stop2 = None
-        self.mainLoop = None
-        self.flagForOutstandingMessageResponseToBeReceived = False
-        # identity/name of the party
+    def start(self, **kwargs):
+        self.receiver.start()
 
-        if 'id' in config:
-            self.party = config['id']
-        else:
-            # TODO: Fix error if no rts id
-            self.party = 'DEFAULT'
+    def stop(self, **kwargs):
+        if self.receiver.stop is not None:
+            self.receiver.stop.set_result
+            self.stopped = True
 
-        # Send Port is used for messages initiated from aggregator
-        if 'wssendport' in config:
-            self.wssendport = config['wssendport']
-        else:
-            # set a default value to be aggregator
-            self.wssendport = "/aggregator"
-        # Recv port is used for messages initiated from party
-        if 'wsrecvport' in config:
-            self.wsrecvport = config['wsrecvport']
+    def get_connection_config(self):
+        return self.settings
+
+    def process_config(self, config):
+        logger.debug("METHOD = SWSConnection.process_config")
+        logger.debug(config)
+
+        if config.get("ip", None) is not None and config.get("port", None) is not None:
+            return config
         else:
-            # set a default value to be aggregator
-            self.wsrecvport = "/aggregator1"
+            return self.DEFAULT_CONFIG
+
 
+class SWSReceiver(FLReceiver):
+    def __init__(self, router, sender, party_id, settings):
+        self.router = router
+        self.ssl_context = True
+        self.sender = sender
+        self.stopped = False
+        self.stopping = False
+        self.party = party_id
+        self.settings = settings
+        self.stop = None
+        self.messageInTransit = None
+        self.stoppedStateObj = None
+        self.registered = False
 
-    async def PartySendLoop(self):
-        """
-        This is started at the party.
-        It reads from the party_sbuffer, and synchronously sends each message to the aggregator
-        and waits for a response from the aggregator.
-        SendLoop is a misnomer, like in Flask Connection.
-        It merely means that messages are initiated by the party.
-        """
-
-        uri = "wss://" + self.aggInfo['ip'] + str(self.wsrecvport)
-        logger.debug("PartySendLoop started = " + uri)
-        ssl_context_party = True
-        if self.ssl_context is not None:
-            ssl_context_party = ssl.SSLContext()
+    def initialize(self, **kwargs):
+        pass
 
-        while not self.stopped:
-            if len(party_sbuffer) == 0:
-                logger.info("PartySendLoop: Holding for message to send" )
-                pSendEvt.wait()
-            try:
-                logger.info("PartySendLoop: Number of active messages ready to send: " + str(len(party_sbuffer)))
-                msg = party_sbuffer.popleft()
-            except IndexError:
-                logger.debug("No message to send")
-                continue
-            pSendEvt.clear()
-            logger.debug("PartySendLoop: Sending message to aggregator")
-            headers = None
-            if self.authToken != None:
-                headers = { 'Authorization': self.authToken, 'rtsid': self.party }
-            try:
-                async with websockets.connect(uri, max_size=2 ** 29, read_limit=2 ** 29, write_limit=2**29, ssl=ssl_context_party, extra_headers=headers) as websocket:
+    def debug_websocket(self, websocket):
+        # pass
+        logger.debug("*************")
+        logger.debug(websocket.remote_address)
+        logger.debug(websocket.request_headers.get("rtsid"))
+        logger.debug("*************")
+
+    def prepare_message_data_structures(self):
+        # party_rbuffer = deque([])
+        # party_sbuffer = deque([])
+        # pRecvEvt = threading.Event()
+        # pRecvEvt.clear()
+        # pSendEvt = threading.Event()
+        # pSendEvt.clear()
+        pass
+
+    def start(self, **kwargs):
+        logger.debug("METHOD = SWSReceiver.start")
+        self.loop = asyncio.new_event_loop()
+        self.stop = self.loop.create_future()
+        try:
+            self.loop.add_signal_handler(signal.SIGTERM, self.stop.set_result, None)
+        except NotImplementedError:
+            logger.debug("Signal handling is not implemented")
+        t1 = threading.Thread(target=self.setup_websocket_connection, args=(self.loop, self.stop), daemon=True)
+        t1.start()
+
+    def stop(self, **kwargs):
+        logger.debug("METHOD = SWSReceiver.stop")
+        self.stopped = True
+        if self.stoppedStateObj is not None:
+            self.stoppedStateObj.stopped = True
 
-                    if self.flagForOutstandingMessageResponseToBeReceived == False:
-                        logger.debug(
-                            "PartySendLoop: Attempting to send message to aggregator")
-                        try:
-                            await websocket.send(msg)
-
-                        except Exception as ex:
-                            logger.error("PartySendLoop exception when attempting to send message: " + str(ex))
-                            party_sbuffer.appendleft(msg)
-                            raise
-                        logger.debug(
-                            "PartySendLoop: Message sent successfully; awaiting message response")
-
-                    self.flagForOutstandingMessageResponseToBeReceived = True
-                    resp = await websocket.recv()
-                    self.flagForOutstandingMessageResponseToBeReceived = False
-                    party_rbuffer.append(resp)
-                    pRecvEvt.set()
+    def setup_websocket_connection(self, loop, stop):
+        logger.debug("METHOD = SWSReceiver.setup_websocket_connection")
+        try:
+            loop.run_until_complete(self.establish_websocket_connection(loop, stop))
+
+            # asyncio.get_event_loop().run_forever()
+        except websockets.exceptions.ConnectionClosedError as ex:
+            if self.stopped != True:
+                logger.error("PartySendLoop : Connection closed abnormally. Exception is " + str(ex))
+                logger.error("Retrying connection to aggregator")
+        except websockets.exceptions.ConnectionClosedOK as ex:
+            if self.stopped != True:
+                logger.error("PartySendLoop : Connection closed unexpectedly. Exception is " + str(ex))
+                logger.error("Retrying connection to aggregator")
+            else:
+                logger.debug("Connection Closed OK")
+        except websockets.exceptions.WebSocketException as ex:
+            logger.error("PartySendLoop : WebsocketException. Details: " + str(ex))
+        except Exception as ex:
+            logger.error("PartySendLoop : Exception is " + str(ex))
 
+        self.stop.set_result
+
+    async def establish_websocket_connection(self, loop, stop):
+        prefix = "wss://" if not TEST_MODE else "ws://"
+        uri = prefix + self.settings.get("ip") + ":" + str(self.settings.get("port"))
+        logger.info("METHOD = SWSReceiver.establish_websocket_connection uri = " + uri)
+
+        headers = {"rtsid": self.party}
+        token = self.settings.get("token")
+        if token is not None:
+            headers["Authorization"] = token
+        if not self.settings.get("verify"):
+            self.ssl_context = ssl.SSLContext() if not TEST_MODE else None
+        logger.debug("SWSReceiver.establish_websocket_connection ssl = " + str(self.ssl_context))
+        apikey_header = None
+        while not self.stopped:
+            try:
+                logger.debug("SWSReceiver.establish_websocket_connection ATTEMPT async websockets.connect")
+                connect_params = {
+                    "open_timeout": 30,
+                    "ping_interval": 20,
+                    "ping_timeout": None,
+                    "max_size": 2**29,
+                    "read_limit": 2**29,
+                    "write_limit": 2**29,
+                    "loop": loop,
+                    "extra_headers": headers,
+                }
+                if not TEST_MODE:
+                    connect_params["ssl"] = self.ssl_context
+                async with websockets.connect(uri, **connect_params) as websocket:
+                    logger.info("Received Heartbeat from Aggregator")
+                    apikey_header = websocket.response_headers.get("apikey", None)
+                    if apikey_header is not None:
+                        logger.debug("API KEY received from Aggregator, for reconnection authorization")
+                        headers["Authorization"] = apikey_header
+                    await self.websocket_message_loop(websocket)
             except websockets.exceptions.ConnectionClosedError as ex:
+                logger.info("ConnectionClosedError encountered.")
+                logger.error(str(ex))
+
                 if self.stopped != True:
-                    logger.error(
-                        "PartySendLoop : Connection closed abnormally. Exception is " + str(ex))
-                    logger.error("Retrying connection to aggregator")
+                    logger.error("PartySendLoop : Connection closed abnormally. Exception is " + str(ex))
+                    logger.info("Retrying connection to aggregator")
                     continue
             except websockets.exceptions.ConnectionClosedOK as ex:
+                logger.info("ConnectionClosedOK encountered.")
+                logger.error(str(ex))
                 if self.stopped != True:
-                    logger.error(
-                        "PartySendLoop : Connection closed unexpectedly. Exception is " + str(ex))
-                    logger.error("Retrying connection to aggregator")
+                    logger.error("PartySendLoop : Connection closed unexpectedly. Exception is " + str(ex))
+                    logger.info("Retrying connection to aggregator")
                     continue
+                else:
+                    logger.info("Connection Closed OK")
+                    break
+            except asyncio.TimeoutError as ex:
+                logger.info("asyncio TimeoutError encountered.")
+                logger.error(str(ex))
+                continue
             except websockets.exceptions.WebSocketException as ex:
-                logger.error(
-                    "PartySendLoop : WebsocketException. Details: " + str(ex))
-                return False
-            except socket.gaierror as ex:
-                logger.error("PartySendLoop : Error connecting to " + uri + ". Exception is " + str(ex))
-                return False
+                logger.info("WebsocketException encountered.")
+                logger.error(str(ex))
+                self.stopped = True
+                continue
+            except ssl.SSLError as ex:
+                logger.info("SSL Error encountered.")
+                pRecvEvt.set()
+                logger.error(str(ex))
+                raise Exception(
+                    "No valid certificate detected. Please replace the default certificate with your own "
+                    "TLS certificate, or set verify to False at your own risk. For more details, please see "
+                    "https://www.ibm.com/docs/en/cloud-paks/cp-data/4.0?topic=client-using-custom-tls-certificate-connect-platform"
+                )
+
             except Exception as ex:
-                logger.error("PartySendLoop : Exception is " + str(ex))
-                return False
-        logger.debug("PartySendLoop ended ")
-        return True
-
-    async def PartyRecvLoop(self):
-        """
-        This is started at the party.
-        It establishes a persistent connection to the aggregator by sending the name of the party
-        The aggregator can then send messages to the party synchronously
-        to which the party replies
-        """
-        pName = self.party
-        uri = "wss://" + self.aggInfo['ip'] + str(self.wssendport)
-        logger.debug("PartyRecvLoop started = " + uri)
-
-        ssl_context_party2 = True
-        if self.ssl_context is not None:
-            ssl_context_party2 = ssl.SSLContext()
-
-        headers = None
-        if self.authToken != None:
-            headers = { 'Authorization': self.authToken }
-        while not self.stopped:
+                logger.info("Exception encountered.")
+                logger.error(repr(ex))
+                logger.exception(ex)
+                break
+
+    async def websocket_message_loop(self, websocket):
+        logger.debug("METHOD = SWSReceiver.websocket_message_loop")
+        self.debug_websocket(websocket)
+
+        receive_message_task = asyncio.ensure_future(self.receive_message_handler(websocket))
+        send_message_task = asyncio.ensure_future(self.send_message_handler(websocket))
+        stop_loop_tasks = False
+        while not stop_loop_tasks:
             try:
-                async with websockets.connect(uri, close_timeout=100, max_size=2 ** 29, read_limit=2 ** 29, write_limit=2**29, ssl=ssl_context_party2, extra_headers=headers) as websocket:
-                    await websocket.send(pName)
-                    async for rmess in websocket:
-                        if rmess == "HEARTBEAT":
-                            logger.info("Received Heartbeat from Aggregator")
-                            continue
-                        serializer = SerializerFactory(
-                            SerializerTypes.JSON_PICKLE).build()
-                        recv_msg = serializer.deserialize(rmess)
-                        logger.debug(
-                            "PartyRecvLoop : Received " + str(recv_msg))
-                        request_path = str(recv_msg.message_type)
-                        logger.debug(
-                            "REQUEST PATH IN PARTY IS " + str(request_path))
-
-                        # check if error
-                        if recv_msg.message_type == MessageType.ERROR_AUTH.value :
-                            logger.info(
-                            "PartyRecvLoop : Auth Error. Please verify training ID, RTS, project permission and token")
-                            self.stop()
-                            break;
-
-                        handler, kwargs = self.router.get_handler(
-                            request_path=request_path)
-
-                        if handler is None:
-                            logger.error(
-                                'PartyRecvLoop : Invalid Request ! Routing it to default handler')
-                            handler, kwargs = self.router.get_handler(
-                                request_path='default')
-                        try:
-                            res_message = handler(recv_msg)
-                        except Exception as ex:
-                            res_message = Message()
-                            data = {'status': 'error', 'message': str(ex)}
-                            res_message.set_data(data)
-
-                        response = serializer.serialize(res_message)
-                        await websocket.send(response)
-                        logger.info("PartyRecvLoop : sent response")
-
-                        if recv_msg.message_type == MessageType.STOP.value :
-                            logger.info(
-                            "PartyRecvLoop : Received STOP message" )
-                            self.stop()
-                            break;
+                done, pending = await asyncio.wait(
+                    [receive_message_task, send_message_task], return_when=asyncio.FIRST_COMPLETED
+                )
+
+                logger.debug("*******************")
+                logger.debug(done)
+                logger.debug(pending)
+                logger.debug("*******************")
+
+                for task in pending:
+                    task.cancel()
+                for future in done:
+                    if not future.cancelled():
+                        logger.debug("Future Processed successfully")
+                if len(pending) == 0:
+                    stop_loop_tasks = True
+            except websockets.exceptions.ConnectionClosedError as ex:
+                logger.exception("$$ EXCEPTION IN websocket_message_loop - ConnectionClosedError encountered.")
+                self.invalidate_message_data_structures(websocket.request_headers.get("rtsid"), True)
+                self.stopped = True
 
+            except Exception as ex:
+                logger.exception(
+                    "$$ EXCEPTION IN WEBSOCKET MESSAGE LOOP for "
+                    + websocket.request_headers.get("rtsid")
+                    + "; Retrying connection"
+                )
+                print(ex)
+
+    async def receive_message_handler(self, websocket):
+        logger.debug("METHOD = SWSReceiver.receive_message_handler")
+        try:
+            async for message in websocket:
+                logger.debug("METHOD = SWSReceiver.receive_message_handler message")
+                self.debug_websocket(websocket)
+                await self.process_received_message(message)
+                # logger.info(message)
+        except concurrent.futures._base.CancelledError as ex:
+            logger.debug("task is being cancelled")
+        except websockets.exceptions.ConnectionClosedError as ex:
+            logger.info("ConnectionClosedError encountered.")
+            if self.messageInTransit is not None:
+                logger.info("Adding in Transit message back to queue")
+                party_sbuffer.append(self.messageInTransit)
+                self.messageInTransit = None
+        except Exception as ex:
+            print(ex)
+
+    async def send_message_handler(self, websocket):
+        while self.stopped is not True:
+            logger.debug("METHOD = SWSReceiver.send_message_handler")
+            self.debug_websocket(websocket)
+            message = await self.process_message_to_send(websocket.request_headers.get("rtsid"))
+            try:
+                if message is not None:
+                    await websocket.send(message)
+                    self.messageInTransit = message
+                    logger.debug("METHOD = SWSReceiver.send_message_handler Message SENT")
+                    if self.stopping == True:
+                        self.stop.set_result(True)
+                        self.stopped = True
+                        if self.stoppedStateObj is not None:
+                            self.stoppedStateObj.stopped = True
             except websockets.exceptions.ConnectionClosedError as ex:
-                if self.stopped != True:
-                    logger.error(
-                        "PartyRecvLoop : Connection closed abnormally. Exception is " + str(ex))
-                    logger.error("Retrying connection to aggregator")
-                    continue
+                logger.info("ConnectionClosedError encountered")
             except websockets.exceptions.ConnectionClosedOK as ex:
-                if self.stopped != True:
-                    logger.error(
-                        "PartyRecvLoop : Connection closed unexpectedly. Exception is " + str(ex))
-                    logger.error("Retrying connection to aggregator")
-                    continue
+                logger.info("ConnectionClosedOK")
             except websockets.exceptions.WebSocketException as ex:
-                logger.error(
-                    "PartyRecvLoop : WebsocketException. Details: " + str(ex))
-                return False
-            except socket.gaierror as ex:
-                logger.error("PartyRecvLoop : Error connecting to " + uri + ". Exception is " + str(ex))
-                return False
+                logger.info("WebSocketException encountered")
             except Exception as ex:
-                logger.error("PartyRecvLoop : Exception is " + str(ex))
-                return False
-        logger.debug("PartyRecvLoop ended ")
-        return True
-
-    def PartySendLoopThread(self, loop):
-        logger.info("**** PartySendLoopThread")
-        # Need to host the loop in its own thread for asyncio to work properly
-        pslt_result = loop.run_until_complete(self.PartySendLoop())
-        logger.debug("**** PartySendLoopThread RESULT = " + str(pslt_result))
-        if pslt_result == False:
-            #if error when sending message, then unblock mainThread so it can fail.
-            pRecvEvt.set()
-            self.stop()
-
-    def PartyRecvLoopThread(self, loop):
-        logger.info("**** PartyRecvLoopThread")
-        # Need to host the loop in its own thread for asyncio to work properly
-        prlt_result = loop.run_until_complete(self.PartyRecvLoop())
-        logger.debug("**** PartyRecvLoopThread RESULT = " + str(prlt_result))
-        if prlt_result == False:
-            self.stop()
-
-
-    def initialize(self, router, aggInfo=None, ssl_context=None, authToken=None):
-        """Initialize receiver and sender """
-        self.router = router
-        self.ssl_context = ssl_context
-
-        if aggInfo == None:
-            logger.error("Aggregator information not specified. Exiting.")
-            exit()
-        logger.info("WSConnection : Initialize Party Communications")
-        self.party = self.settings['id']
-        self.aggInfo = aggInfo
-        self.authToken = authToken
-
-        # Each websocket connection needs a separate asyncio event loop
-        self.loop1 = asyncio.new_event_loop()
-        t1 = threading.Thread(
-            target=self.PartySendLoopThread, args=(self.loop1,), daemon=True)
-        t1.start()
+                logger.info("Exception")
+                logger.info(str(ex))
 
-        # Each websocket connection needs a separate asyncio event loop
-        self.loop2 = asyncio.new_event_loop()
-        t2 = threading.Thread(
-            target=self.PartyRecvLoopThread, args=(self.loop2,), daemon=True)
-        t2.start()
-
-    def initialize_receiver(self, router=None):
-        """Basically does nothing
-        :param router: Router object describing the routes for each request
-            which are passed down to PH
-        :type router: `Router`
-        """
-        # print(router)
-        self.router = router
-        pRouter = router
-        self.receiver = WSReceiver(router, self.settings, self)
-        self.receiver.connection = self
-        logger.debug('Receiver Initialized')
-        self.receiver.initialize()
-        self.status = ConnectionStatus.INITIALIZED
+    async def process_message_to_send(self, rts_id):
+        while self.stopped is not True:
+            # if len(party_sbuffer) == 0:
+            #     logger.info("pSendEvt.wait(), pSendEvt.is_set = " + str(pSendEvt.is_set()))
+            #     pSendEvt.wait()
+            try:
+                msgToBeSent = party_sbuffer.popleft()
+                log_message = f"METHOD = SWSReceiver.process_message_to_send {msgToBeSent}"
+                log_message = (log_message[:300] + "...") if len(log_message) > 300 else log_message
+                logger.debug(log_message)
+                # logger.debug("METHOD = SWSReceiver.process_message_to_send ")
 
-    def initialize_sender(self):
-        """BAsically does nothing
-        """
-        self.sender = WSSender(self.settings)
-        self.sender.initialize()
-        self.status = ConnectionStatus.INITIALIZED
+                return msgToBeSent
+            except IndexError:
+                await asyncio.sleep(1)
+                continue
+            except Exception as ex:
+                logger.exception("$$ EXCEPTION IN process_message_to_send")
+                print(ex)
 
-    def start(self):
-        """
-        Set self.started to True
-        """
-        # self.start_receiver()
-        self.started = True
-        self.status = ConnectionStatus.STARTED
-        self.SENDER_STATUS = ConnectionStatus.STARTED
-
-    def stop(self):
-        """
-        set self.stopped to true
-        """
-        self.stopped = True
-        logger.debug('Stopping Receiver and Sender')
-        pSendEvt.set()
-        self.status = ConnectionStatus.STOPPED
-        try:
-            if self.stop1 is not None:
-                self.stop1.cancel()
-            if self.stop2 is not None:
-                self.stop2.set_result(None)
-            if self.mainLoop is not None:
-                self.mainLoop.call_soon_threadsafe(self.mainLoop.stop)
-        except Exception as ex:
-            logger.error("Exception when stopping connection: " + str(ex))
+    async def process_received_message(self, message):
+        log_message = f"METHOD = SWSReceiver.process_received_message {message}"
+        log_message = (log_message[:300] + "...") if len(log_message) > 300 else log_message
+        logger.debug(log_message)
+        self.messageInTransit = None
+        recv_msg = Message.deserialize(message)
+        message_type = str(recv_msg.message_type)
+        logger.debug("METHOD = SWSReceiver.process_received_message type = " + message_type)
+
+        if recv_msg.message_type == MessageType.STOP.value:
+            handler, kwargs = self.router.get_handler(request_path=message_type)
+            if handler is not None:
+                res_message = handler(recv_msg)
+
+            self.sender.send_message(recv_msg.sender_info, res_message, False)
+            self.stopping = True
+            return
+
+        # check if error
+        if recv_msg.message_type == MessageType.ERROR_AUTH.value:
+            logger.info("Authorization Error. Please verify ip address, training ID, RTS, project permission and token")
+            self.stopped = True
+            party_rbuffer.append(recv_msg)
+            pRecvEvt.set()
 
-    def get_connection_config(self):
-        pass
+            return
 
-class WSReceiver(FLReceiver):
+        # if recv_msg.sender_info is not None and recv_msg.sender_info == self.party:
+        #     logger.info("Received Message is reply to previously sent message")
+        #     return
+
+        # msg_data = recv_msg.get_data()
+        # if msg_data is not None and msg_data.get('status', None) is not None:
+        #     res_message = self.handle_response_message()
+        #     logger.info("status = " + msg_data.get('status'))
+        #     self.sender.send_message(recv_msg.sender_info, res_message, False)
+
+        if not self.registered:
+            if recv_msg.message_type == MessageType.REGISTER.value:
+                logger.info("Processing REGISTER")
+                party_rbuffer.appendleft(recv_msg)
+                logger.info("Signalling for REGISTER")
+                pRecvEvt.set()
+                logger.info("Waiting for REGISTER to be popped to clear backlog")
+                pSendEvt.wait()
+                logger.debug("############# pSendEvt activated")
+                self.registered = True
+                try:
+                    while back_message := party_rbuffer.popleft():
+                        logger.debug("Handling back_message {}".format(back_message))
+                        self.handle_message(back_message)
+                except IndexError:
+                    logger.debug("Popped backlog")
+                logger.debug("Clearing send evt")
+                pSendEvt.clear()
+            else:
+                logger.info("Not registered - stashing message")
+                party_rbuffer.append(recv_msg)
 
-    def __init__(self, router, settings, connection):
-        """
-        Does nothing
-        """
+            return
 
-        self.router = router
-        self.settings = settings
-        self.connection = connection
+        self.handle_message(recv_msg)
 
-    def shutdown_server(self):
-        pass
+    def handle_message(self, recv_msg):
+        message_type = str(recv_msg.message_type)
 
-    def initialize(self):
-        """
-        Does nothing
-        """
-        logger.debug('Initializing WSReceiver')
+        handler, kwargs = self.router.get_handler(request_path=message_type)
 
+        logger.debug("Handler for message type {} is {}".format(message_type, handler.__name__))
 
-    def receive_message(self, party):
-        logger.error('OOOOO We are in trouble. This should not be called')
+        if handler is None:
+            party_rbuffer.append(recv_msg)
+            pRecvEvt.set()
 
-    def start(self):
-        pass
+        else:
+            if recv_msg.sender_info is not None:
+                if recv_msg.sender_info == self.party:
+                    logger.debug("Received Message is reply to previously sent message")
+                else:
+                    logger.warning("Received Message is reply for message from " + recv_msg.sender_info)
+                return
 
-    def stop(self):
-        pass
+            try:
+                res_message = handler(recv_msg)
+            except Exception as ex:
+                res_message = Message()
+                data = {"status": "error", "message": str(ex)}
+                res_message.set_data(data)
+
+            self.sender.send_message(recv_msg.sender_info, res_message, False)
+
+    def handle_stop_message(self):
+        logger.info("Received STOP request from aggregator")
+        response_msg = ResponseMessage(message_type=MessageType.STOP.value, id_request=-1, data={"ACK": True})
+        logger.info("received a STOP request")
+        time.sleep(30)
+        self.stopped = True
+        return response_msg
 
+    def handle_response_message(self):
+        logger.debug("Received response from aggregator with status payload")
+        response_msg = ResponseMessage(message_type=MessageType.ACK.value, id_request=88888, data={"ACK": True})
+        return response_msg
 
-class WSSender(FLSender):
 
-    """
-    Basic implementation using the request package of python to send
-    requests to Rest endpoints.
-    """
-
-    def __init__(self, settings):
-        """
-        Does nothing
-        """
+class SWSSender(FLSender):
+    def __init__(self, source_info, settings):
         self.settings = settings
-        logger.debug("WS Sender Init. My info is " + str(settings))
+        self.source_info = source_info
 
-    def initialize(self):
-        """Does all the setups required for the sender. This method should also
-        check for availability of resources, open ports, certificates etc.,
-        if required
-        """
-        logger.info('Websockets Sender initialized')
-
-    def send_message(self, destination, message):
-        """
-        used for sending all the requests. Message object should be
-        validated and endpoint should be decided based on message codes.
-        :param destination: information about the destination to which message
-        should be forwarded
-        :type destination: `dict`
-        :param message: message object constructed by aggregator/party
-        :type message: `Message`
-        :return : response object
-        :rtype : `Response`
-        """
-        if 'id' in self.settings:
-            message.add_sender_info(self.settings['id'])
-        else:
-            # set a default value to be Aggregator
-            message.add_sender_info('Aggregator')
-
-        serializer = SerializerFactory(SerializerTypes.JSON_PICKLE).build()
-        message = serializer.serialize(message)
+    def initialize(self, **kwargs):
+        pass
 
-        logger.info("Sending serialized message to aggregator")
+    def send_message(self, destination, message, expectResponse=False):
+        message.add_sender_info(self.source_info)
+        message_type = str(message.message_type)
+        logger.debug("SWSSender.send_message message_type = " + message_type + " to destination = " + str(destination))
+        if message.message_type == MessageType.REGISTER.value:
+            expectResponse = True
+            logger.debug("SWSSender.send_message set expectResponse to True for REGISTER message")
+        message = message.serialize()
         party_sbuffer.append(message)
-        pSendEvt.set()
-        pRecvEvt.wait()
-        response = None
-        try:
-            response = party_rbuffer.popleft()
-        except IndexError:
-            logger.debug("[WSSender] No message response received, after attempt to send message")
-
-        pRecvEvt.clear()
-        response_message = None
-        if response is not None:
-            # inspect response
-            if isinstance(response, str):
-                logger.info('Received String message as response: ' + response)
-                response_message = Message(
-                    MessageType.REGISTER.value, data={'status': 'success', 'response': response})
-            else:
-                # deserialize response
-                response_message = serializer.deserialize(response)
-                logger.info(
-                    'Received serialized message as response: ' + str(response_message))
+        # pSendEvt.set()
+        logger.debug("############# Thread switch")
+        if expectResponse:
+            logger.debug("############# Waiting for response")
+            pRecvEvt.wait()
+            logger.debug("############# pRecvEvt activated")
+            response_message = party_rbuffer.popleft()
+            pRecvEvt.clear()
+            logger.debug("Received serialized message as response: " + str(response_message))
+            pSendEvt.set()
 
-        return response_message
+            return response_message
 
     def cleanup(self):
-        """
-        Cleanup the Sender object and close the hanging connections if any
-        """
-        logger.info('Cleaning up Websockets Client')
+        pass
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/data/data_handler.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/data/data_handler.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,19 +1,28 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
-import logging
 import abc
+import logging
+
 import numpy
 
-from ibmfl.data.data_util import get_min, get_max, get_mean,\
-    get_var, get_std, get_quantile, get_normalizer, get_standardscaler, \
-    get_minmaxscaler
+from ibmfl.data.data_util import (
+    get_max,
+    get_mean,
+    get_min,
+    get_minmaxscaler,
+    get_normalizer,
+    get_quantile,
+    get_standardscaler,
+    get_std,
+    get_var,
+)
 from ibmfl.exceptions import FLException
 
 logger = logging.getLogger(__name__)
 
 
 class DataHandler(abc.ABC):
     """
@@ -34,18 +43,48 @@
         """
         Access the local dataset and return the training and testing dataset
         as a tuple.
 
         :param kwargs:
         :return: `tuple`. (training_set, testing_set)
         """
-        raise NotImplemented
+        raise NotImplementedError
+
+    def get_train_counts(self):
+        """
+        Returns the training sample size.
+
+        :return: The training sample size
+        :rtype: `int`
+        """
+        if self.x_train is not None and isinstance(self.x_train, numpy.ndarray):
+            return self.x_train.shape[0]
+        else:
+            logger.info(
+                "The attribute x_train is None or is not of numpy.ndarray format!"
+                "Trying to access training counts assuming data is loaded \
+                as DataGenerator type."
+            )
+            try:
+                counts = len(self.training_generator.filenames)
+                assert isinstance(counts, int)
+                return counts
+            except:
+                raise FLException("Error ocurred during accessing the training sample size.")
+
+    def get_val_data(self, **kwargs):
+        """
+        Access the local dataset and return the validation dataset
+        as a tuple.
+        :param kwargs:
+        :return: `tuple`. (validation_set)
+        """
+        pass
 
-    def get_statistics_of_training_data(self, sample_data_schema,
-                                        lst_stats_name, **kwargs):
+    def get_statistics_of_training_data(self, sample_data_schema, lst_stats_name, **kwargs):
         """
         Return the corresponding statistics, which is specified by the
         provided list of statistics names, of the local training dataset.
 
         :param sample_data_schema: Provided data with only feature values. \
         Assuming the dataset has shape (num_samples, num_features).
         :type sample_data_schema: `np.array`
@@ -57,56 +96,56 @@
         :return: The requested statistics based on the local dataset.
         :rtype: `dict`
         """
         if sample_data_schema is None and self.x_train is None:
             raise FLException("No data is provided!")
         elif sample_data_schema is None and self.x_train is not None:
             sample_data_schema = self.x_train
-            logger.warning('No dataset is provided, use the data schema from '
-                           'training dataset(x_train) as the default one.')
+            logger.warning(
+                "No dataset is provided, use the data schema from " "training dataset(x_train) as the default one."
+            )
 
         if type(lst_stats_name) is not list:
-            raise FLException("list of requested statistics badly form. "
-                              "It should of type list, but it is instead "
-                              "of type " + str(type(lst_stats_name)))
+            raise FLException(
+                "list of requested statistics badly form. "
+                "It should of type list, but it is instead "
+                "of type " + str(type(lst_stats_name))
+            )
         list_stats = {}
         if not isinstance(sample_data_schema, numpy.ndarray):
-            raise FLException("Expecting the local dataset to be of type "
-                              "numpy.ndarray, instead it is of type " +
-                              str(type(sample_data_schema)))
+            raise FLException(
+                "Expecting the local dataset to be of type "
+                "numpy.ndarray, instead it is of type " + str(type(sample_data_schema))
+            )
 
         while len(lst_stats_name) != 0:
             tmp_stat_name = lst_stats_name.pop()
             if type(tmp_stat_name) is not str:
-                logger.warning("Skipping the current requested statistics "
-                               "name. It should be of type string, "
-                               "but now it is instead of type" +
-                               str(type(tmp_stat_name)))
-            elif tmp_stat_name == 'min' or tmp_stat_name == 'minimum':
+                logger.warning(
+                    "Skipping the current requested statistics "
+                    "name. It should be of type string, "
+                    "but now it is instead of type" + str(type(tmp_stat_name))
+                )
+            elif tmp_stat_name == "min" or tmp_stat_name == "minimum":
                 list_stats[tmp_stat_name] = get_min(sample_data_schema)
-            elif tmp_stat_name == 'max' or tmp_stat_name == 'maximum':
+            elif tmp_stat_name == "max" or tmp_stat_name == "maximum":
                 list_stats[tmp_stat_name] = get_max(sample_data_schema)
-            elif tmp_stat_name == 'mean':
+            elif tmp_stat_name == "mean":
                 list_stats[tmp_stat_name] = get_mean(sample_data_schema)
-            elif tmp_stat_name == 'var' or tmp_stat_name == 'variance':
+            elif tmp_stat_name == "var" or tmp_stat_name == "variance":
                 list_stats[tmp_stat_name] = get_var(sample_data_schema)
-            elif tmp_stat_name == 'std' or \
-                    tmp_stat_name == 'standard deviation':
+            elif tmp_stat_name == "std" or tmp_stat_name == "standard deviation":
                 list_stats[tmp_stat_name] = get_std(sample_data_schema)
-            elif tmp_stat_name == 'quantile':
-                if kwargs and 'q' in kwargs:
-                    list_stats[tmp_stat_name] = get_quantile(
-                        sample_data_schema,
-                        percentage=kwargs['q'])
+            elif tmp_stat_name == "quantile":
+                if kwargs and "q" in kwargs:
+                    list_stats[tmp_stat_name] = get_quantile(sample_data_schema, percentage=kwargs["q"])
                 else:
-                    raise FLException('Cannot compute quantile, '
-                                      'missing quantile requirement.')
+                    raise FLException("Cannot compute quantile, " "missing quantile requirement.")
             else:
-                logger.warning("Current required statistics "
-                               "is not supported. Skipping...")
+                logger.warning("Current required statistics " "is not supported. Skipping...")
 
         return list_stats
 
     def get_preprocessor(self, sample_data_schema, preprocessor_name, **kwargs):
         """
         Set the data preprocessor of the data handler class as the requested
         type of preprocessor. The supported preprocessors
@@ -126,48 +165,43 @@
         :type kwargs: `dict`
         :return: None
         """
         if sample_data_schema is None and self.x_train is None:
             raise FLException("No data is provided!")
         elif sample_data_schema is None and self.x_train is not None:
             sample_data_schema = self.x_train
-            logger.warning('No dataset is provided, use the data schema from '
-                           'training dataset(x_train) as the default one.')
+            logger.warning(
+                "No dataset is provided, use the data schema from " "training dataset(x_train) as the default one."
+            )
 
         if type(preprocessor_name) is not str:
-            raise FLException("Expecting the requested preprocessor "
-                              "to be of type string, instead it is of type" +
-                              str(type(preprocessor_name)))
+            raise FLException(
+                "Expecting the requested preprocessor "
+                "to be of type string, instead it is of type" + str(type(preprocessor_name))
+            )
 
         if not isinstance(sample_data_schema, numpy.ndarray):
-            raise FLException("Expecting the local dataset to be of type "
-                              "numpy.ndarray, instead it is of type " +
-                              str(type(sample_data_schema)))
-
-        if preprocessor_name == 'normalizer' or \
-                preprocessor_name == 'normalization':
-            if 'norm' in kwargs:
-                self.preprocessor = get_normalizer(sample_data_schema,
-                                                   norm=kwargs['norm'])
+            raise FLException(
+                "Expecting the local dataset to be of type "
+                "numpy.ndarray, instead it is of type " + str(type(sample_data_schema))
+            )
+
+        if preprocessor_name == "normalizer" or preprocessor_name == "normalization":
+            if "norm" in kwargs:
+                self.preprocessor = get_normalizer(sample_data_schema, norm=kwargs["norm"])
             else:
                 self.preprocessor = get_normalizer(sample_data_schema)
-        elif preprocessor_name == 'standardscaler' or \
-                preprocessor_name == 'standardization':
+        elif preprocessor_name == "standardscaler" or preprocessor_name == "standardization":
             mean_val = None
             std = None
-            if 'mean' in kwargs:
-                mean_val = kwargs['mean']
-            if 'scale' in kwargs:
-                std = kwargs['scale']
-            self.preprocessor = get_standardscaler(sample_data_schema,
-                                                   mean_val=mean_val,
-                                                   std=std)
-        elif preprocessor_name == 'minmaxscaler':
-            if 'feature_range' in kwargs:
-                self.preprocessor = get_minmaxscaler(
-                    sample_data_schema,
-                    feature_range=kwargs['feature_range'])
+            if "mean" in kwargs:
+                mean_val = kwargs["mean"]
+            if "scale" in kwargs:
+                std = kwargs["scale"]
+            self.preprocessor = get_standardscaler(sample_data_schema, mean_val=mean_val, std=std)
+        elif preprocessor_name == "minmaxscaler":
+            if "feature_range" in kwargs:
+                self.preprocessor = get_minmaxscaler(sample_data_schema, feature_range=kwargs["feature_range"])
             else:
                 self.preprocessor = get_minmaxscaler(sample_data_schema)
         else:
-            logger.warning("Required preprocessor is not supported. "
-                           "Skipping...")
+            logger.warning("Required preprocessor is not supported. " "Skipping...")
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/data/data_util.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/federated_learning/data_util.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,22 +1,24 @@
+#  -----------------------------------------------------------------------------------------
+#  (C) Copyright IBM Corp. 2023-2024.
+#  https://opensource.org/licenses/BSD-3-Clause
+#  -----------------------------------------------------------------------------------------
+
 """
 Module providing utility functions helpful for preproccessing data
 """
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 20212022.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
 import numpy as np
-import pandas as pd
-from sklearn import preprocessing
-from diffprivlib import tools as dp
 import logging
 
-from ibmfl.exceptions import FLException
+from ibm_watsonx_ai.federated_learning.FLExceptions import FLException
 
 logger = logging.getLogger(__name__)
 
 # TODO get dp stats
 
 
 def get_min(data, **kwargs):
@@ -34,16 +36,17 @@
     :return: A vector of shape (1, num_features) stores the minimum value \
     of each feature across all samples.
     :rtype: `np.array` of `float`
     """
     try:
         min_vec = np.min(data, axis=0)
     except Exception as ex:
-        raise FLException('Error occurred when calculating '
-                          'the minimum value. ' + str(ex))
+        raise FLException(
+            "Error occurred when calculating " "the minimum value. " + str(ex)
+        )
     return min_vec
 
 
 def get_max(data, **kwargs):
     """
     Assuming the dataset is loaded as type `np.array`, and has shape
      (num_samples, num_features).
@@ -58,16 +61,17 @@
     :return: A vector of shape (1, num_features) stores the maximum value \
     of each feature across all samples.
     :rtype: `np.array` of `float`
     """
     try:
         max_vec = np.max(data, axis=0)
     except Exception as ex:
-        raise FLException('Error occurred when calculating '
-                          'the maximum value. ' + str(ex))
+        raise FLException(
+            "Error occurred when calculating " "the maximum value. " + str(ex)
+        )
     return max_vec
 
 
 def get_mean(data, **kwargs):
     """
     Assuming the dataset is loaded as type `np.array`, and has shape
      (num_samples, num_features).
@@ -82,16 +86,17 @@
     :return: A vector of shape (1, num_features) stores the maximum value \
     of each feature across all samples.
     :rtype: `np.array` of `float`
     """
     try:
         var_vec = np.var(data, axis=0)
     except Exception as ex:
-        raise FLException('Error occurred when calculating '
-                          'the mean value. ' + str(ex))
+        raise FLException(
+            "Error occurred when calculating " "the mean value. " + str(ex)
+        )
     return var_vec
 
 
 def get_var(data, **kwargs):
     """
     Assuming the dataset is loaded as type `np.array`, and has shape
      (num_samples, num_features).
@@ -106,16 +111,15 @@
     :return: A vector of shape (1, num_features) stores the variance
     of each feature across all samples.
     :rtype: `np.array` of `float`
     """
     try:
         var_vec = np.var(data, axis=0)
     except Exception as ex:
-        raise FLException('Error occurred when calculating '
-                          'the variance. ' + str(ex))
+        raise FLException("Error occurred when calculating " "the variance. " + str(ex))
     return var_vec
 
 
 def get_std(data, **kwargs):
     """
     Assuming the dataset is loaded as type `np.array`, and has shape
      (num_samples, num_features).
@@ -130,16 +134,17 @@
     :return: A vector of shape (1, num_features) stores the
     standard deviation of each feature across all samples.
     :rtype: `np.array` of `float`
     """
     try:
         std_vec = np.std(data, axis=0)
     except Exception as ex:
-        raise FLException('Error occurred when calculating '
-                          'the standard deviation. ' + str(ex))
+        raise FLException(
+            "Error occurred when calculating " "the standard deviation. " + str(ex)
+        )
     return std_vec
 
 
 def get_quantile(data, percentage, **kwargs):
     """
     Assuming the dataset is loaded as type `np.array`, and has shape
      (num_samples, num_features).
@@ -157,20 +162,19 @@
     :return: A vector of shape (1, num_features) stores the
     standard deviation of each feature across all samples.
     :rtype: `np.array` of `float`
     """
     try:
         quantile_vec = np.quantile(data, q=percentage, axis=0)
     except Exception as ex:
-        raise FLException('Error occurred when calculating '
-                          'the quantile. ' + str(ex))
+        raise FLException("Error occurred when calculating " "the quantile. " + str(ex))
     return quantile_vec
 
 
-def get_normalizer(data, norm='l2'):
+def get_normalizer(data, norm="l2"):
     """
     Obtain the normalizer that perform the normalization preprocessing
     technique across all features via sklearn.preprocessing.normalizer API.
     A normalizer will scale a dataset w.r.t. features to unit norm.
 
     :param data: Provided dataset, assume each row is a data sample and \
     each column is one feature.
@@ -179,22 +183,23 @@
     By default, norm is set to `l2`.
     :type norm: `str`
     :return: The normalizer preprocessor that can be applied to perform \
     normalizing preprocessing step for the party's local dataset \
     via `transform` method.
     :rtype: `sklearn.preprocessing.data.Normalizer`
     """
+    from sklearn import preprocessing
+
     try:
         normalizer = preprocessing.Normalizer(norm=norm).fit(data)
 
         # test the normalizer
         normalizer.transform(data)
     except Exception as ex:
-        raise FLException('Error occurred when obtaining '
-                          'the normalizer. ' + str(ex))
+        raise FLException("Error occurred when obtaining " "the normalizer. " + str(ex))
     return normalizer
 
 
 def get_standardscaler(data, mean_val=None, std=None):
     """
     Obtain the StandardScaler that perform the standardization preprocessing
     technique with provided mean and standard deviation values
@@ -214,29 +219,35 @@
     The vector should be of shape (1, num_features).
     :type std: `np.ndarray`
     :return: The standard scaler preprocessor that can be applied to perform \
     standardization preprocessing step for the party's local dataset \
     via `transform` method.
     :rtype: 'sklearn.preprocessing.data.StandardScaler'
     """
+    from sklearn import preprocessing
+
     try:
         scaler = preprocessing.StandardScaler().fit(data)
 
         # set scaler with correct mean_val and std values
         if mean_val is not None:
-            logger.info("Set mean_val value of the StandardScaler "
-                        "as the provided mean...")
+            logger.info(
+                "Set mean_val value of the StandardScaler " "as the provided mean..."
+            )
             scaler.mean_ = mean_val
         if std is not None:
-            logger.info("Set standard deviation of the StandardScaler "
-                        "as the provided standard deviation...")
+            logger.info(
+                "Set standard deviation of the StandardScaler "
+                "as the provided standard deviation..."
+            )
             scaler.scale_ = std
     except Exception as ex:
-        raise FLException('Error occurred when obtaining '
-                          'the standardscaler. ' + str(ex))
+        raise FLException(
+            "Error occurred when obtaining " "the standardscaler. " + str(ex)
+        )
 
     return scaler
 
 
 def get_minmaxscaler(data, feature_range=(0, 1)):
     """
     Obtain a MinMaxScaler that perform the MinMaxScale preprocessing technique
@@ -254,20 +265,22 @@
     :param feature_range: Desired range of transformed data.
     :type feature_range: tuple (min, max), default=(0, 1)
     :return: The minmaxscaler preprocessor that can be applied to perform \
     minmax scaling preprocessing step for the party's local dataset \
     via `transform` method.
     :rtype: `sklearn.preprocessing.data.MinMaxScaler`
     """
+    from sklearn import preprocessing
+
     try:
-        scaler = preprocessing.MinMaxScaler(feature_range=feature_range).\
-            fit(data)
+        scaler = preprocessing.MinMaxScaler(feature_range=feature_range).fit(data)
     except Exception as ex:
-        raise FLException('Error occurred when obtaining '
-                          'the minmaxcaler. ' + str(ex))
+        raise FLException(
+            "Error occurred when obtaining " "the minmaxcaler. " + str(ex)
+        )
     return scaler
 
 
 def get_reweighing_weights(data, sensitive_attribute, columns):
     """
     Calculates reweighing weights for points, assuming:
     * privileged group has sensitive attribute value = 1, unprivileged group is 0
@@ -280,62 +293,66 @@
     :param sensitive_attribute: Sensitive attribute
     :type sensitive_attribute: `str`
     :param columns: dataset column names
     :type columns: `list`
     :return: weights
     :rtype: `np.array`
     """
+    # additional imports for reweighing algorithms
+    import pandas as pd
+
     (features, labels) = data
 
     training_dataset = pd.DataFrame(data=features)
     class_values = labels.tolist()
 
     training_dataset.columns = columns
-    training_dataset['class'] = class_values
+    training_dataset["class"] = class_values
 
     nrows = training_dataset.shape[0]
 
-    priv = sum(training_dataset[sensitive_attribute])/nrows
-    unpriv = nrows - sum(training_dataset[sensitive_attribute])/nrows
-    pos = sum(training_dataset['class'])/nrows
-    neg = nrows - sum(training_dataset['class'])/nrows
+    priv = sum(training_dataset[sensitive_attribute]) / nrows
+    unpriv = nrows - sum(training_dataset[sensitive_attribute]) / nrows
+    pos = sum(training_dataset["class"]) / nrows
+    neg = nrows - sum(training_dataset["class"]) / nrows
 
     tmp_unp_train_data = training_dataset[training_dataset[sensitive_attribute] == 0]
     tmp_p_train_data = training_dataset[training_dataset[sensitive_attribute] == 1]
 
     if len(tmp_unp_train_data) > 0:
-        unpriv_neg = tmp_unp_train_data['class'].value_counts()[0]/nrows
-        if sum(tmp_unp_train_data['class']) > 0:
-            unpriv_pos = tmp_unp_train_data['class'].value_counts()[1]/nrows
+        unpriv_neg = tmp_unp_train_data["class"].value_counts()[0] / nrows
+        if sum(tmp_unp_train_data["class"]) > 0:
+            unpriv_pos = tmp_unp_train_data["class"].value_counts()[1] / nrows
         else:
             unpriv_pos = 0
     else:
         unpriv_neg = 0
     if len(tmp_p_train_data) > 0:
-        priv_neg = tmp_p_train_data['class'].value_counts()[0]/nrows
-        if sum(tmp_p_train_data['class']) > 0:
-            priv_pos = tmp_p_train_data['class'].value_counts()[1]/nrows
+        priv_neg = tmp_p_train_data["class"].value_counts()[0] / nrows
+        if sum(tmp_p_train_data["class"]) > 0:
+            priv_pos = tmp_p_train_data["class"].value_counts()[1] / nrows
         else:
             priv_pos = 0
     else:
         priv_pos = 0
 
     weight = []
     for index, row in training_dataset.iterrows():
-        if row[sensitive_attribute] == 0 and row['class'] == 0:
+        if row[sensitive_attribute] == 0 and row["class"] == 0:
             weight.append(unpriv * neg / unpriv_neg)
-        elif row[sensitive_attribute] == 0 and row['class'] == 1:
+        elif row[sensitive_attribute] == 0 and row["class"] == 1:
             weight.append(unpriv * pos / unpriv_pos)
-        elif row[sensitive_attribute] == 1 and row['class'] == 0:
+        elif row[sensitive_attribute] == 1 and row["class"] == 0:
             weight.append(priv * neg / priv_neg)
-        elif row[sensitive_attribute] == 1 and row['class'] == 1:
+        elif row[sensitive_attribute] == 1 and row["class"] == 1:
             weight.append(priv * pos / priv_pos)
 
     return np.array(weight)
 
+
 def get_hist_counts(data, sensitive_attribute, columns, eps):
     """
     Calculates noisy counts for reweighing with differential privacy (epsilon set in
     datahandler), assuming:
     * privileged group has sensitive attribute value = 1, unprivileged group is 0
     * positive class has value = 1, negative class is 0
     weight = P_expected(sensitive_attribute & class)/P_observed(sensitive_attribute & class)
@@ -348,46 +365,50 @@
     :param columns: dataset column names
     :type columns: `list`
     :param eps: epsilon utilized for differential privacy
     :type eps: `float`
     :return: counts
     :rtype: `dict`
     """
+    # additional imports for global reweighing with differential privacy
+    import pandas as pd
+    from diffprivlib import tools as dp
+
     (features, labels) = data
     counts = {}
 
     training_dataset = pd.DataFrame(data=features)
     class_values = labels.tolist()
 
     training_dataset.columns = columns
-    training_dataset['class'] = class_values
+    training_dataset["class"] = class_values
 
-    data_whole = training_dataset[[sensitive_attribute, 'class']]
+    data_whole = training_dataset[[sensitive_attribute, "class"]]
     data_whole_0 = data_whole[data_whole[sensitive_attribute] == 0]
     data_whole_0_t = data_whole_0.T
 
     data_whole_0_t = np.array(data_whole_0_t)
     dp_hist_0, dp_bins_0 = dp.histogram(data_whole_0_t[1], bins=2, epsilon=eps)
-    counts['unp_neg'] = dp_hist_0[0]
-    counts['unp_pos'] = dp_hist_0[1]
+    counts["unp_neg"] = dp_hist_0[0]
+    counts["unp_pos"] = dp_hist_0[1]
 
     data_whole_1 = data_whole[data_whole[sensitive_attribute] == 1]
     data_whole_1_t = data_whole_1.T
 
     data_whole_1_t = np.array(data_whole_1_t)
     dp_hist_1, dp_bins_1 = dp.histogram(data_whole_1_t[1], bins=2, epsilon=eps)
-    counts['p_neg'] = dp_hist_1[0]
-    counts['p_pos'] = dp_hist_1[1]
+    counts["p_neg"] = dp_hist_1[0]
+    counts["p_pos"] = dp_hist_1[1]
 
     tmp_unp_train_data = training_dataset[training_dataset[sensitive_attribute] == 0]
     tmp_p_train_data = training_dataset[training_dataset[sensitive_attribute] == 1]
-    unpriv_neg = tmp_unp_train_data['class'].value_counts()[0]
-    unpriv_pos = tmp_unp_train_data['class'].value_counts()[1]
-    priv_neg = tmp_p_train_data['class'].value_counts()[0]
-    priv_pos = tmp_p_train_data['class'].value_counts()[1]
-
-    counts['unpriv_neg'] = unpriv_neg
-    counts['unpriv_pos'] = unpriv_pos
-    counts['priv_neg'] = priv_neg
-    counts['priv_pos'] = priv_pos
+    unpriv_neg = tmp_unp_train_data["class"].value_counts()[0]
+    unpriv_pos = tmp_unp_train_data["class"].value_counts()[1]
+    priv_neg = tmp_p_train_data["class"].value_counts()[0]
+    priv_pos = tmp_p_train_data["class"].value_counts()[1]
+
+    counts["unpriv_neg"] = unpriv_neg
+    counts["unpriv_pos"] = unpriv_pos
+    counts["priv_neg"] = priv_neg
+    counts["priv_pos"] = priv_pos
 
     return counts
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/model/pytorch_fl_model.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/model/pytorch_fl_model.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,42 +1,41 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
-import torch
+import copy
 import logging
 import time
-import copy
 from importlib import import_module
 
 import numpy as np
+import torch
+import torch.onnx
 from sklearn import metrics
 from skorch import NeuralNet
 from skorch.callbacks import EpochScoring
+from skorch.dataset import CVSplit, Dataset
 from skorch.exceptions import NotInitializedError
+from skorch.helper import predefined_split
 
+from ibmfl.exceptions import FLException, LocalTrainingException, ModelException, ModelInitializationException
 from ibmfl.model.fl_model import FLModel
 from ibmfl.model.model_update import ModelUpdate
 from ibmfl.util import config
-from ibmfl.exceptions import LocalTrainingException, ModelException, FLException
 
 logger = logging.getLogger(__name__)
 
 
 class PytorchFLModel(FLModel):
     """
     Wrapper class for importing a Pytorch based model
     """
 
-    def __init__(self, model_name,
-                 model_spec=None,
-                 pytorch_module=None,
-                 module_init_params=None,
-                 **kwargs):
+    def __init__(self, model_name, model_spec=None, pytorch_module=None, module_init_params=None, **kwargs):
         """
         Create a `PytorchFLModel` instance from a Pytorch model.
         If pytorch_model is provided, it will use it; otherwise it will take
         the model_spec to create the model.
 
         :param model_name: String specifying the type of model e.g., Pytorch_NN
         :type model_name: `str`
@@ -50,46 +49,61 @@
         :type module_init_params: 'dict'
         :param kwargs: A dictionary contains other parameter settings on \
          to initialize a PyTorch model.
         :type kwargs: `dict`
         """
         super().__init__(model_name, model_spec, **kwargs)
 
-        self.model_type = 'PyTorch'
+        self.model_type = "PyTorch"
         self.model = None
         self.module = None
-        self.optimizer = torch.optim.SGD
-        self.criterion = torch.nn.NLLLoss
+        self.input_shape = None
         if pytorch_module is None:
             if model_spec is None or (not isinstance(model_spec, dict)):
-                raise ValueError('Initializing model requires '
-                                 'a model specification or uninstantiated '
-                                 'pytorch model class reference '
-                                 'None was provided')
+                raise ValueError(
+                    "Initializing model requires "
+                    "a model specification or uninstantiated "
+                    "pytorch model class reference "
+                    "None was provided"
+                )
             # In this case we need to recreate the model from model_spec
             self.module = self.load_model_from_spec(model_spec)
         else:
             self.module = pytorch_module
-        self.model = self.initialize_model(self.module, self.optimizer,
-                                           self.criterion, module_init_params)
+
+        self.is_classification = True if not model_spec else model_spec.get("is_classification", True)
+        if ( self.is_classification ) :
+            self.default_optimizer = torch.optim.SGD 
+            self.default_criterion = torch.nn.NLLLoss
+            self.default_metric = metrics.accuracy_score
+        else :
+            self.default_optimizer = torch.optim.Adam
+            self.default_criterion = torch.nn.MSELoss
+            self.default_metric = metrics.r2_score
+
+        self.optimizer = self.load_optimizer_from_spec(model_spec)
+        self.criterion = self.load_loss_criterion_from_spec(model_spec)
+
+        self.model = self.initialize_model(self.module, self.optimizer, self.criterion, module_init_params)
 
         if self.use_gpu_for_training and torch.cuda.device_count() > 0:
             if self.num_gpus > torch.cuda.device_count():
-                logger.error('Selected number of gpus to use for training exceeds number of available gpus, ' +
-                             str(torch.cuda.device_count()) +
-                             'Set number of gpus to maximum available on device.')
+                logger.error(
+                    "Selected number of gpus to use for training exceeds number of available gpus, "
+                    + str(torch.cuda.device_count())
+                    + "Set number of gpus to maximum available on device."
+                )
                 self.num_gpus = torch.cuda.device_count()
 
             device_ids = list(range(self.num_gpus))
             self.model.module_ = torch.nn.DataParallel(self.model.module_, device_ids=device_ids)
-            self.model.set_params(device='cuda')
-            self.model.module_.to('cuda')
+            self.model.set_params(device="cuda")
+            self.model.module_.to("cuda")
 
-    def initialize_model(self, pytorch_module, optimizer, criterion,
-                         module_init_params=None):
+    def initialize_model(self, pytorch_module, optimizer, criterion, module_init_params=None):
         """
         Initializes a pytorch model via skorch library
 
         :param pytorch_module: uninstantiated pytorch model class
         :type pytorch_module: torch.nn.Module class reference
         :param optimizer: the optimizer to use
         :type optimizer: pytorch optimizer class
@@ -105,24 +119,26 @@
         if module_init_params is None:
             module_init_params = {}
         model = NeuralNet(
             module=pytorch_module,
             optimizer=optimizer,
             criterion=criterion,
             warm_start=True,
-            callbacks=[('valid_acc', EpochScoring(self.valid_acc,
-                                                  lower_is_better=False,
-                                                  use_caching=True,
-                                                  name='test_valid_acc'))],
+            callbacks=[
+                (
+                    "valid_acc",
+                    EpochScoring(self.valid_acc, lower_is_better=False, use_caching=True, name="test_valid_acc"),
+                )
+            ],
             **module_init_params,
         )
         model.initialize()
         return model
 
-    def fit_model(self, train_data, fit_params=None, **kwargs):
+    def fit_model(self, train_data, fit_params=None, validation_data=None, **kwargs):
         """
         Fits current model with provided training data.
 
         :param train_data: Training data, a tuple \
         given in the form (x_train, y_train). otherwise, input compatible with skorch.dataset.Dataset
         :type train_data: `(np.ndarray, np.ndarray)`
         :param fit_params: (optional) Dictionary with hyperparameters \
@@ -134,62 +150,80 @@
         :type fit_params: `dict`
         :return: None
         """
         # Initialized with default values
         batch_size = 128
         epochs = 1
         lr = 0.01
+        train_split = None
         optimizer_params = {}
-        hyperparams = fit_params.get('hyperparams', {}) if fit_params else {}
+        hyperparams = fit_params.get("hyperparams", {}) if fit_params else {}
+        party_params = kwargs.get("local_params", {}) or {} if fit_params else {}
 
         if hyperparams:
-            local_params = hyperparams.get('local', {}) or {}
-            training_hp = local_params.get('training', {}) or {}
-            
-            epochs = training_hp.get('epochs', epochs)
-            batch_size = training_hp.get('batch_size', batch_size)
-            lr = training_hp.get('lr', lr)
-            
-            if 'optimizer' in local_params:
-                optimizer = local_params['optimizer']
+            local_params = hyperparams.get("local", {}) or {}
+            training_hp = local_params.get("training", {}) or {}
+
+            epochs = training_hp.get("epochs", epochs)
+            batch_size = training_hp.get("batch_size", batch_size)
+            lr = training_hp.get("lr", lr)
+
+            if "validation_split" in training_hp:
+                validation_split = training_hp.get("validation_split")
+                try:
+                    if float(validation_split) != 0:
+                        train_split = CVSplit(float(validation_split), random_state=42)
+                except (TypeError, ValueError):
+                    raise ValueError("Validation split cannot be a NoneType")
+            if "validation_split" in party_params:
+                validation_split = party_params.get("validation_split")
+                try:
+                    if float(validation_split) != 0:
+                        train_split = CVSplit(float(validation_split), random_state=42)
+                except (TypeError, ValueError):
+                    raise ValueError("Validation split cannot be a NoneType")
+            if validation_data is not None:
+                validation_ds = Dataset(validation_data[0], validation_data[1])
+                train_split = predefined_split(validation_ds)
+
+            if "optimizer" in local_params:
+                optimizer = local_params["optimizer"]
                 if isinstance(optimizer, str):
                     try:
-                        optimizer = getattr(import_module(
-                            'torch.optim'), optimizer.split('.')[-1])
+                        optimizer = getattr(import_module("torch.optim"), optimizer.split(".")[-1])
 
                     except Exception as e:
                         optimizer = self.optimizer
                         logger.exception(str(e))
-                        print('selected optimizer not found. using default')
+                        print("selected optimizer not found. using default")
                 if optimizer != self.optimizer:
                     self.optimizer = optimizer
                     self.model.set_params(optimizer=self.optimizer)
 
                     if self.use_gpu_for_training and torch.cuda.device_count() > 0:
                         self.model.module_.to(self.model.device)
 
-            if 'criterion' in local_params:
-                criterion = local_params['criterion']
+            if "criterion" in local_params:
+                criterion = local_params["criterion"]
                 if isinstance(criterion, str):
                     try:
-                        criterion = getattr(import_module(
-                            'torch.nn'), criterion.split('.')[-1])
+                        criterion = getattr(import_module("torch.nn"), criterion.split(".")[-1])
 
                     except Exception as e:
                         criterion = self.criterion
                         logger.exception(str(e))
-                        print('selected criterion not found. using default')
+                        print("selected criterion not found. using default")
 
                 if criterion != self.criterion:
                     self.criterion = criterion
                     self.model.set_params(criterion=self.criterion)
 
-            if 'optimizer_params' in local_params:
-                optimizer_params = local_params['optimizer_params']
-        self.model.set_params(batch_size=batch_size, lr=lr, **optimizer_params)
+            if "optimizer_params" in local_params:
+                optimizer_params = local_params["optimizer_params"]
+        self.model.set_params(batch_size=batch_size, lr=lr, train_split=train_split, **optimizer_params)
 
         try:
             if type(train_data) is tuple:
                 # Extract x_train and y_train, by default,
                 # label is stored in the last column
                 x = train_data[0]
                 y = train_data[1]
@@ -197,47 +231,47 @@
 
             else:
                 # otherwise, expect that input is a pytorch Dataset generator
                 self.model.fit_loop(train_data, epochs=epochs)
 
         except Exception as e:
             logger.exception(str(e))
-            raise LocalTrainingException(
-                'Error occurred while performing model.fit')
+            raise LocalTrainingException("Error occurred while performing model.fit")
 
     def update_model(self, model_update):
         """
         Update model with provided model_update, where model_update
         should be generated according to `PytorchFLModel.get_model_update()`.
 
         :param model_update: `ModelUpdate` object that contains the weights \
         that will be used to update the model.
         :type model_update: `ModelUpdate`
         :return: None
         """
         if isinstance(model_update, ModelUpdate):
-            for p1, p2 in zip(self.get_weights(), model_update.get('weights')):
+            for p1, p2 in zip(self.get_weights(), model_update.get("weights")):
                 p1.data = torch.from_numpy(p2)
                 p1.data.requires_grad = True
 
             if self.use_gpu_for_training and torch.cuda.device_count() > 0:
                 self.model.module_.to(self.model.device)
         else:
-            raise ValueError('Provided model_update should be of type Model.'
-                             'Instead they are:{0}'.format(str(type(model_update))))
+            raise ValueError(
+                "Provided model_update should be of type Model." "Instead they are:{0}".format(str(type(model_update)))
+            )
 
     def get_model_update(self):
         """
         Generates a `ModelUpdate` object that will be sent to other entities.
 
         :return: ModelUpdate
         :rtype: `ModelUpdate`
         """
         weights = self.get_weights(to_numpy=True)
-        update = {'weights': weights}
+        update = {"input_shape": self.input_shape, "weights": weights}
 
         return ModelUpdate(**update)
 
     def predict(self, x):
         """
         Perform prediction for a batch of inputs. Note that for classification
         problems, it returns the resulting probabilities.
@@ -258,16 +292,16 @@
 
         :param x: A pytorch dataloader with a dataset for predicting
         :type x: 'torch.utils.data.DataLoader'
         :return: Array of predictions
         :rtype: 'np.ndarray'
         """
         predictions = None
-        for x_batch, _ in dataloader:
-            y_pred = self.model.evaluation_step(x_batch)
+        for batch in dataloader:
+            y_pred = self.model.evaluation_step(batch)
             y_pred = y_pred.numpy()
             if predictions is None:
                 predictions = y_pred
             else:
                 predictions = np.append(predictions, y_pred, 0)
 
         return predictions
@@ -303,40 +337,44 @@
         :type y: `np.ndarray`
         :param eval_metrics: A list of sklearn.metric class or a functions with the signature \
                         (y_true, y_pred, **kwargs) => float
         :type eval_metrics: `sklearn.metrics` or 'function'
         :return: dictionary of metrics
         :rtype: 'dict'
         """
+        self.input_shape = list(x.shape)
+        self.input_shape[0] = 1
+        
         if eval_metrics is None or len(eval_metrics) == 0:
-            eval_metrics = [metrics.accuracy_score]
+            eval_metrics = [self.default_metric]
 
         # get names for eval_metrics
-        metric_names = [getattr(metric, '__name__', repr(metric))
-                        for metric in eval_metrics]
+        metric_names = [getattr(metric, "__name__", repr(metric)) for metric in eval_metrics]
 
         y_pred = self.predict(x)
         y_pred_exp = np.exp(y_pred)
         y_pred_argmax = np.argmax(y_pred_exp, axis=1)
 
         try:
             metric_dict = dict(zip(metric_names, eval_metrics))
             # NOTE: Had to replace comprehension with for loop to tackle loss fn case which uses y_pred not argmax
             for metric, fn in metric_dict.items():
-                if metric is 'loss':
+                if metric == "loss":
                     metric_dict[metric] = fn(y, y_pred, self.model)
                 else:
                     metric_dict[metric] = fn(y, y_pred_argmax, **kwargs)
             # metric_dict = {metric_name: metric(y, y_pred, **kwargs) for metric_name, metric in
             #                zip(metric_names, eval_metrics)}
             return metric_dict
         except TypeError as exc:
             logger.exception(str(exc))
-            raise TypeError("eval_metrics must be an sklearn.eval_metrics class, or a function with the signature "
-                            "(y_true, y_pred, **kwargs)'")
+            raise TypeError(
+                "eval_metrics must be an sklearn.eval_metrics class, or a function with the signature "
+                "(y_true, y_pred, **kwargs)'"
+            )
         except ValueError as exc:
             logger.exception(exc)
             raise ValueError("arguments not in the correct format for metric")
 
     def evaluate_generator_model(self, dataloader, eval_metrics=None, **kwargs):
         """
         evaluates the model based on the provided dataloader
@@ -346,64 +384,85 @@
         must have the signature (x_batch, y_batch) => float
         :type eval_metrics: 'function'
         :return: dictionary of metrics
         :rtype: 'dict'
         """
         metric_score = 0
         for x_batch, y_batch in dataloader:
-            y_pred = self.model.evaluation_step(x_batch)
+            y_pred = self.model.evaluation_step((x_batch, y_batch))
 
             if eval_metrics is None:
                 y_pred = np.exp(y_pred)
                 y_pred = np.argmax(y_pred, axis=1)
-                equals = (y_pred == y_batch)
+                equals = y_pred == y_batch
                 metric_score += torch.mean(equals.type(torch.FloatTensor))
 
             else:
                 try:
                     metric_score += eval_metrics(x_batch, y_batch)
 
                 except TypeError as exc:
                     logger.exception(str(exc))
-                    raise TypeError("eval_metrics must be a function "
-                                    "with the signature (x_batch, y_batch, **kwargs)'")
+                    raise TypeError(
+                        "eval_metrics must be a function " "with the signature (x_batch, y_batch, **kwargs)'"
+                    )
 
-        metric_dict = {'metric_score': metric_score / len(dataloader)}
+        metric_dict = {"metric_score": metric_score / len(dataloader)}
 
         return metric_dict
 
-    def save_model(self, filename=None, optimizer_filename=None, history_filename=None):
+    def save_model(self, filename=None, optimizer_filename=None, history_filename=None, onnx_export=True):
         """
         Save a model to file in the format specific to the backend framework.
 
         :param filename: Name of the file that contains the model to be loaded.
         :type filename: `str`
         :param optimizer_filename: Name of the file that contains the optimizer to be loaded.
         :type optimizer_filename: `str`
         :param history_filename: Name of the file that contains the model history to be loaded.
         :type history_filename: `str`
         :return: None
         """
         if filename is None:
             file = self.model_name if self.model_name else self.model_type
-            filename = '{}_{}.pt'.format(file, time.time())
+            if onnx_export:
+                filename = "{}_{}.onnx".format(file, time.time())
+            else:
+                filename = "{}_{}.pt".format(file, time.time())
+
+        if onnx_export:
+            x = torch.randn(self.input_shape)
+            torch.onnx.export(
+                self.model.module_,
+                x,
+                super().get_model_absolute_path(filename),
+                export_params=True,
+                keep_initializers_as_inputs=True,
+            )
+        else:
+            f_params = super().get_model_absolute_path(filename)
+            f_optimizer = None
+            f_history = None
+            if optimizer_filename is not None:
+                f_optimizer = super().get_model_absolute_path(optimizer_filename)
+            if history_filename is not None:
+                f_history = super().get_model_absolute_path(history_filename)
+            self.model.save_params(f_params=f_params, f_optimizer=f_optimizer, f_history=f_history)
 
-        f_params = super().get_model_absolute_path(filename)
-        f_optimizer = None
-        f_history = None
-        if optimizer_filename is not None:
-            f_optimizer = super().get_model_absolute_path(optimizer_filename)
-        if history_filename is not None:
-            f_history = super().get_model_absolute_path(history_filename)
-        self.model.save_params(
-            f_params=f_params, f_optimizer=f_optimizer, f_history=f_history)
         return filename
 
-    def load_model(self, pytorch_module, model_filename, optimizer_filename=None, history_filename=None, optimizer=None,
-                   module_init_params=None):
+    def load_model(
+        self,
+        pytorch_module,
+        model_filename,
+        optimizer_filename=None,
+        history_filename=None,
+        optimizer=None,
+        module_init_params=None,
+    ):
         """
         Loads a model from disk given the specified file_name
 
         :param pytorch_module: uninstantiated pytorch model class
         :type pytorch_module: torch.nn.Module class reference
         :param model_filename: Name of the file that contains the model to be loaded.
         :type model_filename: `str`
@@ -423,55 +482,56 @@
         f_optimizer = None
         f_history = None
         if optimizer is not None and optimizer_filename is not None:
             self.optimizer = optimizer
             f_optimizer = optimizer_filename
         if history_filename is not None:
             f_history = history_filename
-        model = self.initialize_model(pytorch_module, self.optimizer, self.criterion,
-                                      module_init_params=module_init_params)
-        model.load_params(f_params=f_params,
-                          f_optimizer=f_optimizer, f_history=f_history)
+        model = self.initialize_model(
+            pytorch_module, self.optimizer, self.criterion, module_init_params=module_init_params
+        )
+        model.load_params(f_params=f_params, f_optimizer=f_optimizer, f_history=f_history)
 
         if self.use_gpu_for_training and torch.cuda.device_count() > 0:
             if self.num_gpus > torch.cuda.device_count():
-                logger.error('Selected number of gpus to use for training exceeds number of available gpus, ' +
-                             str(torch.cuda.device_count()) +
-                             'Set number of gpus to maximum available on device.')
+                logger.error(
+                    "Selected number of gpus to use for training exceeds number of available gpus, "
+                    + str(torch.cuda.device_count())
+                    + "Set number of gpus to maximum available on device."
+                )
                 self.num_gpus = torch.cuda.device_count()
             device_ids = list(range(self.num_gpus))
             model.module_ = torch.nn.DataParallel(model.module_, device_ids=device_ids)
-            model.set_params(device='cuda')
-            model.module_.to('cuda')
+            model.set_params(device="cuda")
+            model.module_.to("cuda")
 
         self.model = model
 
     def get_weights(self, to_numpy=False):
         """
         Returns the weights of the model
 
         :param to_numpy; Determines whether the weights should be returned as numpy array, or tensor
         :type to_numpy: `boolean`
         :return: list of model weights
         """
         if self.use_gpu_for_training and torch.cuda.device_count() > 0:
             module = copy.deepcopy(self.model.module_).cpu()
-
         else:
             module = self.model.module_
         if to_numpy:
             return self.parameters_to_numpy(module.parameters())
         return list(module.parameters())
 
     def parameters_to_numpy(self, params):
         """
         Transforms parameter tensors to numpy arrays
 
         :param params: The parameter tensor to be transformed
-        :return: numpy array of parameters 
+        :return: numpy array of parameters
         """
         np_params = []
         for layer in params:
             np_params.append(layer.detach().numpy())
         return np_params
 
     def load_model_from_spec(self, model_spec):
@@ -480,19 +540,74 @@
         that contains one item: model_spec['model_definition'], which has a
         pointer to the file where an nn.sequence container is saved
 
         :return: model
         :rtype: `nn.sequence`
         """
 
-        model_file = model_spec['model_definition']
+        model_file = model_spec["model_definition"]
         model_absolute_path = config.get_absolute_path(model_file)
         model = torch.load(model_absolute_path)
         return model
 
+    def load_optimizer_from_spec(self, model_spec):
+        """
+        Loads optimizer class from provided model_spec, where model_spec is a `dict`
+        that contains an item: model_spec['optimizer_class'], which has a
+        pointer to the file where an optimizer class object is saved
+
+        :return: model
+        :rtype: `torch.optim`
+        """
+        if model_spec is None or (not isinstance(model_spec, dict)) or model_spec.get("optimizer") is None:
+            logger.info("No optimizer found in the config file. Using default optimizer class: {}".format(getattr(self.default_optimizer,"__name__")))
+            optimizer = self.default_optimizer
+            return optimizer
+        else:
+            optimizer = model_spec.get("optimizer")
+        if not isinstance(optimizer, str):
+            raise ModelInitializationException("Optimizer is not specified as a string")
+        else:
+            try:
+                optimizer = getattr(import_module("torch.optim"), optimizer.split(".")[-1])
+            except Exception as e:
+                logger.exception(str(e))
+                logger.info("Selected optimizer not found. Using default optimizer class: {}".format(getattr(self.default_optimizer,"__name__")))
+                optimizer = self.default_optimizer
+
+        return optimizer
+
+    def load_loss_criterion_from_spec(self, model_spec):
+        """
+        Loads loss criterion from provided model_spec, where model_spec is a `dict`
+        that contains an item: model_spec['loss_criterion'], which has a
+        pointer to the file where an loss criterion class object is saved
+
+        :return: model
+        :rtype: `torch.nn` loss class object
+        """
+        if model_spec is None or (not isinstance(model_spec, dict)) or model_spec.get("loss_criterion") is None:
+            logger.info("No loss criterion found in the config file. Using default loss criterion class: {}".format(getattr(self.default_criterion,"__name__")))
+            criterion = self.default_criterion
+            return criterion
+        else:
+            criterion = model_spec.get("loss_criterion")
+        if not isinstance(criterion, str):
+            raise ModelInitializationException("Criterion is not specified as a string")
+        else:
+            try:
+                criterion = getattr(import_module("torch.nn"), criterion.split(".")[-1])
+            except Exception as e:
+                criterion = self.criterion
+                logger.exception(str(e))
+                logger.info("selected criterion not found.  Using default loss criterion class: {}".format(getattr(self.default_criterion,"__name__")))
+                criterion = self.default_criterion
+
+        return criterion
+
     def get_gradient(self, train_data):
         """
         Returns the gradients for each layer in the model
 
         :return: gradients
         :rtype: `list`. numpy array list of model's gradients
         """
@@ -544,54 +659,47 @@
         :type layer_idx: `int`
         :return: None
         """
 
         original_lin_layer = getattr(net, layer_name)
 
         if not isinstance(original_lin_layer, torch.nn.Linear):
-            raise FLException('Received a non-linear layer to expand '
-                              'whereas the method expects a linear layer')
+            raise FLException("Received a non-linear layer to expand " "whereas the method expects a linear layer")
 
-        new_ip_dim = original_lin_layer.in_features if layer_idx == 0 else new_dimensions[
-            layer_idx-1]
+        new_ip_dim = original_lin_layer.in_features if layer_idx == 0 else new_dimensions[layer_idx - 1]
         new_op_dim = new_dimensions[layer_idx]
         bias = original_lin_layer.bias is not None
 
-        new_lin_layer = type(original_lin_layer)(
-            in_features=new_ip_dim, out_features=new_op_dim, bias=bias
-        )
+        new_lin_layer = type(original_lin_layer)(in_features=new_ip_dim, out_features=new_op_dim, bias=bias)
 
         setattr(net, layer_name, new_lin_layer)
 
     def expand_model_by_layer_name(self, new_dimension, layer_name="dense"):
         """
         Expands the current PyTorch models layers with the provided dimensions
         :param new_dimension: new dimensions of the particular `layer_name`
         :type new_dimension: `list`
         :param layer_name: layer name which needs to be expanded
         :type layer_name: `str`
         :return: None
         """
 
         if new_dimension is None:
-            raise FLException('No information is provided for '
-                              'the new expanded model. '
-                              'Please provide the new dimension of '
-                              'the resulting expanded model')
-
-        layer_maps = {
-            'dense': {
-                'class': torch.nn.Linear,
-                'expansion_fn': self.__expand_linear_layer__
-            }
-        }
-
-        layer_cls = layer_maps[layer_name]['class']
-        layer_exp_fn = layer_maps[layer_name]['expansion_fn']
-        net = self.model.module_    # get instantiated module
+            raise FLException(
+                "No information is provided for "
+                "the new expanded model. "
+                "Please provide the new dimension of "
+                "the resulting expanded model"
+            )
+
+        layer_maps = {"dense": {"class": torch.nn.Linear, "expansion_fn": self.__expand_linear_layer__}}
+
+        layer_cls = layer_maps[layer_name]["class"]
+        layer_exp_fn = layer_maps[layer_name]["expansion_fn"]
+        net = self.model.module_  # get instantiated module
         idx = 0
 
         for layer_varname, layer in net.named_modules():
             # skip layer if not instance of layer desired to be expanded
             if not isinstance(layer, layer_cls):
                 continue
 
@@ -622,13 +730,19 @@
 
         :param train_data: Training data, a tuple \
         given in the form (x_train, y_train). otherwise, input compatible with skorch.dataset.Dataset
         :type train_data: `(np.ndarray, np.ndarray)`
         :return: The resulting loss.
         :rtype: `float`
         """
-        
+
         metrics_list = [PytorchFLModel.loss]
-        orig_loss_value = self.evaluate(dataset, eval_metrics=metrics_list)[
-                    'loss']
-        
+        orig_loss_value = self.evaluate(dataset, eval_metrics=metrics_list)["loss"]
+
         return orig_loss_value
+    
+    def update_model_metadata(self, model_update):
+        """
+        """
+        if "input_shape" in model_update :
+            self.input_shape = model_update.get('input_shape')
+            logger.info("update_model_metadata: input_shape = {}".format(model_update.get('input_shape')))
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/model/tensorflow_fl_model.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/model/tensorflow_fl_model.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,26 +1,30 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
+import inspect
+import json
 import logging
 import time
-import json
+
 import numpy as np
 import tensorflow as tf
-import inspect
-# if tf.__version__ != "2.1.0":
-# raise ImportError("This function requires TensorFlow v2.1.0.")
+from pandas.core.frame import DataFrame
+from tensorflow.keras import backend as K
 
-from ibmfl.util import config
-from ibmfl.util import fl_metrics
+from ibmfl.exceptions import FLException, LocalTrainingException, ModelException
 from ibmfl.model.fl_model import FLModel
 from ibmfl.model.model_update import ModelUpdate
-from ibmfl.exceptions import FLException, LocalTrainingException, ModelException
+from ibmfl.util import config, fl_metrics
+
+# if tf.__version__ != "2.1.0":
+# raise ImportError("This function requires TensorFlow v2.1.0.")
+
 
 logger = logging.getLogger(__name__)
 
 
 class TensorFlowFLModel(FLModel):
     """
     Wrapper class for importing tensorflow models.
@@ -40,41 +44,48 @@
         :param tf_model: Compiled TensorFlow model.
         :type tf_model: `tf.keras.Model`
         """
 
         super().__init__(model_name, model_spec, **kwargs)
         if tf_model is None:
             if model_spec is None or (not isinstance(model_spec, dict)):
-                raise ValueError('Initializing model requires '
-                                 'a model specification or '
-                                 'compiled TensorFlow model. '
-                                 'None was provided')
+                raise ValueError(
+                    "Initializing model requires "
+                    "a model specification or "
+                    "compiled TensorFlow model. "
+                    "None was provided"
+                )
             # In this case we need to recreate the model from model_spec
             self.model = self.load_model_from_spec(model_spec)
         else:
             if not issubclass(type(tf_model), tf.keras.Model):
-                raise ValueError('Compiled TensorFlow model needs to be '
-                                 'provided of type `tensorflow.keras.models`.'
-                                 ' Type provided: ' + str(type(tf_model)))
+                raise ValueError(
+                    "Compiled TensorFlow model needs to be "
+                    "provided of type `tensorflow.keras.models`."
+                    " Type provided: " + str(type(tf_model))
+                )
 
             if self.use_gpu_for_training and self.num_gpus >= 1:
                 strategy = tf.distribute.MirroredStrategy()
                 with strategy.scope():
                     self.model = tf_model
             else:
                 self.model = tf_model
-        self.model_type = 'TensorFlow-2.1.0'
+
+        self.model_type = "TensorFlow-2.1.0"
         # Default values for local training
         self.batch_size = 128
         self.epochs = 1
-        self.steps_per_epoch = 100
-        self.is_classification = True if not (model_spec and model_spec.get(
-            'is_classification')) else model_spec.get('is_classification')
+        self.validation_split = 0
+        self.steps_per_epoch = None
+        self.is_classification = (
+            True if not (model_spec and model_spec.get("is_classification")) else model_spec.get("is_classification")
+        )
 
-    def fit_model(self, train_data, fit_params=None, **kwargs):
+    def fit_model(self, train_data, fit_params=None, validation_data=None, **kwargs):
         """
         Fits current model with provided training data.
 
         :param train_data: Training data, a tuple\
         given in the form (x_train, y_train).
         :type train_data: `np.ndarray`
         :param fit_params: (optional) Dictionary with hyperparameters\
@@ -82,68 +93,88 @@
         Hyperparameter parameters should match  expected values\
         e.g., `epochs`, which specifies the number of epochs to be run.\
         If no `epochs` or `batch_size` are provided, a default value\
         will be used (1 and 128, respectively).
         :type fit_params: `dict`
         :return: None
         """
-
-        hyperparams = fit_params.get('hyperparams', {}) or {
-        } if fit_params else {}
-        local_hp = hyperparams.get('local', {}) or {}
-        training_hp = local_hp.get('training', {}) or {}
-
-        # Initialized with default values if not in training_hp
-        batch_size = training_hp.get('batch_size', self.batch_size)
-        epochs = training_hp.get('epochs', self.epochs)
-        steps_per_epoch = training_hp.get(
-            'steps_per_epoch', self.steps_per_epoch)
-
-        logger.info('Training hps for this round => '
-                    'batch_size: {}, epochs {}, steps_per_epoch {}'
-                    .format(batch_size, epochs, steps_per_epoch))
+        fit_args = self.get_fit_args(fit_params, **kwargs)
+        if validation_data is not None:
+            fit_args.pop("validation_split", None)
+            fit_args["validation_data"] = validation_data
+
+        logger.info(
+            "Training hps for this round => "
+            "batch_size: {}, epochs {}, steps_per_epoch {}".format(
+                fit_args.get("batch_size"), fit_args.get("epochs"), fit_args.get("steps_per_epoch")
+            )
+        )
 
         try:
             if type(train_data) is tuple and type(train_data[0]) is np.ndarray:
                 # Extract x_train and y_train, by default,
                 # label is stored in the last column
                 x = train_data[0]
                 y = train_data[1]
-                self.model.fit(x, y, batch_size=batch_size, epochs=epochs)
+                self.model.fit(x, y, **fit_args)
             else:
-                self.model.fit(train_data, epochs=epochs,
-                               steps_per_epoch=steps_per_epoch)
+                if isinstance(train_data, (tf.keras.utils.Sequence)) and hasattr(train_data, "set_batch_size"):
+                    train_data.set_batch_size(fit_args.get("batch_size"))
+
+                fit_args.pop("batch_size", None)
+                fit_args.pop("validation_split", None)
+                self.model.fit(train_data, **fit_args)
 
         except Exception as e:
             logger.exception(str(e))
-            if epochs is None:
-                logger.exception('epochs need to be provided')
-
-            raise LocalTrainingException(
-                'Error occurred while performing model.fit')
+            raise LocalTrainingException("Error occurred while performing model.fit")
 
     def update_model(self, model_update):
         """
         Update TensorFlow model with provided model_update, where model_update \
         should be generated according to \
-        `TensorFlowFLModel.get_model_update()`.
+        `TensorFlowFLModel.get_model_update()`
 
         :param model_update: `ModelUpdate` object that contains the weights \
         that will be used to update the model.
-        :type model_update: `ModelUpdate`
+        :type model_update: `ModelUpdate`, `numpy array`, or 'list'
         :return: None
         """
         if isinstance(model_update, ModelUpdate):
             w = model_update.get("weights")
             self.model.set_weights(w)
         else:
-            raise LocalTrainingException('Provided model_update should be of '
-                                         'type ModelUpdate. '
-                                         'Instead they are: ' +
-                                         str(type(model_update)))
+            raise LocalTrainingException(
+                "Provided model_update should be of "
+                "type ModelUpdate. "
+                "Instead they are: " + str(type(model_update))
+            )
+
+    def update_model_gradient(self, grads):
+        """
+        Update TensorFlow model with provided gradients,
+        where grads is a vector of gradients to be applied
+        based on current optimizer.
+
+        :param grads: Numpy array or list of gradient arrays that contains \
+        gradients to update model weights.
+        :type grads: `numpy array`, or 'list'
+        :return: None
+        """
+        if isinstance(grads, np.ndarray):
+            grads = self.reshape_to_model(grads, "model")
+            self.model.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))
+        elif isinstance(grads, list):
+            self.model.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))
+        else:
+            raise LocalTrainingException(
+                "Provided gradients should be of "
+                "type np.ndarray or list of np.ndarrays. "
+                "Instead they are: " + str(type(grads))
+            )
 
     def get_model_update(self):
         """
         Generates a `ModelUpdate` object that will be sent to other entities.
 
         :return: ModelUpdate
         :rtype: `ModelUpdate`
@@ -183,16 +214,15 @@
         if type(test_dataset) is tuple:
             x_test = test_dataset[0]
             y_test = test_dataset[1]
 
             return self.evaluate_model(x_test, y_test, **kwargs)
 
         else:
-            return self.evaluate_generator_model(
-                test_dataset, **kwargs)
+            return self.evaluate_generator_model(test_dataset, **kwargs)
 
     def evaluate_model(self, x, y, batch_size=128, **kwargs):
         """
         Evaluates the model given x and y.
 
         :param x: Samples with shape as expected by the model.
         :type x: `np.ndarray`
@@ -202,35 +232,32 @@
         :type batch_size: `int`
         :param kwargs: Dictionary of metrics available for the model
         :type kwargs: `dict`
         :return: metrics
         :rtype: `dict`
         """
 
-        metrics = self.model.evaluate(
-            x, y, batch_size=batch_size, **kwargs)
+        metrics = self.model.evaluate(x, y, batch_size=batch_size, verbose=0, **kwargs)
         names = self.model.metrics_names
         dict_metrics = {}
         additional_metrics = {}
         if type(metrics) == list:
             for metric, name in zip(metrics, names):
                 # metric = metric.item()
-                if name == 'accuracy':
-                    dict_metrics['acc'] = round(metric, 2)
+                if name == "accuracy":
+                    dict_metrics["acc"] = round(metric, 2)
                 dict_metrics[name] = metric
         else:
             dict_metrics[names[0]] = metrics
 
         y_pred = self.predict(x, batch_size=batch_size)
         if self.is_classification:
-            additional_metrics = fl_metrics.get_eval_metrics_for_classificaton(
-                y, y_pred)
+            additional_metrics = fl_metrics.get_eval_metrics_for_classificaton(y, y_pred)
         else:
-            additional_metrics = fl_metrics.get_eval_metrics_for_regression(
-                y, y_pred)
+            additional_metrics = fl_metrics.get_eval_metrics_for_regression(y, y_pred)
 
         logger.info(additional_metrics)
         dict_metrics = {**dict_metrics, **additional_metrics}
         logger.info(dict_metrics)
 
         return dict_metrics
 
@@ -244,27 +271,26 @@
         :type test_generator: `ImageDataGenerator` or `keras.utils.Sequence`
         :return: metrics
         :rtype: `dict`
         """
 
         steps = self.steps_per_epoch
         if steps in kwargs:
-            steps = kwargs.get('steps')
+            steps = kwargs.get("steps")
 
-        metrics = self.model.evaluate_generator(
-            test_generator, steps=steps)
+        metrics = self.model.evaluate_generator(test_generator, steps=steps)
         names = self.model.metrics_names
         dict_metrics = {}
         additional_metrics = {}
 
         if type(metrics) == list:
             for metric, name in zip(metrics, names):
                 # metric = metric.item()
-                if name == 'accuracy':
-                    dict_metrics['acc'] = round(metric, 2)
+                if name == "accuracy":
+                    dict_metrics["acc"] = round(metric, 2)
                 dict_metrics[name] = metric
         else:
             dict_metrics[names[0]] = metrics
 
         return dict_metrics
 
     @staticmethod
@@ -274,38 +300,36 @@
 
         :param file_name: Name of the file that contains the model to be loaded.
         :type file_name: `str`
         :return: TensorFlow model loaded to memory
         :rtype: `tf.keras.models.Model`
         """
         try:
-            model = tf.keras.models.load_model(
-                file_name, custom_objects=custom_objects)
+            model = tf.keras.models.load_model(file_name, custom_objects=custom_objects)
         except Exception as ex:
             logger.exception(str(ex))
-            logger.error(
-                'Loading model via tf.keras.models.load_model failed!')
+            logger.error("Loading model via tf.keras.models.load_model failed!")
         return model
 
     def save_model(self, filename=None):
         """
         Save a model to file in the format specific to the backend framework.
 
         :param filename: Name of the file where to store the model.
         :type filename: `str`
         :return: filename
         :rtype `string`
         """
         if filename is None:
             file = self.model_name if self.model_name else self.model_type
-            filename = '{}'.format(file)
+            filename = "{}".format(file)
 
         full_path = super().get_model_absolute_path(filename)
         self.model.save(full_path)
-        logger.info('Model saved in path: %s.', full_path)
+        logger.info("Model saved in path: %s.", full_path)
         return filename
 
     @staticmethod
     def model_from_json_via_tf_keras(json_file_name, custom_objects={}):
         """
         Loads a model architecture from disk via tf.keras \
         given the specified json file name.
@@ -315,73 +339,64 @@
         :type json_file_name: `str`
         :param custom_objects: Dictionary of custom objects required for loading arch
         :type custom_objects: `dict`
         :return: tf.keras model with only model architecture loaded to memory
         :rtype: `tf.keras.models.Model`
         """
         model = None
-        json_file = open(json_file_name, 'r')
+        json_file = open(json_file_name, "r")
         f = json_file.read()
         json_file.close()
         try:
-            model = tf.keras.models.model_from_json(
-                f, custom_objects=custom_objects)
+            model = tf.keras.models.model_from_json(f, custom_objects=custom_objects)
         except Exception as ex:
-            logger.error(
-                'Loading model via tf.keras.models.model_from_json failed! ')
+            logger.error("Loading model via tf.keras.models.model_from_json failed! ")
 
         return model
 
     def load_model_from_spec(self, model_spec):
         """
         Loads model from provided model_spec, where model_spec is a `dict` \
         that contains the following items: \
             'model_definition': the path where the tf model is stored, \
                 usually in a `SavedModel` format.
         :return: model
         :rtype: `keras.models.Model`
         """
         custom_objects = {}
-        if 'custom_objects' in model_spec:
-
-            custom_objects_config = model_spec['custom_objects']
+        if "custom_objects" in model_spec:
+            custom_objects_config = model_spec["custom_objects"]
             for custom_object in custom_objects_config:
-                key = custom_object['key']
-                value = custom_object['value']
-                path = custom_object['path']
-                custom_objects[key] = config.get_attr_from_path(
-                    path, value)
+                key = custom_object["key"]
+                value = custom_object["value"]
+                path = custom_object["path"]
+                custom_objects[key] = config.get_attr_from_path(path, value)
 
-        if 'model_definition' in model_spec:
+        if "model_definition" in model_spec:
             try:
-                model_file = model_spec['model_definition']
+                model_file = model_spec["model_definition"]
                 model_absolute_path = config.get_absolute_path(model_file)
 
                 if self.use_gpu_for_training:
                     strategy = tf.distribute.MirroredStrategy()
                     with strategy.scope():
-                        model = TensorFlowFLModel.load_model(
-                            model_absolute_path, custom_objects=custom_objects)
+                        model = TensorFlowFLModel.load_model(model_absolute_path, custom_objects=custom_objects)
                 else:
-                    model = TensorFlowFLModel.load_model(
-                        model_absolute_path, custom_objects=custom_objects)
+                    model = TensorFlowFLModel.load_model(model_absolute_path, custom_objects=custom_objects)
 
             except Exception as ex:
                 logger.exception(str(ex))
-                raise FLException('Failed to load TensorFlow model!')
+                raise FLException("Failed to load TensorFlow model!")
         else:
-
             if self.use_gpu_for_training:
                 strategy = tf.distribute.MirroredStrategy()
-                model = self.load_model_from_architecture(
-                    model_spec, custom_objects)
+                model = self.load_model_from_architecture(model_spec, custom_objects)
 
             else:
-                model = self.load_model_from_architecture(
-                    model_spec, custom_objects)
+                model = self.load_model_from_architecture(model_spec, custom_objects)
 
         return model
 
     def load_model_from_architecture(self, model_spec, custom_objects):
         """
         Loads model from provided model_spec, where model_spec is a `dict` \
         that contains the following items: \
@@ -395,74 +410,94 @@
         :type custom_objects: `dict`
         :return: model
         :rtype: `keras.models.Model`
         """
 
         try:
             model = TensorFlowFLModel.model_from_json_via_tf_keras(
-                model_spec['model_architecture'], custom_objects=custom_objects)
+                model_spec["model_architecture"], custom_objects=custom_objects
+            )
 
             if model is None:
-                logger.error('An acceptable compiled model should be of type '
-                             'tensorflow.keras.models!')
+                logger.error("An acceptable compiled model should be of type " "tensorflow.keras.models!")
         except Exception as ex:
             logger.error(str(ex))
-            raise FLException('Unable to load the provided uncompiled model!')
+            raise FLException("Unable to load the provided uncompiled model!")
 
             # Load weights from provided path
-        if 'model_weights' in model_spec:
-            model.load_weights(model_spec['model_weights'])
+        if "model_weights" in model_spec:
+            model.load_weights(model_spec["model_weights"])
 
-        if 'compile_model_options' in model_spec:
+        if "compile_model_options" in model_spec:
             # Load compile options:
             try:
-                compiled_options = model_spec['compile_model_options']
-                optimizer = self.get_custom_attribute(
-                    compiled_options.get('optimizer'))
-                loss = self.get_custom_attribute(compiled_options.get('loss'))
-                metrics = self.get_custom_attribute(
-                    compiled_options.get('metrics'))
-                metrics = [metrics] if not isinstance(
-                    metrics, list) else metrics
-                model.compile(optimizer=optimizer,
-                              loss=loss,
-                              metrics=metrics)
+                compiled_options = model_spec["compile_model_options"]
+                optimizer = self.get_custom_attribute(compiled_options.get("optimizer"))
+                loss = self.get_custom_attribute(compiled_options.get("loss"))
+                metrics = self.get_custom_attribute(compiled_options.get("metrics"))
+                metrics = [metrics] if not isinstance(metrics, list) else metrics
+                model.compile(optimizer=optimizer, loss=loss, metrics=metrics)
             except Exception as ex:
                 logger.exception(str(ex))
-                logger.exception(
-                    'Failed to compiled the TensorFlow.keras model.')
+                logger.exception("Failed to compiled the TensorFlow.keras model.")
         else:
-            raise ModelException('Failed to compile keras model, '
-                                 'no compile options provided.')
+            raise ModelException("Failed to compile keras model, " "no compile options provided.")
 
         return model
 
-    def get_gradient(self, train_data):
+    def get_gradient(self, train_data, wrt="trainable_weights"):
         """
         Compute the gradient with the provided dataset at the current local \
-        model's weights.
+        model's weights. Can calculate the gradient with respect to the trainable \
+        weights, or with respect to the input to the model.
 
         :param train_data: Training data, a tuple \
         given in the form (x_train, y_train).
         :type train_data: `np.ndarray`
+        :param wrt: Specifying which variable the gradient is computed for.
+        The default 'trainable_weights' will calculate the gradient of the trainable weights.
+        Setting wrt to 'model_input' will calculate the gradient with respect to the input to the model.
+        :type wrt: `str`
         :return: gradients
         :rtype: `list` of `tf.Tensor`
         """
         try:
             x, y = train_data[0], train_data[1]
         except Exception as ex:
             logger.exception(str(ex))
-            raise FLException('Provided dataset has incorrect format. '
-                              'It should be a tuple in the form of '
-                              '(x_train, y_train).')
-        with tf.GradientTape() as tape:
-            predictions = self.model(x, training=True)
-            loss = self.model.loss(y, predictions)
+            raise FLException(
+                "Provided dataset has incorrect format. " "It should be a tuple in the form of " "(x_train, y_train)."
+            )
 
-        gradients = tape.gradient(loss, self.model.trainable_variables)
+        # Check if input length matches input shape of model
+        if hasattr(self.model.layers[0], "input_shape") and x.shape[1:] != self.model.layers[0].input_shape[1:]:
+            raise FLException(
+                "Input data does not match model input shape."
+                f"Input data shape: {x.shape[1:]}."
+                f"Model input shape: {self.model.layers[0].input_shape[1:]}."
+            )
+        if wrt == "trainable_weights":
+            with tf.GradientTape() as tape:
+                predictions = self.model(x, training=True)
+                loss = self.model.loss(y, predictions)
+                gradients = tape.gradient(loss, self.model.trainable_variables)
+        elif wrt == "model_input":
+            # TODO see https://github.com/tensorflow/tensorflow/issues/36596
+            x_input = tf.Variable(tf.convert_to_tensor(x))
+            with tf.GradientTape() as tape:
+                tape.watch(x_input)
+                predictions = self.model(x_input, training=False)
+                loss = self.model.loss(y, predictions)
+                gradients = [tape.gradient(loss, x_input)]
+        else:
+            raise FLException(
+                "Gradient can only be computed "
+                "with respect to trainable_weights or "
+                "model_input. However, " + str(wrt) + "was provided."
+            )
         return gradients
 
     def expand_model_by_layer_name(self, new_dimension, layer_name="dense"):
         """
         Expand the current Keras model with provided dimension of
         the hidden layers or model weights.
         This method by default expands the dense layer of
@@ -475,72 +510,65 @@
         the fully connected layers
         :type new_dimension: `list`
         :param layer_name: layer's name to be expanded
         :type layer_name: `str`
         :return: None
         """
         if new_dimension is None:
-            raise FLException('No information is provided for '
-                              'the new expanded model. '
-                              'Please provide the new dimension of '
-                              'the resulting expanded model.')
+            raise FLException(
+                "No information is provided for "
+                "the new expanded model. "
+                "Please provide the new dimension of "
+                "the resulting expanded model."
+            )
         try:
             model_config = json.loads(self.model.to_json())
         except NotImplementedError:
             raise ModelException(
                 "Please construct the model config for models in "
                 "`SavedModel` format. "
                 "Details about how to construct the model config can be found"
-                " in TensorFlowFLModel tutorials.")
+                " in TensorFlowFLModel tutorials."
+            )
         except Exception as ex:
             logger.exception(str(ex))
-            raise FLException("Error occurred during extracting "
-                              "the model architecture.")
+            raise FLException("Error occurred during extracting " "the model architecture.")
         i = 0
 
-        for layer in model_config['config']['layers']:
+        for layer in model_config["config"]["layers"]:
             # find the specified layers
-            if 'class_name' in layer and \
-                    layer['class_name'].strip().lower() == layer_name:
-                layer['config']['units'] = new_dimension[i]
+            if "class_name" in layer and layer["class_name"].strip().lower() == layer_name:
+                layer["config"]["units"] = new_dimension[i]
                 i += 1
 
-        custom_obj = {
-            self.model.__class__.__name__: self.model.__class__
-        }
+        custom_obj = {self.model.__class__.__name__: self.model.__class__}
 
         try:
-            new_model = tf.keras.models.model_from_json(
-                json.dumps(model_config), custom_objects=custom_obj)
+            new_model = tf.keras.models.model_from_json(json.dumps(model_config), custom_objects=custom_obj)
         except Exception as ex:
             logger.exception(str(ex))
-            raise FLException("Error occurred during loading model from "
-                              "the new config.")
+            raise FLException("Error occurred during loading model from " "the new config.")
 
         metrics = self.model.metrics_names
-        if 'loss' in metrics:
-            metrics.remove('loss')
+        if "loss" in metrics:
+            metrics.remove("loss")
         if not self.use_gpu_for_training or self.num_gpus == 1:
-            new_model.compile(optimizer=self.model.optimizer,
-                              loss=self.model.loss,
-                              metrics=metrics)
+            new_model.compile(optimizer=self.model.optimizer, loss=self.model.loss, metrics=metrics)
         else:
             strategy = tf.distribute.MirroredStrategy()
             with strategy.scope():
-                new_model.compile(optimizer=self.model.optimizer,
-                                  loss=self.model.loss,
-                                  metrics=metrics)
+                new_model.compile(optimizer=self.model.optimizer, loss=self.model.loss, metrics=metrics)
 
         self.model = new_model
 
     def is_fitted(self):
         """
         Return a boolean value indicating if the model is fitted or not. \
         In particular, check if the tensorflow model has weights. \
-        If it has, return True; otherwise return false. 
+        If it has, return True; otherwise return false.
 
         :return: res
         :rtype: `bool`
         """
         try:
             self.model.get_weights()
         except Exception:
@@ -554,24 +582,23 @@
         :param dataset: Provided dataset, a tuple given in the form \
         (x_test, y_test) or a datagenerator of type `keras.utils.Sequence`, \
         `keras.preprocessing.image.ImageDataGenerator`.
         :type dataset: `np.ndarray`
         :return: The resulting loss.
         :rtype: `float`
         """
-        if 'loss' not in self.model.metrics_names:
-            self.model.metrics_names.append('loss')
+        if "loss" not in self.model.metrics_names:
+            self.model.metrics_names.append("loss")
         res = self.evaluate(dataset)
 
-        if 'loss' in res:
-            loss = round(res['loss'], 2)
+        if "loss" in res:
+            loss = round(res["loss"], 2)
             return loss
         else:
-            raise FLException(
-                "Loss is not listed in the model's metrics_names.")
+            raise FLException("Loss is not listed in the model's metrics_names.")
 
     def get_custom_attribute(self, attr):
         """
         Load compiled options which are provided as config.
         :param attr: Attribute config provided in config
         :type attr: dict or key
         :return: Attribute loaded and returned back for compilation
@@ -579,32 +606,141 @@
 
         """
         if attr is None:
             raise ModelException("Invalid Model config exception")
 
         if isinstance(attr, dict):
             try:
-
-                value = attr.get('value')
-                path = attr.get('path')
-                args = attr.get('args') if 'args' in attr else {}
-                attribute = config.get_attr_from_path(
-                    path, value)
+                value = attr.get("value")
+                path = attr.get("path")
+                args = attr.get("args") if "args" in attr else {}
+                attribute = config.get_attr_from_path(path, value)
 
             except Exception as ex:
-                logger.error(
-                    "Error occurred while loading the custom attribute!")
+                logger.error("Error occurred while loading the custom attribute!")
                 logger.error("Custom attribute : " + attr)
                 logger.error()
 
             logger.debug(type(attribute))
 
             if inspect.isclass(attribute):
                 return attribute(**args)
             else:
                 return attribute
 
         else:
             return attr
 
+    def get_fit_args(self, global_params, **kwargs):
+        fit_args = {}
+        local_params = kwargs.get("local_params", {}) or {} if global_params else {}
+        hyperparams = global_params.get("hyperparams", {}) or {} if global_params else {}
+        local_hp = hyperparams.get("local", {}) or {}
+        training_hp = local_hp.get("training", {}) or {}
+        optimizer_hp = local_hp.get("optimizer", {}) or {}
+
+        lr = optimizer_hp.get("lr", None)
+        if lr:
+            K.set_value(self.model.optimizer.learning_rate, lr)
+        logger.info("Learning rate of optimizer is set as {}".format(self.model.optimizer.learning_rate))
+
+        validation_split = training_hp.get("validation_split", self.validation_split)
+        try:
+            if float(validation_split) != 0:
+                fit_args["validation_split"] = float(validation_split)
+        except (TypeError, ValueError):
+            raise ValueError("Validation split cannot be a NoneType")
+        if "validation_split" in local_params:
+            validation_split = local_params.get("validation_split")
+            try:
+                if float(validation_split) != 0:
+                    fit_args["validation_split"] = float(validation_split)
+            except (TypeError, ValueError):
+                raise ValueError("Validation split cannot be a NoneType")
+
+        fit_args["batch_size"] = training_hp.get("batch_size", self.batch_size)
+        if "batch_size" in local_params:
+            fit_args["batch_size"] = local_params.get("batch_size")
+
+        fit_args["epochs"] = training_hp.get("epochs", self.epochs)
+        if "epochs" in local_params:
+            fit_args["epochs"] = local_params.get("epochs")
 
-        
+        fit_args["steps_per_epoch"] = training_hp.get("steps_per_epoch", self.steps_per_epoch)
+        if "steps_per_epoch" in local_params:
+            fit_args["steps_per_epoch"] = local_params.get("steps_per_epoch")
+
+        return fit_args
+
+    def get_model_output(self, x, learning_phase_flag=False):
+        """
+        Return the resulting last layer output of the model when passing the
+        provided set of features.
+
+        :param x: The provided set of features to obtain the last layer output.
+        :type x: `np.ndarray`
+        :param learning_phase_flag: The keras.backend.learning_phase flag to \
+        indicate if this is training or inference phase, \
+        as 'some Keras layers (e.g. Dropout, BatchNormalization) behave \
+        differently at training time and testing time' quoted from Keras.io.
+        :type learning_phase_flag: `boolean`
+        :return: The resulting last layer output
+        :rtype: `list`
+        """
+        x_train = self._data_format_check(x)
+        last_layer = self.model(x_train, training=learning_phase_flag)
+        return [last_layer]
+
+    def get_gradient_of_output(self, x):
+        """
+        Compute the gradient of output with respect to model weights.
+        Uses tape.jacobian to obtain per-sample gradients.
+
+        :param x: The provided set of features to compute gradients.
+        :type x: `np.ndarray`
+        :return: The resulting gradient.
+        :rtype: `list`
+        """
+        with tf.GradientTape(persistent=True) as tape:
+            preds = self.model(x, training=False)
+
+        gradients = tape.jacobian(preds, self.model.trainable_variables)
+        gradients = self.reshape_to_model(gradients, "flatten")
+        return gradients
+
+    def reshape_to_model(self, vals, reshape_type):
+        """
+        Reshapes input values into shape that matches the model weights,
+        or flattens input values that are currently in the shape of model weights.
+
+        :param vals: Input values to be reshaped.
+        :type vals: `np.ndarray`
+        :param reshape_type: Indicates if input should be flattened or placed into model weights shape.
+        :type vals: `str`
+        :return: Reshaped values.
+        :rtype: `np.ndarray`
+        """
+        if reshape_type == "flatten":
+            l = []
+            for val in vals:
+                batch_size = val.shape[0]
+                embedding_length = val.shape[1]
+                # Flatten only axis 2 and higher,
+                # maintain the batch size and embedding length dimensions
+                l.append(tf.reshape(val, (batch_size, embedding_length, -1)))
+            return np.concatenate(l, axis=2)
+        else:
+            l = []
+            elem = 0
+            for weights in self.model.trainable_variables:
+                end = elem + tf.size(weights)
+                l.append(vals[elem:end].reshape(weights.shape))
+                elem = end
+            return l
+
+    @staticmethod
+    def _data_format_check(data):
+        """
+        Check the data format: if the data in format of `DataFrame`,
+        covert it `numpy.ndarray`
+        """
+        return data.to_numpy(dtype=float) if isinstance(data, DataFrame) else data
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/model/v2/xgb_fl_model.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/model/xgb_fl_model.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,37 +1,38 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
 from __future__ import print_function
+
+import logging
 import time
 import uuid
-import pickle
-import logging
+from abc import ABC, abstractmethod
 from functools import partial
 
-from sklearn.utils.validation import check_is_fitted
-from sklearn.ensemble._hist_gradient_boosting.loss import *
+from sklearn._loss.loss import *
 from sklearn.ensemble._hist_gradient_boosting.common import *
-from sklearn.preprocessing import OrdinalEncoder, LabelEncoder
-from sklearn.utils.multiclass import check_classification_targets
-from sklearn.metrics import check_scoring, accuracy_score, r2_score
-from sklearn.utils import check_X_y, check_random_state, check_array
+from sklearn.ensemble._hist_gradient_boosting.common import X_BINNED_DTYPE, X_DTYPE
 from sklearn.ensemble._hist_gradient_boosting.grower import TreePredictor
-from sklearn.ensemble._hist_gradient_boosting.common import X_DTYPE, X_BINNED_DTYPE
+from sklearn.metrics import accuracy_score, check_scoring, r2_score
+from sklearn.preprocessing import LabelEncoder, OrdinalEncoder
+from sklearn.utils import check_array, check_random_state, check_X_y
+from sklearn.utils.multiclass import check_classification_targets
+from sklearn.utils.validation import check_is_fitted
 
-from ibmfl.util import fl_metrics
-from ibmfl.util import config
+from ibmfl.exceptions import ModelException, ModelInitializationException
 from ibmfl.model.fl_model import FLModel
-from ibmfl.util.xgboost.utils import is_classifier
+from ibmfl.util import config, fl_metrics
 from ibmfl.util.xgboost.export import export_sklearn
-from ibmfl.exceptions import ModelInitializationException, ModelException
-from ibmfl.util.xgboost.hyperparams import init_parameters, \
-    validate_parameters
+from ibmfl.util.xgboost.hyperparams import init_parameters, validate_parameters
+from ibmfl.util.xgboost.utils import _LOSSES
+from ibmfl.util.persistence.sklearn import SKLearnPersistence
+
 
 logger = logging.getLogger(__name__)
 
 
 class XGBFLModel(FLModel, ABC):
     """
     Wrapper class implementation for XGBoost containing the XGBoost Model Object
@@ -55,15 +56,15 @@
         super().__init__(model_name, model_spec, **kwargs)
 
         # Initialize Input Model Object Parameters
         # Note: For model loading process, onboard from `xgb_model` parameter.
         self._predictors = []
 
         # Initialize Additional Internal Model Parameters
-        self.model_type = 'XGBFLModel'
+        self.model_type = "XGBFLModel"
         self._baseline_prediction = None
         self._raw_predictions = None
         self.n_features_ = 0
         self.loss_ = None
         self._in_fit = False
         self.bin_mapper_ = None
         # Validate and Initialize Model Hyperparameters
@@ -92,54 +93,56 @@
         the corresponding FLModel class.
         :type `ModelUpdate`
         :param kwargs: Dictionary of model-specific arguments.
         :type kwargs: `dict`
         :return: None
         """
         if model_update is not None:
-            if isinstance(model_update.get('xgb_model'), list):
+            xgb_model = model_update.get("xgb_model")
+            if isinstance(xgb_model, list):
                 # Perform Model Update
-                self._predictors.append(model_update.get('xgb_model'))
+                self._predictors.append(xgb_model)
             else:
-                raise ValueError('Provided model object is not of the correct '
-                                 'data type. Should be a `list`. '
-                                 'Type provided: ' + str(type(model_update)))
+                raise ValueError(
+                    "Provided model object is not of the correct "
+                    "data type. Should be a `list`. "
+                    "Type provided: " + str(type(model_update))
+                )
 
     def _raw_predict(self, x):
         """
         Internal helper function for generating predictions for the model.
 
         :param x: The input data for the prediction model.
         :type x: `np.array`
         :param model: Model object which is used for generating the prediction. \
         If not provided the default model assigned internally will be used.
         :type model: `XGBFLModel`
         :return: raw_predictions
         :rtype: `np.array`
         """
-        logger.info('Performing Model Inference Process')
+        logger.info("Performing Model Inference Process")
 
         # Initialize Prediction Object
         n_samples = x.shape[0]
-        is_binned = getattr(self, '_in_fit', False)
+        is_binned = getattr(self, "_in_fit", False)
 
         # Generate Predictions
-        preds = np.zeros(shape=(self.n_trees, n_samples))
+        preds = np.zeros(shape=(n_samples, self.n_trees))
         if self._baseline_prediction is not None:
             preds += self._baseline_prediction
         for p in self._predictors:
             for k, p_i in enumerate(p):
                 if is_binned and self.bin_mapper_:
                     predict = partial(
-                        p_i.predict_binned,
-                        missing_values_bin_idx=self.bin_mapper_.missing_values_bin_idx_
+                        p_i.predict_binned, missing_values_bin_idx=self.bin_mapper_.missing_values_bin_idx_
                     )
                 else:
                     predict = p_i.predict
-                preds[k, :] += predict(x)
+                preds[:, k] += predict(x, self.known_cat_bitsets, self.f_idx_map, self.n_threads)
 
         return preds
 
     def get_model_update(self):
         """
         Since we are not using the conventional train() function at the local
         training handler, we respectively do not implement anything within here,
@@ -172,25 +175,25 @@
         is specified, the model will be stored in the default data location of \
         the library `DATA_PATH`.
         :type path: `str`
         :return: filename
         """
         if filename is None:
             file = self.model_name if self.model_name else self.model_type
-            filename = '{}_{}.pickle'.format(file, self.party_uuid)
+            filename = SKLearnPersistence.model_filename(file)
 
         full_path = super().get_model_absolute_path(filename)
 
         # Scikit-Learn Export Demo
         export_sklearn(self, full_path)
 
         if len(self._predictors) > 0:
-            logger.info('Model saved in path: %s.', full_path)
+            logger.info("Model saved in path: %s.", full_path)
         else:
-            logger.info('Persisting empty model in path: %s.', full_path)
+            logger.info("Persisting empty model in path: %s.", full_path)
 
         return filename
 
     @staticmethod
     def load_model(filename):
         """
         Load model from provided filename
@@ -201,21 +204,19 @@
         is specified, the model will be stored in the default data location of \
         the library `DATA_PATH`.
         :type path: `str`
         :return: Returns the corresponding model object.
         :rtype: `XGBFLModel`
         """
         absolute_path = config.get_absolute_path(filename)
-
-        with open(absolute_path, 'rb') as f:
-            model = pickle.load(f)
-            self = model
+        model = SKLearnPersistence.load_model(absolute_path)
+        self = model
 
         if len(self._predictors) == 0:
-            logger.info('Model does not contain any predictors.')
+            logger.info("Model does not contain any predictors.")
 
         return model
 
     @abstractmethod
     def get_loss(self):
         """
         Internal helper function used to obtain the corresponding loss function
@@ -265,15 +266,15 @@
         implementation.
         :rtype: `dict`
         """
         return NotImplementedError
 
 
 class XGBRegressorFLModel(XGBFLModel):
-    _VALID_LOSSES = ('least_squares', 'least_absolute_deviation')
+    _VALID_LOSSES = "least_squares"
 
     def predict(self, x):
         """
         Perform prediction for a batch of inputs.
 
         :param x: Samples with shape as expected by the model.
         :type x: Data structure as expected by the model
@@ -282,15 +283,15 @@
         :return: Predictions
         :rtype: Data structure the same as the type defines labels
         in testing data.
         """
         if len(self._predictors) > 0:
             return self._raw_predict(x).ravel()
         else:
-            raise Exception('Model has not been trained yet.')
+            raise Exception("Model has not been trained yet.")
 
     def encode_target(self, y):
         """
         Converts the input y to the expected dtype.
 
         :param y: The corresponding target data from the dataset to encode.
         :type y: `np.array`
@@ -327,26 +328,25 @@
         input and target values.
         :rtype: `dict`
         """
         metrics = {}
         if len(self._predictors) > 0:
             y_hat = self.predict(x)
             score = r2_score(y, y_hat)
-            metrics['r2_score'] = score
-            additional_metrics = fl_metrics.get_eval_metrics_for_regression(
-                y, y_hat)
+            metrics["r2_score"] = score
+            additional_metrics = fl_metrics.get_eval_metrics_for_regression(y, y_hat)
             metrics = {**metrics, **additional_metrics}
             return metrics
         else:
-            logger.info('Model has not been trained yet.')
+            logger.info("Model has not been trained yet.")
             return {}
 
 
 class XGBClassifierFLModel(XGBFLModel):
-    _VALID_LOSSES = ('binary_crossentropy', 'categorical_crossentropy', 'auto')
+    _VALID_LOSSES = ("binary_crossentropy", "categorical_crossentropy", "auto")
 
     def encode_target(self, y):
         """
         Converts the input y to the expected dtype and performs a label
         encoding. Here, we assume that each party has at least one sample of
         the corresponding class label type for each different classes.
 
@@ -361,21 +361,22 @@
         # Apply Label Encoder Transformation
         lab_enc = LabelEncoder()
         enc_y = lab_enc.fit_transform(y).astype(np.float64, copy=False)
 
         # Extract Encoded Target Sizes
         self.classes_ = lab_enc.classes_
         if self.classes_.shape[0] != self.num_classes:
-            raise ValueError('Number of classes defined in configuration file '
-                             'and the classes derived from the data does not '
-                             'match. Found {0} classes, while config file '
-                             'is defined as {1} classes.'.format(
-                             self.classes_.shape[0], self.num_classes))
+            raise ValueError(
+                "Number of classes defined in configuration file "
+                "and the classes derived from the data does not "
+                "match. Found {0} classes, while config file "
+                "is defined as {1} classes.".format(self.classes_.shape[0], self.num_classes)
+            )
 
-        if self.loss == 'auto':
+        if self.loss == "auto":
             self.n_trees = 1 if self.classes_.shape[0] <= 2 else self.classes_.shape[0]
         else:
             self.n_trees = 1 if self.num_classes <= 2 else self.num_classes
 
         return enc_y
 
     def get_loss(self, sample_weight):
@@ -388,20 +389,19 @@
 
         :param sample_weight: Weights of training data
         :type sample_weight: `np.ndarray`
         :return: Returns the respective loss object as defined in the FL \
         hyperparameters.
         :rtype: Derivation of `BaseLoss`
         """
-        if (self.loss == 'categorical_crossentropy' and self.n_trees == 1):
+        if self.loss == "categorical_crossentropy" and self.n_trees == 1:
             raise ValueError("Incompatible loss and target variable counts.")
 
-        if self.loss == 'auto':
-            return _LOSSES['binary_crossentropy']() if self.n_trees == 1 else \
-                _LOSSES['categorical_crossentropy']()
+        if self.loss == "auto":
+            return _LOSSES["binary_crossentropy"]() if self.n_trees == 1 else _LOSSES["categorical_crossentropy"]()
         else:
             return _LOSSES[self.loss](sample_weight=sample_weight)
 
     def predict(self, x, **kwargs):
         """
         Perform prediction for a batch of inputs returned as class values.
 
@@ -413,15 +413,15 @@
         :rtype: Data structure the same as the type defines labels
         in testing data.
         """
         if len(self._predictors) > 0:
             encoded_classes = np.argmax(self.predict_proba(x), axis=1)
             return self.classes_[encoded_classes]
         else:
-            raise Exception('Model has not been trained yet.')
+            raise Exception("Model has not been trained yet.")
 
     def predict_proba(self, x, **kwargs):
         """
         Perform prediction for a batch of inputs as probabilities.
 
         :param x: Samples with shape as expected by the model.
         :type x: Data structure as expected by the model
@@ -431,15 +431,15 @@
         :rtype: Data structure the same as the type defines labels \
         in testing data.
         """
         if len(self._predictors) > 0:
             raw_predictions = self._raw_predict(x)
             return self.loss_.predict_proba(raw_predictions)
         else:
-            raise Exception('Model has not been trained yet.')
+            raise Exception("Model has not been trained yet.")
 
     def evaluate_model(self, x, y, **kwargs):
         """
         Given an input set of values and their values, generate the prediction
         and compute the accuracy.
 
         :param x: The input data for the prediction model.
@@ -455,15 +455,14 @@
             y_hat = self.predict(x)
 
             correct = 0
             for i in range(x.shape[0]):
                 if y_hat[i] == y[i]:
                     correct += 1
 
-            acc = {'acc': correct/float(len(y))}
-            additional_metrics = fl_metrics.get_eval_metrics_for_classificaton(
-                y, y_hat)
+            acc = {"acc": correct / float(len(y))}
+            additional_metrics = fl_metrics.get_eval_metrics_for_classificaton(y, y_hat)
             acc = {**acc, **additional_metrics}
             return acc
         else:
-            logger.info('No models have been trained yet.')
+            logger.info("No models have been trained yet.")
             return {}
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/model/xgb_fl_model.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/model/sklearn_SGD_linear_fl_model.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,458 +1,388 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
-from __future__ import print_function
-import time
-import pickle
 import logging
+import time
 
+import numpy as np
+from sklearn.exceptions import NotFittedError
+from sklearn.linear_model import SGDClassifier, SGDRegressor
 from sklearn.utils.validation import check_is_fitted
-from sklearn.ensemble._hist_gradient_boosting.loss import *
-from sklearn.ensemble._hist_gradient_boosting.common import *
-from sklearn.preprocessing import OrdinalEncoder, LabelEncoder
-from sklearn.utils.multiclass import check_classification_targets
-from sklearn.metrics import check_scoring, accuracy_score, r2_score
-from sklearn.utils import check_X_y, check_random_state, check_array
-from sklearn.ensemble._hist_gradient_boosting.grower import TreePredictor
-
-from ibmfl.util import fl_metrics
-from ibmfl.util import config
-from ibmfl.model.fl_model import FLModel
-from ibmfl.util.xgboost.utils import is_classifier
-from ibmfl.util.xgboost.export import export_sklearn
-from ibmfl.exceptions import ModelInitializationException, ModelException
-from ibmfl.util.xgboost.hyperparams import init_parameters, \
-    validate_parameters
+
+from ibmfl.exceptions import LocalTrainingException, ModelException, ModelInitializationException
+from ibmfl.model.model_update import ModelUpdate
+from ibmfl.model.sklearn_fl_model import SklearnFLModel
+from ibmfl.util import config, fl_metrics
+from ibmfl.util.persistence.sklearn import SKLearnPersistence
 
 logger = logging.getLogger(__name__)
 
 
-class XGBFLModel(FLModel, ABC):
+class SklearnSGDFLModel(SklearnFLModel):
     """
-    Wrapper class implementation for XGBoost containing the XGBoost Model Object
+    Wrapper class for sklearn.linear_model.SGDClassifier and
+    sklearn.linear_model.SGDRegressor.
     """
 
-    def __init__(self, model_name, model_spec, xgb_model=None, **kwargs):
+    def __init__(self, model_name, model_spec, sklearn_model=None, **kwargs):
         """
-        Create a XGBFLModel instance for XGBoost model based either on an
-        existing model object or an entirely new model object.
-
-        :param model_type: String specifying the name of the model
-        :type model_type: `str`
-        :param model_spec: Hyperparameters associated with the model.
+        Create a `SklearnSGDFLModel` instance from a
+        sklearn.linear_model.SGDClassifier or a
+        sklearn.linear_model.SGDRegressor model.
+        If sklearn_model is provided, it will use it; otherwise it will take
+        the model_spec to create the model.
+
+        :param model_name: A name specifying the type of model, e.g., \
+        linear_SVM
+        :type model_name: `str`
+        :param model_spec: A dictionary contains model specification
         :type model_spec: `dict`
-        :param xgb_model: List of predictors existing predictor structures.
-        :type xgb_model: `list`
+        :param sklearn_model: Compiled sklearn model
+        :type sklearn_model: `sklearn.linear_model`
         :param kwargs: A dictionary contains other parameter settings on \
-         to initialize a XGBoost model.
+         to initialize a sklearn.linear_model model.
         :type kwargs: `dict`
         """
-        super().__init__(model_name, model_spec, **kwargs)
-
-        # Initialize Input Model Object Parameters
-        # Note: For model loading process, onboard from `xgb_model` parameter.
-        self._predictors = []
-
-        # Initialize Additional Internal Model Parameters
-        self.model_type = 'XGBFLModel'
-        self._baseline_prediction = None
-        self._raw_predictions = None
-        self.n_features_ = 0
-        self.loss_ = None
-
-        # Validate and Initialize Model Hyperparameters
-        validate_parameters(model_spec)
-        init_parameters(self, model_spec)
+        super().__init__(model_name, model_spec, sklearn_model=sklearn_model, **kwargs)
+        self.model_type = "Sklearn-linear-SGD"
+        if sklearn_model:
+            if not issubclass(type(sklearn_model), (SGDClassifier, SGDRegressor)):
+                raise ValueError(
+                    "Compiled sklearn model needs to be provided"
+                    "(sklearn.linear_model). "
+                    "Type provided: " + str(type(sklearn_model))
+                )
+
+            self.model = sklearn_model
+
+        if type(self.model) is SGDClassifier:
+            self.is_classification = True
+        elif type(self.model) is SGDRegressor:
+            self.is_classification = False
+
+        # checking if classes_ is provided for classification problem
+        if isinstance(self.model, SGDClassifier) and hasattr(self.model, "classes_"):
+            # check the type
+            if isinstance(self.model.classes_, np.ndarray) or self.model.classes_ is None:
+                self.classes = self.model.classes_
+            else:
+                raise ValueError(
+                    "Provided SGDClassifier model has wrong type of "
+                    "`classes_` attribute. "
+                    "`classes_` should be of type `numpy.ndarray`. "
+                    "Instead it is of type " + str(type(self.model.classes_))
+                )
+        else:
+            self.classes = None
 
     def fit_model(self, train_data, fit_params=None, **kwargs):
         """
-        This function is not implemented as model training is not operated
-        within the FL Model object and is part of an external process.
+        Fits current model with provided training data.
 
-        :return: `NotImplementedError`
-        """
-        return NotImplementedError
-
-    def update_model(self, model_update=None, **kwargs):
-        """
-        Updates model using provided `model_update`. Additional arguments
-        specific to the model can be added through `**kwargs`
-
-        :param model_update: Model with update. This is specific to each model \
-        type e.g., `ModelUpdateSGD`. The specific type should be checked by \
-        the corresponding FLModel class.
-        :type `ModelUpdate`
-        :param kwargs: Dictionary of model-specific arguments.
-        :type kwargs: `dict`
+        :param train_data: Training data a tuple \
+        given in the form (x_train, y_train).
+        :type train_data: `np.ndarray`
+        :param fit_params: (Optional) Dictionary with hyperparameters that \
+        will be used to call sklearn.linear_model fit function. \
+        Provided hyperparameter should only contains parameters that \
+        match sklearn expected values, e.g., `learning_rate`, which provides \
+        the learning rate schedule. \
+        If no `learning_rate` or `max_iter` is provided, a default value will \
+        be used ( `optimal` and `1`, respectively).
         :return: None
         """
-        if model_update is not None:
-            if isinstance(model_update.get('xgb_model'), list):
-                # Perform Model Update
-                self._predictors.append(model_update.get('xgb_model'))
+        # Default values
+        max_iter = 1
+        warm_start = True
+
+        # Extract x_train and y_train, by default,
+        # label is stored in the last column
+        x = train_data[0]
+        y = train_data[1]
+
+        hyperparams = fit_params.get("hyperparams", {}) or {} if fit_params else {}
+        sample_weight = fit_params.get("sample_weight", None) if fit_params else {}
+
+        local_hp = hyperparams.get("local", {}) or {}
+        training_hp = local_hp.get("training", {}) or {}
+
+        try:
+            if "max_iter" not in training_hp:
+                training_hp["max_iter"] = max_iter
+                logger.info("Using default max_iter: " + str(max_iter))
+
+            # set warm_start to True
+            training_hp["warm_start"] = warm_start
+            logger.info("Set warm_start as " + str(warm_start))
+
+            for key, val in training_hp.items():
+                self.model.set_params(**{key: val})
+        except Exception as e:
+            logger.exception(str(e))
+            raise LocalTrainingException("Error occurred while setting up model parameters")
+
+        try:
+            # Compute the classes based on labels if classification problem
+            if isinstance(self.model, SGDClassifier):
+                if self.classes is None:
+                    logger.warning(
+                        "Obtaining class labels based on local dataset. "
+                        "This may cause failures during aggregation "
+                        "when parties have distinctive class labels. "
+                    )
+                    self.classes = self.get_classes(labels=y)
+                # sklearn `partial_fit` uses max_iter = 1,
+                # manually start the local training cycles
+                for iter in range(training_hp["max_iter"]):
+                    logger.info("Local training epoch " + str(iter + 1) + ":")
+                    self.model.partial_fit(x, y, classes=self.classes, sample_weight=sample_weight)
+            elif isinstance(self.model, SGDRegressor):
+                for iter in range(training_hp["max_iter"]):
+                    logger.info("Local training epoch " + str(iter + 1) + ":")
+                    self.model.partial_fit(x, y, sample_weight=sample_weight)
             else:
-                raise ValueError('Provided model object is not of the correct '
-                                 'data type. Should be a `list`. '
-                                 'Type provided: ' + str(type(model_update)))
-
-    def _raw_predict(self, x):
-        """
-        Internal helper function for generating predictions for the model.
-
-        :param x: The input data for the prediction model.
-        :type x: `np.array`
-        :param model: Model object which is used for generating the prediction. \
-        If not provided the default model assigned internally will be used.
-        :type model: `XGBFLModel`
-        :return: raw_predictions
-        :rtype: `np.array`
-        """
-        logger.info('Performing Model Inference Process')
-
-        # Initialize Prediction Object
-        n_samples = x.shape[0]
-        preds = np.zeros(shape=(self.n_trees, n_samples))
-
-        # Append Null Model Predictions
-        if self._baseline_prediction is not None:
-            preds += self._baseline_prediction
-        elif self._raw_predictions is not None:
-            preds += self._raw_predictions
-
-        # Generate Predictions
-        for p in self._predictors:
-            for k, p_i in enumerate(p):
-                preds[k, :] += p_i.predict(x.astype(np.float64))
-
-        return preds
-
-    def get_model_update(self):
-        """
-        Since we are not using the conventional train() function at the local
-        training handler, we respectively do not implement anything within here,
-        nor this function is utilized anywhere else.
-        """
-        return NotImplementedError
-
-    @abstractmethod
-    def predict(self, x):
-        """
-        Given a set of inputs, generate inference for the given set of samples.
-
-        :param x: The input data for the prediction model.
-        :type x: `np.array`
-        :param model: Model object which is used for generating the prediction. \
-        If not provided the default model assigned internally will be used.
-        :type model: `XGBFLModel`
-        :return: `np.array`
-        """
-        return NotImplementedError
-
-    def save_model(self, filename=None, path=None):
-        """
-        Saves a sklearn model to file in the format specific
-        to the framework requirement as a pickle file (only for inference use).
-
-        :param filename: Name of the file where to store the model.
-        :type filename: `str`
-        :param path: Path of the folder where to store the model. If no path \
-        is specified, the model will be stored in the default data location of \
-        the library `DATA_PATH`.
-        :type path: `str`
-        :return: filename
+                raise NotImplementedError
+        except Exception as e:
+            logger.info(str(e))
+            raise LocalTrainingException("Error occurred while performing model.fit")
+
+    def update_model(self, model_update):
+        """
+        Update sklearn model with provided model_update, where model_update
+        should contains `coef_` and `intercept_` having the same dimension
+        as expected by the sklearn.linear_model.
+        `coef_` : np.ndarray, shape (1, n_features) if n_classes == 2
+        else (n_classes, n_features)
+        `intercept_` : np.ndarray, shape (1,) if n_classes == 2 else (n_classes,)
+
+        :param model_update: `ModelUpdate` object that contains the coef_ and \
+        the intercept vectors that will be used to update the model.
+        :type model_update: `ModelUpdate`
+        :return: None
         """
-        if filename is None:
-            file = self.model_name if self.model_name else self.model_type
-            filename = '{}_{}.pickle'.format(file, time.time())            
-            
-        full_path = super().get_model_absolute_path(filename)
-
-        # Scikit-Learn Export Demo
-        export_sklearn(self, full_path)
+        if isinstance(model_update, ModelUpdate):
+            weights = model_update.get("weights")
 
-        if len(self._predictors) > 0:
-            logger.info('Model saved in path: %s.', full_path)
+            if isinstance(self.model, SGDClassifier):
+                coef = np.array(weights)[:, :-1]
+                intercept = np.array(weights)[:, -1]
+            elif isinstance(self.model, SGDRegressor):
+                coef = np.array(weights)[:-1]
+                intercept = np.array(weights)[-1].reshape(
+                    1,
+                )
+            else:
+                raise LocalTrainingException(
+                    "Expecting scitkit-learn model of "
+                    "type either "
+                    "sklearn.linear_model.SGDClassifier "
+                    "or sklearn.linear_model.SGDRegressor."
+                    "Instead provided model is of type " + str(type(self.model))
+                )
+            try:
+                self.model.coef_ = coef
+                self.model.intercept_ = intercept
+            except Exception as e:
+                raise LocalTrainingException("Error occurred during " "updating the model weights. " + str(e))
         else:
-            logger.info('Persisting empty model in path: %s.', full_path)
+            raise LocalTrainingException(
+                "Provided model_update should be of "
+                "type ModelUpdate. "
+                "Instead they are: " + str(type(model_update))
+            )
 
-        return filename
-
-    @staticmethod
-    def load_model(filename):
+    def get_model_update(self):
         """
-        Load model from provided filename
+        Generates a `ModelUpdate` object that will be sent to other entities.
 
-        :param filename: Name of the file where to store the model.
-        :type filename: `str`
-        :param path: Path of the folder where to store the model. If no path \
-        is specified, the model will be stored in the default data location of \
-        the library `DATA_PATH`.
-        :type path: `str`
-        :return: Returns the corresponding model object.
-        :rtype: `XGBFLModel`
+        :return: ModelUpdate
+        :rtype: `ModelUpdate`
         """
-        absolute_path = config.get_absolute_path(filename)
-
-        with open(absolute_path, 'rb') as f:
-            model = pickle.load(f)
-            self = model
 
-        if len(self._predictors) == 0:
-            logger.info('Model does not contain any predictors.')
-
-        return model
+        coef = self.model.coef_
+        intercept = self.model.intercept_
+        if isinstance(self.model, SGDClassifier):
+            n_classes = len(self.get_classes())
+            # intercept is of shape (1,) if n_classes == 2 else (n_classes,)
+            if n_classes == 2:
+                intercept = np.reshape(intercept, [1, 1])
+            else:
+                intercept = np.reshape(intercept, [n_classes, 1])
+            w = np.append(coef, intercept, axis=1)
+        elif isinstance(self.model, SGDRegressor):
+            w = np.append(coef, intercept)
+        else:
+            raise LocalTrainingException(
+                "Expecting scitkit-learn model of "
+                "type either "
+                "sklearn.linear_model.SGDClassifier "
+                "or sklearn.linear_model.SGDRegressor."
+                "Instead provided model is of type " + str(type(self.model))
+            )
 
-    @abstractmethod
-    def get_loss(self):
-        """
-        Internal helper function used to obtain the corresponding loss function
-        which is dependent on the learning task set by the hyperparameters.
-
-        :return: Returns the loss object used to compute the loss function.
-        :rtype: `BaseLoss` based object
-        """
-        return NotImplementedError
+        return ModelUpdate(weights=w.tolist(), coef=self.model.coef_, intercept=self.model.intercept_)
 
     def evaluate(self, test_dataset, **kwargs):
         """
         Evaluates the model given testing data.
         :param test_dataset: Testing data, a tuple given in the form \
-        (x_test, test) or a datagenerator of of type `keras.utils.Sequence`, \
+        (x_test, test) or a datagenerator of of type `keras.utils.Sequence`,
         `keras.preprocessing.image.ImageDataGenerator`
         :type test_dataset: `np.ndarray`
+
         :param kwargs: Dictionary of metrics available for the model
         :type kwargs: `dict`
         """
 
         if type(test_dataset) is tuple:
             x_test = test_dataset[0]
             y_test = test_dataset[1]
 
             return self.evaluate_model(x_test, y_test)
 
         else:
             raise ModelException("Invalid test dataset!")
 
-    @abstractmethod
     def evaluate_model(self, x, y, **kwargs):
         """
-        Evaluates model given the samples x and true labels y.
-        Multiple evaluation metrics are returned in a dictionary
+        Evaluates the model given test data x and the corresponding labels y.
 
         :param x: Samples with shape as expected by the model.
-        :type x: Data structure as expected by the model \
-        :param y: Corresponding labels to x
-        :type y: Data structure the same as the type defines labels \
-        in testing data.
-        :param batch_size: Size of batches.
-        :type batch_size: `int`
-        :param kwargs: Dictionary of model-specific arguments.
-        :type kwargs: `dict`
-        :return: Dictionary with all evaluation metrics provided by specific \
-        implementation.
+        :type x: `np.ndarray`
+        :param y: Corresponding true labels to x
+        :type y: `np.ndarray`
+        :param kwargs: Optional sample weights accepted by model.score
+        :return: score, mean accuracy on the given test data and labels
         :rtype: `dict`
         """
-        return NotImplementedError
-
-
-class XGBRegressorFLModel(XGBFLModel):
-    _VALID_LOSSES = ('least_squares', 'least_absolute_deviation')
-
-    def predict(self, x):
-        """
-        Perform prediction for a batch of inputs.
-
-        :param x: Samples with shape as expected by the model.
-        :type x: Data structure as expected by the model
-        :param kwargs: Dictionary of model-specific arguments.
-        :type kwargs: `dict`
-        :return: Predictions
-        :rtype: Data structure the same as the type defines labels
-        in testing data.
-        """
-        if len(self._predictors) > 0:
-            return self._raw_predict(x).ravel()
-        else:
-            raise Exception('Model has not been trained yet.')
-
-    def encode_target(self, y):
-        """
-        Converts the input y to the expected dtype.
-
-        :param y: The corresponding target data from the dataset to encode.
-        :type y: `np.array`
-        :return: Returns the corresponding encoded y values.
-        :rtype: `np.array`
-        """
-        self.n_trees = 1
-        return y.astype(Y_DTYPE, copy=False)
-
-    def get_loss(self, sample_weight):
-        """
-        Given the initialized loss type defined under the hyerparameters, we
-        return the corresponding loss function to dictate the corresponding
-        learning task of the model.
+        acc = {}
+        acc["score"] = self.model.score(x, y, **kwargs)
 
-        :param sample_weight: Weights of training data
-        :type sample_weight: `np.ndarray`
-        :return: Returns the respective loss object as defined in the FL \
-        hyperparameters.
-        :rtype: Derivation of `BaseLoss`
-        """
-        return _LOSSES[self.loss](sample_weight=sample_weight)
+        y_pred = self.predict(x, **kwargs)
 
-    def evaluate_model(self, x, y, **kwargs):
-        """
-        Given an input set of values and their values, generate the prediction
-        and compute the R^2 metric.
-
-        :param x: The input data for the prediction model.
-        :type x: `np.array`
-        :param y: The corresponding target value of the prediction.
-        :type y: `np.array`
-        :return: Return a dictionary containing the R^2 metric for the provided
-        input and target values.
-        :rtype: `dict`
-        """
-        metrics = {}
-        if len(self._predictors) > 0:
-            y_hat = self.predict(x)
-            score = r2_score(y, y_hat)
-            metrics['r2_score'] = score
-            additional_metrics = fl_metrics.get_eval_metrics_for_regression(
-                y, y_hat)
-            metrics = {**metrics, **additional_metrics}
-            return metrics
+        if self.is_classification:
+            acc["acc"] = acc["score"]
+            additional_metrics = fl_metrics.get_eval_metrics_for_classificaton(y, y_pred)
         else:
-            logger.info('Model has not been trained yet.')
-            return {}
+            additional_metrics = fl_metrics.get_eval_metrics_for_regression(y, y_pred)
 
+        dict_metrics = {**acc, **additional_metrics}
 
-class XGBClassifierFLModel(XGBFLModel):
-    _VALID_LOSSES = ('binary_crossentropy', 'categorical_crossentropy', 'auto')
+        return dict_metrics
 
-    def encode_target(self, y):
+    def predict_proba(self, x):
         """
-        Converts the input y to the expected dtype and performs a label
-        encoding. Here, we assume that each party has at least one sample of
-        the corresponding class label type for each different classes.
-
-        :param y: The corresponding target data from the dataset to encode.
-        :type  y: `np.array`
-        :return y: Returns the corresponding encoded y values.
-        :rtype  y: `np.array`
-        """
-        # Validate Classification Target Values
-        check_classification_targets(y)
-
-        # Apply Label Encoder Transformation
-        lab_enc = LabelEncoder()
-        enc_y = lab_enc.fit_transform(y).astype(np.float64, copy=False)
-
-        # Extract Encoded Target Sizes
-        self.classes_ = lab_enc.classes_
-        if self.classes_.shape[0] != self.num_classes:
-            raise ValueError('Number of classes defined in configuration file '
-                             'and the classes derived from the data does not '
-                             'match. Found {0} classes, while config file '
-                             'is defined as {1} classes.'.format(
-                             self.classes_.shape[0], self.num_classes))
+        Perform prediction for the given input.
 
-        if self.loss == 'auto':
-            self.n_trees = 1 if self.classes_.shape[0] <= 2 else self.classes_.shape[0]
+        :param x: Samples with shape as expected by the model.
+        :type x: `np.ndarray`
+        :return: Array of predictions
+        :rtype: `np.ndarray`
+        """
+        return self.model.predict_proba(x)
+
+    def get_classes(self, labels=None):
+        """
+        Returns an array of shape (n_classes,). If self.classes is not None,
+        return self.classes, else obtains the array based on provided labels.
+
+        :param labels: Provided class labels to obtain the array of classes.
+        :type labels: `numpy.ndarray`
+        :return: An array of shape `(n_classes,)`.
+        :rtype: `numpy.ndarray`
+        """
+        if isinstance(self.model, SGDRegressor):
+            raise ValueError("SGDRegressor is used for regression problems and " "does not have `classes_`.")
+        if self.classes is not None:
+            return self.classes
+        elif hasattr(self.model, "classes_"):
+            return self.model.classes_
+        elif labels is not None:
+            return np.unique(labels)
         else:
-            self.n_trees = 1 if self.num_classes <= 2 else self.num_classes
-
-        return enc_y
+            raise NotFittedError(
+                "The scikit-learn model has not been initialized with "
+                "`classes_` attribute, "
+                "please either manually specify `classes_` attribute as "
+                "an array of shape (n_classes,) or "
+                "provide labels to obtain the array of classes. "
+            )
 
-    def get_loss(self, sample_weight):
+    def save_model(self, filename=None, path=None):
         """
-        Given the initialized loss type defined under the hyerparameters, we
-        return the corresponding loss function to dictate the corresponding
-        learning task of the model. If auto is selected, then we will
-        automatically determine whether the classification task is binary or
-        multiclass given the label encoding cardinality.
-
-        :param sample_weight: Weights of training data
-        :type sample_weight: `np.ndarray`
-        :return: Returns the respective loss object as defined in the FL \
-        hyperparameters.
-        :rtype: Derivation of `BaseLoss`
-        """
-        if (self.loss == 'categorical_crossentropy' and self.n_trees == 1):
-            raise ValueError("Incompatible loss and target variable counts.")
-
-        if self.loss == 'auto':
-            return _LOSSES['binary_crossentropy']() if self.n_trees == 1 else \
-                _LOSSES['categorical_crossentropy']()
-        else:
-            return _LOSSES[self.loss](sample_weight=sample_weight)
+        Save a sklearn model to file in the format specific
+        to the framework requirement.
 
-    def predict(self, x, **kwargs):
+        :param filename: Name of the file where to store the model.
+        :type filename: `str`
+        :param path: Path of the folder where to store the model. If no path \
+        is specified, the model will be stored in the default data location of \
+        the library `DATA_PATH`.
+        :type path: `str`
+        :return: filename
         """
-        Perform prediction for a batch of inputs returned as class values.
+        if filename is None:
+            file = self.model_name if self.model_name else self.model_type
+            filename = SKLearnPersistence.model_filename(file)
 
-        :param x: Samples with shape as expected by the model.
-        :type x: Data structure as expected by the model
-        :param kwargs: Dictionary of model-specific arguments.
-        :type kwargs: `dict`
-        :return: Predictions based on class values.
-        :rtype: Data structure the same as the type defines labels
-        in testing data.
-        """
-        if len(self._predictors) > 0:
-            encoded_classes = np.argmax(self.predict_proba(x), axis=1)
-            return self.classes_[encoded_classes]
-        else:
-            raise Exception('Model has not been trained yet.')
+        full_path = super().get_model_absolute_path(filename)
 
-    def predict_proba(self, x, **kwargs):
-        """
-        Perform prediction for a batch of inputs as probabilities.
+        # check if the classes_ attribute exists for SGDClassifier
+        # this attribute is required for prediction and scoring
+        if isinstance(self.model, SGDClassifier) and not hasattr(self.model, "classes_"):
+            logger.warning(
+                "The classification model to be saved has no `classes_` " "attribute and cannot be used for prediction!"
+            )
 
-        :param x: Samples with shape as expected by the model.
-        :type x: Data structure as expected by the model
-        :param kwargs: Dictionary of model-specific arguments.
-        :type kwargs: `dict`
-        :return: A probabilistic set of predictions
-        :rtype: Data structure the same as the type defines labels \
-        in testing data.
-        """
-        if len(self._predictors) > 0:
-            raw_predictions = self._raw_predict(x)
-            return self.loss_.predict_proba(raw_predictions)
-        else:
-            raise Exception('Model has not been trained yet.')
+        SKLearnPersistence.save_model(self.model, full_path)
+        logger.info("Model saved in path: %s.", full_path)
+        return filename
 
-    def evaluate_model(self, x, y, **kwargs):
+    @staticmethod
+    def load_model_from_spec(model_spec):
         """
-        Given an input set of values and their values, generate the prediction
-        and compute the accuracy.
+        Loads model from provided model_spec, where model_spec is a `dict`
+        that contains the following items: model_spec['model_definition']
+        contains the model definition as
+        type sklearn.linear_model.SGDClassifier
+        or sklearn.linear_model.SGDRegressor.
+
+        :param model_spec: Model specification contains \
+        a compiled sklearn model.
+        :param model_spec: `dict`
+        :return: model
+        :rtype: `sklearn.cluster`
+        """
+        model = None
+        try:
+            if "model_definition" in model_spec:
+                model_file = model_spec["model_definition"]
+                model_absolute_path = config.get_absolute_path(model_file)
+
+                model = SKLearnPersistence.load_model(model_absolute_path)
+                
+                if not issubclass(type(model), (SGDClassifier, SGDRegressor)):
+                    raise ValueError(
+                        "Provided compiled model in model_spec "
+                        "should be of type sklearn.linear_model."
+                        "Instead it is: " + str(type(model))
+                    )
+        except Exception as ex:
+            raise ModelInitializationException("Model specification was " "badly formed. " + str(ex))
+        return model
 
-        :param x: The input data for the prediction model.
-        :type x: `np.array`
-        :param y: The corresponding target value of the prediction.
-        :type y: `np.array`
-        :return: Return a dictionary containing the accuracy for the provided \
-        input and target values.
-        :rtype: `dict`
+    def is_fitted(self):
         """
-        acc = {}
-        if len(self._predictors) > 0:
-            y_hat = self.predict(x)
-            correct = 0
-            for i in range(x.shape[0]):
-                if y_hat[i] == y[i]:
-                    correct += 1
-
-            acc = {'acc': correct/float(len(y))}
-            additional_metrics = fl_metrics.get_eval_metrics_for_classificaton(
-                y, y_hat)
+        Return a boolean value indicating if the model is fitted or not.
 
-            acc = {**acc, **additional_metrics}
-            return acc
-        else:
-            logger.info('No models have been trained yet.')
-            return {}
+        :return: res
+        :rtype: `bool`
+        """
+        try:
+            check_is_fitted(self.model, ("coef_", "intercept_"))
+        except NotFittedError as ex:
+            logger.warning("Model has no attribute `coef_` and `intercept_`, " "and hence is not fitted yet.")
+            return False
+        return True
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/party/party.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/party/party.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,217 +1,226 @@
-#  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
-#  https://opensource.org/licenses/BSD-3-Clause
-#  -----------------------------------------------------------------------------------------
+#  (C) Copyright IBM Corp. 2024.
+#
+#  Licensed under the Apache License, Version 2.0 (the "License");
+#  you may not use this file except in compliance with the License.
+#  You may obtain a copy of the License at
+#
+#       http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
 
+import argparse
+import logging
+import os
 import re
 import sys
-import os
 import time
-import logging
-import argparse
 from zipfile import ZipFile
 
-fl_path = os.path.abspath('.')
+fl_path = os.path.abspath(".")
 if fl_path not in sys.path:
     sys.path.append(fl_path)
 
 import ibmfl.envs as fl_envs
+import ibmfl.util.config as fl_config
 from ibmfl._version import __version__
+from ibmfl.connection.route_declarations import get_party_router
 from ibmfl.connection.router_handler import Router
+from ibmfl.exceptions import FLException, InvalidConfigurationException
 from ibmfl.message.message import Message
 from ibmfl.message.message_type import MessageType
-from ibmfl.exceptions import FLException
-from ibmfl.connection.route_declarations import get_party_router
-import ibmfl.util.config as fl_config
-from ibmfl.util.config import read_yaml_config, get_class_by_name, \
-	configure_logging_from_file, get_party_config, convert_bytes_to_zip
+from ibmfl.util.config import (
+    configure_logging_from_file,
+    convert_bytes_to_zip,
+    get_class_by_name,
+    get_lt_from_config,
+    get_model_from_config,
+    get_party_config,
+    read_yaml_config,
+)
 
 logger = logging.getLogger(__name__)
 
 
 class Party:
     """
-    Party participates in Federated Learning training.  
+    Party participates in Federated Learning training.
     """
 
     def __init__(self, **kwargs):
         """
-        Initializes a party based on the configuration provided in 
+        Initializes a party based on the configuration provided in
         either a yaml file or a dictionary.
         :param config_file: path to yml file with the configuration of the party
         :type config_file: `str`
         :param config_dict: dictionary containing the configuration of the party
         :type config_dict: `dict`
         :return None
         """
-        self.isCloud = False
-        self.isXGB = False
-        log_level = kwargs.get('log_level', 'INFO')
+        log_level = kwargs.get("log_level", "INFO")
         configure_logging_from_file(log_level=log_level)
 
-        config_file = kwargs.get('config_file')
-        config_dict = kwargs.get('config_dict')
-        if config_file != None: 
+        config_file = kwargs.get("config_file")
+        config_dict = kwargs.get("config_dict")
+        if config_file != None:
             config_dict = fl_config.read_yaml_config(config_file=config_file)
 
-        config_dict['connection']['name'] = 'WSConnection'
-        config_dict['connection']['path'] = 'ibmfl.connection.websockets_connection'
-
-        if config_dict.get('aggregator', None) is not None:
-            if config_dict.get('aggregator').get('ip', None) is not None:
-                if "cloud.ibm.com" in config_dict.get('aggregator').get('ip'):
-                    self.isCloud = True       
-                    if config_dict.get('local_training', None) is not None:
-                        if config_dict.get('local_training').get('path', None) is not None:
-                            if "ibmfl.party.training.xgboost_local_training_handler" in config_dict.get('local_training').get('path'):
-                                self.isXGB = True
-                                config_dict['local_training']['path'] = 'ibmfl.party.training.v2.xgboost_local_training_handler'
-        
-        logger.info("IS CLOUD = " + str(self.isCloud))
-        logger.info("IS XGB = " + str(self.isXGB))
+        config_dict["connection"]["name"] = "SWSConnection"
+        config_dict["connection"]["path"] = "ibmfl.connection.websockets_connection"
+        # Remove and save possible incomplete local training to be set later with agg config
+        self.local_training = config_dict.pop("local_training", {})
+        self.local_training_handler = None
 
         cls_config = fl_config.get_cls_by_config(config_dict)
 
         self.party_config = cls_config
 
         self.data_handler = None
         self.fl_model = None
         self.is_running = False
 
-        data_config = cls_config.get('data')
-        connection_config = cls_config.get('connection')
-        ph_config = cls_config.get('protocol_handler')
+        data_config = cls_config.get("data")
+        connection_config = cls_config.get("connection")
+        ph_config = cls_config.get("protocol_handler")
+        mrec_config = cls_config.get("metrics_recorder")
 
         try:
             # Load data (optional field)
             # - in some cases the aggregator doesn't have data for testing purposes
-            data_cls_ref = data_config.get('cls_ref')
-            data_info = data_config.get('info')
+            data_cls_ref = data_config.get("cls_ref")
+            data_info = data_config.get("info")
             self.data_handler = data_cls_ref(data_config=data_info)
-
             # Read and create model (optional field)
             # In some cases aggregator doesn't need to load the model:
-            
+
             self.fl_model = None
 
             # Load hyperparams
-            self.hyperparams = cls_config.get('hyperparams')
-            self.agg_info = cls_config.get('aggregator')
+            self.hyperparams = cls_config.get("hyperparams")
+            self.agg_info = cls_config.get("aggregator")
+            self.agg_info["ip"] = self.agg_info["ip"] + "/aggregator"
+            self.agg_info["port"] = 443
 
-            connection_cls_ref = connection_config.get('cls_ref')
-            self.connection_info = connection_config.get('info')
+            connection_cls_ref = connection_config.get("cls_ref")
+            self.connection_info = connection_config.get("info")
             self.connection_info["VERSION"] = __version__
-            connection_synch = connection_config.get('sync')
-            # Use opt-in privacy for watsonx.ai, to get metrics by default
-            connection_private = self.connection_info.get('private',False)
+            self.connection_info["ip"] = self.agg_info["ip"]
+            self.connection_info["port"] = self.agg_info["port"]
+            connection_synch = connection_config.get("sync")
+            # Use opt-in privacy for WML, to get metrics by default
+            connection_private = self.connection_info.get("private", False)
             self.connection = connection_cls_ref(self.connection_info)
             self.connection.initialize_sender()
 
+            ph_cls_ref = ph_config.get("cls_ref")
+            if mrec_config:
+                mrec_cls_ref = mrec_config.get("cls_ref")
+                metrics_recorder = mrec_cls_ref(
+                    mrec_config.get("output_file"),
+                    mrec_config.get("output_type"),
+                    mrec_config.get("compute_pre_train_eval"),
+                    mrec_config.get("compute_post_train_eval"),
+                )
+            else:
+                metrics_recorder = None
 
-            self.local_training_handler = None
-        
-            ph_cls_ref = ph_config.get('cls_ref')
-
-            self.proto_handler = ph_cls_ref(self.fl_model,
-                                            self.connection.sender,
-                                            self.data_handler,
-                                            self.local_training_handler,
-                                            agg_info=self.agg_info,
-                                            synch=connection_synch,
-                                            is_private=connection_private)
+            self.proto_handler = ph_cls_ref(
+                self.fl_model,
+                self.connection.sender,
+                self.data_handler,
+                self.local_training_handler,
+                metrics_recorder,
+                agg_info=self.agg_info,
+                synch=connection_synch,
+                is_private=connection_private,
+            )
 
             self.router = Router()
             get_party_router(self.router, self.proto_handler)
 
-            self.connection.initialize_receiver(router=self.router)
-
             # check for token
-            token = kwargs.get('token', None)
-            self_signed_cert_flag = kwargs.get('self_signed_cert', None)
-            self.connection.initialize(self.router, self.agg_info, self_signed_cert_flag, token)
+            token = kwargs.get("token", None)
+            self_signed_cert_flag = kwargs.get("self_signed_cert", None)
+            # self.connection.initialize(router=self.router, aggIinfo=self.agg_info, ssl_context=self_signed_cert_flag, authToken=token)
+            self.connection.initialize_receiver(router=self.router, verify=not self_signed_cert_flag, token=token)
+
+            self.connection.start()
 
         except Exception as ex:
-            logger.info('Error occurred '
-                        'while loading aggregator configuration')
+            logger.info("Error occurred " "while loading aggregator configuration")
             logger.exception(ex)
         else:
             logger.info("Party initialization successful")
 
-        self.agg_info = cls_config.get('aggregator')
-
+        self.agg_info = cls_config.get("aggregator")
 
     def initialize_model_config(self):
-        logger.info('Initializing model configuration')
+        logger.info("Initializing model configuration")
         cls_config = self.party_config
-        model_config = cls_config.get('model')
-        lt_config = cls_config.get('local_training')
-
+        model_config = cls_config.get("model")
+        lt_config = cls_config.get("local_training")
         try:
-            
             # Read and create model (optional field)
             # In some cases aggregator doesn't need to load the model:
-            model_cls_ref = model_config.get('cls_ref')
-            if self.isCloud and self.isXGB:
-                previous_cls_ref = fl_config.get_class_by_name("ibmfl.model.xgb_fl_model", "XGBClassifierFLModel")
-                if model_cls_ref == previous_cls_ref:
-                    model_cls_ref = fl_config.get_class_by_name("ibmfl.model.v2.xgb_fl_model", "XGBClassifierFLModel")
-                else:
-                    previous_cls_ref = fl_config.get_class_by_name("ibmfl.model.xgb_fl_model", "XGBRegressorFLModel")
-                    if model_cls_ref == previous_cls_ref:
-                        model_cls_ref = fl_config.get_class_by_name("ibmfl.model.v2.xgb_fl_model", "XGBRegressorFLModel")
+            model_cls_ref = get_model_from_config(model_config).get("cls_ref")
+            spec = model_config.get("spec")
+            info = model_config.get("info")
+            self.fl_model = model_cls_ref("", spec, info=info)
 
-            spec = model_config.get('spec')
-            self.fl_model = model_cls_ref('', spec)
+        except Exception as ex:
+            logger.error("Error occurred while loading model config. Please check for missing libraries.")
+            logger.exception(ex)
+            raise InvalidConfigurationException("Error occurred while loading model config.")
 
-            # Load hyperparams
-            self.hyperparams = cls_config.get('hyperparams')
+        try:
+            self.hyperparams = cls_config.get("hyperparams")
 
-            lt_cls_ref = lt_config.get('cls_ref')
+            lt_cls_ref = lt_config.get("cls_ref")
+            lt_info = lt_config.get("info")
             self.local_training_handler = lt_cls_ref(
-                self.fl_model, self.data_handler)
+                self.fl_model, self.data_handler, hyperparams=self.hyperparams, info=lt_info
+            )
 
             self.proto_handler.set_model(self.fl_model)
             self.proto_handler.set_training_handler(self.local_training_handler)
-            
 
         except Exception as ex:
-            logger.info('Error occurred '
-                        'while loading model and lth configuration')
+            logger.info("Error occurred " "while loading model and lth configuration")
             logger.exception(ex)
         else:
             logger.info("Party model initialization successful")
 
     def register_party(self):
         """
         Registers party with the aggregator.
 
         :param: None
         :return: None
         """
-        logger.info('Registering party...')
+        logger.info("Registering party...")
         returnValue = False
-        register_message = Message(
-            MessageType.REGISTER.value, data=self.connection_info)
+        register_message = Message(MessageType.REGISTER.value, data=self.connection_info)
         try:
-            response = self.connection.sender.send_message(
-                self.agg_info, register_message)
-            if response is not None and response.get_data()['status'] == 'success':
-
+            response = self.connection.sender.send_message(self.agg_info, register_message)
+            if response is not None and response.get_data()["status"] == "success":
                 data = response.get_data()
-                if self.extract_model_from_stream(data):
-                    logger.info('Registration Successful')
+                if self.extract_config_from_stream(data):
+                    logger.info("Registration Successful")
                     returnValue = True
                 else:
-                    logger.info('Registration Failed: Model processing error')
+                    logger.info("Registration Failed: Model processing error")
                     returnValue = False
             else:
-                logger.error('Registration Failed: Failure status from aggregator')
+                logger.error("Registration Failed: Failure status from aggregator")
                 if response is not None:
-                    logger.error(response.get_data().get('detail', "No Detail Provided"))
+                    logger.error(response.get_data().get("detail", "No Detail Provided"))
         except Exception as ex:
             logger.error("Error occurred during registration" + str(ex))
 
         return returnValue
 
     def stop_connection(self):
         """
@@ -243,138 +252,177 @@
         except Exception as ex:
             logger.error("Error occurred during evaluation.")
             logger.error(ex)
 
     def start(self):
         """
         Initializes connection and registers with the aggregator,
-        then accept commands from the aggregator to effect training.  
+        then accept commands from the aggregator to effect training.
 
         :param: None
         :return: None
         """
 
         if not self.is_running:
-            logger.info('Party not registered yet.')
+            logger.info("Party not registered yet.")
             if self.register_party():
-                logger.info('Listening for commands from Aggregator')
+                logger.info("Listening for commands from Aggregator")
                 self.is_running = True
                 self.initialize_model_config()
             else:
                 self.stop_connection()
+                raise FLException("Error occurred while registering party. ")
         else:
-            logger.info('Party already running.')
+            logger.info("Party already running.")
 
-    def extract_model_from_stream(self, data):
+    def extract_config_from_stream(self, data):
         """Read model from response stream and extract the content.
         :param data: response data recieved from aggregator.
-        :type data: `json` 
+        :type data: `json`
         :return: None
         """
         returnValue = False
         working_dir = fl_envs.working_directory
 
-        if 'model_package' in data:
-            logger.info(
-                'Model Package found, placing in working_dir ' + working_dir)
-            model_file_name = 'model_package.zip'
+        if "model_package" in data:
+            logger.info("Model Package found, placing in working_dir " + working_dir)
+            model_file_name = "model_package.zip"
             model_file_op = os.path.join(working_dir, model_file_name)
-            model_pk_bytes = data.get('model_package')
+            model_pk_bytes = data.get("model_package")
 
             try:
                 convert_bytes_to_zip(model_pk_bytes, model_file_op)
 
-                with ZipFile(model_file_op, 'r') as o:
+                with ZipFile(model_file_op, "r") as o:
                     o.extractall(working_dir)
 
             except Exception as ex:
-                logger.exception(
-                    "Error occurred while unpacking model package from aggregator.")
-                raise FLException(
-                    "Error occurred while downloading model package. ")
+                logger.exception("Error occurred while unpacking model package from aggregator.")
+                raise FLException("Error occurred while downloading model package. ")
         else:
             logger.info("No model package received from aggregator.")
 
-        logger.info('Checking for model config in payload.')
+        logger.info("Checking for model config in payload.")
 
-        model_config_agg = data.get('model_config', None)
+        model_config_agg = data.get("model_config", None)
         if model_config_agg == None:
             logger.info("No model config received from aggregator.")
             return returnValue
-        
-        if 'is_model_working_dir' in model_config_agg.get('spec'):
-            model_config_agg['spec']['model_definition'] = working_dir
-            
-        elif 'model_definition' in  model_config_agg.get('spec'):
-            model_def = os.path.join(working_dir, model_config_agg['spec']['model_definition'])
-            model_config_agg['spec']['model_definition'] = model_def
 
-        logger.info(model_config_agg)
-        self.party_config['model'] = model_config_agg
+        if "is_model_working_dir" in model_config_agg.get("spec"):
+            model_config_agg["spec"]["model_definition"] = working_dir
+
+        elif "model_definition" in model_config_agg.get("spec"):
+            model_def = os.path.join(working_dir, model_config_agg["spec"]["model_definition"])
+            model_config_agg["spec"]["model_definition"] = model_def
+
+        logger.info("Aggregator model config: %s" % str(model_config_agg))
+        model_config_local = self.party_config.get("model") or {}
+        logger.info("Local model config: %s" % str(model_config_local))
+        model_config_agg.update(model_config_local)
+        logger.info("Merged model config: %s" % str(model_config_agg))
+
+        self.party_config["model"] = model_config_agg
+
+        local_training_agg = data.get("local_training")
+        logger.info("Default local training: %s" % str(local_training_agg))
+        logger.info("Configured local training: %s" % str(self.local_training))
+
+        # Merge local trainging config:
+        # name and path may be overridden:
+        if "name" in self.local_training:
+            local_training_agg["name"] = self.local_training["name"]
+        if "path" in self.local_training:
+            local_training_agg["path"] = self.local_training["path"]
+        # Crypto key_mgr_info may be overridden
+        if (
+            "info" in self.local_training
+            and ("crypto" in self.local_training["info"])
+            and ("key_manager" in self.local_training["info"]["crypto"])
+            and ("key_mgr_info" in self.local_training["info"]["crypto"]["key_manager"])
+            and ("info" in local_training_agg)
+            and ("crypto" in local_training_agg["info"])
+            and ("key_manager" in local_training_agg["info"]["crypto"])
+        ):
+            local_training_agg["info"]["crypto"]["key_manager"]["key_mgr_info"] = self.local_training["info"]["crypto"][
+                "key_manager"
+            ]["key_mgr_info"]
+
+        logger.info("Merged local training: %s" % str(local_training_agg))
+
+        if local_training_agg == None:
+            logger.error("No local training config received from aggregator.")
+            return returnValue
+        self.party_config["local_training"] = get_lt_from_config(local_training_agg)
+
+        logger.info("config extract finished!")
 
-        logger.info('model extract finished!')
         returnValue = True
         return returnValue
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     """
     Main function to run a party-side application in a Federated Learning training job.
     The application can either be run in an interactive ( -i ), or a non-interactive
-    mode.  
+    mode.
 
     In interactive mode, enter commands:
 
     START     - Start the connection for communication with the aggregator
-    STOP      - Stop the connection 
+    STOP      - Stop the connection
     REGISTER  - Register with the aggregator and start accepting commands
     EVAL      - Evaluate the model with local test data
 
     In non-interactive mode, the party will be started, and registered with
     the aggregator.  After training is complete and a STOP message is received
     from the aggregator, the application will exit.
 
     """
 
-    parser = argparse.ArgumentParser(
-        description='Federated Learning Party')
-    parser.add_argument('config_file', help='yaml configuration file')
-    parser.add_argument('token', help='authentication token')
-    parser.add_argument('-s', '--self_signed_cert', help='flag for self-signed certificate', action='store_true')
-    parser.add_argument('-i', '--interactive',
-                        help='run interactively', action='store_true')
-    parser.add_argument('-l', '--log_level', type=str, default="INFO", required=False,
-                        help='log_level should be a value from [DEBUG, INFO, WARNING, ERROR, CRITICAL]')
+    parser = argparse.ArgumentParser(description="WML Federated Learning Party")
+    parser.add_argument("config_file", help="yaml configuration file")
+    parser.add_argument("token", help="authentication token")
+    parser.add_argument("-s", "--self_signed_cert", help="flag for self-signed certificate", action="store_true")
+    parser.add_argument("-i", "--interactive", help="run interactively", action="store_true")
+    parser.add_argument(
+        "-l",
+        "--log_level",
+        type=str,
+        default="INFO",
+        required=False,
+        help="log_level should be a value from [DEBUG, INFO, WARNING, ERROR, CRITICAL]",
+    )
     args = parser.parse_args()
 
-    if (args.self_signed_cert):
+    if args.self_signed_cert:
         self_signed_cert_arg = True
     else:
         self_signed_cert_arg = None
 
-    p = Party(config_file=args.config_file, token=args.token, self_signed_cert=self_signed_cert_arg,
-              log_level=args.log_level)
+    p = Party(
+        config_file=args.config_file, token=args.token, self_signed_cert=self_signed_cert_arg, log_level=args.log_level
+    )
 
-    if (args.interactive):
+    if args.interactive:
         # Indefinite loop to accept user commands to execute
         while 1:
-        	
             try:
                 msg = sys.stdin.readline()
-                if re.match('START', msg):
+                if re.match("START", msg):
                     p.start()
 
-                if re.match('STOP', msg):
+                if re.match("STOP", msg):
                     p.stop_connection()
                     break
 
-                if re.match('REGISTER', msg):
+                if re.match("REGISTER", msg):
                     p.register_party()
 
-                if re.match('EVAL', msg):
+                if re.match("EVAL", msg):
                     p.evaluate_model()
 
             except Exception as ex:
                 logger.error("Error occurred during " + msg)
                 logger.error(ex)
     else:
         try:
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/party/party_protocol_handler.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/party/party_protocol_handler.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,35 +1,45 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
 import abc
-import time
 import logging
 import threading
-
+import time
 from multiprocessing.pool import ThreadPool
+
+from ibmfl.exceptions import LocalTrainingException, PreprocessException
 from ibmfl.message.message import Message, ResponseMessage
 from ibmfl.message.message_type import MessageType
-from ibmfl.exceptions import LocalTrainingException
 from ibmfl.party.status_type import StatusType
 
 logger = logging.getLogger(__name__)
 
 
 class PartyProtocolHandler(abc.ABC):
     """
     Base class for all PartyProtocolHandlers
     """
 
-    def __init__(self, fl_model, connection, data_handler,
-                 local_training_handler, metrics_recorder=None,
-                 hyperparams=None, agg_info=None, synch=False, is_private=True,
-                 **kwargs):
+    def __init__(
+        self,
+        fl_model,
+        connection,
+        data_handler,
+        local_training_handler,
+        metrics_recorder=None,
+        hyperparams=None,
+        agg_info=None,
+        synch=False,
+        is_private=True,
+        local_preprocess_handler=None,
+        **kwargs
+    ):
         """
         Initiate PartyProtocolHandler with provided fl_model, connection,
         data_handler and local_training_handler, hyperparams, aggregator
         information and synchronous flag are optional.
 
         :param fl_model: model to be trained
         :type fl_model: `model.FLModel`
@@ -58,16 +68,17 @@
         self.agg_info = agg_info
         self.lock = threading.Lock()
         self.status = StatusType.IDLE
         self.synch = synch
         self.pool = ThreadPool(processes=1)
         self.local_training_handler = local_training_handler
         self.metrics_recorder = metrics_recorder
-        if ( self.local_training_handler is not None ):
+        if self.local_training_handler is not None:
             self.local_training_handler.set_metrics_recorder_obj(self.metrics_recorder)
+        self.local_preprocess_handler = local_preprocess_handler
         self.is_private = is_private
 
     def get_handle(self, message_type):
         """
         Get handler for given message type.
 
         :param message_type: request message type
@@ -78,115 +89,116 @@
             return self.local_training_handler.save_model
         elif message_type == MessageType.SYNC_MODEL.value:
             return self.local_training_handler.sync_model
         elif message_type == MessageType.EVAL_MODEL.value:
             return self.local_training_handler.eval_model
         elif message_type == MessageType.TRAIN.value:
             return self.local_training_handler.train
+        elif message_type == MessageType.PREPROCESS.value:
+            return self.local_preprocess_handler.local_preprocess
+        elif message_type == MessageType.REQUEST_CERT.value:
+            return self.local_training_handler.request_cert
+        elif message_type == MessageType.GENERATE_KEYS.value:
+            return self.local_training_handler.generate_keys
+        elif message_type == MessageType.DISTRIBUTE_KEYS.value:
+            return self.local_training_handler.distribute_keys
+        elif message_type == MessageType.SET_KEYS.value:
+            return self.local_training_handler.set_keys
         else:
             raise LocalTrainingException("Unsupported message type!")
 
     def handle_request(self, msg):
         """
         Handle all incoming requests and route it to respective methods
         in local training handler.
 
         :param msg: Message object form connection
         :type msg: `Message`
         :return: Response message sent back to requester
         :rtype: ResponseMessage
 
         """
-        logger.info("Received request from aggregator")
+
         message_type = msg.message_type
-        logger.info("Received request in with message_type:  " +
-                    str(message_type))
+        logger.info("Received request from aggregator with message_type:  " + str(message_type))
 
         data = msg.get_data()
 
         response_msg = ResponseMessage(req_msg=msg)
         response_data = {"status": "success"}
         status = True
-        logger.info("Received request in PH " + str(message_type))
 
         try:
-            if message_type is MessageType.TRAIN.value:
-                if self.metrics_recorder:
-                    self.metrics_recorder.add_entry()
-                    self.metrics_recorder.set_round_no(self.local_training_handler.n_completed_trains)
-            elif message_type is MessageType.STOP.value:
+            if message_type is MessageType.STOP.value:
                 self.status = StatusType.STOPPING
                 # May be no need to send response
-                response_msg = ResponseMessage(message_type=MessageType.ACK.value,
-                                               id_request=-1,
-                                               data={'ACK': True})
+                response_msg = ResponseMessage(
+                    message_type=MessageType.ACK.value,
+                    id_request=-1,
+                    data={"ACK": True, "id_request": msg.get_header()["id_request"]},
+                )
                 logger.info("received a STOP request")
                 return response_msg
-        
+
             self.wait_for_model_initialization()
+
+            (first_train_msg, last_train_msg) = self.local_training_handler.determine_train_msg_seq(
+                message_type, data.get("payload")
+            )
+            if first_train_msg and self.metrics_recorder:
+                self.metrics_recorder.add_entry()
+                self.metrics_recorder.set_round_no(self.local_training_handler.get_n_completed_trains())
+
             handler = self.get_handle(message_type)
+            response = handler(data.get("payload"))
 
-            response = handler(data.get('payload'))
-            if message_type is MessageType.TRAIN.value:
+            if last_train_msg:
                 self.local_training_handler.n_completed_trains += 1
                 if not self.is_private:
                     metrics_handler = self.get_handle(MessageType.EVAL_MODEL.value)
-                    metrics = metrics_handler(data.get('payload'))
-                    response_data['metrics'] = metrics
+                    metrics = metrics_handler(data.get("payload"))
+                    response_data["metrics"] = metrics
+                if self.metrics_recorder:
+                    self.metrics_recorder.write_metrics()
             elif message_type is MessageType.EVAL_MODEL.value:
                 self.local_training_handler.n_completed_evals += 1
 
         except Exception as ex:
             logger.exception(ex)
-            raise LocalTrainingException(
-                "Error occurred while handling request")
+            raise LocalTrainingException("Error occurred while handling request")
 
-        response_data['payload'] = response
+        response_data["payload"] = response
         if not status:
-            response_data['status'] = "error"
+            response_data["status"] = "error"
 
         response_msg.set_data(response_data)
         return response_msg
 
     def execute_async(self, id_request, msg):
         """
         Handle run in a different thread to allow asynchronous requests.
 
         :param msg: Message object form connection
         :type msg: `Message`
         """
         try:
             # Acquire lock so that we do not run train twice
             self.lock.acquire()
-            logger.info("Handling async request in a separate thread")
+            logger.debug("Handling async request in a separate thread")
             response_msg = self.handle_request(msg)
 
         except Exception as ex:
-            logger.info("Exception occurred while async handling of msg: "
-                        + msg)
+            logger.info("Exception occurred while async handling of msg: " + msg)
             logger.exception(ex)
 
             response_msg = ResponseMessage(msg)
-            response_msg.set_data({
-                "status": "error",
-                "payload": None
-            })
-        logger.info("successfully finished async request")
+            response_msg.set_data({"status": "error", "payload": None})
+        logger.debug("successfully finished async request")
         self.connection.send_message(self.agg_info, response_msg)
 
-        if self.metrics_recorder:
-            with open(self.metrics_recorder.get_output_file(), 'w') as metrics_file:
-                metrics_output_type = self.metrics_recorder.get_output_type()
-                if metrics_output_type == 'json':
-                    metrics_output_data = self.metrics_recorder.to_json()
-                else:
-                    logger.info("Bad metrics output filetype. Defaulting to json.")
-                    metrics_output_data = self.metrics_recorder.to_json()
-                metrics_file.write('{}\n'.format(metrics_output_data))
-
         # Release lock
         self.lock.release()
         return
 
     def handle_async_request(self, msg):
         """
         Handle all incoming requests asynchronously and route it to respective
@@ -194,44 +206,64 @@
 
         :param msg: Message object form connection
         :type msg: `Message`
         :return: Response message sent back to requester
         :rtype: ResponseMessage
         """
         try:
-            response_msg = ResponseMessage(message_type=MessageType.ACK.value,
-                                           id_request=-1,
-                                           data={'ACK': True})
-            logger.info("received a async request")
-
-            id_request = msg.get_header()['id_request']
-
-            self.pool.apply_async(
-                self.execute_async, args=(id_request, msg))
-            logger.info("finished async request")
+            response_msg = ResponseMessage(
+                message_type=MessageType.ACK.value,
+                id_request=-1,
+                data={"ACK": True, "id_request": msg.get_header()["id_request"]},
+            )
+            logger.debug("received a async request")
+
+            id_request = msg.get_header()["id_request"]
+
+            self.pool.apply_async(self.execute_async, args=(id_request, msg))
+            logger.debug("finished async request")
         except Exception as ex:
             logger.info(ex)
 
         return response_msg
 
+    def handle_prepreocess_request(self, msg):
+        logger.info("Received preprocess request from aggregator")
+        message_type = msg.message_type
+        logger.info("Received request in with message_type: {}".format(message_type))
+
+        data = msg.get_data()
+
+        response_msg = ResponseMessage(req_msg=msg)
+        response_data = {"status": "success"}
+        try:
+            handler = self.get_handle(message_type)
+            response = handler(data.get("payload"))
+            response_data["payload"] = response
+        except Exception as ex:
+            logger.exception(ex)
+            response_data["status"] = "error"
+            response_data["payload"] = {"error message": "Error occurred at local preprocess"}
+            raise PreprocessException("Error occurred while handling preprocess request")
+        response_msg.set_data(response_data)
+        return response_msg
+
     def print_evaluate_local_model(self, hyperparams=None):
         """
         Print local evaluations in console
 
         :param hyperparams: hyperparams for evaluation on the local model
         :type hyperparams: `dict`
         :return: None
         """
         try:
             evaluations = self.local_training_handler.eval_model(hyperparams)
             # logger.info(evaluations)
         except Exception as ex:
-            logger.info('Error occurred on evaluating model on local data. ' +
-                        str(ex))
-
+            logger.info("Error occurred on evaluating model on local data. " + str(ex))
 
     def set_model(self, model):
         """Set model instance
         :param model: model to be trained
         :type model: `model.FLModel`
         :return: None
         """
@@ -242,42 +274,41 @@
 
         :param local_training_handler: local training handler class that \
         handles the access of the local model, for example, local training \
         of the model, prediction based on the model, etc.
         :type local_training_handler: `LocalTrainingHandler`        
         """
         self.local_training_handler = training_handler
-        if ( self.local_training_handler is not None ):
+        if self.local_training_handler is not None:
             self.local_training_handler.set_metrics_recorder_obj(self.metrics_recorder)
 
     def wait_for_model_initialization(self):
-        """Wait until model and localtraininghandler are initialized
-        """
+        """Wait until model and localtraininghandler are initialized"""
         logger.debug("Waiting for model initialization to finish")
 
         while not (self.fl_model and self.local_training_handler):
             time.sleep(10)
 
 
 class PartyProtocolHandlerRabbitMQ(PartyProtocolHandler):
     """
     Extended class for PartyProtocolHandler for using with RabbitMQ connection
     """
 
     def __init__(
-            self,
-            fl_model,
-            connection,
-            data_handler,
-            local_training_handler,
-            hyperparams=None,
-            agg_info=None,
-            synch=True,
-            is_private=True,
-            **kwargs
+        self,
+        fl_model,
+        connection,
+        data_handler,
+        local_training_handler,
+        hyperparams=None,
+        agg_info=None,
+        synch=True,
+        is_private=True,
+        **kwargs
     ):
         """
         Initiate PartyProtocolHandlerRabbitMQ with provided fl_model,
         connection and data_handler and hyperparams
 
         :param fl_model: model to be trained
         :type fl_model: `model.FLModel`
@@ -302,37 +333,36 @@
                 agg_info,
                 synch,
                 is_private,
                 **kwargs
             )
 
         else:
-            raise Exception(
-                'RabbitMQ connection currently only supports synchronous mode'
-            )
+            raise Exception("RabbitMQ connection currently only supports synchronous mode")
 
     def handle_async_request(self, msg):
         """Handle all incoming requests asynchronously and route it to
         respective methods in local training handler
 
         :param msg: Message object form connection
         :type msg: `Message`
         :return: Response message sent back to requester
         :rtype: ResponseMessage
         """
         try:
             response_msg = ResponseMessage(
                 message_type=MessageType.ACK.value,
-                id_request=-1, data={'ACK': True}
+                id_request=-1,
+                data={"ACK": True, "id_request": msg.get_header()["id_request"]},
             )
-            logger.info("received a async request")
+            logger.debug("received a async request")
 
-            id_request = msg.get_header()['id_request']
+            id_request = msg.get_header()["id_request"]
 
             self.execute_async(id_request, msg)
 
-            logger.info("finished async request")
+            logger.debug("finished async request")
 
         except Exception as ex:
             logger.info(ex)
 
         return response_msg
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/party/training/local_training_handler.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/party/training/local_training_handler.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,45 +1,53 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
 import logging
 
-from ibmfl.exceptions import LocalTrainingException, \
-    ModelUpdateException
+import numpy as np
+
+from ibmfl.exceptions import LocalTrainingException, ModelUpdateException
+from ibmfl.message.message_type import MessageType
 
 logger = logging.getLogger(__name__)
 
-class LocalTrainingHandler():
 
-    def __init__(self, fl_model, data_handler, hyperparams=None, **kwargs):
+class LocalTrainingHandler:
+    def __init__(self, fl_model, data_handler, hyperparams=None, evidencia=None, **kwargs):
         """
         Initialize LocalTrainingHandler with fl_model, data_handler
 
         :param fl_model: model to be trained
         :type fl_model: `model.FLModel`
         :param data_handler: data handler that will be used to obtain data
         :type data_handler: `DataHandler`
         :param hyperparams: Hyperparameters used for training.
         :type hyperparams: `dict`
+        :param evidencia: evidencia to use
+        :type evidencia: `evidencia.EvidenceRecorder`
         :param kwargs: Additional arguments to initialize a local training \
         handler, e.g., a crypto library object to help with encryption and \
         decryption.
         :type kwargs: `dict`
         :return None
         """
         self.fl_model = fl_model
         self.data_handler = data_handler
         self.hyperparams = hyperparams
+        self.evidencia = evidencia
 
         self.metrics_recorder = None
         self.n_completed_trains = 0
         self.n_completed_evals = 0
 
+        if self.evidencia:
+            from ibmfl.evidencia.util.hashing import hash_model_update, hash_np_array
+
     def set_metrics_recorder_obj(self, metrics_recorder):
         """
         Set metrics instance variable to the input parameter. We do this because the \
         party_protocol_handler tells the local_training_handler which metrics object to use; the \
         local_training_handler can be constructed somewhere else, so we don't want to force the \
         metrics object to necessarily exist at that time.
 
@@ -54,19 +62,19 @@
 
         :param model_update: ModelUpdate
         :type model_update: `ModelUpdate`
         """
         try:
             if model_update is not None:
                 self.fl_model.update_model(model_update)
-                logger.info('Local model updated.')
+                logger.info("Local model updated.")
             else:
-                logger.info('No model update was provided.')
+                logger.info("No model update was provided.")
         except Exception as ex:
-            raise LocalTrainingException('No query information is provided. '+ str(ex))
+            raise LocalTrainingException("No query information is provided. " + str(ex))
 
     def get_train_metrics_pre(self):
         """
         Call the post-train metrics hook. This hook runs immediately before the training starts at
         each party during the routine corresponding to the TRAIN command.
 
         :param: None
@@ -86,28 +94,30 @@
                 else:
                     pre_eval_results = None
                 # collect metrics specific to the model class, that the user may customize
                 additional_metrics = self.fl_model.get_custom_metrics_pre()
                 self.metrics_recorder.pre_train_hook(pre_eval_results, additional_metrics)
             except Exception as e:
                 logger.exception(str(e))
-                raise LocalTrainingException(
-                    'Error occurred while running pre-train hooks')
+                raise LocalTrainingException("Error occurred while running pre-train hooks")
 
     def get_train_metrics_post(self):
         """
         Call the post-train metrics hook. This hook runs immediately after the training finishes at
         each party during the routine corresponding to the TRAIN command.
 
         :param: None
         :return: None
         """
         if self.metrics_recorder:
             try:
-                train_result = self.fl_model.get_train_result()
+                try:
+                    train_result = self.fl_model.get_train_result()
+                except NotImplementedError:
+                    train_result = None
                 # TODO: find sound way of determining if we really want to do a post_train (i.e.
                 # "locally-trained") eval
                 if self.metrics_recorder.compute_post_train_eval:
                     ret = self.data_handler.get_data()
                     if ret is not None:
                         (_), test_dataset = ret
                     else:
@@ -115,40 +125,70 @@
                     post_eval_results = self.fl_model.evaluate(test_dataset)
                 else:
                     post_eval_results = None
                 additional_metrics = self.fl_model.get_custom_metrics_post()
                 self.metrics_recorder.post_train_hook(train_result, post_eval_results, additional_metrics)
             except Exception as e:
                 logger.exception(str(e))
-                raise LocalTrainingException(
-                    'Error occurred while running post-train hooks')
+                raise LocalTrainingException("Error occurred while running post-train hooks")
 
     def train(self, fit_params=None):
         """
         Train locally using fl_model. At the end of training, a
         model_update with the new model information is generated and
         send through the connection.
 
         :param fit_params: (optional) Query instruction from aggregator
         :type fit_params: `dict`
         :return: ModelUpdate
         :rtype: `ModelUpdate`
         """
         train_data, (_) = self.data_handler.get_data()
 
-        self.update_model(fit_params.get('model_update'))
+        if self.evidencia:
+            self.evidencia.add_claim("training_data_hash", "'{}'".format(hash_np_array(train_data[0])))
+            self.evidencia.add_claim("training_data_labels_hash", "'{}'".format(hash_np_array(train_data[1])))
+            self.evidencia.add_claim("training_data_size", str(train_data[0].shape[0]))
+            self.evidencia.add_claim("training_data_labels_number", str(np.unique(train_data[1], axis=0).shape[0]))
+            # TODO labels are hardcoded
+            labels_text = [
+                "Verkehr & Mobilitt",
+                "Stdtebau & Stadtraum",
+                "Sonstiges",
+                "Grn & Erholung",
+                "Soziales & Kultur",
+                "Wohnen & Arbeiten",
+                "Sport & Freizeit",
+                "Klima & Umweltschutz",
+            ]
+            self.evidencia.add_claim("labels_list", "'{}'".format(str(labels_text).replace("'", '"')))
+            # also log number of training instances per label
+            (labels, counts) = np.unique(np.argmax(train_data[1], axis=1), return_counts=True)
+
+            for idx, _ in np.ndenumerate(labels):
+                self.evidencia.add_claim("training_data_count_per_label", "{}, {}".format(labels[idx], counts[idx]))
+
+        self.update_model(fit_params.get("model_update"))
+
+        if self.evidencia:
+            self.evidencia.add_claim(
+                "received_model_update", "'\"{}\"'".format(hash_model_update(self.fl_model.get_model_update()))
+            )
 
         self.get_train_metrics_pre()
 
-        logger.info('Local training started...')
+        logger.info("Local training started...")
 
-        self.fl_model.fit_model(train_data, fit_params)
+        self.fl_model.fit_model(train_data, fit_params, local_params=self.hyperparams)
 
         update = self.fl_model.get_model_update()
-        logger.info('Local training done, generating model update...')
+        logger.info("Local training done, generating model update...")
+
+        if self.evidencia:
+            self.evidencia.add_claim("sent_model_update", "'\"{}\"'".format(hash_model_update(update)))
 
         self.get_train_metrics_post()
 
         return update
 
     def save_model(self, payload=None):
         """
@@ -157,15 +197,17 @@
         :param payload: data payload received from Aggregator
         :type payload: `dict`
         :return: Status of save model request
         :rtype: `boolean`
         """
         status = False
         try:
-            self.fl_model.save_model()
+            logger.info("save_model({})".format(payload))
+            filename = payload["filename"] if payload and "filename" in payload else None
+            self.fl_model.save_model(filename=filename)
             status = True
         except Exception as ex:
             logger.error("Error occurred while saving local model")
             logger.exception(ex)
 
         return status
 
@@ -178,64 +220,61 @@
         :return: None
         """
         if self.metrics_recorder:
             try:
                 self.metrics_recorder.pre_update_hook()
             except Exception as e:
                 logger.exception(str(e))
-                raise LocalTrainingException(
-                    'Error occurred while running pre-update hooks')
+                raise LocalTrainingException("Error occurred while running pre-update hooks")
 
     def get_update_metrics_post(self):
         """
         Call the post-update metrics hook. This hook runs after the model update from the SYNC
         command, but still during the routine corresponding to that SYNC.
 
         :param: None
         :return: None
         """
         if self.metrics_recorder:
             try:
                 self.metrics_recorder.post_update_hook()
             except Exception as e:
                 logger.exception(str(e))
-                raise LocalTrainingException(
-                    'Error occurred while running post-update hooks')
+                raise LocalTrainingException("Error occurred while running post-update hooks")
 
     def sync_model_impl(self, payload=None):
         """
         Update the local model with global ModelUpdate received
-        from the Aggregator. This function is meant to be 
+        from the Aggregator. This function is meant to be
         overridden in base classes as opposed to sync_model, which
         contains boilerplate for exception handling and metrics.
 
         :param payload: data payload received from Aggregator
         :type payload: `dict`
         :return: Status of sync model request
         :rtype: `boolean`
         """
         status = False
-        model_update = payload['model_update']
+        model_update = payload["model_update"]
         status = self.fl_model.update_model(model_update)
         return status
-    
+
     def sync_model(self, payload=None):
         """
         Update the local model with global ModelUpdate received
         from the Aggregator.
 
         :param payload: data payload received from Aggregator
         :type payload: `dict`
         :return: Status of sync model request
         :rtype: `boolean`
         """
         status = False
-        if payload is None or 'model_update' not in payload:
-            raise ModelUpdateException(
-                "Invalid Model update request aggregator")
+        if payload is None or "model_update" not in payload:
+            raise ModelUpdateException("Invalid Model update request aggregator")
 
         self.get_update_metrics_pre()
 
         try:
             status = self.sync_model_impl(payload)
         except Exception as ex:
             logger.error("Exception occurred while sync model")
@@ -258,21 +297,38 @@
         (_), test_dataset = self.data_handler.get_data()
         evaluations = dict()
         try:
             evaluations = self.fl_model.evaluate(test_dataset)
             logger.info(evaluations)
 
         except Exception as ex:
-            logger.error("Expecting the test dataset to be of type tuple. "
-                         "However, test dataset is of type "
-                         + str(type(test_dataset)))
+            logger.error(
+                "Expecting the test dataset to be of type tuple. "
+                "However, test dataset is of type " + str(type(test_dataset))
+            )
             logger.exception(ex)
 
         return evaluations
 
+    def determine_train_msg_seq(self, message_type, payload: dict):
+        """
+        Determines if the training message is the first and / or the last in a training round sequence.
+
+        :param message_type: The type of the message.
+        :type message_type: `enum`
+        :param payload: Payload of the message.
+        :type payload: `dict`
+        :return: Tuple indicating if the message is first or last or both (i.e. single train message in a sequence).
+        :rtype: `tuple`
+        """
+        first_step = last_step = False
+        if message_type is MessageType.TRAIN.value:
+            first_step = last_step = True
+        return (first_step, last_step)
+
     def get_n_completed_trains(self):
         """
         Return the number of completed executions of the TRAIN command at the party side
 
         :param: None
         :return: Number indicating how many TRAINs have been completed
         :rtype: `int`
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/util/config.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/config.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,28 +1,28 @@
-"""
-Module providing utility functions helpful for both party and aggregator
-"""
-
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
-import os
-import yaml
+"""
+Module providing utility functions helpful for both party and aggregator
+"""
+
+import importlib
 import logging
 import logging.config
-import importlib
+import os
 from importlib.machinery import SourceFileLoader
-from pathlib import PurePath, Path
+from pathlib import Path, PurePath
+
+import yaml
 
 import ibmfl.envs as fl_envs
 import ibmfl.util.log_config as log_config
-from ibmfl.exceptions import InvalidConfigurationException, FLException
-
+from ibmfl.exceptions import FLException, InvalidConfigurationException
 
 """
 Module providing utility functions helpful for both party and aggregator
 """
 logger = logging.getLogger(__name__)
 
 
@@ -31,28 +31,27 @@
     reads the yaml config file and generates a dictionary
 
     :param config_file: yaml file containing the definitions of formatter and
     handler
     :return : dictionary object containing the config details
     """
     if config_file and os.path.exists(config_file):
-        with open(config_file, 'rt') as cf:
+        with open(config_file, "rt") as cf:
             try:
                 return yaml.safe_load(cf.read())
             except InvalidConfigurationException as ice:
-                logger.error('Failed to load yaml configurations. ' + str(ice))
-                logging.error('Could not load yaml configuration')
+                logger.error("Failed to load yaml configurations. " + str(ice))
+                logging.error("Could not load yaml configuration")
     else:
-        logger.info('Yaml configuration file not found: ' + str(config_file))
+        logger.warn("Yaml configuration file not found: " + str(config_file))
 
     return None
 
 
-def configure_logging_from_file(config_file='log_config.yaml',
-                                log_level='INFO'):
+def configure_logging_from_file(config_file="log_config.yaml", log_level="INFO"):
     """
     configures logging for application based on the configuration file
 
     :param config_file: yaml file containing the definitions of formatter and handler
     :param log_level: should be a value from [DEBUG, INFO, WARNING, ERROR, CRITICAL]
             based on the required granularity
     :return: a boolean object. False for default basic config True otherwise
@@ -68,18 +67,17 @@
 def get_aggregator_config(**kwargs):
     """Retrieve connections settings from config file or arguments
 
     :param \\*\\*kwargs: Arguments that are passed into the aggregator instance
     """
 
     if not kwargs:
-        raise InvalidConfigurationException(
-            'No arguments given to Aggregator at runtime')
+        raise InvalidConfigurationException("No arguments given to Aggregator at runtime")
 
-    config_file = kwargs.get('config_file')
+    config_file = kwargs.get("config_file")
 
     if config_file:
         return get_config_from_file(config_file=config_file)
     else:
         return get_config_from_args(**kwargs)
 
 
@@ -87,57 +85,36 @@
     """
     Retrieve connections settings from config file or arguments
 
     :param \\*\\*kwargs: Arguments that are passed into the party instance
     """
 
     if not kwargs:
-        raise InvalidConfigurationException(
-            'No arguments given to Party at runtime')
+        raise InvalidConfigurationException("No arguments given to Party at runtime")
 
-    config_file = kwargs.get('config_file')
+    config_file = kwargs.get("config_file")
 
     if config_file:
-        logger.info('Getting config from file')
-        return get_config_from_file(config_file=config_file)
-    else:
-        return get_config_from_args(**kwargs)
-
-
-def get_authority_config(**kwargs):
-    """
-    Retrieve connections settings from config file or arguments
-
-    :param \\*\\*kwargs: Arguments that are passed into the party instance
-    """
-
-    if not kwargs:
-        raise InvalidConfigurationException(
-            'No arguments given to Authority at runtime')
-
-    config_file = kwargs.get('config_file')
-
-    if config_file:
-        logger.info('Getting config from file')
+        logger.info("Getting config from file")
         return get_config_from_file(config_file=config_file)
     else:
         return get_config_from_args(**kwargs)
 
 
 def get_config_from_file(config_file):
     """
     Reads a yaml file and resolves the string configuration to appropriate
     class instances.
     :param config_file: yaml file containing the configurations
     :type config_file: `str`
     :return: dictionary containing class references and additional information
     :rtype: `dict`
     """
-    logger.info('Getting details from config file.')
-    logger.debug('Reading Configuration File : %s', config_file)
+    logger.info("Getting details from config file.")
+    logger.debug("Reading Configuration File : %s", config_file)
     config_dict = read_yaml_config(config_file)
 
     cls_config = get_cls_by_config(config_dict)
 
     return cls_config
 
 
@@ -146,15 +123,15 @@
     Reads arguments and resolves the string configuration to appropriate
     class instances.
     :param config_file: yaml file containing the configurations
     :type config_file: `str`
     :return: dictionary of class references
     :rtype: `dict`
     """
-    logger.info('Getting Aggregator details from arguments.')
+    logger.info("Getting Aggregator details from arguments.")
 
     cls_config = {}
     if config_dict:
         cls_config = get_cls_by_config(config_dict)
     else:
         cls_config = get_cls_by_config(kwargs)
 
@@ -171,117 +148,111 @@
     :type config_dict: `dict`
     :return: dictionary of class references
     :rtype: `dict`
     """
     cls_config = {}
     # TODO: since all the configuration blocks carry same signature this
     # should be done in iterative way for all keys in the dictionary
-    cls_config['data'] = get_data_from_config(config_dict.get('data'))
-    cls_config['metrics_recorder'] = get_mrec_from_config(config_dict.get('metrics_recorder'))
-    cls_config['model'] = get_model_from_config(config_dict.get('model'))
-    cls_config['fusion'] = get_fusion_from_config(config_dict.get('fusion'))
-    cls_config['connection'] = get_connection_from_config(
-        config_dict.get('connection'))
-    cls_config['protocol_handler'] = get_ph_from_config(
-        config_dict.get('protocol_handler'))
-    cls_config['hyperparams'] = config_dict.get('hyperparams')
-    cls_config['aggregator'] = config_dict.get('aggregator')
-    cls_config['local_training'] = get_lt_from_config(
-        config_dict.get('local_training'))
-    cls_config['privacy'] = config_dict.get('privacy')
-    cls_config['metrics'] = get_mh_from_config(config_dict.get('metrics'))
-    cls_config['key_generators'] = config_dict.get('key_generators')
-    cls_config['access_white_lst'] = config_dict.get('access_white_lst')
+    cls_config["data"] = get_data_from_config(config_dict.get("data"))
+    cls_config["metrics_recorder"] = get_mrec_from_config(config_dict.get("metrics_recorder"))
+    cls_config["model"] = get_model_from_config(config_dict.get("model"))
+    cls_config["fusion"] = get_fusion_from_config(config_dict.get("fusion"))
+    cls_config["connection"] = get_connection_from_config(config_dict.get("connection"))
+    cls_config["protocol_handler"] = get_ph_from_config(config_dict.get("protocol_handler"))
+    cls_config["hyperparams"] = config_dict.get("hyperparams")
+    cls_config["aggregator"] = config_dict.get("aggregator")
+    cls_config["local_training"] = get_lt_from_config(config_dict.get("local_training"))
+    cls_config["preprocess"] = get_preprocess_from_config(config_dict.get("preprocess"))
+    cls_config["privacy"] = config_dict.get("privacy")
+    cls_config["metrics"] = get_mh_from_config(config_dict.get("metrics"))
+    cls_config["evidencia"] = get_evidencia_from_config(config_dict.get("evidencia"))
 
     return cls_config
 
 
 def get_connection_from_config(config):
-    """ Load connection class information from params provided in config file
+    """Load connection class information from params provided in config file
     :param config: dictionary of configuration
     :type config: `dict`
     :return: dictionary of class references
     :rtype: `dict`
     """
     # TODO: validation needs to be added
     connection_config = {}
     if config:
         try:
-            connection_config['cls_ref'] = get_class_by_name(
-                config['path'], config['name'])
-            connection_config['info'] = config['info']
-            connection_config['sync'] = bool(config.get('sync', False))
+            connection_config["cls_ref"] = get_class_by_name(config["path"], config["name"])
+            connection_config["info"] = config["info"]
+            connection_config["sync"] = bool(config.get("sync", False))
 
         except InvalidConfigurationException as ex:
             logger.exception(ex)
 
-            raise InvalidConfigurationException(
-                'Error occurred while loading connection config.')
+            raise InvalidConfigurationException("Error occurred while loading connection config.")
 
     else:
-        logger.info('No connection config provided for this setup.')
+        logger.info("No connection config provided for this setup.")
 
     return connection_config
 
 
 def get_data_from_config(config):
-    """ Load data class information from params provided in config file
+    """Load data class information from params provided in config file
     :param config: dictionary of configuration
     :type config: `dict`
     :return: dictionary of class references
     :rtype: `dict`
     """
     # TODO: validation needs to be added
     data_config = None
     if config:
         data_config = {}
         try:
-            data_config['cls_ref'] = get_class_by_name(
-                config['path'], config['name'])
-            data_config['info'] = config['info']
+            if "class" in config:
+                data_config["cls_ref"] = config["class"]
+            else:
+                data_config["cls_ref"] = get_class_by_name(config["path"], config["name"])
+            data_config["info"] = config["info"]
         except InvalidConfigurationException as ex:
             logger.exception(ex)
-            raise InvalidConfigurationException(
-                'Error occurred while loading data config.')
+            raise InvalidConfigurationException("Error occurred while loading data config.")
 
     else:
-        logger.info('No data config provided for this setup.')
+        logger.info("No data config provided for this setup.")
     return data_config
 
 
 def get_model_from_config(config):
-    """ Load model class information from params provided in config file
+    """Load model class information from params provided in config file
     :param config: dictionary of configuration
     :type config: `dict`
     :return: dictionary of class references
     :rtype: `dict`
     """
     # TODO: validation needs to be added
     model_config = None
     if config:
         model_config = {}
         try:
-            model_config['cls_ref'] = get_class_by_name(
-                config['path'], config['name'])
+            model_config["cls_ref"] = get_class_by_name(config["path"], config["name"])
 
-            model_config['spec'] = config['spec']
+            model_config["spec"] = config["spec"]
 
-            if 'info' in config:
-                model_config['info'] = config['info']
+            if "info" in config:
+                model_config["info"] = config["info"]
 
-            if 'model_file' in config:
-                model_config['model_file'] = config['model_file']
+            if "model_file" in config:
+                model_config["model_file"] = config["model_file"]
 
         except Exception as ex:
             logger.exception(ex)
-            raise InvalidConfigurationException(
-                'Error occurred while loading model config.')
+            raise InvalidConfigurationException("Error occurred while loading model config.")
 
     else:
-        logger.info('No model config provided for this setup.')
+        logger.info("No model config provided for this setup.")
 
     return model_config
 
 
 def get_fusion_from_config(config):
     """
     Load fusion class information from params provided in config file.
@@ -291,180 +262,191 @@
     :return: dictionary of class references
     :rtype: `dict`
     """
     # TODO: validation needs to be added
     fusion_config = {}
     if config:
         try:
-            fusion_config['cls_ref'] = get_class_by_name(
-                config['path'], config['name'])
-            if 'info' in config:
-                fusion_config['info'] = config['info']
+            fusion_config["cls_ref"] = get_class_by_name(config["path"], config["name"])
+            if "info" in config:
+                fusion_config["info"] = config["info"]
         except Exception as ex:
             logger.exception(ex)
 
-            raise InvalidConfigurationException(
-                'Error occurred while loading fusion config.')
+            raise InvalidConfigurationException("Error occurred while loading fusion config.")
 
     else:
-        logger.debug('No fusion config provided for this setup.')
+        logger.debug("No fusion config provided for this setup.")
 
     return fusion_config
 
 
 def get_ph_from_config(config):
-    """ Load ph class information from params provided in config file
+    """Load ph class information from params provided in config file
     :param config: dictionary of configuration
     :type config: `dict`
     :return: dictionary of class references
     :rtype: `dict`
     """
     # TODO: validation needs to be added
     ph_config = {}
     if config:
         try:
-            ph_config['cls_ref'] = get_class_by_name(
-                config['path'], config['name'])
-            if 'info' in config:
-                ph_config['info'] = config['info']
+            ph_config["cls_ref"] = get_class_by_name(config["path"], config["name"])
+            if "info" in config:
+                ph_config["info"] = config["info"]
         except Exception as ex:
             logger.exception(ex)
 
-            raise InvalidConfigurationException(
-                'Error occurred while loading protocol handler config.')
+            raise InvalidConfigurationException("Error occurred while loading protocol handler config.")
 
     else:
-        logger.info('No ph config provided for this setup.')
+        logger.info("No ph config provided for this setup.")
 
     return ph_config
 
 
+def get_evidencia_from_config(config):
+    """Evidencia recorder class information from params provided in config file
+    :param config: dictionary of configuration
+    :type config: `dict`
+    :return: dictionary of class references
+    :rtype: `dict`
+    """
+    # TODO: validation needs to be added
+    evidencia_config = {}
+    if config:
+        try:
+            evidencia_config["cls_ref"] = get_class_by_name(config["path"], config["name"])
+            if "info" in config:
+                evidencia_config["info"] = config["info"]
+
+        except Exception as ex:
+            logger.exception(ex)
+
+            raise InvalidConfigurationException("Error occurred while loading evidencia recorder config.")
+
+    else:
+        logger.info("No evidencia recordeer config provided for this setup.")
+
+    return evidencia_config
+
+
 def get_lt_from_config(config):
-    """ Load local training class information from params provided in config file
+    """Load local training class information from params provided in config file
     :param config: dictionary of configuration
     :type config: `dict`
     :return: dictionary of class references
     :rtype: `dict`
     """
     # TODO: validation needs to be added
     lt_config = {}
     if config:
         try:
-            lt_config['cls_ref'] = get_class_by_name(
-                config['path'], config['name'])
-            if 'info' in config:
-                lt_config['info'] = config['info']
+            lt_config["cls_ref"] = get_class_by_name(config["path"], config["name"])
+            if "info" in config:
+                lt_config["info"] = config["info"]
 
         except Exception as ex:
             logger.exception(ex)
 
-            raise InvalidConfigurationException(
-                'Error occurred while loading local training config.')
+            raise InvalidConfigurationException("Error occurred while loading local training config.")
 
     else:
-        logger.debug('No local training config provided for this setup.')
+        logger.debug("No local training config provided for this setup.")
 
     return lt_config
 
 
-def get_mh_from_config(config):
-    """ Load data class information from params provided in config file
+def get_preprocess_from_config(config):
+    """Load preprocess class information from params provided in config file
     :param config: dictionary of configuration
     :type config: `dict`
     :return: dictionary of class references
     :rtype: `dict`
     """
-    # TODO: validation needs to be added
-    mh_config = None
+    preprocess_config = {}
     if config:
-        mh_config = {}
         try:
-            mh_config['cls_ref'] = get_class_by_name(
-                config['path'], config['name'])
-            mh_config['info'] = config.get('info')
-        except InvalidConfigurationException as ex:
+            preprocess_config["cls_ref"] = get_class_by_name(config["path"], config["name"])
+            if "spec" in config:
+                preprocess_config["spec"] = config["spec"]
+            return preprocess_config
+        except Exception as ex:
             logger.exception(ex)
-            raise InvalidConfigurationException(
-                'Error occurred while loading mh config.')
-
+            raise InvalidConfigurationException("Error occurred while loading local training config.")
     else:
-        logger.info('No data config provided for this setup.')
-    return mh_config
+        logger.debug("No local training config provided for this setup.")
 
 
-def get_mrec_from_config(config):
-    """ Load metrics recorder class information from params provided in config file
+def get_mh_from_config(config):
+    """Load metrics handler class information from params provided in config file
     :param config: dictionary of configuration
     :type config: `dict`
     :return: dictionary of class references
     :rtype: `dict`
     """
     # TODO: validation needs to be added
-    pm_config = None
+    mh_config = None
     if config:
-        pm_config = {}
+        mh_config = {}
         try:
-            pm_config['cls_ref'] = get_class_by_name(
-                config['path'], config['name'])
-            pm_config['output_file'] = config['output_file']
-            pm_config['output_type'] = config['output_type']
-            pm_config['compute_pre_train_eval'] = config['compute_pre_train_eval']
-            pm_config['compute_post_train_eval'] = config['compute_post_train_eval']
+            mh_config["cls_ref"] = get_class_by_name(config["path"], config["name"])
+            mh_config["info"] = config.get("info")
         except InvalidConfigurationException as ex:
             logger.exception(ex)
-            raise InvalidConfigurationException(
-                'Error occured while loading data config.')
+            raise InvalidConfigurationException("Error occurred while loading mh config.")
 
     else:
-        logger.info('No metrics recorder config provided for this setup.')
-    return pm_config
+        logger.info("No metrics config provided for this setup.")
+    return mh_config
 
 
-def get_key_generator_from_config(config):
-    """ Load data class information from params provided in config file
+def get_mrec_from_config(config):
+    """Load metrics recorder class information from params provided in config file
     :param config: dictionary of configuration
     :type config: `dict`
     :return: dictionary of class references
     :rtype: `dict`
     """
-    key_generator_config = None
+    # TODO: validation needs to be added
+    pm_config = None
     if config:
-        key_generator_config = {}
+        pm_config = {}
         try:
-            key_generator_config['cls_ref'] = get_class_by_name(
-                config['path'], config['name'])
-            key_generator_config['sec_config'] = config.get('sec_config')
+            pm_config["cls_ref"] = get_class_by_name(config["path"], config["name"])
+            pm_config["output_file"] = config["output_file"]
+            pm_config["output_type"] = config["output_type"]
+            pm_config["compute_pre_train_eval"] = config["compute_pre_train_eval"]
+            pm_config["compute_post_train_eval"] = config["compute_post_train_eval"]
         except InvalidConfigurationException as ex:
             logger.exception(ex)
-            raise InvalidConfigurationException(
-                'Error occurred while loading mh config.')
+            raise InvalidConfigurationException("Error occurred while loading data config.")
 
     else:
-        logger.info('No data config provided for this setup.')
-    return key_generator_config
+        logger.info("No metrics recorder config provided for this setup.")
+    return pm_config
 
 
 def get_class_by_name(path, name_class):
     """
     Gets object according to the name_class and the path where it is located.
 
     :param path: logical location of the object
     :type path: `str`
     :param name_class: name of the object
     :type name_class: `str`
 
     :return: Object
     """
     try:
-
         cls_ref = get_attr_from_path(path, name_class)
     except Exception as ex:
         logger.exception(ex)
-        raise InvalidConfigurationException(
-            'Error occurred while loading class '+name_class+'from path ' + path)
+        raise InvalidConfigurationException("Error occurred while loading class " + name_class + "from path " + path)
     return cls_ref
 
 
 def get_attr_from_path(path, attr):
     """
     Gets attr from a file with given path.
 
@@ -473,23 +455,23 @@
     :param attr: name of the attr
     :type attr: `str`
 
     :return: Object/method
     """
     file_path = Path(PurePath(path))
     if is_file_path(path):
-
         absolute_path = get_absolute_path(path)
 
         if absolute_path:
             loader = SourceFileLoader(file_path.stem, absolute_path)
             module = loader.load_module()
         else:
             raise InvalidConfigurationException(
-                "File does not exist. "+path+" not found in current or working directory.")
+                "File does not exist. " + path + " not found in current or working directory."
+            )
 
     else:
         module = importlib.import_module(path)
 
     return getattr(module, attr)
 
 
@@ -500,15 +482,15 @@
     :param path: path given in configuration
     :type path: `str`
 
     :return: Boolean
     """
     file_path = Path(PurePath(path))
 
-    return file_path.suffix == '.py'
+    return file_path.suffix == ".py"
 
 
 def get_absolute_path(path):
     """
     Get absolute path for `file path` given in config.
     If given path is relative- then absolute path is constructed by adding
     either the current directory or working directory to the path.
@@ -520,26 +502,26 @@
 
     :return: absolute path
     """
     file_path = Path(PurePath(path))
     absolute_path = path  # when absolute path is provided
 
     if file_path.is_absolute == True and not file_path.exists():
-        raise InvalidConfigurationException(
-            "File does not exist. "+path+" not found.")
+        raise InvalidConfigurationException("File does not exist. " + path + " not found.")
 
     elif get_current_dir_ap(path):
         absolute_path = get_current_dir_ap(path)
 
     else:
         absolute_path = get_working_dir_ap(path)
 
     if absolute_path is None or os.path.exists(absolute_path) is False:
         raise InvalidConfigurationException(
-            "File does not exist. "+path+" not found in current or working directory.")
+            "File does not exist. " + path + " not found in current or working directory."
+        )
 
     return absolute_path
 
 
 def get_current_dir_ap(path):
     """
     Get absolute path using current directory for `file path` given in config.
@@ -550,16 +532,15 @@
     :type path: `str`
 
     :return: absolute path
     """
     file_path = Path(PurePath(path))
 
     current_dir = os.getcwd()
-    absolute_path = Path(
-        PurePath(current_dir).joinpath(PurePath(file_path)))
+    absolute_path = Path(PurePath(current_dir).joinpath(PurePath(file_path)))
     if absolute_path.exists():
         return str(absolute_path)
 
     return None
 
 
 def get_working_dir_ap(path):
@@ -571,25 +552,24 @@
     :param path: logical location of the object
     :type path: `str`
 
     :return: absolute path
     """
     file_path = Path(PurePath(path))
     working_dir = fl_envs.working_directory
-    absolute_path = Path(
-        PurePath(working_dir).joinpath(PurePath(file_path)))
+    absolute_path = Path(PurePath(working_dir).joinpath(PurePath(file_path)))
     if absolute_path.exists():
         return str(absolute_path)
 
     return None
 
 
 def convert_zip_to_bytes(filename):
     """
-    Open a zip file and return a byte array. 
+    Open a zip file and return a byte array.
     :param filename: name of the file
     :type filename: `str`
     :return: byte array
     """
     byte_arr = bytearray()
 
     if filename is not None:
@@ -598,29 +578,27 @@
             with open(abs_path, "rb") as f:
                 byte_arr = f.read()
 
         except InvalidConfigurationException as ex:
             logger.error(str(ex))
             raise InvalidConfigurationException("Zip file does not exist.")
         except Exception as identifier:
-            raise FLException(
-                "Error while converting zip file to bytes.")
+            raise FLException("Error while converting zip file to bytes.")
 
     return byte_arr
 
 
 def convert_bytes_to_zip(byte_arr, destination_file):
     """
-    Copy the bytes into a file 
+    Copy the bytes into a file
     :param byte_arr: byte array which needs to be copied into destination file
     :param destination_file: name of the file
     :type destination_file: `str`
     :return: byte array
     """
     try:
-        f_out = open(destination_file, 'w+b')
+        f_out = open(destination_file, "w+b")
         f_out.write(byte_arr)
         f_out.close
 
     except Exception as identifier:
-        raise FLException(
-            "Error while creating a file at " + destination_file)
+        raise FLException("Error while creating a file at " + destination_file)
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/util/pfnm/core.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/pfnm/core.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
 from __future__ import print_function
+
 import numpy as np
 from scipy.optimize import linear_sum_assignment
 
 
 def build_init(hungarian_weights, assignments, j):
     """
     Create local network weights from the matched global network weights
@@ -32,22 +33,18 @@
         return batch_init
 
     for c in range(num_assignments):
         if c == 0:
             batch_init.append(hungarian_weights[c][:, assignments[c][j]])
             batch_init.append(hungarian_weights[c + 1][assignments[c][j]])
         else:
-            batch_init.append(hungarian_weights[2 * c]
-                              [assignments[c - 1][j]]
-                              [:, assignments[c][j]])
-            batch_init.append(hungarian_weights[2 * c + 1]
-                              [assignments[c][j]])
+            batch_init.append(hungarian_weights[2 * c][assignments[c - 1][j]][:, assignments[c][j]])
+            batch_init.append(hungarian_weights[2 * c + 1][assignments[c][j]])
             if c == num_assignments - 1:
-                batch_init.append(hungarian_weights[2 * c + 2]
-                                  [assignments[c][j]])
+                batch_init.append(hungarian_weights[2 * c + 2][assignments[c][j]])
                 batch_init.append(hungarian_weights[-1])
                 return batch_init
 
 
 def row_param_cost(global_weights, weights_j_l, global_sigmas, sigma_inv_j):
     """
     Computes the cost defined in equation 6 of PFNM paper
@@ -59,22 +56,23 @@
     :type global_sigmas: `numpy.ndarray`
     :param sigma_inv_j:
     :type sigma_inv_j: `numpy.ndarray`
     :return Cost value
     :rtype: `numpy.ndarray`
     """
 
-    match_norms = ((weights_j_l + global_weights) ** 2 /
-                   (sigma_inv_j + global_sigmas)).sum(axis=1) - \
-                  (global_weights ** 2 / global_sigmas).sum(axis=1)
+    match_norms = ((weights_j_l + global_weights) ** 2 / (sigma_inv_j + global_sigmas)).sum(axis=1) - (
+        global_weights**2 / global_sigmas
+    ).sum(axis=1)
     return match_norms
 
 
-def compute_cost(global_weights, weights_j, global_sigmas, sigma_inv_j,
-                 prior_mean_norm, prior_inv_sigma, popularity_counts, gamma, J):
+def compute_cost(
+    global_weights, weights_j, global_sigmas, sigma_inv_j, prior_mean_norm, prior_inv_sigma, popularity_counts, gamma, J
+):
     """
     Computes full cost to be used by Hungarian algorithm. Refer Equation (8) in the PFNM paper
     :param global_weights: weight matrix
     :type global_weights: `numpy.ndarray`
     :param weights_j: Jth network weights
     :type weights_j: `numpy.ndarray`
     :param global_sigmas: global sigma values
@@ -93,42 +91,40 @@
     :type J: `int`
     :return: Cost value
     :rtype: `numpy.ndarray`
     """
 
     Lj = weights_j.shape[0]
     counts = np.minimum(np.array(popularity_counts), 10)
-    param_cost = np.array(
-        [
-            row_param_cost(global_weights, weights_j[l], global_sigmas, sigma_inv_j)
-            for l in range(Lj)
-        ]
-    )
+    param_cost = np.array([row_param_cost(global_weights, weights_j[l], global_sigmas, sigma_inv_j) for l in range(Lj)])
     param_cost += np.log(counts / (J - counts))
 
     # Nonparametric cost
     L = global_weights.shape[0]
     max_added = min(Lj, max(700 - L, 1))
 
-    nonparam_cost = np.outer((((weights_j + prior_mean_norm) ** 2 /
-                               (prior_inv_sigma + sigma_inv_j)).sum(axis=1)
-                              - (prior_mean_norm ** 2 /
-                                 prior_inv_sigma).sum()),
-                             np.ones(max_added))
+    nonparam_cost = np.outer(
+        (
+            ((weights_j + prior_mean_norm) ** 2 / (prior_inv_sigma + sigma_inv_j)).sum(axis=1)
+            - (prior_mean_norm**2 / prior_inv_sigma).sum()
+        ),
+        np.ones(max_added),
+    )
     cost_pois = 2 * np.log(np.arange(1, max_added + 1))
     nonparam_cost -= cost_pois
     nonparam_cost += 2 * np.log(gamma / J)
 
     full_cost = np.hstack((param_cost, nonparam_cost))
 
     return full_cost
 
 
-def matching_upd_j(weights_j, global_weights, sigma_inv_j, global_sigmas,
-                   prior_mean_norm, prior_inv_sigma, popularity_counts, gamma, J):
+def matching_upd_j(
+    weights_j, global_weights, sigma_inv_j, global_sigmas, prior_mean_norm, prior_inv_sigma, popularity_counts, gamma, J
+):
     """
     Computes cost [Equation 8] and solves the linear assignment problem
     using hungarian algo
     :param weights_j: Jth network weights
     :type weights_j: `numpy.ndarray`
     :param global_weights: global network weights
     :type global_weights: `numpy.ndarray`
@@ -148,16 +144,25 @@
     :type J: `int`
     :return a tuple of global weights, sigma values, popularity counts, and assignment values
     :rtype: `tuple`
     """
 
     L = global_weights.shape[0]
 
-    full_cost = compute_cost(global_weights, weights_j, global_sigmas, sigma_inv_j,
-                             prior_mean_norm, prior_inv_sigma, popularity_counts, gamma, J)
+    full_cost = compute_cost(
+        global_weights,
+        weights_j,
+        global_sigmas,
+        sigma_inv_j,
+        prior_mean_norm,
+        prior_inv_sigma,
+        popularity_counts,
+        gamma,
+        J,
+    )
 
     row_ind, col_ind = linear_sum_assignment(-full_cost)
 
     assignment_j = []
     new_L = L
 
     for l, i in zip(row_ind, col_ind):
@@ -166,18 +171,16 @@
             assignment_j.append(i)
             global_weights[i] += weights_j[l]
             global_sigmas[i] += sigma_inv_j
         else:  # new neuron
             popularity_counts += [1]
             assignment_j.append(new_L)
             new_L += 1
-            global_weights = np.vstack((global_weights,
-                                        prior_mean_norm + weights_j[l]))
-            global_sigmas = np.vstack((global_sigmas,
-                                       prior_inv_sigma + sigma_inv_j))
+            global_weights = np.vstack((global_weights, prior_mean_norm + weights_j[l]))
+            global_sigmas = np.vstack((global_sigmas, prior_inv_sigma + sigma_inv_j))
 
     return global_weights, global_sigmas, popularity_counts, assignment_j
 
 
 def patch_weights(w_j, L_next, assignment_j_c):
     """
     Patch weights for different layers of the network together into a
@@ -218,23 +221,20 @@
 
     J = len(batch_weights)
     sigma_bias = sigma
     sigma0_bias = sigma0
     mu0_bias = 0.1
     softmax_bias = [batch_weights[j][-1] for j in range(J)]
     softmax_inv_sigma = [s / sigma_bias for s in last_layer_const]
-    softmax_bias = sum(
-        [b * s for b, s in zip(softmax_bias, softmax_inv_sigma)]
-    ) + mu0_bias / sigma0_bias
+    softmax_bias = sum([b * s for b, s in zip(softmax_bias, softmax_inv_sigma)]) + mu0_bias / sigma0_bias
     softmax_inv_sigma = 1 / sigma0_bias + sum(softmax_inv_sigma)
     return softmax_bias, softmax_inv_sigma
 
 
-def init_from_assignments(batch_weights_norm, sigma_inv_layer, prior_mean_norm,
-                          sigma_inv_prior, assignment):
+def init_from_assignments(batch_weights_norm, sigma_inv_layer, prior_mean_norm, sigma_inv_prior, assignment):
     """
     Initialize a global network using the assignment matrix of
     local networks found by hungarian algorithm
     :param batch_weights_norm: network weights
     :type batch_weights_norm: `numpy.ndarray`
     :param sigma_inv_layer: inverse sigma values for layer
     :type sigma_inv_layer: `list`
@@ -257,9 +257,7 @@
     for j, a_j in enumerate(assignment):
         for l, i in enumerate(a_j):
             popularity_counts[i] += 1
             global_weights[i] += batch_weights_norm[j][l]
             global_sigmas[i] += sigma_inv_layer[j]
 
     return popularity_counts, global_weights, global_sigmas
-
-
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/util/pfnm/matching.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/pfnm/matching.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,24 +1,26 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
 from __future__ import print_function
+
 import logging
+
 import numpy as np
 
 from ibmfl.util.pfnm import core as pfnm_core
 
-
 logger = logging.getLogger(__name__)
 
 
-def match_network(batch_weights, batch_frequencies, sigma_layers,
-                  sigma0_layers, gamma_layers, iters, assignments_old=None):
+def match_network(
+    batch_weights, batch_frequencies, sigma_layers, sigma0_layers, gamma_layers, iters, assignments_old=None
+):
     """
     Main function to match fully-connected network weights
 
     :param batch_weights: a list of network weights. Each networks weights
     are a numpy array
     :type batch_weights: `list` of `np.array`
     :param batch_frequencies: a list of list with each outer list the
@@ -61,15 +63,15 @@
     if not isinstance(sigma0_layers, list):
         sigma0_layers = (n_layers - 1) * [sigma0_layers]
     if not isinstance(gamma_layers, list):
         gamma_layers = (n_layers - 1) * [gamma_layers]
 
     sigma_bias_layers = sigma_layers
     sigma0_bias_layers = sigma0_layers
-    mu0 = 0.
+    mu0 = 0.0
     mu0_bias = 0.1
     assignment_c = [None for _ in range(j_client_cnts)]
     l_next = None
     assignment_all = []
 
     if batch_frequencies is None:
         last_layer_const = [np.ones(k_hiddendim) for _ in range(j_client_cnts)]
@@ -84,122 +86,99 @@
         sigma = sigma_layers[c - 1]
         sigma_bias = sigma_bias_layers[c - 1]
         gamma = gamma_layers[c - 1]
         sigma0 = sigma0_layers[c - 1]
         sigma0_bias = sigma0_bias_layers[c - 1]
         if c == (n_layers - 1) and n_layers > 2:
             weights_bias = [
-                np.hstack((
-                    batch_weights[j][c * 2 - 1].reshape(-1, 1),
-                    batch_weights[j][c * 2]
-                ))
+                np.hstack((batch_weights[j][c * 2 - 1].reshape(-1, 1), batch_weights[j][c * 2]))
                 for j in range(j_client_cnts)
             ]
-            sigma_inv_prior = np.array(
-                [1 / sigma0_bias] +
-                (weights_bias[0].shape[1] - 1) * [1 / sigma0]
-            )
-            mean_prior = np.array(
-                [mu0_bias] + (weights_bias[0].shape[1] - 1) * [mu0])
+            sigma_inv_prior = np.array([1 / sigma0_bias] + (weights_bias[0].shape[1] - 1) * [1 / sigma0])
+            mean_prior = np.array([mu0_bias] + (weights_bias[0].shape[1] - 1) * [mu0])
             sigma_inv_layer = [
-                np.array([1 / sigma_bias] +
-                         [y / sigma for y in last_layer_const[j]]
-                         )
-                for j in range(j_client_cnts)
+                np.array([1 / sigma_bias] + [y / sigma for y in last_layer_const[j]]) for j in range(j_client_cnts)
             ]
         elif c > 1:
             weights_bias = [
                 np.hstack(
-                    (batch_weights[j][c * 2 - 1].reshape(-1, 1),
-                     pfnm_core.patch_weights(batch_weights[j][c * 2], l_next, assignment_c[j])))
+                    (
+                        batch_weights[j][c * 2 - 1].reshape(-1, 1),
+                        pfnm_core.patch_weights(batch_weights[j][c * 2], l_next, assignment_c[j]),
+                    )
+                )
                 for j in range(j_client_cnts)
             ]
-            sigma_inv_prior = np.array(
-                [1 / sigma0_bias] +
-                (weights_bias[0].shape[1] - 1) * [1 / sigma0]
-            )
-            mean_prior = np.array(
-                [mu0_bias] + (weights_bias[0].shape[1] - 1) * [mu0])
+            sigma_inv_prior = np.array([1 / sigma0_bias] + (weights_bias[0].shape[1] - 1) * [1 / sigma0])
+            mean_prior = np.array([mu0_bias] + (weights_bias[0].shape[1] - 1) * [mu0])
             sigma_inv_layer = [
-                np.array(
-                    [1 / sigma_bias] +
-                    (weights_bias[j].shape[1] - 1) * [1 / sigma]
-                ) for j in range(j_client_cnts)
+                np.array([1 / sigma_bias] + (weights_bias[j].shape[1] - 1) * [1 / sigma]) for j in range(j_client_cnts)
             ]
         else:
             weights_bias = [
-                np.hstack((
-                    batch_weights[j][0].T,
-                    batch_weights[j][c * 2 - 1].reshape(-1, 1),
-                    pfnm_core.patch_weights(batch_weights[j][c * 2], l_next, assignment_c[j])
-                ))
+                np.hstack(
+                    (
+                        batch_weights[j][0].T,
+                        batch_weights[j][c * 2 - 1].reshape(-1, 1),
+                        pfnm_core.patch_weights(batch_weights[j][c * 2], l_next, assignment_c[j]),
+                    )
+                )
                 for j in range(j_client_cnts)
             ]
             sigma_inv_prior = np.array(
-                d_datadim * [1 / sigma0] +
-                [1 / sigma0_bias] +
-                (weights_bias[0].shape[1] - 1 - d_datadim) * [1 / sigma0]
-            )
-            mean_prior = np.array(
-                d_datadim * [mu0] +
-                [mu0_bias] +
-                (weights_bias[0].shape[1] - 1 - d_datadim) * [mu0]
+                d_datadim * [1 / sigma0] + [1 / sigma0_bias] + (weights_bias[0].shape[1] - 1 - d_datadim) * [1 / sigma0]
             )
+            mean_prior = np.array(d_datadim * [mu0] + [mu0_bias] + (weights_bias[0].shape[1] - 1 - d_datadim) * [mu0])
             if n_layers == 2:
                 sigma_inv_layer = [
-                    np.array(
-                        d_datadim * [1 / sigma] +
-                        [1 / sigma_bias] +
-                        [y / sigma for y in last_layer_const[j]]
-                    )
-                    for j in range(j_client_cnts)]
+                    np.array(d_datadim * [1 / sigma] + [1 / sigma_bias] + [y / sigma for y in last_layer_const[j]])
+                    for j in range(j_client_cnts)
+                ]
             else:
                 sigma_inv_layer = [
                     np.array(
-                        d_datadim * [1 / sigma] +
-                        [1 / sigma_bias] +
-                        (weights_bias[j].shape[1] - 1 - d_datadim) *
-                        [1 / sigma])
+                        d_datadim * [1 / sigma]
+                        + [1 / sigma_bias]
+                        + (weights_bias[j].shape[1] - 1 - d_datadim) * [1 / sigma]
+                    )
                     for j in range(j_client_cnts)
                 ]
 
-        assignment_c, global_weights_c, global_sigmas_c = \
-            match_layer(weights_bias, sigma_inv_layer, mean_prior, sigma_inv_prior,
-                        gamma, iters, assignment=assignments_old[c - 1])
+        assignment_c, global_weights_c, global_sigmas_c = match_layer(
+            weights_bias, sigma_inv_layer, mean_prior, sigma_inv_prior, gamma, iters, assignment=assignments_old[c - 1]
+        )
         l_next = global_weights_c.shape[0]
         assignment_all = [assignment_c] + assignment_all
 
         if c == (n_layers - 1) and n_layers > 2:
-            softmax_bias, softmax_inv_sigma = \
-                pfnm_core.process_softmax_bias(batch_weights, last_layer_const, sigma, sigma0)
-            global_weights_out = [global_weights_c[:, 0],
-                                  global_weights_c[:, 1:], softmax_bias]
-            global_inv_sigmas_out = [global_sigmas_c[:, 0],
-                                     global_sigmas_c[:, 1:],
-                                     softmax_inv_sigma]
+            softmax_bias, softmax_inv_sigma = pfnm_core.process_softmax_bias(
+                batch_weights, last_layer_const, sigma, sigma0
+            )
+            global_weights_out = [global_weights_c[:, 0], global_weights_c[:, 1:], softmax_bias]
+            global_inv_sigmas_out = [global_sigmas_c[:, 0], global_sigmas_c[:, 1:], softmax_inv_sigma]
         elif c > 1:
-            global_weights_out = [global_weights_c[:, 0],
-                                  global_weights_c[:, 1:]] + \
-                                 global_weights_out
-            global_inv_sigmas_out = [global_sigmas_c[:, 0],
-                                     global_sigmas_c[:, 1:]] + global_inv_sigmas_out
+            global_weights_out = [global_weights_c[:, 0], global_weights_c[:, 1:]] + global_weights_out
+            global_inv_sigmas_out = [global_sigmas_c[:, 0], global_sigmas_c[:, 1:]] + global_inv_sigmas_out
         else:
             if n_layers == 2:
-                softmax_bias, softmax_inv_sigma = \
-                    pfnm_core.process_softmax_bias(batch_weights, last_layer_const, sigma, sigma0)
+                softmax_bias, softmax_inv_sigma = pfnm_core.process_softmax_bias(
+                    batch_weights, last_layer_const, sigma, sigma0
+                )
                 global_weights_out = [softmax_bias]
                 global_inv_sigmas_out = [softmax_inv_sigma]
-            global_weights_out = [global_weights_c[:, :d_datadim].T,
-                                  global_weights_c[:, d_datadim],
-                                  global_weights_c[:, (d_datadim + 1):]
-                                  ] + global_weights_out
-            global_inv_sigmas_out = [global_sigmas_c[:, :d_datadim].T,
-                                     global_sigmas_c[:, d_datadim],
-                                     global_sigmas_c[:, (d_datadim + 1):]
-                                     ] + global_inv_sigmas_out
+            global_weights_out = [
+                global_weights_c[:, :d_datadim].T,
+                global_weights_c[:, d_datadim],
+                global_weights_c[:, (d_datadim + 1) :],
+            ] + global_weights_out
+            global_inv_sigmas_out = [
+                global_sigmas_c[:, :d_datadim].T,
+                global_sigmas_c[:, d_datadim],
+                global_sigmas_c[:, (d_datadim + 1) :],
+            ] + global_inv_sigmas_out
 
     map_out = [g_w / g_s for g_w, g_s in zip(global_weights_out, global_inv_sigmas_out)]
 
     return map_out, assignment_all
 
 
 def match_layer(weights_bias, sigma_inv_layer, mean_prior, sigma_inv_prior, gamma, iters, assignment=None):
@@ -226,35 +205,40 @@
     J = len(weights_bias)
     group_order = sorted(range(J), key=lambda x: -weights_bias[x].shape[0])
     batch_weights_norm = [w * s for w, s in zip(weights_bias, sigma_inv_layer)]
     prior_mean_norm = mean_prior * sigma_inv_prior
 
     if assignment is None:
         global_weights = prior_mean_norm + batch_weights_norm[group_order[0]]
-        global_sigmas = np.outer(
-            np.ones(global_weights.shape[0]),
-            sigma_inv_prior + sigma_inv_layer[group_order[0]]
-        )
+        global_sigmas = np.outer(np.ones(global_weights.shape[0]), sigma_inv_prior + sigma_inv_layer[group_order[0]])
 
         popularity_counts = [1] * global_weights.shape[0]
 
         assignment = [[] for _ in range(J)]
 
         assignment[group_order[0]] = list(range(global_weights.shape[0]))
 
         # Initialize
         for j in group_order[1:]:
             global_weights, global_sigmas, popularity_counts, assignment_j = pfnm_core.matching_upd_j(
-                batch_weights_norm[j], global_weights, sigma_inv_layer[j], global_sigmas,
-                prior_mean_norm, sigma_inv_prior, popularity_counts, gamma, J)
+                batch_weights_norm[j],
+                global_weights,
+                sigma_inv_layer[j],
+                global_sigmas,
+                prior_mean_norm,
+                sigma_inv_prior,
+                popularity_counts,
+                gamma,
+                J,
+            )
             assignment[j] = assignment_j
     else:
-        popularity_counts, global_weights, global_sigmas = \
-            pfnm_core.init_from_assignments(batch_weights_norm, sigma_inv_layer, mean_prior,
-                                            sigma_inv_prior, assignment)
+        popularity_counts, global_weights, global_sigmas = pfnm_core.init_from_assignments(
+            batch_weights_norm, sigma_inv_layer, mean_prior, sigma_inv_prior, assignment
+        )
 
     # Iterate over groups
     for _ in range(iters):
         random_order = np.random.permutation(J)
         for j in random_order:  # random_order:
             to_delete = []
             # Remove j
@@ -265,23 +249,30 @@
                     del popularity_counts[i]
                     to_delete.append(i)
                     for j_clean in range(J):
                         for idx, l_ind in enumerate(assignment[j_clean]):
                             if i < l_ind and j_clean != j:
                                 assignment[j_clean][idx] -= 1
                             elif i == l_ind and j_clean != j:
-                                logger.warning('Weird unmatching detected')
+                                logger.warning("Weird unmatching detected")
                 else:
-                    global_weights[i] = global_weights[i] - \
-                                        batch_weights_norm[j][l]
+                    global_weights[i] = global_weights[i] - batch_weights_norm[j][l]
                     global_sigmas[i] -= sigma_inv_layer[j]
 
             global_weights = np.delete(global_weights, to_delete, axis=0)
             global_sigmas = np.delete(global_sigmas, to_delete, axis=0)
 
             # Match j
             global_weights, global_sigmas, popularity_counts, assignment_j = pfnm_core.matching_upd_j(
-                batch_weights_norm[j], global_weights, sigma_inv_layer[j], global_sigmas,
-                prior_mean_norm, sigma_inv_prior, popularity_counts, gamma, J)
+                batch_weights_norm[j],
+                global_weights,
+                sigma_inv_layer[j],
+                global_sigmas,
+                prior_mean_norm,
+                sigma_inv_prior,
+                popularity_counts,
+                gamma,
+                J,
+            )
             assignment[j] = assignment_j
 
     return assignment, global_weights, global_sigmas
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/util/pfnm/utils.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/pfnm/utils.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
 from __future__ import print_function
+
 import numpy as np
 
 
 def change_global_dtypes(w_global, w_client):
     """
     Converts w_global to the same dtype as w_client. PFNM procedure changes the dtype
     causing discrepancies between data dtype and model dtype.
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/util/xgboost/export.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/xgboost/export.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,25 +1,23 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
 from __future__ import print_function
-import joblib
+
 import logging
 
 from sklearn.base import is_classifier
-from sklearn.experimental import enable_hist_gradient_boosting
-from sklearn.ensemble import HistGradientBoostingRegressor
-from sklearn.ensemble import HistGradientBoostingClassifier
-from sklearn.ensemble._hist_gradient_boosting.predictor import TreePredictor
+from sklearn.ensemble import HistGradientBoostingClassifier, HistGradientBoostingRegressor
 from sklearn.ensemble._hist_gradient_boosting.common import PREDICTOR_RECORD_DTYPE
+from sklearn.ensemble._hist_gradient_boosting.predictor import TreePredictor
 
-from sklearn.ensemble._hist_gradient_boosting.loss import BinaryCrossEntropy, \
-    CategoricalCrossEntropy, LeastSquares
+from ibmfl.util.xgboost.utils import _LOSSES, is_classifier
+from ibmfl.util.persistence.sklearn import SKLearnPersistence
 
 logger = logging.getLogger(__name__)
 
 
 def export_sklearn(model, full_path=None):
     """
     Auxillary function to first convert the FL XGBoost Model to Scikit Learn's
@@ -39,64 +37,63 @@
     :param model: The FL XGBoost model object to convert from.
     :type  model: `XGBFLModel`
     :param full_path: The full absolute path of where the model will persist.
     :type  full_path: `str`
     :return: `None`
     """
     # Initialize Model Object (Parameter Transfer from FL Model to Sklearn)
-    if model.loss == 'least_squares':
+    if model.loss == "least_squares":
         export_model = HistGradientBoostingRegressor(
             loss=model.loss,
             learning_rate=model.learning_rate,
             max_iter=model.max_iter,
             max_leaf_nodes=model.max_leaf_nodes,
             max_depth=model.max_depth,
             min_samples_leaf=model.min_samples_leaf,
             l2_regularization=model.l2_regularization,
             max_bins=model.max_bins,
             verbose=model.verbose,
-            random_state=model.random_state)
-    elif model.loss == 'categorical_crossentropy' or \
-            model.loss == 'binary_crossentropy' or \
-            model.loss == 'auto':
+            random_state=model.random_state,
+        )
+    elif model.loss == "categorical_crossentropy" or model.loss == "binary_crossentropy" or model.loss == "auto":
         export_model = HistGradientBoostingClassifier(
             loss=model.loss,
             learning_rate=model.learning_rate,
             max_iter=model.max_iter,
             max_leaf_nodes=model.max_leaf_nodes,
             max_depth=model.max_depth,
             min_samples_leaf=model.min_samples_leaf,
             l2_regularization=model.l2_regularization,
             max_bins=model.max_bins,
             verbose=model.verbose,
-            random_state=model.random_state)
+            random_state=model.random_state,
+        )
 
     # Attribute Transfer
     export_model._baseline_prediction = model._baseline_prediction
     export_model.n_trees_per_iteration_ = model.n_trees
-    export_model.n_features_ = model.n_features_
+    export_model._n_features = model.n_features_
+    export_model._bin_mapper = model.bin_mapper_
 
     # Model Predictor Object (Deep Object Rebuild)
     export_model._predictors = []
     for i, pred in enumerate(model._predictors):
         export_model._predictors.append([])
         for j, p in enumerate(pred):
-            nodes = model._predictors[i][j].nodes.astype(
-                PREDICTOR_RECORD_DTYPE).copy()
-            export_model._predictors[i].append(TreePredictor(nodes))
+            nodes = model._predictors[i][j].nodes.astype(PREDICTOR_RECORD_DTYPE).copy()
+            export_model._predictors[i].append(TreePredictor(nodes, model.known_cat_bitsets, model.known_cat_bitsets))
 
     # Model Loss Function
-    if hasattr(model, 'classes_'):
-        if model.loss == 'binary_crossentropy' or len(model.classes_) == 2:
-            export_model.loss_ = BinaryCrossEntropy(None)
-        elif model.loss == 'categorical_crossentropy':
-            export_model.loss_ = CategoricalCrossEntropy(None)
-    elif model.loss == 'least_squares':
-        export_model.loss_ = LeastSquares(None)
+    if hasattr(model, "classes_"):
+        if model.loss == "binary_crossentropy" or len(model.classes_) == 2:
+            export_model._loss = _LOSSES["binary_crossentropy"](None)
+        elif model.loss == "categorical_crossentropy":
+            export_model._loss = _LOSSES["categorical_crossentropy"](None)
+    elif model.loss == "least_squares":
+        export_model._loss = _LOSSES["least_squares"](None)
 
     # Target Class Encoding (For Classification Models)
-    if hasattr(model, 'classes_'):
-        export_model.classes_ = model.classes_
+    if hasattr(model, "classes_"):
+        export_model.classes_ = model.classes_.ravel()
 
-    # Perform Model Pickling
-    with open(full_path, 'wb') as f:
-        joblib.dump(export_model, f)
+    SKLearnPersistence.save_model(export_model, full_path)
+
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/util/xgboost/hyperparams.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/xgboost/hyperparams.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,18 +1,19 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
 from __future__ import print_function
+
 import logging
+
 from ibmfl.exceptions import HyperparamsException
 
-VALID_LOSSES = ['binary_crossentropy', 'categorical_crossentropy',
-                'least_squares', 'least_absolute_deviation', 'auto']
+VALID_LOSSES = ["binary_crossentropy", "categorical_crossentropy", "least_squares", "least_absolute_deviation", "auto"]
 VALID_SCORES = []
 
 logger = logging.getLogger(__name__)
 
 
 def validate_parameters(hyperparams):
     """
@@ -23,161 +24,169 @@
     :param hyperparams: Dictionary containing values for the hyperparameters.
     :type  hyperparams: `dict`
     :rtype: None
     """
     try:
         # Check Hyperparameter Type
         if not isinstance(hyperparams, dict):
-            raise ValueError('Provided hyperparameter is not valid.')
+            raise ValueError("Provided hyperparameter is not valid.")
 
         # Global Hyperparameter Check
-        if 'global' in hyperparams:
-            params = hyperparams['global']
+        if "global" in hyperparams:
+            params = hyperparams["global"]
         else:
-            raise ValueError('Global parameters have not been defined.')
+            raise ValueError("Global parameters have not been defined.")
 
-        if 'learning_rate' in params:
-            if params['learning_rate'] <= 0:
-                raise ValueError('learning_rate={} must be strictly '
-                                 'positive'.format(params['learning_rate']))
+        if "learning_rate" in params:
+            if params["learning_rate"] <= 0:
+                raise ValueError("learning_rate={} must be strictly " "positive".format(params["learning_rate"]))
         else:
-            raise ValueError('learning_rate has not been defined.')
+            raise ValueError("learning_rate has not been defined.")
 
-        if 'loss' in params:
-            if params['loss'] not in VALID_LOSSES:
-                raise ValueError('Loss {} is currently not supported.'
-                                 'Accpted losses: {}'.format(params['loss'],
-                                                             ', '.join(VALID_LOSSES)))
+        if "loss" in params:
+            if params["loss"] not in VALID_LOSSES:
+                raise ValueError(
+                    "Loss {} is currently not supported."
+                    "Accpted losses: {}".format(params["loss"], ", ".join(VALID_LOSSES))
+                )
         else:
-            raise ValueError('loss has not been defined.')
+            raise ValueError("loss has not been defined.")
 
-        if 'num_classes' in params:
+        if "num_classes" in params:
             # Validate Class Parameter Types
-            if type(params['num_classes']) is not int:
-                raise ValueError('Provided classes value\'s type is not valid, '
-                                 'should be an int value >= 2 for classification.')
+            if type(params["num_classes"]) is not int:
+                raise ValueError(
+                    "Provided classes value's type is not valid, " "should be an int value >= 2 for classification."
+                )
 
             # Validate Classification Case
-            if params['loss'] != 'least_squares':
-                if params['num_classes'] < 0:
-                    raise ValueError('Provided class value must be >= 2 for '
-                                     'classification.')
-
-                if params['loss'] == 'binary_crossentropy' and \
-                    params['num_classes'] != 2:
-                    raise ValueError('Binary class models must have class of 2.')
-                elif params['loss'] == 'categorical_crossentropy' and \
-                    params['num_classes'] <= 2:
-                    raise ValueError('Multiclass models must have class > 2.')
-                elif params['loss'] == 'auto':
-                    if params['num_classes'] < 2:
-                        raise ValueError('Class value must be >= 2.')
+            if params["loss"] != "least_squares":
+                if params["num_classes"] < 0:
+                    raise ValueError("Provided class value must be >= 2 for " "classification.")
+
+                if params["loss"] == "binary_crossentropy" and params["num_classes"] != 2:
+                    raise ValueError("Binary class models must have class of 2.")
+                elif params["loss"] == "categorical_crossentropy" and params["num_classes"] <= 2:
+                    raise ValueError("Multiclass models must have class > 2.")
+                elif params["loss"] == "auto":
+                    if params["num_classes"] < 2:
+                        raise ValueError("Class value must be >= 2.")
                     else:
                         logging.warning(
-                            'Obtaining class labels based on local dataset. '
-                            'This may cause failures during aggregation '
-                            'when parties have distinctive class labels.')
+                            "Obtaining class labels based on local dataset. "
+                            "This may cause failures during aggregation "
+                            "when parties have distinctive class labels."
+                        )
         else:
             # Handle Classes Not Defined Case
-            if params['loss'] != 'least_squares':
-                raise ValueError('Classes has not been defined. Should provide '
-                                 'a value >= 2 for classification models.')
-
-        if 'max_bins' in params:
-            if not (2 <= params['max_bins'] and params['max_bins'] <= 255):
-                raise ValueError('max_bins={} should be no smaller than 2 '
-                                 'and no larger than 255.'.format(params['max_bins']))
-
-        if 'max_iter' in params:
-            if params['max_iter'] < 1:
-                raise ValueError('max_iter={} must not be smaller '
-                                 'than 1.'.format(params['max_iter']))
+            if params["loss"] != "least_squares":
+                raise ValueError(
+                    "Classes has not been defined. Should provide " "a value >= 2 for classification models."
+                )
+
+        if "max_bins" in params:
+            if not (2 <= params["max_bins"] and params["max_bins"] <= 255):
+                raise ValueError(
+                    "max_bins={} should be no smaller than 2 " "and no larger than 255.".format(params["max_bins"])
+                )
+
+        if "max_iter" in params:
+            if params["max_iter"] < 1:
+                raise ValueError("max_iter={} must not be smaller " "than 1.".format(params["max_iter"]))
         else:
-            raise ValueError('max_iter has not been defined.')
+            raise ValueError("max_iter has not been defined.")
 
-        if 'max_depth' in params:
-            if params['max_depth'] is not None and params['max_depth'] < 0:
-                raise ValueError('max_depth={} must be strictly greater'
-                                 'than 0.'.format(params['max_depth']))
-
-        if 'max_leaf_nodes' in params:
-            if params['max_leaf_nodes'] is not None and params['max_leaf_nodes'] <= 1:
-                raise ValueError('max_leaf_nodes={} must be strictly greater'
-                                 'than 1.'.format(params['max_leaf_nodes']))
-
-        if 'min_samples_leaf' in params:
-            if params['min_samples_leaf'] is not None and params['min_samples_leaf'] < 0:
-                raise ValueError('min_sample_leaf={} must not be smaller '
-                                 'than 0'.format(params['min_samples_leaf']))
+        if "max_depth" in params:
+            if params["max_depth"] is not None and params["max_depth"] < 0:
+                raise ValueError("max_depth={} must be strictly greater" "than 0.".format(params["max_depth"]))
+
+        if "max_leaf_nodes" in params:
+            if params["max_leaf_nodes"] is not None and params["max_leaf_nodes"] <= 1:
+                raise ValueError(
+                    "max_leaf_nodes={} must be strictly greater" "than 1.".format(params["max_leaf_nodes"])
+                )
+
+        if "min_samples_leaf" in params:
+            if params["min_samples_leaf"] is not None and params["min_samples_leaf"] < 0:
+                raise ValueError("min_sample_leaf={} must not be smaller " "than 0".format(params["min_samples_leaf"]))
+
+        if "data_sketch_accuracy" in params and params["data_sketch_accuracy"] is not None:
+            if params["data_sketch_accuracy"] <= 0 or params["data_sketch_accuracy"] >= 1:
+                raise ValueError("data_sketch_accuracy={} must be strictly greater than 0 and less than 1")
 
     except Exception as ex:
         logger.exception(str(ex))
-        raise HyperparamsException('Defined global hyperparameters malformed. '+str(ex))
+        raise HyperparamsException("Defined global hyperparameters malformed. " + str(ex))
 
 
 def init_parameters(obj, hyperparameters):
     """
     Initializes the hyperparameters to the corresponding object as an attribute.
     The parameters are propagated by reference, so changes should propagate to
     respective object passed into the function.
 
     :param obj: Any class object which to tie initialize the hyperparameters with.
     :type  obj: Object
     :param hyperparameters: Dictionary containing values for the hyperparameters.
     :type  hyperparameters: `dict`
     """
     # Initialize Global Configuration Parameter
-    params = hyperparameters['global']
-    setattr(obj, 'param', params)
+    params = hyperparameters["global"]
+    setattr(obj, "param", params)
 
     # Initialize Attributes (Pre-Checked Parameters)
-    setattr(obj, 'learning_rate', params['learning_rate'])
-    setattr(obj, 'loss', params['loss'])
-    setattr(obj, 'max_iter', params['max_iter'])
-
-    if params['loss'] == 'least_squares':
-        setattr(obj, 'num_classes', 1)
-    elif params['loss'] in ['binary_crossentropy', 'categorical_crossentropy', 'auto']:
-        setattr(obj, 'num_classes', params['num_classes'])
+    setattr(obj, "learning_rate", params["learning_rate"])
+    setattr(obj, "loss", params["loss"])
+    setattr(obj, "max_iter", params["max_iter"])
+
+    if params["loss"] == "least_squares":
+        setattr(obj, "num_classes", 1)
+    elif params["loss"] in ["binary_crossentropy", "categorical_crossentropy", "auto"]:
+        setattr(obj, "num_classes", params["num_classes"])
 
     # Initialize Attributes (Optional Values - Based on Default Parameters)
-    if 'l2_regularization' not in params or params['l2_regularization'] is None:
-        setattr(obj, 'l2_regularization', 0)
+    if "l2_regularization" not in params or params["l2_regularization"] is None:
+        setattr(obj, "l2_regularization", 0)
+    else:
+        setattr(obj, "l2_regularization", params["l2_regularization"])
+
+    if "max_bins" not in params:
+        setattr(obj, "max_bins", 255)
     else:
-        setattr(obj, 'l2_regularization', params['l2_regularization'])
+        setattr(obj, "max_bins", params["max_bins"])
 
-    if 'max_bins' not in params:
-        setattr(obj, 'max_bins', 255)
+    if "max_depth" not in params or params["max_depth"] is None:
+        setattr(obj, "max_depth", None)
     else:
-        setattr(obj, 'max_bins', params['max_bins'])
+        setattr(obj, "max_depth", params["max_depth"])
 
-    if 'max_depth' not in params or params['max_depth'] is None:
-        setattr(obj, 'max_depth', None)
+    if "max_leaf_nodes" not in params or params["max_leaf_nodes"] is None:
+        setattr(obj, "max_leaf_nodes", 31)
     else:
-        setattr(obj, 'max_depth', params['max_depth'])
+        setattr(obj, "max_leaf_nodes", params["max_leaf_nodes"])
 
-    if 'max_leaf_nodes' not in params or params['max_leaf_nodes'] is None:
-        setattr(obj, 'max_leaf_nodes', 31)
+    if "min_samples_leaf" not in params or params["min_samples_leaf"] is None:
+        setattr(obj, "min_samples_leaf", 20)
     else:
-        setattr(obj, 'max_leaf_nodes', params['max_leaf_nodes'])
+        setattr(obj, "min_samples_leaf", params["min_samples_leaf"])
 
-    if 'min_samples_leaf' not in params or params['min_samples_leaf'] is None:
-        setattr(obj, 'min_samples_leaf', 20)
+    if "random_state" in params:
+        setattr(obj, "random_state", params["random_state"])
     else:
-        setattr(obj, 'min_samples_leaf', params['min_samples_leaf'])
+        setattr(obj, "random_state", None)
 
-    if 'random_state' in params:
-        setattr(obj, 'random_state', params['random_state'])
+    if "scoring" in params:
+        setattr(obj, "scoring", params["scoring"])
     else:
-        setattr(obj, 'random_state', None)
+        setattr(obj, "scoring", None)
 
-    if 'scoring' in params:
-        setattr(obj, 'scoring', params['scoring'])
+    if "verbose" not in params or params["verbose"] is None:
+        setattr(obj, "verbose", False)
     else:
-        setattr(obj, 'scoring', None)
+        setattr(obj, "verbose", True)
 
-    if 'verbose' not in params or params['verbose'] is None:
-        setattr(obj, 'verbose', False)
+    if "data_sketch_accuracy" not in params or params["data_sketch_accuracy"] is None:
+        setattr(obj, "data_sketch_accuracy", None)  # default
     else:
-        setattr(obj, 'verbose', True)
+        setattr(obj, "data_sketch_accuracy", params["data_sketch_accuracy"])
 
     return obj
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/cpd403/ibmfl/party/metrics/metrics_recorder.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/party/metrics/metrics_recorder.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
 import json
 import sys
 import time
 import logging
@@ -255,8 +255,8 @@
         """
         Update the most recent entry with the round number.
 
         :param round_no: the current round number
         :type round_no: `int`
         :return: None
         """
-        return self._data[-1].round_no if self._data else 1
+        return self._data[-1].round_no if self._data else 1
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/party/party.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/party/party.py`

 * *Files 0% similar despite different names*

```diff
@@ -3,14 +3,15 @@
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
 import logging
 
 logger = logging.getLogger(__name__)
 
+
 class Party:
 
     def __init__(self, **kwargs):
         from ibm_watsonx_ai.party_wrapper import Party
         self.party = Party(**kwargs)
         self.connection = self.party.connection
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/party_env_validator.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/party_env_validator.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/crypto_enumeration.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/crypto_enumeration.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 """
  An enumeration class for the crypto type field which describe what
  kind of data is being sent inside the Message
 """
 from enum import Enum
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/crypto_exceptions.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/crypto_exceptions.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
 from ibmfl.exceptions import FLException
 
 
 class CryptoException(FLException):
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/crypto_library.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/crypto_library.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
+
 """
 Module where the crypto library are defined.
 """
 
 import abc
 import logging
 import sys
@@ -102,17 +103,17 @@
         """
         if not isinstance(model_update, ModelUpdate):
             raise CryptoException("not a ModelUpdate instance to be encrypted")
 
         model_weights = model_update.get(key)
         ct_model_update = ModelUpdate()
         if isinstance(model_weights, list):
-            ct_model_update.add("ct_{}".format(key), self.cipher.encrypt_weights(model_weights))
+            ct_model_update["ct_{}".format(key)] = self.cipher.encrypt_weights(model_weights)
         else:
-            ct_model_update.add("ct_{}".format(key), self.cipher.encrypt_weights([model_weights]))
+            ct_model_update["ct_{}".format(key)] = self.cipher.encrypt_weights([model_weights])
         return ct_model_update
 
     @abc.abstractmethod
     def decrypt(self, ct_model_update, key="ct_weights", **kwargs):
         """
         Abstract method to perform decryption or partial decryption operations
         FL party will perform during training.
@@ -145,15 +146,15 @@
             to be averaged.
         :type lst_model_updates:  `list`
 
         :return: results of fused encrypted model update
         :rtype: `ModelUpdate`
         """
         fused_model_update = self.aggregate_collected_ciphertext_response(lst_model_updates)
-        if fused_model_update.exist_key("weights"):
+        if "weights" in fused_model_update :
             avg_model_weights = fused_model_update.get("weights")
             for i in range(len(avg_model_weights)):
                 avg_model_weights[i] = avg_model_weights[i] / len(lst_model_updates)
             avg_model_update = ModelUpdate(weights=avg_model_weights)
             return avg_model_update
         else:
             # fused model update is still in ciphertext format
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/helayer/fhe.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/helayer/fhe.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
+
 """
 Fully homomorphic encryption cryptosystem, built on helayer library.
 
 * type:     public-key encryption
 * setting:  Integer based
 
 :Date: 2021
@@ -19,14 +20,15 @@
 import numpy as np
 
 from ibmfl.crypto.crypto_enumeration import CryptoEnum
 from ibmfl.crypto.crypto_exceptions import CryptoException, KeyManagerException
 from ibmfl.crypto.crypto_library import Cipher, Crypto
 from ibmfl.model.model_update import ModelUpdate
 
+
 logger = logging.getLogger(__name__)
 
 
 class CryptoFHE(Crypto):
     """
     The crypto class for FHE crypto system, where its KeyGenerator and
     Cipher can be initialized.
@@ -81,21 +83,21 @@
             raise CryptoException("not a ModelUpdate instance to be encrypted")
 
         ct_model_update = ModelUpdate()
         weights = model_update.get(key)
         counts = kwargs.get("counts")
         if self.private_fusion_weight:
             ct_weights, ct_counts = self.cipher.encrypt_weights(weights, counts=counts)
-            ct_model_update.add("ct_weights", ct_weights)
-            ct_model_update.add("ct_counts", ct_counts)
+            ct_model_update["ct_weights"] = ct_weights
+            ct_model_update["ct_counts"] = ct_counts
         else:
             ct_weights = self.cipher.encrypt_weights(weights)
-            ct_model_update.add("ct_weights", ct_weights)
-            ct_model_update.add("counts", counts)
-        ct_model_update.add("weights_dtype", self.cipher.weights_dtype)
+            ct_model_update["ct_weights"] = ct_weights
+            ct_model_update["counts"] = counts
+        ct_model_update["weights_dtype"] = self.cipher.weights_dtype
 
         return ct_model_update
 
     def decrypt(self, ct_model_update, **kwargs):
         """
         Method for processing fused model update in ciphertext that
         FL party will perform during training.
@@ -107,15 +109,15 @@
         :return: A `ModelUpdate` object containing the resulting decrypted \
         plain text, where the plain text is of type `list` of `np.ndarray`.
         :rtype: `ModelUpdate`
         """
         ct_weights = ct_model_update.get("ct_weights")
         weights = self.cipher.decrypt_weights(ct_weights, weights_dtype=ct_model_update.get("weights_dtype"))
         res_model_update = ModelUpdate()
-        res_model_update.add("weights", weights)
+        res_model_update["weights"] = weights
 
         return res_model_update
 
     def aggregate_collected_ciphertext_response(self, lst_model_updates, key="ct_weights", **kwargs):
         """
         Receives a list of `model update`, where a `model_update` is of type
         `ModelUpdate`, using the encrypted values (indicating by the `key`)
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/infra/crypto_asym_imp_rsa.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/infra/crypto_asym_imp_rsa.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
+
 from cryptography.exceptions import InvalidSignature
 from cryptography.hazmat.primitives import hashes, serialization
 from cryptography.hazmat.primitives.asymmetric import padding, rsa, utils
 from cryptography.hazmat.primitives.serialization import load_pem_public_key
 
 from ibmfl.crypto.crypto_exceptions import *
 from ibmfl.crypto.infra.crypto_asym_int import CryptoAsym
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/infra/crypto_asym_int.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/infra/crypto_asym_int.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
+
 import abc
 
 
 class CryptoAsym(abc.ABC):
     """
     This class defines an interface for asymmetric encryption functions.
     """
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/infra/crypto_cert_imp_rsa.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/infra/crypto_cert_imp_rsa.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
+
 from cryptography import x509
 from cryptography.exceptions import InvalidSignature
 from cryptography.hazmat.primitives import serialization
 from cryptography.hazmat.primitives.asymmetric import padding, rsa
 
 from ibmfl.crypto.crypto_exceptions import *
 from ibmfl.crypto.infra.crypto_cert_int import CryptoCert
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/infra/crypto_cert_int.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/infra/crypto_cert_int.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
+
 import abc
 
 from ibmfl.crypto.crypto_exceptions import *
 
 
 class CryptoCert:
     """
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/infra/crypto_he_imp_hely.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/infra/crypto_he_imp_hely.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
+
 import importlib
 
 if importlib.util.find_spec("pyhelayers") is not None:
     import pyhelayers as pyhe
 
 from ibmfl.crypto.crypto_exceptions import *
 from ibmfl.crypto.infra.crypto_he_int import CryptoHe
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/infra/crypto_he_int.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/infra/crypto_he_int.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
+
 import abc
 
 
 class CryptoHe(abc.ABC):
     """
     This class defines an interface for HE keys generation functions.
     """
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/infra/crypto_sym_imp_fernet.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/infra/crypto_sym_imp_fernet.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
+
 from cryptography.fernet import Fernet
 
 from ibmfl.crypto.infra.crypto_sym_int import CryptoSym
 
 
 class CryptoSymFernet(CryptoSym):
     """
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/infra/crypto_sym_int.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/infra/crypto_sym_int.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
+
 import abc
 
 from ibmfl.crypto.crypto_exceptions import *
 
 
 class CryptoSym(abc.ABC):
     """
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/keys_mng/crypto_key_mng_dst.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/keys_mng/crypto_key_mng_dst.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
+
 from ibmfl.crypto.crypto_exceptions import KeyManagerException
 from ibmfl.crypto.keys_mng.crypto_key_mng_int import KeyManager
 
 
 class DistributionKeyManager(KeyManager):
     def __init__(self, config):
         """Initialize Key from local key file"""
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/keys_mng/crypto_key_mng_int.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/keys_mng/crypto_key_mng_int.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
+
 import abc
 
 
 class KeyManager(abc.ABC):
     """
     The abstract class for `KeyManager` object.
     """
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/keys_mng/crypto_keys_proto_party.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/keys_mng/crypto_keys_proto_party.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
+
 import base64
 import logging
 import os
 import random
 import string
 from datetime import datetime
 
@@ -271,15 +272,15 @@
         """
 
         self.crypto_sym.generate_key()
         self.crypto_he.generate_keys()
         self.he_pb = self.crypto_he.get_public_key()
         self.he_pr = self.crypto_he.get_private_key()
         self.keys_id = generate_random_string(64)
-        self.keys_time = datetime.now(pytz.timezone("America/New_York"))
+        self.keys_time = str(datetime.now(pytz.timezone("America/New_York")))
         return
 
     def parse_keys(self, keys_msg: dict) -> bool:
         """
         Parses a keys distribution message.
         Incoming message structure:
         {gen_cert|he_pb|he_pb_sign|he_pr_enc|he_pr_enc_sign|sym_enc|sym_enc_sign|keys_id|keys_time}
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/exceptions.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/exceptions.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
+
 """
 This module will host all the exceptions which are raised by fl components
 """
 
 
 class FLException(Exception):
     pass
@@ -65,7 +66,10 @@
 
 class QuorumException(FLException):
     pass
 
 
 class PreprocessException(FLException):
     pass
+
+class SerializationException(FLException):
+    pass
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/message/message_type.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/message/message_type.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
+
 """
  An enumeration class for the message type field which describe what
  kind of data is being sent inside the Message
 """
 from enum import Enum
 
 __author__ = "Supriyo Chakraborty, Shalisha Witherspoon, Dean Steuer"
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/model/pytorch_fl_model.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/utils/utils.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,748 +1,917 @@
 #  -----------------------------------------------------------------------------------------
 #  (C) Copyright IBM Corp. 2023-2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
-import copy
+from __future__ import annotations
+import re
+import os
+import sys
+import shutil
+import tarfile
 import logging
-import time
-from importlib import import_module
+import importlib.util
+import json
 
-import numpy as np
-import torch
-import torch.onnx
-from sklearn import metrics
-from skorch import NeuralNet
-from skorch.callbacks import EpochScoring
-from skorch.dataset import CVSplit, Dataset
-from skorch.exceptions import NotInitializedError
-from skorch.helper import predefined_split
-
-from ibmfl.exceptions import FLException, LocalTrainingException, ModelException, ModelInitializationException
-from ibmfl.model.fl_model import FLModel
-from ibmfl.model.model_update import ModelUpdate
-from ibmfl.util import config
+import numpy
+import importlib
 
-logger = logging.getLogger(__name__)
+from typing import (
+    TYPE_CHECKING,
+    Any,
+    Callable,
+    Iterable,
+    Type,
+    Generator,
+    TypeAlias,
+    cast,
+)
+from subprocess import check_call
+from packaging import version
+from warnings import warn
+
+import ibm_watsonx_ai._wrappers.requests as requests
+
+from ibm_watsonx_ai.wml_client_error import (
+    WMLClientError,
+    CannotInstallLibrary,
+)
+
+if TYPE_CHECKING:
+    import pyspark
+    import collections
+    from types import TracebackType
+    from ibm_watsonx_ai import APIClient
+    from IPython.display import HTML
+    from requests import Response
+
+    PipelineType: TypeAlias = Any
+    MLModelType: TypeAlias = Any
+
+INSTANCE_DETAILS_TYPE = "instance_details_type"
+PIPELINE_DETAILS_TYPE = "pipeline_details_type"
+DEPLOYMENT_DETAILS_TYPE = "deployment_details_type"
+EXPERIMENT_RUN_DETAILS_TYPE = "experiment_run_details_type"
+MODEL_DETAILS_TYPE = "model_details_type"
+DEFINITION_DETAILS_TYPE = "definition_details_type"
+EXPERIMENT_DETAILS_TYPE = "experiment_details_type"
+TRAINING_RUN_DETAILS_TYPE = "training_run_details_type"
+FUNCTION_DETAILS_TYPE = "function_details_type"
+DATA_ASSETS_DETAILS_TYPE = "data_assets_details_type"
+SW_SPEC_DETAILS_TYPE = "sw_spec_details_type"
+HW_SPEC_DETAILS_TYPE = "hw_spec_details_type"
+RUNTIME_SPEC_DETAILS_TYPE = "runtime_spec_details_type"
+LIBRARY_DETAILS_TYPE = "library_details_type"
+SPACES_DETAILS_TYPE = "spaces_details_type"
+MEMBER_DETAILS_TYPE = "member_details_type"
+CONNECTION_DETAILS_TYPE = "connection_details_type"
+PKG_EXTN_DETAILS_TYPE = "pkg_extn_details_type"
+UNKNOWN_ARRAY_TYPE = "resource_type"
+UNKNOWN_TYPE = "unknown_type"
+SPACES_IMPORTS_DETAILS_TYPE = "spaces_imports_details_type"
+SPACES_EXPORTS_DETAILS_TYPE = "spaces_exports_details_type"
+
+SPARK_MLLIB = "mllib"
+SPSS_FRAMEWORK = "spss-modeler"
+TENSORFLOW_FRAMEWORK = "tensorflow"
+XGBOOST_FRAMEWORK = "xgboost"
+SCIKIT_LEARN_FRAMEWORK = "scikit-learn"
+PMML_FRAMEWORK = "pmml"
+
+
+def _get_id_from_deprecated_uid(
+    kwargs: dict, resource_id: str | None, resource_name: str, can_be_none: bool = False
+) -> str:
+    if (resource_uid := kwargs.get(resource_name + "_uid")) is not None:
+        warn(
+            f"`{resource_name}_uid` parameter is deprecated, please use `{resource_name}_id`",
+            category=DeprecationWarning,
+        )
+        if not resource_id:
+            resource_id = resource_uid
+    elif not can_be_none and resource_uid is None and resource_id is None:
+        raise TypeError(
+            f"Function missing 1 required positional argument: '{resource_name}_id'"
+        )
 
+    return resource_id
 
-class PytorchFLModel(FLModel):
-    """
-    Wrapper class for importing a Pytorch based model
-    """
 
-    def __init__(self, model_name, model_spec=None, pytorch_module=None, module_init_params=None, **kwargs):
-        """
-        Create a `PytorchFLModel` instance from a Pytorch model.
-        If pytorch_model is provided, it will use it; otherwise it will take
-        the model_spec to create the model.
-
-        :param model_name: String specifying the type of model e.g., Pytorch_NN
-        :type model_name: `str`
-        :param model_spec: A dictionary specifying path to saved nn.sequence container
-        :type model_spec: 'dict'
-        :param pytorch_module: uninstantiated pytorch model class
-        :type pytorch_module: torch.nn.Module class reference \
-        :param module_init_params: A dictionary with the values for the model's init \
-        arguments. The key for the init parameters must be prefixed with module__ \
-        (ex. if the parameter name is hidden_size, then key must be module__hidden_size)
-        :type module_init_params: 'dict'
-        :param kwargs: A dictionary contains other parameter settings on \
-         to initialize a PyTorch model.
-        :type kwargs: `dict`
-        """
-        super().__init__(model_name, model_spec, **kwargs)
-
-        self.model_type = "PyTorch"
-        self.model = None
-        self.module = None
-        self.input_shape = None
-        if pytorch_module is None:
-            if model_spec is None or (not isinstance(model_spec, dict)):
-                raise ValueError(
-                    "Initializing model requires "
-                    "a model specification or uninstantiated "
-                    "pytorch model class reference "
-                    "None was provided"
+def get_url(
+    url: str, headers: dict, params: dict | None = None, isIcp: bool = False
+) -> Response:
+
+    if isIcp:
+        return requests.get(url, headers=headers, params=params)
+    else:
+        return requests.get(url, headers=headers, params=params)
+
+
+def print_text_header_h1(title: str) -> None:
+    print("\n\n" + ("#" * len(title)) + "\n")
+    print(title)
+    print("\n" + ("#" * len(title)) + "\n\n")
+
+
+def print_text_header_h2(title: str) -> None:
+    print("\n\n" + ("-" * len(title)))
+    print(title)
+    print(("-" * len(title)) + "\n\n")
+
+
+def get_type_of_details(details: dict) -> str:
+    if "resources" in details:
+        return UNKNOWN_ARRAY_TYPE
+    elif details is None:
+        raise WMLClientError("Details doesn't exist.")
+    else:
+        try:
+            plan = "plan" in details["entity"]
+
+            if plan:
+                return INSTANCE_DETAILS_TYPE
+
+            if (
+                re.search("\/wml_instances\/[^\/]+$", details["metadata"]["url"])
+                is not None
+            ):
+                return INSTANCE_DETAILS_TYPE
+        except:
+            pass
+        try:
+            if (
+                re.search("\/pipelines\/[^\/]+$", details["metadata"]["href"])
+                is not None
+            ):
+                return PIPELINE_DETAILS_TYPE
+        except:
+            pass
+        try:
+            if (
+                "href" in details["metadata"]
+                and re.search("\/deployments\/[^\/]+$", details["metadata"]["href"])
+                is not None
+                or re.search("\/deployments\/[^\/]+$", details["metadata"]["id"])
+                is not None
+                or "virtual_deployment_downloads" in details["entity"]["status"]
+            ):
+                return DEPLOYMENT_DETAILS_TYPE
+        except:
+            pass
+
+        try:
+            if (
+                re.search("\/experiments\/[^\/]+$", details["metadata"]["href"])
+                is not None
+            ):
+                return EXPERIMENT_DETAILS_TYPE
+        except:
+            pass
+
+        try:
+            if (
+                re.search("\/trainings\/[^\/]+$", details["metadata"]["href"])
+                is not None
+            ):
+                return TRAINING_RUN_DETAILS_TYPE
+        except:
+            pass
+
+        try:
+            if re.search("\/models\/[^\/]+$", details["metadata"]["href"]) is not None:
+                return MODEL_DETAILS_TYPE
+        except:
+            pass
+
+        try:
+            if (
+                re.search("\/functions\/[^\/]+$", details["metadata"]["href"])
+                is not None
+            ):
+                return FUNCTION_DETAILS_TYPE
+        except:
+            pass
+
+        try:
+            if (
+                re.search("\/runtimes\/[^\/]+$", details["metadata"]["href"])
+                is not None
+            ):
+                return RUNTIME_SPEC_DETAILS_TYPE
+        except:
+            pass
+
+        try:
+            if (
+                re.search("\/libraries\/[^\/]+$", details["metadata"]["href"])
+                is not None
+            ):
+                return LIBRARY_DETAILS_TYPE
+        except:
+            pass
+
+        try:
+            if re.search("\/spaces\/[^\/]+$", details["metadata"]["href"]) is not None:
+                return SPACES_DETAILS_TYPE
+        except:
+            pass
+
+        try:
+            if re.search("\/members\/[^\/]+$", details["metadata"]["href"]) is not None:
+                return MEMBER_DETAILS_TYPE
+        except:
+            pass
+
+        try:
+            if re.search("\/members\/[^\/]+$", details["metadata"]["href"]) is not None:
+                return MEMBER_DETAILS_TYPE
+        except:
+            pass
+
+        try:
+            if re.search("\/assets\/[^\/]+$", details["metadata"]["href"]) is not None:
+                return DATA_ASSETS_DETAILS_TYPE
+        except:
+            pass
+
+        try:
+            if (
+                re.search(
+                    "\/software_specifications\/[^\/]+$", details["metadata"]["href"]
                 )
-            # In this case we need to recreate the model from model_spec
-            self.module = self.load_model_from_spec(model_spec)
-        else:
-            self.module = pytorch_module
+                is not None
+            ):
+                return SW_SPEC_DETAILS_TYPE
+        except:
+            pass
 
-        self.is_classification = True if not model_spec else model_spec.get("is_classification", True)
-        if ( self.is_classification ) :
-            self.default_optimizer = torch.optim.SGD 
-            self.default_criterion = torch.nn.NLLLoss
-            self.default_metric = metrics.accuracy_score
-        else :
-            self.default_optimizer = torch.optim.Adam
-            self.default_criterion = torch.nn.MSELoss
-            self.default_metric = metrics.r2_score
-
-        self.optimizer = self.load_optimizer_from_spec(model_spec)
-        self.criterion = self.load_loss_criterion_from_spec(model_spec)
-
-        self.model = self.initialize_model(self.module, self.optimizer, self.criterion, module_init_params)
-
-        if self.use_gpu_for_training and torch.cuda.device_count() > 0:
-            if self.num_gpus > torch.cuda.device_count():
-                logger.error(
-                    "Selected number of gpus to use for training exceeds number of available gpus, "
-                    + str(torch.cuda.device_count())
-                    + "Set number of gpus to maximum available on device."
+        try:
+            if (
+                re.search(
+                    "\/hardware_specifications\/[^\/]+$", details["metadata"]["href"]
                 )
-                self.num_gpus = torch.cuda.device_count()
+                is not None
+            ):
+                return HW_SPEC_DETAILS_TYPE
+        except:
+            pass
 
-            device_ids = list(range(self.num_gpus))
-            self.model.module_ = torch.nn.DataParallel(self.model.module_, device_ids=device_ids)
-            self.model.set_params(device="cuda")
-            self.model.module_.to("cuda")
-
-    def initialize_model(self, pytorch_module, optimizer, criterion, module_init_params=None):
-        """
-        Initializes a pytorch model via skorch library
-
-        :param pytorch_module: uninstantiated pytorch model class
-        :type pytorch_module: torch.nn.Module class reference
-        :param optimizer: the optimizer to use
-        :type optimizer: pytorch optimizer class
-        :param criterion: the loss function to use
-        :type criterion: pytorch loss function class
-        :param module_init_params: A dictionary with the values for the model's init arguments. \
-        The key for the init parameters must be prefixed with module__  \
-        (ex. if the parameter name is hidden_size, then key must be module__hidden_size)
-        :type module_init_params: 'dict'
-        :return: an initialized skorch model
-        :rtype: 'skorch.NeuralNet'
-        """
-        if module_init_params is None:
-            module_init_params = {}
-        model = NeuralNet(
-            module=pytorch_module,
-            optimizer=optimizer,
-            criterion=criterion,
-            warm_start=True,
-            callbacks=[
-                (
-                    "valid_acc",
-                    EpochScoring(self.valid_acc, lower_is_better=False, use_caching=True, name="test_valid_acc"),
+        try:
+            if (
+                re.search(
+                    "\/package_extension\/[^\/]+$",
+                    details["entity"]["package_extension"]["href"],
                 )
-            ],
-            **module_init_params,
-        )
-        model.initialize()
-        return model
+                is not None
+            ):
+                return PKG_EXTN_DETAILS_TYPE
+        except:
+            pass
 
-    def fit_model(self, train_data, fit_params=None, validation_data=None, **kwargs):
-        """
-        Fits current model with provided training data.
-
-        :param train_data: Training data, a tuple \
-        given in the form (x_train, y_train). otherwise, input compatible with skorch.dataset.Dataset
-        :type train_data: `(np.ndarray, np.ndarray)`
-        :param fit_params: (optional) Dictionary with hyperparameters \
-        that will be used to call fit function. \
-        Hyperparameter parameters should match pytorch expected values \
-        e.g., `epochs`, which specifies the number of epochs to be run. \
-        If no `epochs` or `batch_size` are provided, a default value \
-        will be used (1 and 128, respectively).
-        :type fit_params: `dict`
-        :return: None
-        """
-        # Initialized with default values
-        batch_size = 128
-        epochs = 1
-        lr = 0.01
-        train_split = None
-        optimizer_params = {}
-        hyperparams = fit_params.get("hyperparams", {}) if fit_params else {}
-        party_params = kwargs.get("local_params", {}) or {} if fit_params else {}
-
-        if hyperparams:
-            local_params = hyperparams.get("local", {}) or {}
-            training_hp = local_params.get("training", {}) or {}
-
-            epochs = training_hp.get("epochs", epochs)
-            batch_size = training_hp.get("batch_size", batch_size)
-            lr = training_hp.get("lr", lr)
-
-            if "validation_split" in training_hp:
-                validation_split = training_hp.get("validation_split")
-                try:
-                    if float(validation_split) != 0:
-                        train_split = CVSplit(float(validation_split), random_state=42)
-                except (TypeError, ValueError):
-                    raise ValueError("Validation split cannot be a NoneType")
-            if "validation_split" in party_params:
-                validation_split = party_params.get("validation_split")
-                try:
-                    if float(validation_split) != 0:
-                        train_split = CVSplit(float(validation_split), random_state=42)
-                except (TypeError, ValueError):
-                    raise ValueError("Validation split cannot be a NoneType")
-            if validation_data is not None:
-                validation_ds = Dataset(validation_data[0], validation_data[1])
-                train_split = predefined_split(validation_ds)
-
-            if "optimizer" in local_params:
-                optimizer = local_params["optimizer"]
-                if isinstance(optimizer, str):
-                    try:
-                        optimizer = getattr(import_module("torch.optim"), optimizer.split(".")[-1])
-
-                    except Exception as e:
-                        optimizer = self.optimizer
-                        logger.exception(str(e))
-                        print("selected optimizer not found. using default")
-                if optimizer != self.optimizer:
-                    self.optimizer = optimizer
-                    self.model.set_params(optimizer=self.optimizer)
-
-                    if self.use_gpu_for_training and torch.cuda.device_count() > 0:
-                        self.model.module_.to(self.model.device)
-
-            if "criterion" in local_params:
-                criterion = local_params["criterion"]
-                if isinstance(criterion, str):
-                    try:
-                        criterion = getattr(import_module("torch.nn"), criterion.split(".")[-1])
-
-                    except Exception as e:
-                        criterion = self.criterion
-                        logger.exception(str(e))
-                        print("selected criterion not found. using default")
-
-                if criterion != self.criterion:
-                    self.criterion = criterion
-                    self.model.set_params(criterion=self.criterion)
-
-            if "optimizer_params" in local_params:
-                optimizer_params = local_params["optimizer_params"]
-        self.model.set_params(batch_size=batch_size, lr=lr, train_split=train_split, **optimizer_params)
-
-        try:
-            if type(train_data) is tuple:
-                # Extract x_train and y_train, by default,
-                # label is stored in the last column
-                x = train_data[0]
-                y = train_data[1]
-                self.model.fit_loop(x, y, epochs=epochs)
+        try:
+            if re.search("\/imports\/[^\/]+$", details["metadata"]["href"]) is not None:
+                return SPACES_IMPORTS_DETAILS_TYPE
+        except:
+            pass
 
-            else:
-                # otherwise, expect that input is a pytorch Dataset generator
-                self.model.fit_loop(train_data, epochs=epochs)
+        try:
+            if re.search("\/exports\/[^\/]+$", details["metadata"]["href"]) is not None:
+                return SPACES_EXPORTS_DETAILS_TYPE
+        except:
+            pass
+
+        return UNKNOWN_TYPE
+
+
+def load_model_from_directory(
+    framework: dict, directory_path: str
+) -> pyspark.ml.pipeline.PipelineModel | None:
+    if "mllib" in framework:
+        from pyspark.ml import PipelineModel
+
+        return PipelineModel.read().load(directory_path)
+    if "spss" in framework:
+        pass
+    if "tensorflow" in framework:
+        pass
+    if "scikit" in framework or "xgboost" in framework:
+        try:
+            try:
+                from sklearn.externals import joblib
+            except ImportError:
+                import joblib
+            pkl_files = [x for x in os.listdir(directory_path) if x.endswith(".pkl")]
+
+            if len(pkl_files) < 1:
+                raise WMLClientError("No pkl files in directory.")
 
+            model_id = pkl_files[0]
+            return joblib.load(os.path.join(directory_path, model_id))
         except Exception as e:
-            logger.exception(str(e))
-            raise LocalTrainingException("Error occurred while performing model.fit")
+            raise WMLClientError("Cannot load model from pkl file.", e)
+    if "pmml" in framework:
+        return None
+    else:
+        raise WMLClientError("Invalid framework specified: '{}'.".format(framework))
+
+
+def save_model_to_file(
+    model: MLModelType, framework: str, base_path: str, filename: str
+) -> None:
+    if filename.find(".") != -1:
+        base_name = filename[: filename.find(".") + 1]
+    else:
+        base_name = filename
+
+    if framework == SPARK_MLLIB:
+        model.write.overwrite.save(os.path.join(base_path, base_name))
+    elif framework == SPSS_FRAMEWORK:
+        pass
+    elif framework == TENSORFLOW_FRAMEWORK:
+        pass
+    elif framework == XGBOOST_FRAMEWORK:
+        pass
+    elif framework == SCIKIT_LEARN_FRAMEWORK:
+        os.makedirs(os.path.join(base_path, base_name))
+        try:
+            from sklearn.externals import joblib
+        except ImportError:
+            import joblib
+        joblib.dump(model, os.path.join(base_path, base_name, base_name + ".pkl"))
+    elif framework == PMML_FRAMEWORK:
+        pass
+    else:
+        raise WMLClientError("Invalid framework specified: '{}'.".format(framework))
+
+
+def format_metrics(latest_metrics_list: list[dict]) -> str:
+    formatted_metrics = ""
+
+    for i in latest_metrics_list:
 
-    def update_model(self, model_update):
-        """
-        Update model with provided model_update, where model_update
-        should be generated according to `PytorchFLModel.get_model_update()`.
-
-        :param model_update: `ModelUpdate` object that contains the weights \
-        that will be used to update the model.
-        :type model_update: `ModelUpdate`
-        :return: None
-        """
-        if isinstance(model_update, ModelUpdate):
-            for p1, p2 in zip(self.get_weights(), model_update.get("weights")):
-                p1.data = torch.from_numpy(p2)
-                p1.data.requires_grad = True
+        values = i["values"]
 
-            if self.use_gpu_for_training and torch.cuda.device_count() > 0:
-                self.model.module_.to(self.model.device)
+        if len(values) > 0:
+            sorted_values = sorted(values, key=lambda k: k["name"])
         else:
-            raise ValueError(
-                "Provided model_update should be of type Model." "Instead they are:{0}".format(str(type(model_update)))
+            sorted_values = values
+
+        for j in sorted_values:
+            formatted_metrics = (
+                formatted_metrics
+                + i["phase"]
+                + ":"
+                + j["name"]
+                + "="
+                + "{0:.4f}".format(j["value"])
+                + "\n"
             )
 
-    def get_model_update(self):
-        """
-        Generates a `ModelUpdate` object that will be sent to other entities.
-
-        :return: ModelUpdate
-        :rtype: `ModelUpdate`
-        """
-        weights = self.get_weights(to_numpy=True)
-        update = {"input_shape": self.input_shape, "weights": weights}
-
-        return ModelUpdate(**update)
-
-    def predict(self, x):
-        """
-        Perform prediction for a batch of inputs. Note that for classification
-        problems, it returns the resulting probabilities.
-
-        :param x: Samples with shape as expected by the model.
-        :type x: `np.ndarray`, input compatible with skorch.dataset.Dataset, or pytorch dataloader
-        :return: Array of predictions
-        :rtype: `np.ndarray`
-        """
-        if isinstance(x, torch.utils.data.DataLoader):
-            return self.predict_generator(x)
-
-        return self.model.predict(x)
-
-    def predict_generator(self, dataloader):
-        """
-        Performs predictions using a pytorch dataloader
-
-        :param x: A pytorch dataloader with a dataset for predicting
-        :type x: 'torch.utils.data.DataLoader'
-        :return: Array of predictions
-        :rtype: 'np.ndarray'
-        """
-        predictions = None
-        for batch in dataloader:
-            y_pred = self.model.evaluation_step(batch)
-            y_pred = y_pred.numpy()
-            if predictions is None:
-                predictions = y_pred
-            else:
-                predictions = np.append(predictions, y_pred, 0)
+    return formatted_metrics
 
-        return predictions
 
-    def evaluate(self, test_dataset, eval_metrics=None, **kwargs):
-        """
-        Evaluates the model given testing data.
-        :param test_dataset: Testing data, a tuple given in the form \
-        (x_test, y_test) or a pytorch DataLoader
-        :type test_dataset: `np.ndarray`
-        :param eval_metrics: a list of sklearn.metric class, or a function for evaluating a pytorch dataloader batch
-        :type eval_metrics: 'sklearn.metrics' or 'function'
-        :param kwargs: Dictionary of metrics available for the model
-        :type kwargs: `dict`
-        """
-
-        if type(test_dataset) is tuple:
-            x_test = test_dataset[0]
-            y_test = test_dataset[1]
+def inherited_docstring(f: Callable, mapping: dict = {}) -> Callable:
+    def dec(obj: Callable) -> Callable:
+        if not obj.__doc__:
+            possible_types = {
+                "model": "model",
+                "function": "function",
+                "space": "space",
+                "pipeline": "pipeline",
+                "experiment": "experiment",
+                "member": "space",
+            }
+
+            available_metanames = {
+                "model": "ModelMetaNames",
+                "experiment": "ExperimentMetaNames",
+                "function": "FunctionMetaNames",
+                "pipeline": "PipelineMetaNames",
+            }
+
+            actual_type = None
+
+            for t in possible_types:
+                if t in obj.__name__:
+                    actual_type = possible_types[t]
+
+            docs = cast(str, f.__doc__)
+
+            if actual_type:
+                docs = docs.replace(
+                    f"client.{actual_type}s.{f.__name__}",
+                    "client.repository." + obj.__name__,
+                )
+                docs = docs.replace(
+                    f"client._{actual_type}s.{f.__name__}",
+                    "client.repository." + obj.__name__,
+                )
 
-            return self.evaluate_model(x_test, y_test, eval_metrics)
+                if actual_type in available_metanames:
+                    repository_meta_names = available_metanames[actual_type]
+                    docs = docs.replace(
+                        f"_{actual_type}s.ConfigurationMetaNames",
+                        f"repository.{repository_meta_names}",
+                    )
+                    docs = docs.replace(
+                        f"{actual_type}s.ConfigurationMetaNames",
+                        f"repository.{repository_meta_names}",
+                    )
+                    docs = docs.replace("ConfigurationMetaNames", repository_meta_names)
 
-        else:
-            return self.evaluate_generator_model(test_dataset, eval_metrics)
+                for k in mapping:
+                    docs = docs.replace(k, mapping[k])
+            obj.__doc__ = docs
+        return obj
 
-    def evaluate_model(self, x, y, eval_metrics=None, **kwargs):
-        """
-        Evaluates the model given x and y.
-
-        :param x:  Samples with shape as expected by the model.
-        :type x: `np.ndarray`
-        :param y: Corresponding labels to x
-        :type y: `np.ndarray`
-        :param eval_metrics: A list of sklearn.metric class or a functions with the signature \
-                        (y_true, y_pred, **kwargs) => float
-        :type eval_metrics: `sklearn.metrics` or 'function'
-        :return: dictionary of metrics
-        :rtype: 'dict'
-        """
-        self.input_shape = list(x.shape)
-        self.input_shape[0] = 1
-        
-        if eval_metrics is None or len(eval_metrics) == 0:
-            eval_metrics = [self.default_metric]
-
-        # get names for eval_metrics
-        metric_names = [getattr(metric, "__name__", repr(metric)) for metric in eval_metrics]
-
-        y_pred = self.predict(x)
-        y_pred_exp = np.exp(y_pred)
-        y_pred_argmax = np.argmax(y_pred_exp, axis=1)
-
-        try:
-            metric_dict = dict(zip(metric_names, eval_metrics))
-            # NOTE: Had to replace comprehension with for loop to tackle loss fn case which uses y_pred not argmax
-            for metric, fn in metric_dict.items():
-                if metric == "loss":
-                    metric_dict[metric] = fn(y, y_pred, self.model)
-                else:
-                    metric_dict[metric] = fn(y, y_pred_argmax, **kwargs)
-            # metric_dict = {metric_name: metric(y, y_pred, **kwargs) for metric_name, metric in
-            #                zip(metric_names, eval_metrics)}
-            return metric_dict
-        except TypeError as exc:
-            logger.exception(str(exc))
-            raise TypeError(
-                "eval_metrics must be an sklearn.eval_metrics class, or a function with the signature "
-                "(y_true, y_pred, **kwargs)'"
-            )
-        except ValueError as exc:
-            logger.exception(exc)
-            raise ValueError("arguments not in the correct format for metric")
-
-    def evaluate_generator_model(self, dataloader, eval_metrics=None, **kwargs):
-        """
-        evaluates the model based on the provided dataloader
-        :param dataloader: a pytorch dataloader with test dataset and labels
-        :type dataloader: torch.utils.data.DataLoader
-        :param eval_metrics: a function for how to evaluate the batched examples and labels
-        must have the signature (x_batch, y_batch) => float
-        :type eval_metrics: 'function'
-        :return: dictionary of metrics
-        :rtype: 'dict'
-        """
-        metric_score = 0
-        for x_batch, y_batch in dataloader:
-            y_pred = self.model.evaluation_step((x_batch, y_batch))
-
-            if eval_metrics is None:
-                y_pred = np.exp(y_pred)
-                y_pred = np.argmax(y_pred, axis=1)
-                equals = y_pred == y_batch
-                metric_score += torch.mean(equals.type(torch.FloatTensor))
+    return dec
 
-            else:
-                try:
-                    metric_score += eval_metrics(x_batch, y_batch)
 
-                except TypeError as exc:
-                    logger.exception(str(exc))
-                    raise TypeError(
-                        "eval_metrics must be a function " "with the signature (x_batch, y_batch, **kwargs)'"
-                    )
+def group_metrics(metrics: list[dict]) -> list | collections.defaultdict:
+    grouped_metrics: list | collections.defaultdict = []
 
-        metric_dict = {"metric_score": metric_score / len(dataloader)}
+    if len(metrics) > 0:
+        import collections
 
-        return metric_dict
+        grouped_metrics = collections.defaultdict(list)
+        for d in metrics:
+            k = d["phase"]
+            grouped_metrics[k].append(d)
 
-    def save_model(self, filename=None, optimizer_filename=None, history_filename=None, onnx_export=True):
-        """
-        Save a model to file in the format specific to the backend framework.
-
-        :param filename: Name of the file that contains the model to be loaded.
-        :type filename: `str`
-        :param optimizer_filename: Name of the file that contains the optimizer to be loaded.
-        :type optimizer_filename: `str`
-        :param history_filename: Name of the file that contains the model history to be loaded.
-        :type history_filename: `str`
-        :return: None
-        """
-        if filename is None:
-            file = self.model_name if self.model_name else self.model_type
-            if onnx_export:
-                filename = "{}_{}.onnx".format(file, time.time())
-            else:
-                filename = "{}_{}.pt".format(file, time.time())
+    return grouped_metrics
 
-        if onnx_export:
-            x = torch.randn(self.input_shape)
-            torch.onnx.export(
-                self.model.module_,
-                x,
-                super().get_model_absolute_path(filename),
-                export_params=True,
-                keep_initializers_as_inputs=True,
-            )
+
+class StatusLogger:
+    def __init__(self, initial_state: str):
+        self.last_state = initial_state
+        print(initial_state, end="")
+
+    def log_state(self, state: str) -> None:
+        if state == self.last_state:
+            print(".", end="")
         else:
-            f_params = super().get_model_absolute_path(filename)
-            f_optimizer = None
-            f_history = None
-            if optimizer_filename is not None:
-                f_optimizer = super().get_model_absolute_path(optimizer_filename)
-            if history_filename is not None:
-                f_history = super().get_model_absolute_path(history_filename)
-            self.model.save_params(f_params=f_params, f_optimizer=f_optimizer, f_history=f_history)
+            print("\n{}".format(state), end="")
+            self.last_state = state
 
-        return filename
+    def __enter__(self) -> StatusLogger:
+        return self
 
-    def load_model(
+    def __exit__(
         self,
-        pytorch_module,
-        model_filename,
-        optimizer_filename=None,
-        history_filename=None,
-        optimizer=None,
-        module_init_params=None,
-    ):
-        """
-        Loads a model from disk given the specified file_name
-
-        :param pytorch_module: uninstantiated pytorch model class
-        :type pytorch_module: torch.nn.Module class reference
-        :param model_filename: Name of the file that contains the model to be loaded.
-        :type model_filename: `str`
-        :param optimizer_filename: Name of the file that contains the optimizer to be loaded.
-        :type optimizer_filename: `str`
-        :param history_filename: Name of the file that contains the model history to be loaded.
-        :type history_filename: `str`
-        :param optimizer: the optimizer that should be loaded
-        :type optimizer: pytorch optimizer class
-        :param module_init_params: A dictionary with the values for the model's init arguments. \
-        The key for the init parameters must be prefixed with module__ \
-        (ex. if the parameter name is hidden_size, then key must be module__hidden_size)
-        :type module_init_params: 'dict'
-        :return: None
-        """
-        f_params = model_filename
-        f_optimizer = None
-        f_history = None
-        if optimizer is not None and optimizer_filename is not None:
-            self.optimizer = optimizer
-            f_optimizer = optimizer_filename
-        if history_filename is not None:
-            f_history = history_filename
-        model = self.initialize_model(
-            pytorch_module, self.optimizer, self.criterion, module_init_params=module_init_params
-        )
-        model.load_params(f_params=f_params, f_optimizer=f_optimizer, f_history=f_history)
+        exc_type: Type[BaseException] | None,
+        exc_val: BaseException | None,
+        exc_tb: TracebackType | None,
+    ) -> None:
+        pass
+
+
+def get_file_from_cos(cos_credentials: dict) -> str:
+    import ibm_boto3
+    from ibm_botocore.client import Config
+
+    client_cos = ibm_boto3.client(
+        service_name="s3",
+        ibm_api_key_id=cos_credentials["IBM_API_KEY_ID"],
+        ibm_auth_endpoint=cos_credentials["IBM_AUTH_ENDPOINT"],
+        config=Config(signature_version="oauth"),
+        endpoint_url=cos_credentials["ENDPOINT"],
+    )
+
+    streaming_body = client_cos.get_object(
+        Bucket=cos_credentials["BUCKET"], Key=cos_credentials["FILE"]
+    )["Body"]
+    training_definition_bytes = streaming_body.read()
+    streaming_body.close()
+    filename = cos_credentials["FILE"]
+    f = open(filename, "wb")
+    f.write(training_definition_bytes)
+    f.close()
+
+    return filename
+
+
+def extract_model_from_repository(
+    model_id: str, client: APIClient, **kwargs: Any
+) -> str:
+    """Download and extract archived model from wml repository.
+
+    :param model_id: ID of model
+    :type model_id: str
+    :param client: client instance
+    :type client: APIClient
+
+    :return: extracted directory path
+    :rtype: str
+    """
+    model_id = _get_id_from_deprecated_uid(kwargs, model_id, "model")
+    create_empty_directory(model_id)
+    current_dir = os.getcwd()
+
+    os.chdir(model_id)
+    model_dir = os.getcwd()
+
+    fname = "downloaded_" + model_id + ".tar.gz"
+    client.repository.download(model_id, filename=fname)
+
+    if fname.endswith("tar.gz"):
+        tar = tarfile.open(fname)
+        tar.extractall()
+        tar.close()
+    else:
+        raise WMLClientError("Invalid type. Expected tar.gz")
+
+    os.chdir(current_dir)
+    return model_dir
+
+
+def extract_mlmodel_from_archive(
+    archive_path: str, model_id: str, **kwargs: Any
+) -> str:
+    """Extract archived model under model id directory.
+
+    :param model_id: ID of model
+    :type model_id: str
+    :param archive_path: path to archived model
+    :type archive_path: str
 
-        if self.use_gpu_for_training and torch.cuda.device_count() > 0:
-            if self.num_gpus > torch.cuda.device_count():
-                logger.error(
-                    "Selected number of gpus to use for training exceeds number of available gpus, "
-                    + str(torch.cuda.device_count())
-                    + "Set number of gpus to maximum available on device."
+    :return: extracted directory path
+    :rtype: str
+    """
+    model_id = _get_id_from_deprecated_uid(kwargs, model_id, "model")
+    create_empty_directory(model_id)
+    current_dir = os.getcwd()
+
+    os.rename(archive_path, os.path.join(model_id, archive_path))
+
+    os.chdir(model_id)
+
+    if archive_path.endswith("tar.gz"):
+        tar = tarfile.open(archive_path)
+        tar.extractall()
+        tar.close()
+    else:
+        raise WMLClientError("Invalid type. Expected tar.gz")
+
+    os.chdir(current_dir)
+    return os.path.join(model_id, "model.mlmodel")
+
+
+def get_model_filename(directory: str, model_extension: str) -> str:
+    logger = logging.getLogger(__name__)
+    model_filepath = None
+
+    for file in os.listdir(directory):
+        if file.endswith(model_extension):
+            if model_filepath is None:
+                model_filepath = os.path.join(directory, file)
+            else:
+                logger.warning(
+                    "More than one file with extension '{}'.".format(model_extension)
                 )
-                self.num_gpus = torch.cuda.device_count()
-            device_ids = list(range(self.num_gpus))
-            model.module_ = torch.nn.DataParallel(model.module_, device_ids=device_ids)
-            model.set_params(device="cuda")
-            model.module_.to("cuda")
-
-        self.model = model
-
-    def get_weights(self, to_numpy=False):
-        """
-        Returns the weights of the model
-
-        :param to_numpy; Determines whether the weights should be returned as numpy array, or tensor
-        :type to_numpy: `boolean`
-        :return: list of model weights
-        """
-        if self.use_gpu_for_training and torch.cuda.device_count() > 0:
-            module = copy.deepcopy(self.model.module_).cpu()
-        else:
-            module = self.model.module_
-        if to_numpy:
-            return self.parameters_to_numpy(module.parameters())
-        return list(module.parameters())
-
-    def parameters_to_numpy(self, params):
-        """
-        Transforms parameter tensors to numpy arrays
-
-        :param params: The parameter tensor to be transformed
-        :return: numpy array of parameters
-        """
-        np_params = []
-        for layer in params:
-            np_params.append(layer.detach().numpy())
-        return np_params
-
-    def load_model_from_spec(self, model_spec):
-        """
-        Loads model from provided model_spec, where model_spec is a `dict`
-        that contains one item: model_spec['model_definition'], which has a
-        pointer to the file where an nn.sequence container is saved
-
-        :return: model
-        :rtype: `nn.sequence`
-        """
-
-        model_file = model_spec["model_definition"]
-        model_absolute_path = config.get_absolute_path(model_file)
-        model = torch.load(model_absolute_path)
-        return model
-
-    def load_optimizer_from_spec(self, model_spec):
-        """
-        Loads optimizer class from provided model_spec, where model_spec is a `dict`
-        that contains an item: model_spec['optimizer_class'], which has a
-        pointer to the file where an optimizer class object is saved
-
-        :return: model
-        :rtype: `torch.optim`
-        """
-        if model_spec is None or (not isinstance(model_spec, dict)) or model_spec.get("optimizer") is None:
-            logger.info("No optimizer found in the config file. Using default optimizer class: {}".format(getattr(self.default_optimizer,"__name__")))
-            optimizer = self.default_optimizer
-            return optimizer
-        else:
-            optimizer = model_spec.get("optimizer")
-        if not isinstance(optimizer, str):
-            raise ModelInitializationException("Optimizer is not specified as a string")
+
+    if model_filepath is None:
+        raise WMLClientError("No file with extension '{}'.".format(model_extension))
+
+    return model_filepath
+
+
+def delete_directory(directory: str) -> None:
+    if os.path.exists(directory):
+        shutil.rmtree(directory)
+
+
+def create_empty_directory(directory: str) -> None:
+    delete_directory(directory)
+    os.makedirs(directory)
+
+
+def install_package(package: str) -> None:
+    import importlib
+
+    try:
+        importlib.import_module(package)
+    except ImportError:
+        import pip
+
+        pip.main(["install", package])
+
+
+def is_ipython() -> bool:
+    # checks if the code is run in the notebook
+    try:
+        get_ipython  # type: ignore[name-defined]
+        return True
+    except Exception:
+        return False
+
+
+def create_download_link(file_path: str, title: str = "Download file.") -> HTML | None:
+    # creates download link for binary files on notebook filesystem (Watson Studio)
+
+    if is_ipython():
+        from IPython.display import HTML
+        import base64
+
+        filename = os.path.basename(file_path)
+
+        with open(file_path, "rb") as file:
+            b_model = file.read()
+        b64 = base64.b64encode(b_model)
+        payload = b64.decode()
+        html = '<a download="{file_path}" href="data:binary;base64,{payload}" target="_blank">{title}</a>'
+        html = html.format(payload=payload, title=title, file_path=filename)
+
+        return HTML(html)
+
+    return None
+
+
+def convert_metadata_to_parameters(meta_data: dict) -> list:
+    parameters = []
+
+    if meta_data is not None:
+        for key, value in meta_data.items():
+            parameters.append({"name": str(key), "value": value})
+
+    return parameters
+
+
+def is_of_python_basic_type(el: object | list | None) -> bool:
+    if el is None:
+        return True
+    elif type(el) in [int, float, bool, str]:
+        return True
+    elif type(el) in [list, tuple]:
+        return all([is_of_python_basic_type(t) for t in cast(Iterable, el)])
+    elif type(el) is dict:
+        if not all(type(k) == str for k in el.keys()):
+            return False
+
+        return is_of_python_basic_type(list(el.values()))
+    else:
+        return False
+
+
+def next_resource_generator(
+    client: APIClient,
+    url: str,
+    href: str,
+    params: dict | None = None,
+    _all: bool = False,
+    _filter_func: Callable | None = None,
+) -> Generator[dict, None, None]:
+    """
+    Generator to produce next list of resources from REST API.
+
+    :param client: Client Instance
+    :type client: APIClient
+
+    :param url: URL to the resource
+    :type url: str
+
+    :param href: href to the resource
+    :type href: str
+
+    :param params: parameters of request
+    :type params: dict
+
+    :param _all: if `True`, it will get all entries in 'limited' chunks
+    :type _all: bool, optional
+
+    :param _filter_func: filtering function
+    :type _filter_func: function, optional
+    """
+    next_href: str | None = href
+
+    while next_href is not None:
+        if "http" not in next_href:
+            next_href = f"{url}/{next_href}"
+        response = requests.get(
+            url=next_href,
+            headers=client._get_headers(),
+            params=(params if params is not None else client._params()),
+        )
+        details_json = client.training._handle_response(
+            200, "Get next details", response
+        )
+
+        if _all:
+            next_href = details_json.get("next", {"href": None})["href"]
+
         else:
-            try:
-                optimizer = getattr(import_module("torch.optim"), optimizer.split(".")[-1])
-            except Exception as e:
-                logger.exception(str(e))
-                logger.info("Selected optimizer not found. Using default optimizer class: {}".format(getattr(self.default_optimizer,"__name__")))
-                optimizer = self.default_optimizer
-
-        return optimizer
-
-    def load_loss_criterion_from_spec(self, model_spec):
-        """
-        Loads loss criterion from provided model_spec, where model_spec is a `dict`
-        that contains an item: model_spec['loss_criterion'], which has a
-        pointer to the file where an loss criterion class object is saved
-
-        :return: model
-        :rtype: `torch.nn` loss class object
-        """
-        if model_spec is None or (not isinstance(model_spec, dict)) or model_spec.get("loss_criterion") is None:
-            logger.info("No loss criterion found in the config file. Using default loss criterion class: {}".format(getattr(self.default_criterion,"__name__")))
-            criterion = self.default_criterion
-            return criterion
+            next_href = None
+
+        if "resources" in details_json:
+            resources = details_json["resources"]
+
+        elif "metadata" in details_json:
+            resources = [details_json]
+
         else:
-            criterion = model_spec.get("loss_criterion")
-        if not isinstance(criterion, str):
-            raise ModelInitializationException("Criterion is not specified as a string")
+            resources = details_json.get("results", [])
+
+        yield {"resources": (_filter_func(resources) if _filter_func else resources)}
+
+
+class DisableWarningsLogger:
+    """Class which disables logging warnings (for example for silent handling WMLClientErrors in try except).
+
+    **Example**
+
+    .. code-block:: python
+
+        try:
+            with DisableWarningsLogger():
+                throw_wml_error()
+        except WMLClientError:
+            success = False
+
+    """
+
+    def __enter__(self) -> None:
+        logging.disable(logging.WARNING)
+
+    def __exit__(
+        self,
+        exit_type: Type[BaseException] | None,
+        exit_value: BaseException | None,
+        exit_traceback: TracebackType | None,
+    ) -> None:
+        logging.disable(logging.NOTSET)
+
+
+def is_lib_installed(
+    lib_name: str,
+    minimum_version: str | None = None,
+    install: bool = False,
+) -> bool:
+    """Check if provided library is installed on user environment. If not, tries to install it.
+
+    :param lib_name: library name to check
+    :type lib_name: str
+
+    :param minimum_version: minimum version of library to check, default: None - check if library is installed in overall
+    :type minimum_version: str, optional
+
+    :param install: indicates to install missing or to low version library
+    :type install: bool, optional
+
+    :return: information if library is installed: `True` is library is installed, `False` otherwise
+    :rtype: bool
+    """
+    if lib_name in sys.modules:
+        installed = True
+
+    elif importlib.util.find_spec(lib_name) is not None:
+        installed = True
+
+    else:
+        installed = False
+
+    if installed:
+        installed_module_version = get_module_version(lib_name)
+
+        if minimum_version is not None:
+            if version.parse(installed_module_version) < version.parse(minimum_version):
+                if install:
+                    install_library(
+                        lib_name=lib_name, version=minimum_version, strict=False
+                    )
+
+    else:
+        if install:
+            install_library(lib_name=lib_name, version=minimum_version, strict=False)
+            installed = True
+
+    return installed
+
+
+def install_library(
+    lib_name: str, version: str | None = None, strict: bool = False
+) -> None:
+    """Try to install library.
+
+    :param lib_name: library name to install
+    :type lib_name: str
+
+    :param version: version of the library to install
+    :type version: str, optional
+
+    :param strict: indicates if we want to install specific version or higher version if available
+    :type strict: bool, optional
+    """
+    try:
+        if version is not None:
+            check_call(
+                [
+                    sys.executable,
+                    "-m",
+                    "pip",
+                    "install",
+                    f"{lib_name}{'==' if strict else '>='}{version}",
+                ]
+            )
+
         else:
-            try:
-                criterion = getattr(import_module("torch.nn"), criterion.split(".")[-1])
-            except Exception as e:
-                criterion = self.criterion
-                logger.exception(str(e))
-                logger.info("selected criterion not found.  Using default loss criterion class: {}".format(getattr(self.default_criterion,"__name__")))
-                criterion = self.default_criterion
-
-        return criterion
-
-    def get_gradient(self, train_data):
-        """
-        Returns the gradients for each layer in the model
-
-        :return: gradients
-        :rtype: `list`. numpy array list of model's gradients
-        """
-
-        x = train_data[0]
-        y = train_data[1]
-        gradients = []
-        y_infer = self.model.module_(torch.from_numpy(x))
-        loss = self.model.criterion_(y_infer, torch.from_numpy(y))
-        loss.backward()
-
-        for layer in self.model.module_.parameters():
-            gradients.append(layer.grad.numpy())
-
-        return gradients
-
-    @staticmethod
-    def valid_acc(net, x, y):
-        """
-        Callback scoring function that returns the validation accuracy during training.
-        :param net: The model that will be used
-        :param x: the validation data set
-        :param y: the training targets
-        :return: the accuracy of the validation training pass
-        """
-        y_pred = net.predict(x)
-        y_pred = np.exp(y_pred)
-        y_pred = np.argmax(y_pred, axis=1)
-        return metrics.accuracy_score(y, y_pred)
-
-    @staticmethod
-    def loss(y, y_pred, net):
-        loss = net.get_loss(torch.from_numpy(y_pred), torch.from_numpy(y)).numpy()
-
-        return loss
-
-    @staticmethod
-    def __expand_linear_layer__(net, layer_name, new_dimensions, layer_idx):
-        """
-        Expands a linear layer of a pytorch module
-        :param net: an instantiated pytorch module
-        :type net: `torch.nn.Module`
-        :param layer_name: layer variable name which needs to be expanded
-        :type layer_name: `str`
-        :param new_dimensions: new dimensions
-        :type new_dimensions: `list`
-        :param layer_idx: layer idx which needs to be expanded. this corresponds with
-        the index of `new_dimensions`
-        :type layer_idx: `int`
-        :return: None
-        """
-
-        original_lin_layer = getattr(net, layer_name)
-
-        if not isinstance(original_lin_layer, torch.nn.Linear):
-            raise FLException("Received a non-linear layer to expand " "whereas the method expects a linear layer")
-
-        new_ip_dim = original_lin_layer.in_features if layer_idx == 0 else new_dimensions[layer_idx - 1]
-        new_op_dim = new_dimensions[layer_idx]
-        bias = original_lin_layer.bias is not None
-
-        new_lin_layer = type(original_lin_layer)(in_features=new_ip_dim, out_features=new_op_dim, bias=bias)
-
-        setattr(net, layer_name, new_lin_layer)
-
-    def expand_model_by_layer_name(self, new_dimension, layer_name="dense"):
-        """
-        Expands the current PyTorch models layers with the provided dimensions
-        :param new_dimension: new dimensions of the particular `layer_name`
-        :type new_dimension: `list`
-        :param layer_name: layer name which needs to be expanded
-        :type layer_name: `str`
-        :return: None
-        """
-
-        if new_dimension is None:
-            raise FLException(
-                "No information is provided for "
-                "the new expanded model. "
-                "Please provide the new dimension of "
-                "the resulting expanded model"
+            check_call([sys.executable, "-m", "pip", "install", lib_name])
+
+    except Exception as e:
+        raise CannotInstallLibrary(lib_name=lib_name, reason=str(e))
+
+
+def get_module_version(lib_name: str) -> str:
+    """Use only when you need to check package version by package name with pip."""
+    from importlib.metadata import version
+
+    return version(lib_name)
+
+
+def prepare_interaction_props_for_cos(source_params: dict, file_name: str) -> dict:
+    """If user specified properties for dataset as sheet_name, delimiter etc. we need to
+    pass them as interaction properties for Flight Service.
+
+    :param source_params: data source parameters describe data (eg. excel_sheet, encoding etc.)
+    :type source_params: dict
+
+    :param file_name: name of the file to download, should consist of file extension
+    :type file_name: str
+
+    :return: COS interaction properties for Flight Service
+    :rtype: dict
+    """
+    interaction_properties = {}
+    file_format = None
+
+    encoding = source_params.get("encoding", None)
+
+    if ".xls" in file_name or ".xlsx" in file_name:
+        file_format = "excel"
+        if source_params.get("excel_sheet"):
+            interaction_properties["sheet_name"] = str(source_params.get("excel_sheet"))
+
+    elif ".csv" in file_name:
+        if source_params.get("quote_character"):
+            interaction_properties["quote_character"] = str(
+                source_params.get("quote_character")
             )
+        if encoding is not None:
+            interaction_properties["encoding"] = encoding
 
-        layer_maps = {"dense": {"class": torch.nn.Linear, "expansion_fn": self.__expand_linear_layer__}}
+        input_file_separator = source_params.get("input_file_separator", ",")
+        if input_file_separator != ",":
+            file_format = "delimited"
+            interaction_properties["field_delimiter"] = input_file_separator
+        else:
+            file_format = "csv"
 
-        layer_cls = layer_maps[layer_name]["class"]
-        layer_exp_fn = layer_maps[layer_name]["expansion_fn"]
-        net = self.model.module_  # get instantiated module
-        idx = 0
-
-        for layer_varname, layer in net.named_modules():
-            # skip layer if not instance of layer desired to be expanded
-            if not isinstance(layer, layer_cls):
-                continue
-
-            layer_exp_fn(net, layer_varname, new_dimension, idx)
-            idx += 1
-
-        self.model.initialize_optimizer()
-
-    def is_fitted(self):
-        """
-        Return a boolean value indicating if the model is fitted or not.
-        In particular, it calls `skorch.utils.check_is_fitted` to checks
-        whether the net is initialized.
-        If it has, return True; otherwise return false.
-
-        :return: res
-        :rtype: `bool`
-        """
-        try:
-            self.model.check_is_fitted()
-        except NotInitializedError:
-            return False
-        return True
+    elif ".parquet" in file_name or ".prq" in file_name:
+        file_format = "parquet"
+
+    if file_format is not None:
+        interaction_properties["file_format"] = file_format
+
+    return interaction_properties
 
-    def get_loss(self, dataset):
-        """
-        Return the resulting loss computed based on the provided dataset.
-
-        :param train_data: Training data, a tuple \
-        given in the form (x_train, y_train). otherwise, input compatible with skorch.dataset.Dataset
-        :type train_data: `(np.ndarray, np.ndarray)`
-        :return: The resulting loss.
-        :rtype: `float`
-        """
-
-        metrics_list = [PytorchFLModel.loss]
-        orig_loss_value = self.evaluate(dataset, eval_metrics=metrics_list)["loss"]
-
-        return orig_loss_value
-    
-    def update_model_metadata(self, model_update):
-        """
-        """
-        if model_update.exist_key("input_shape"):
-            self.input_shape = model_update.get('input_shape')
-            logger.info("update_model_metadata: input_shape = {}".format(model_update.get('input_shape')))
+
+def modify_details_for_script_and_shiny(details_from_get: dict) -> dict:
+    """Add the href and id of and asset to the same position as it is returned from the POST method
+    it allows the `get_id`/`get_href` method to work with details returned by GET method.
+
+    :param details_from_get: details of script/shiny app acquired using GET method
+    :type details_from_get: dict
+
+    :return: details with 'guid' and 'href' key added to 'metadata'
+    :rtype: dict
+    """
+    try:
+        details_from_get["metadata"]["href"] = details_from_get["href"]
+        details_from_get["metadata"]["guid"] = details_from_get["metadata"]["asset_id"]
+    except KeyError:
+        pass
+
+    return details_from_get
+
+
+def is_lale_pipeline(pipeline: PipelineType) -> bool:
+    return (
+        type(pipeline).__module__ == "lale.operators"
+        and type(pipeline).__qualname__ == "TrainedPipeline"
+    )
+
+
+class NumpyTypeEncoder(json.JSONEncoder):
+    """Extended json.JSONEncoder to encode correctly numpy types."""
+
+    def default(
+        self, obj: numpy.integer | numpy.bool_ | numpy.floating | numpy.ndarray
+    ) -> int | bool | float | list | None:
+        if isinstance(obj, numpy.integer):
+            return int(obj)
+        elif isinstance(obj, numpy.bool_):
+            return bool(obj)
+        elif isinstance(obj, numpy.floating):
+            return None if numpy.isnan(obj) else float(obj)
+        elif isinstance(obj, numpy.ndarray):
+            return obj.tolist()
+        else:
+            return super().default(obj)
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/party/training/crypto_local_training_handler.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/party/training/crypto_local_training_handler.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
+
 import logging
 
 from ibmfl.crypto.crypto_exceptions import CryptoException
 from ibmfl.crypto.keys_mng.crypto_keys_proto_party import CryptoKeysProtoParty
 from ibmfl.model.model_update import ModelUpdate
 from ibmfl.party.training.local_training_handler import LocalTrainingHandler
 from ibmfl.util.config import get_class_by_name
@@ -73,27 +74,27 @@
             return True
 
         train_data, (_) = self.data_handler.get_data()
 
         model_update = fit_params.get("model_update")
 
         # Check if received fused model update in ciphertext
-        if model_update and model_update.exist_key("ct_weights"):
+        if model_update and "ct_weights" in model_update :
             logger.info("train: Received encrypted model update")
             if self.keys_proto is not None and self.keys_proto.get_keys_id() is None:
                 # Handle the possible race for a rejoining party between the register sequence
                 # and train messages. Discard the train messages until crypto keys are received.
                 logger.info("train: Keys were not yet received. Returning empty model update")
                 return ModelUpdate(empty_model=True)
             logger.info("train: Decrypting - " + str(type(self.crypto)))
             model_update = self.crypto.decrypt(model_update, num_parties=fit_params.get("num_parties"))
             logger.info("train: Decryption done")
             # Check if partial decryption occurred
             # - the case of Threshold Paillier decryption style
-            if model_update.exist_key("partial_ct_weights"):
+            if "partial_ct_weights" in model_update:
                 logger.info("train: Partially decrypted model update. Send to aggregator for final operation")
                 return model_update
 
         self.update_model(model_update)
 
         logger.info("train: Local training started...")
         self.fl_model.fit_model(train_data, fit_params)
@@ -134,17 +135,18 @@
         this method decrypts it.
 
         :param payload: Data payload received from Aggregator.
         :type payload: `dict`
         :return: Status of the sync model request, and returned plaintext model if requested.
         :rtype: `dict`
         """
-        logger.info("sync_model_impl: start [payload: " + str(payload) + "]")
+        logger.info("sync_model_impl: start")
+        logger.debug("sync_model_impl: [payload: " + str(payload) + "]")
         model_update = payload["model_update"]
-        if model_update.exist_key("ct_weights"):
+        if "ct_weights" in model_update:
             if self.keys_proto is not None and self.keys_proto.get_keys_id() is None:
                 # Handle the possible race for a rejoining party between the register sequence
                 # and sync messages.
                 logger.info("sync: Keys were not yet received. Cannot decrypt model")
                 return {"status": False, "model_return": None}
             else:
                 logger.info("sync_model_impl: decrypting model - " + str(type(self.crypto)))
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/party/training/pfnm_local_training_handler.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/party/training/pfnm_local_training_handler.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
+
 import logging
 
 import numpy as np
 
 from ibmfl.party.training.local_training_handler import LocalTrainingHandler
 
 logger = logging.getLogger(__name__)
@@ -40,20 +41,20 @@
         self.get_train_metrics_pre()
 
         logger.info("Local training started...")
 
         self.fl_model.fit_model(train_data, fit_params, local_params=self.hyperparams)
 
         update = self.fl_model.get_model_update()
-        update.add("class_counts", _class_counts)
+        update["class_counts"] = _class_counts
 
         # Different frameworks have different return dimension.
         # PyTorch returns (features_out, features_in) dimensional weights for linear layers
         # Keras returns (features_in, features_out) dimensional weights for linear layers
         # Thus, this flag indicates if a transpose of the weight matrix is required or not
 
-        update.add("transpose_weight", self.fl_model.model_type == "PyTorch")
+        update["transpose_weight"] = self.fl_model.model_type == "PyTorch"
         logger.info("Local training done, generating model update...")
 
         self.get_train_metrics_post()
 
         return update
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/party/training/xgboost_local_training_handler.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/party/training/xgboost_local_training_handler.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,22 +1,24 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
+
 from __future__ import print_function
 
 import inspect
 import logging
 import sys
 from abc import ABC, abstractmethod
 from functools import partial
 
 import numpy as np
 import scipy.stats.mstats as mstats
 from ddsketch.ddsketch import DDSketch
+
 from numcompress import compress
 from scipy import stats
 from sklearn.ensemble._hist_gradient_boosting.binning import _BinMapper
 from sklearn.ensemble._hist_gradient_boosting.common import G_H_DTYPE, X_DTYPE, Y_DTYPE
 from sklearn.metrics import check_scoring
 from sklearn.model_selection import train_test_split
 from sklearn.preprocessing import LabelEncoder, OrdinalEncoder
@@ -26,14 +28,15 @@
 from ibmfl.exceptions import LocalTrainingException
 from ibmfl.message.message_type import MessageType
 from ibmfl.model.xgb_fl_model import XGBFLModel
 from ibmfl.party.training.local_training_handler import LocalTrainingHandler
 from ibmfl.util.xgboost.hyperparams import init_parameters
 from ibmfl.util.xgboost.utils import _LOSSES, is_classifier
 
+
 logger = logging.getLogger(__name__)
 
 
 class XGBoostBaseLocalTrainingHandler(LocalTrainingHandler, ABC):
     """
     Class implementation for XGBoost Base Local Training Handler
     """
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/util/xgboost/utils.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/xgboost/utils.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
 from __future__ import print_function
 
 from sklearn._loss.loss import (
     _LOSSES,
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/crypto/crypto_library.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/model/fl_model.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,258 +1,277 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
-"""
-Module where the crypto library are defined.
-"""
 
 import abc
 import logging
-import sys
+import os
+from pathlib import Path
 
-from ibmfl.crypto.crypto_exceptions import CryptoException, KeyManagerException
-from ibmfl.model.model_update import ModelUpdate
-from ibmfl.util.config import get_class_by_name
+import ibmfl.envs as fl_envs
 
 logger = logging.getLogger(__name__)
 
-# the default precision setting.
-PRECISION = 5
 
-
-class Crypto(abc.ABC):
+class FLModel(abc.ABC):
     """
-    The base class for a crypto system implementation that will be exposed to
-    other FL modules.
+    Base class for all FLModels. This includes supervised and unsupervised ones.
     """
 
-    def __init__(self, config, **kwargs):
-        self.cipher = None
-        if not config:
-            raise CryptoException("No crypto config is provided")
-        if "precision" in config:
-            if isinstance(config["precision"], int) and (config["precision"] > 0):
-                self.precision = config["precision"]
-            else:
-                logger.warning(
-                    "Provided precision is not in the correct format. "
-                    "An accepted precision should be a positive integer. "
-                    "Setting the precision to default precision: " + str(PRECISION)
-                )
-                self.precision = PRECISION
-        else:
-            self.precision = PRECISION
+    def __init__(self, model_name, model_spec, **kwargs):
+        """
+        Initializes an `FLModel` object
 
-        self.idx = None
-        logger.info("Initializing a key manager")
-        if "key_manager" not in config:
-            raise CryptoException("no key manager provided")
-        key_mgr_config = config["key_manager"]
-        try:
-            key_mgr_cls_ref = get_class_by_name(key_mgr_config["path"], key_mgr_config["name"])
-            self.key_manager = key_mgr_cls_ref(key_mgr_config["key_mgr_info"])
-            self.idx = key_mgr_config["key_mgr_info"].get("idx", None)
-        except KeyManagerException as ex:
-            logger.exception(ex)
+        :param model_name: String describing the model e.g., keras_cnn
+        :type model_name: `str`
+        :param model_spec: Specification of the the model
+        :type model_spec: `dict`
+        :param kwargs: Dictionary of model-specific arguments.
+        :type kwargs: `dict`
+        """
+        self.model_name = model_name
+        # reserve to specify the model type by FLModel class
+        self.model_type = None
+        self.model_spec = model_spec if model_spec else {}
+        self.use_gpu_for_training = False
+        if "info" in kwargs and kwargs["info"] is not None:
+            gpu_config = kwargs["info"].get("gpu")
+            # load GPU setting
+            if gpu_config:
+                self.use_gpu_for_training = True
+                if "selection" in gpu_config and gpu_config.get("selection") == "auto":
+                    self.num_gpus, self.gpu_ids = self.get_gpu_config_auto(gpu_config)
+                else:
+                    self.num_gpus, self.gpu_ids = self.get_gpu_config_manual(gpu_config)
 
-        self.initialize(config)
+                self.num_gpus, self.gpu_ids = self.get_gpu_config_auto(gpu_config)
 
-        self.ph = kwargs.get("proto_handler")
+                os.environ["CUDA_VISIBLE_DEVICES"] = self.gpu_ids
 
-    @abc.abstractmethod
-    def initialize(self, config):
+    def get_custom_metrics_pre(self):
         """
-        Initialize a `KeyManager` object to generate keys for `Cipher` object;
-        Initialize a 'Cipher' object
+        Return pre-training metrics to log via metrics manager.
 
-        :param config: A dictionary containing the provided crypto_keys,
-            including public and private key(s), for encryption and decryption;
-         etc., to initialize a cipher of a specific cryptosystem.
-        :type config: `dict`
-        :param kwargs: Dictionary of arguments required by a crypto system \
-        implementation for encryption and decryption.
-        :return: None
+        :return: Dictionary of custom metrics to pass through straight to metrics manager.
+        :rtype: `dict`
         """
-        raise NotImplementedError
 
-    def set_keys(self, keys):
-        """
-        Set the keys in use cases where the keys are not set during initialization
-        of this object, e.g. for the keys generation and distribution protocol use cases.
+        return {}
 
-        :param keys: The provided keys.
-        :type keys: `dict`
-        :return: None
+    def get_custom_metrics_post(self):
         """
-        self.cipher.set_keys(keys)
-        return
+        Return post-training metrics to log via metrics manager.
 
-    def encrypt(self, model_update, key="weights", **kwargs):
+        :return: Dictionary of custom metrics to pass through straight to metrics manager.
+        :rtype: `dict`
         """
-        Method for the encryption operations FL party will perform
-        during training. It currently calls the `encrypt_weights` method
-        defined inside the Cipher class.
 
-        :param model_update: ModelUpdate object to be encrypted.
-        :type model_update: `ModelUpdate`
-        :param key: model
-        :type key: `str`
-        :return: The resulting ciphertext of model update.
-        :type: `ModelUpdate`
-        """
-        if not isinstance(model_update, ModelUpdate):
-            raise CryptoException("not a ModelUpdate instance to be encrypted")
-
-        model_weights = model_update.get(key)
-        ct_model_update = ModelUpdate()
-        if isinstance(model_weights, list):
-            ct_model_update.add("ct_{}".format(key), self.cipher.encrypt_weights(model_weights))
-        else:
-            ct_model_update.add("ct_{}".format(key), self.cipher.encrypt_weights([model_weights]))
-        return ct_model_update
+        return {}
+
+    def get_train_result(self):
+        raise NotImplementedError
 
     @abc.abstractmethod
-    def decrypt(self, ct_model_update, key="ct_weights", **kwargs):
+    def fit_model(self, train_data, **kwargs):
         """
-        Abstract method to perform decryption or partial decryption operations
-        FL party will perform during training.
-        Decrypts the ciphertext included inside the provided `model_update`.
-        The ciphertext is indicated by the dict_key.
-        :param ct_model_update: A `ModelUpdate` object containing the provided \
-        ciphertext to be decrypted. The ciphertext should be of \
-        type `list` of `np.ndarray`.
-        :type ct_model_update: `ModelUpdate`
-        :param key: A dictionary key value indicating what encrypted \
-        values inside the `model_update` objects the method will decrypt.
-        :type key: `str`
-        :param kwargs: Dictionary of arguments required to perform decryption \
-        or partial decryption operations.
+        Fits current model with provided training data.
+
+        :param train_data: Training data.
+        :type train_data: Data structure containing training data, \
+        varied based on model types.
+        :param kwargs: Dictionary of model-specific arguments for fitting, \
+        e.g., hyperparameters for local training, information provided \
+        by aggregator, etc.
         :type kwargs: `dict`
-        :return: The resulting decrypted plain text.
-        :rtype: `list` of `np.ndarray`
+        :return: None
         """
         raise NotImplementedError
 
-    def avg_collected_ciphertext_response(self, lst_model_updates, **kwargs):
+    @abc.abstractmethod
+    def update_model(self, model_update, **kwargs):
         """
-        Receives a list of `model update`, where a `model_update` is of type
-        `ModelUpdate`, using the encrypted values (indicating by the `key`)
-        included in each `model_update`,
-        it returns the decrypted resulting mean.
-
-        :param: lst_model_updates. List of `model_update` of type
-            `ModelUpdate` including the encrypted values (indicating by the `key`)
-            to be averaged.
-        :type lst_model_updates:  `list`
+        Updates model using provided `model_update`. Additional arguments
+        specific to the model can be added through `**kwargs`
 
-        :return: results of fused encrypted model update
-        :rtype: `ModelUpdate`
+        :param model_update: Model with update. This is specific to each model \
+        type e.g., `ModelUpdateSGD`. The specific type should be checked by \
+        the corresponding FLModel class.
+        :type model_update: `ModelUpdate`
+        :param kwargs: Dictionary of model-specific arguments.
+        :type kwargs: `dict`
+        :return: None
         """
-        fused_model_update = self.aggregate_collected_ciphertext_response(lst_model_updates)
-        if fused_model_update.exist_key("weights"):
-            avg_model_weights = fused_model_update.get("weights")
-            for i in range(len(avg_model_weights)):
-                avg_model_weights[i] = avg_model_weights[i] / len(lst_model_updates)
-            avg_model_update = ModelUpdate(weights=avg_model_weights)
-            return avg_model_update
-        else:
-            # fused model update is still in ciphertext format
-            return fused_model_update
-
-    @abc.abstractmethod
-    def aggregate_collected_ciphertext_response(self, lst_model_updates, key="ct_weights", **kwargs):
         raise NotImplementedError
 
+    @abc.abstractmethod
+    def get_model_update(self):
+        """
+        Generate a `ModelUpdate` object specific to the FLModel being trained.
+        This object will be shared with other parties.
 
-class Cipher(abc.ABC):
-    """
-    This class creates a crypto `Cipher` that encrypts and decrypts
-    the message content exchanged during FL training.
-    """
+        :return: Model Update. Object specific to model being trained.
+        :rtype: `ModelUpdate`
+        """
+        raise NotImplementedError
 
-    def __init__(self, crypto_keys, precision, **kwargs):
+    @abc.abstractmethod
+    def predict(self, x, **kwargs):
         """
-        Initializes a crypto `Cipher` for a crypto system with
-        provided crypto keys, and set the `NEGATIVE_THRESHOLD` as the
-        maximum size allowed by the system.
-
-        :param crypto_keys: Provided crypto_keys, including public and \
-        private key(s), for encryption and decryption.
-        :type crypto_keys: `dict`
-        :param precision: Provided precision for encoding and decoding, \
-        default value is set to 6.
-        :type precision: `int`
-        :param kwargs: A dictionary contains additional arguments to \
-        initialize a crypto `Cipher`.
+        Perform prediction for a batch of inputs.
+
+        :param x: Samples with shape as expected by the model.
+        :type x: Data structure as expected by the model
+        :param kwargs: Dictionary of model-specific arguments.
         :type kwargs: `dict`
+
+        :return: Predictions
+        :rtype: Data structure the same as the type defines labels \
+        in testing data.
         """
-        self.NEGATIVE_THRESHOLD = sys.maxsize
-        self.keys = crypto_keys
-        self.precision = precision
+        raise NotImplementedError
 
-    def set_keys(self, keys):
+    @abc.abstractmethod
+    def evaluate(self, test_dataset, batch_size=128, **kwargs):
         """
-        Set the keys in use cases where the keys are not set during initialization
-        of this object, e.g. for the keys generation and distribution protocol use cases.
+        Evaluates model given the test dataset.
+        Multiple evaluation metrics are returned in a dictionary
 
-        :param keys: The provided keys.
-        :type keys: `dict`
-        :return: None
+        :param test_dataset: Provided test dataset to evalute the model.
+        :type test_dataset: `tuple` of `np.ndarray` or data generator.
+        :param batch_size: batch_size: Size of batches.
+        :type batch_size: `int`
+        :param kwargs: Dictionary of model-specific arguments.
+        :type kwargs: `dict`
+        :return: Dictionary with all evaluation metrics provided by specific \
+        implementation.
+        :rtype: `dict`
         """
         raise NotImplementedError
 
-    def encrypt(self, plaintext, **kwargs):
-        raise NotImplementedError
+    @abc.abstractmethod
+    def save_model(self, filename, path=None):
+        """
+        Save a model to file in the format specific to the backend framework.
 
-    def decrypt(self, ciphertext, **kwargs):
+        :param filename: Name of the file where to store the model.
+        :type filename: `str`
+        :param path: Path of the folder where to store the model. If no path \
+        is specified, the model will be stored in \
+                     the default data location of the library `DATA_PATH`.
+        :type path: `str`
+        :return: filename
+        """
         raise NotImplementedError
 
     @abc.abstractmethod
-    def encrypt_weights(self, model_weights, **kwargs):
+    def load_model(self, filename):
         """
-        Encrypts the provided plain_text according to the crypto system setup,
-         the encoding precision, and other encryption requirements defined
-         inside the kwargs.
-
-        :param model_weights: Plain text values to be encrypted.
-        :type model_weights: `list` of `np.ndarray`
-        :param kwargs: Encryption requirements
-        :type kwargs: `dict`
-        :return: Encrypted text
-        :rtype: `list`
+        Load model from provided filename
+
+        :param filename: Name of the file where to store the model.
+        :type filename: `str`
+        :param path: Path of the folder where to store the model. If no path is specified, \
+        the model will be stored in the default data location of the library `DATA_PATH`.
+        :type path: `str`
+        :return: model
         """
         raise NotImplementedError
 
-    @abc.abstractmethod
-    def decrypt_weights(self, model_weights_ciphertext, **kwargs):
+    def is_fitted(self):
         """
-        Decrypts the provided cipher_text according to the crypto system
-        setup, the encoding precision, and other decryption requirements
-        defined inside the kwargs.
-
-        :param model_weights_ciphertext: Cipher text to be decrypted.
-        :type model_weights_ciphertext: `list`
-        :param kwargs: Decryption requirements
-        :type kwargs: `dict`
-        :return: Decrypted text
-        :rtype: `list`
+        Return a boolean value indicating if the model is fitted or not.
+
+        :return: res
+        :rtype: `bool`
         """
         raise NotImplementedError
 
-    def _has_public_key(self):
+    @staticmethod
+    def get_model_absolute_path(filename):
+        """Construct absolute path using MODEL_DIR env variable
+
+        :param filename: Name of the file
+        :type filename: `str`
+        :return: absolute_path; constructed absolute path using model_dir
+        :rtype: `str``
+
+        """
+        if "MODEL_DIR" in os.environ:
+            model_path = Path(os.environ["MODEL_DIR"])
+        else:
+            model_path = Path(fl_envs.model_directory)
+
+        model_path.mkdir(parents=True, exist_ok=True)
+        absolute_path = model_path.joinpath(filename)
+        return str(absolute_path)
+
+    def get_loss(self, dataset):
         """
-        Checks the existence of the public key(s).
+        Return the resulting loss computed based on the provided dataset.
 
-        :return: True if the public key(s) exists otherwise returns False.
-        :rtype: `boolean`
+        :param dataset: Provided dataset, a tuple given in the form \
+        (x_test, y_test) or a datagenerator
+        :type dataset: `np.ndarray`
+        :return: The resulting loss.
+        :rtype: `float`
         """
-        return "pk" in self.keys or "pp" in self.keys
+        raise NotImplementedError
 
-    def _has_private_key(self):
+    def get_gpu_config_auto(self, gpu_config):
+        """
+        Extract num of gpus and gpu ids automatically using gpu util.
+        :param gpu_config: Provided gpu config
+        :type gpu_config:  `dict`
         """
-        Checks the existence of the private key(s).
+        num_gpus = 1
+        gpu_ids = ""
 
-        :return: True if the private key(s) exists otherwise returns False.
-        :rtype: `boolean`
+        import GPUtil
+
+        try:
+            _gpu_ids = GPUtil.getFirstAvailable(order="memory")
+            num_gpus = gpu_config.get("num_gpus") if "num_gpus" in gpu_config else len(_gpu_ids)
+            gpu_ids = str(_gpu_ids[0:num_gpus]).replace("[", "").replace("]", "")
+        except RuntimeError:
+            logger.error("No gpu was detected on the node.")
+        return num_gpus, gpu_ids
+
+    def get_gpu_config_manual(self, gpu_config):
+        """
+        Extract num of gpus and gpu ids from user provided gpu configuration.
+        :param gpu_config: Provided gpu config
+        :type gpu_config:  `dict`
+        """
+        num_gpus = 1
+        gpu_ids = ""
+        if not gpu_config:
+            logger.error("No gpu configuration provided for this job.")
+
+        if "num_gpus" not in gpu_config and "gpu_id" not in gpu_config:
+            logger.error("No valid gpu configuration provided for this job.")
+
+        if "gpu_id" in gpu_config:
+            gpu_ids = gpu_config.get("gpu_ids")
+            num_gpus = len(gpu_ids.split(","))
+
+        elif "num_gpus" in gpu_config:
+            if not isinstance(gpu_config.get("num_gpus"), int):
+                logger.error(
+                    "Provided number of gpu should be of type int. "
+                    "Instead it is of type "
+                    + str(type(gpu_config.get("num_gpus")))
+                    + "Set number of gpu to default value 1."
+                )
+                self.num_gpus = 1
+            else:
+                self.num_gpus = gpu_config.get("num_gpus")
+
+        return num_gpus, gpu_ids
+
+    def update_model_metadata(self, model_update):
+        """
+        Update to model metadata
+        :param model_update: Dictionary of model-specific arguments.
+        :type model_update: `ModelUpdate`
         """
-        return "sk" in self.keys
+        pass
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/crypto/infra/crypto_sym_imp_fernet.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepository/meta_props.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,26 +1,38 @@
 #  -----------------------------------------------------------------------------------------
 #  (C) Copyright IBM Corp. 2023-2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
-from cryptography.fernet import Fernet
 
-from ibmfl.crypto.infra.crypto_sym_int import CryptoSym
-
-
-class CryptoSymFernet(CryptoSym):
+class MetaProps:
     """
-    This class implements the interface for symmetric encryption functions using Fernet.
+    Holder for props used during creation of ML artifacts.
+
+    :param map meta: Map of pair key and value where key is taken from MetaNames.
     """
+    def __init__(self, meta):
+        self.meta = meta
+
+    def available_props(self):
+        """Return list of strings with names of available props."""
+        return self.meta.keys()
+
+    def prop(self, name):
+        """Get prop value by name."""
+        return self.meta.get(name)
+
+    def merge(self, other):
+        """Merge other MetaProp object to first one. Modify first MetaProp object, doesn't return anything."""
+        self.meta.update(other.meta)
+
+    def add(self, name, value):
+        """
+        Add new prop.
+
+        :param str name: Key for value. Should be one of the values from MetaNames.
+        :param object value: Any type of object
+        """
+        self.meta[name] = value
 
-    def __init__(self, key: bytes = None, **kwargs):
-        super(CryptoSymFernet, self).__init__(key)
-        if key is None:
-            self.generate_key()
-        else:
-            self.key = key
-            self.cipher = Fernet(self.key)
-
-    def generate_key(self):
-        self.key = Fernet.generate_key()
-        self.cipher = Fernet(self.key)
-        return
+    def get(self):
+        """returns meta prop dict"""
+        return self.meta
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/connection/connection.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/connection/connection.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,123 +1,130 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
 import abc
 from enum import Enum
 
 
 class FLConnection(abc.ABC):
-
     SENDER_STATUS = None
     RECEIVER_STATUS = None
 
     """
     Initialze sender and receiver
 
     :param kwargs: Dictionary of arguments required by each type of
         sender implementation
     """
+
     @abc.abstractmethod
     def initialize(self, **kwargs):
         pass
 
     """
     Validate config dictionary and initialize receiver
 
     :param kwargs: Dictionary of arguments required by each type of
         receiver implementation
     """
+
     @abc.abstractmethod
     def initialize_receiver(self, **kwargs):
         pass
 
     """
     Validate config dictionary and initialize sender
 
     :param kwargs: Dictionary of arguments required by each type of
         sender implementation
     """
+
     @abc.abstractmethod
     def initialize_sender(self, **kwargs):
         pass
 
     """
     Start connection
     """
+
     @abc.abstractmethod
     def start(self, **kwargs):
         pass
 
     """Stop and cleanup connection object
     """
+
     @abc.abstractmethod
     def stop(self, **kwargs):
         pass
 
     """Provide a connection information such that a node can communicate 
         to other nodes on how to communicate with it.
     """
+
     @abc.abstractmethod
     def get_connection_config(self):
         pass
 
 
 class FLReceiver(abc.ABC):
-
     @abc.abstractmethod
     def initialize(self, **kwargs):
         pass
 
     """
     Start the server as per configuration after a clean intialize_server
     call
     :param kwargs: Dictionary of arguments required by each type of
         server implementation
     """
+
     @abc.abstractmethod
     def start(self, **kwargs):
         pass
 
     """
     Cleanup all the resources and stop serving
     :param kwargs: Dictionary of arguments required by each type of
         server implementation
     """
+
     @abc.abstractmethod
     def stop(self, **kwargs):
         pass
 
 
 class FLSender(abc.ABC):
-
     @abc.abstractmethod
     def initialize(self, **kwargs):
         pass
 
     """
     Requests the end point as per the message forwarded to this module.
     Should have a logic to incorporate additional authentication details as
     per the configuration.
 
     :param kwargs: Dictionary of arguments required by each type of
         connection implementation
     """
+
     @abc.abstractmethod
     def send_message(self, **kwargs):
         pass
 
     """
     Cleanup the Sender object and close the hanging connections if any
     """
+
     @abc.abstractmethod
     def cleanup(self):
         pass
 
 
 class ConnectionStatus(Enum):
-    """Enum Class to describe the current status of the connection
-    """
+    """Enum Class to describe the current status of the connection"""
+
     INITIALIZED = 1
     STARTED = 2
     STOPPED = 3
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/connection/router_handler.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/connection/router_handler.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,21 +1,22 @@
+#  -----------------------------------------------------------------------------------------
+#  (C) Copyright IBM Corp. 2024.
+#  https://opensource.org/licenses/BSD-3-Clause
+#  -----------------------------------------------------------------------------------------
+
 """
 Module acting as a bridge between Server and ProtocolHandler
 Routes for all the requests received by the server are redirected
 to designated handler in PH using routers
 """
-#  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
-#  https://opensource.org/licenses/BSD-3-Clause
-#  -----------------------------------------------------------------------------------------
-
 import logging
+
 from parse import parse
-from ibmfl.exceptions import DuplicateRouteException
 
+from ibmfl.exceptions import DuplicateRouteException
 
 logger = logging.getLogger(__name__)
 
 
 class Router(object):
     """
     Container used to add and match a group of routes.
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/data/env_data_handler.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/data/env_data_handler.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,15 +1,15 @@
-"""
-Module to where data handler are implemented.
-"""
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
+"""
+Module to where data handler are implemented.
+"""
 import abc
 
 from ibmfl.data.data_handler import DataHandler
 from ibmfl.data.env_spec import EnvHandler
 
 
 class EnvDataHandler(DataHandler):
@@ -23,10 +23,10 @@
         Read train data and test data for reinforcement learning
         :return:
         """
 
     @abc.abstractmethod
     def get_env_class_ref(self) -> EnvHandler:
         """
-           Get environment reference for RL trainer, the instance is created in
-           model class as part of trainer initialization
+        Get environment reference for RL trainer, the instance is created in
+        model class as part of trainer initialization
         """
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/data/env_spec.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/data/env_spec.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,15 +1,15 @@
-"""
-EnvHandler for OpenAI gym interface
-"""
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
+"""
+EnvHandler for OpenAI gym interface
+"""
 import abc
 
 import gym
 
 
 class EnvHandler(gym.Env):
     """
@@ -42,19 +42,19 @@
             (helpful for debugging, and sometimes learning)
         """
         raise NotImplementedError
 
     @abc.abstractmethod
     def reset(self):
         """
-        Resets the state of the environment and returns an initial observation.
-       Returns:
-           observation (object): the initial observation.
+         Resets the state of the environment and returns an initial observation.
+        Returns:
+            observation (object): the initial observation.
         """
         raise NotImplementedError
 
     @abc.abstractmethod
-    def render(self, mode='human'):
+    def render(self, mode="human"):
         """
         Render one frame of the environment
         """
         raise NotImplementedError
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/data/pandas_data_handler.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/data/pandas_data_handler.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
 import abc
 import logging
 
 from ibmfl.data.data_handler import DataHandler
@@ -58,11 +58,11 @@
         of each feature across all samples.
         :rtype: `pandas.Series` where each entry matches the original type \
         of the corresponding feature.
         """
         train_data, (_) = self.get_data()
 
         if not dp_flag:
-            logger.info('Calculating minimum values.')
+            logger.info("Calculating minimum values.")
             min_vec = train_data.min(axis=0)
             # TODO dp minimum calculation
         return min_vec
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/envs.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/envs.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
 import os
 import pathlib
 from environs import Env
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/message/message.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/message/message.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,27 +1,34 @@
+#  -----------------------------------------------------------------------------------------
+#  (C) Copyright IBM Corp. 2024.
+#  https://opensource.org/licenses/BSD-3-Clause
+#  -----------------------------------------------------------------------------------------
+
 """
 The Message class is the essential information passing object,
 it contains information about the data sent from a node,
 the id_request, and the rank associated with the node in the network
 """
+from threading import Lock
 
-__author__ = "Supriyo Chakraborty, Shalisha Witherspoon, Dean Steuer, all ARC team"
+from ibmfl.message.message_type import MessageType
+from ibmfl.util.serialization.serializer import Serializer
+from ibmfl.util.serialization.pack import pack, unpack
 
+__author__ = "Supriyo Chakraborty, Shalisha Witherspoon, Dean Steuer, all ARC team"
 
-#  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
-#  https://opensource.org/licenses/BSD-3-Clause
-#  -----------------------------------------------------------------------------------------
 
-class Message(object):
+class Message(Serializer):
     """
     Class to create message for communication between party and aggregator
     """
+
     # Counter to keep track of the request
     request_id = 0
+    request_id_lock = Lock()
 
     def __init__(self, message_type=None, id_request=None, data=None, sender_info=None):
         """
         Initializes an `Message` object
 
         :param message_type: type of message
         :type message_type: `int`
@@ -45,39 +52,33 @@
         """
         Get header information for the message
 
         :param: None
         :return: information about rank, id_request, and message_type
         :rtype: `dict`
         """
-        return {
-            'id_request': self.id_request,
-            'message_type': self.message_type,
-            'sender_info': self.sender_info
-        }
+        return {"id_request": self.id_request, "message_type": self.message_type, "sender_info": self.sender_info}
 
     def set_data(self, data):
         """set data into the message
-
         :param data:
         :type data: `dict`
-
         """
         # replace this with methods from the data class
         self.data = data
 
     def set_header(self, header):
         """update message information using contents in header
 
         :param header: dictionary with message information
         :type header: `dict`
         """
-        self.id_request = header['id_request']
-        self.message_type = header['message_type']
-        self.sender_info = header['sender_info']
+        self.id_request = header["id_request"]
+        self.message_type = header["message_type"]
+        self.sender_info = header["sender_info"]
 
     def get_data(self):
         """
         Get actual data from the message
 
         :param: None
         :return: data
@@ -101,29 +102,56 @@
         :return: info. Sender information
         :rtype: `dict`
         """
         return self.sender_info
 
     @staticmethod
     def get_request_id():
-        Message.request_id = Message.request_id + 1
+        with Message.request_id_lock:
+            Message.request_id = Message.request_id + 1
         return Message.request_id
 
     def __getstate__(self):
-        print('called')
+        print("called")
         msg_dict = self.__dict__.copy()
 
         return msg_dict
 
     def __setstate__(self, dict):
         self.__dict__.update(dict)
 
+    @staticmethod
+    def get_msg_sync_mode(message_type):
+        if message_type == MessageType.TRAIN.value or message_type == MessageType.STOP.value:
+            return "async"
+        else:
+            return "sync"
+
+    def serialize(self):
+        msg_header = self.get_header()
+        data = self.get_data()
+
+        return pack(
+            {
+                "header": msg_header,
+                "data": data,
+            }
+        )
+        
+    @staticmethod
+    def deserialize(serialization):
+        data_dict = unpack(serialization)
+
+        msg = Message(data=data_dict["data"]) 
+        msg.set_header(data_dict["header"])
+        return msg
+        
 
-class ResponseMessage(Message):
 
+
+class ResponseMessage(Message):
     def __init__(self, req_msg=None, message_type=None, id_request=None, data=None):
         if req_msg and isinstance(req_msg, Message):
-            super().__init__(message_type=req_msg.message_type,
-                             id_request=req_msg.id_request)
+            super().__init__(message_type=req_msg.message_type, id_request=req_msg.id_request)
         else:
             super().__init__(message_type, id_request, data)
         return
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/model/fl_model.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/model/naive_bayes_fl_model.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,270 +1,202 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
-import abc
-import os
-from pathlib import Path
 import logging
 
-import ibmfl.envs as fl_envs
+import numpy as np
+from diffprivlib.models import GaussianNB
+
+from ibmfl.exceptions import ModelException, ModelInitializationException
+from ibmfl.model.model_update import ModelUpdate
+from ibmfl.model.sklearn_fl_model import SklearnFLModel
+from ibmfl.util.persistence.sklearn import SKLearnPersistence
 
 logger = logging.getLogger(__name__)
 
 
-class FLModel(abc.ABC):
+class NaiveBayesFLModel(SklearnFLModel):
     """
-    Base class for all FLModels. This includes supervised and unsupervised ones.
+    Wrapper class for diffprivlib.models.GaussianNB classifier, which itself
+    is an implementation of sklearn.naive_bayes.GaussianNB with differential
+    privacy.
     """
 
-    def __init__(self, model_name, model_spec, **kwargs):
+    def __init__(self, model_name, model_spec, model=None, **kwargs):
         """
-        Initializes an `FLModel` object
+        Create a `NaiveBayesFLModel` instance from a diffprivlib.models.GaussianNB
+        model. If model is provided, it will use it; otherwise it will take
+        the model_spec to create the model.
 
-        :param model_name: String describing the model e.g., keras_cnn
+        :param model_name: A name specifying the type of model, e.g., \
+        naive_bayes
         :type model_name: `str`
-        :param model_spec: Specification of the the model
+        :param model_spec: A dictionary contains model specification
         :type model_spec: `dict`
-        :param kwargs: Dictionary of model-specific arguments.
+        :param model: Compiled GaussianNB model
+        :type model: `diffprivlib.models.GaussianNB`
+        :param kwargs: A dictionary contains other parameter settings on \
+         to initialize a Naive Bayes model.
         :type kwargs: `dict`
         """
-        self.model_name = model_name
-        # reserve to specify the model type by FLModel class
-        self.model_type = None
-        self.model_spec = model_spec if model_spec else {}
-        self.use_gpu_for_training = False
-        if 'info' in kwargs and kwargs['info'] is not None:
-            gpu_config = kwargs['info'].get('gpu')
-            # load GPU setting
-            if gpu_config:
-                self.use_gpu_for_training = True
-                if 'selection' in gpu_config and gpu_config.get('selection') == 'auto':
-                    self.num_gpus, self.gpu_ids = self.get_gpu_config_auto(gpu_config)
-                else:
-                    self.num_gpus, self.gpu_ids = self.get_gpu_config_manual(gpu_config)
-
-                os.environ['CUDA_VISIBLE_DEVICES'] = self.gpu_ids
-
-    def get_custom_metrics_pre(self):
-        """
-        Return pre-training metrics to log via metrics manager.
-
-        :return: Dictionary of custom metrics to pass through straight to metrics manager.
-        :rtype: `dict`
-        """
-
-        return {}
-
-    def get_custom_metrics_post(self):
-        """
-        Return post-training metrics to log via metrics manager.
+        if model:
+            if not issubclass(type(model), GaussianNB):
+                raise ValueError(
+                    "Compiled GaussianNB needs to be provided"
+                    "(diffprivlib.models.GaussianNB). "
+                    "Type provided" + str(type(model))
+                )
+
+        super().__init__(model_name, model_spec, sklearn_model=model, **kwargs)
+
+        self.old_vals = {"theta": None, "var": None, "class_count": None}
+        self.model_trained = False
+        self.model_type = "Sklearn-NaiveBayes"
 
-        :return: Dictionary of custom metrics to pass through straight to metrics manager.
-        :rtype: `dict`
-        """
-
-        return {}
-
-    def get_train_result(self):
-        raise NotImplementedError
-
-    @abc.abstractmethod
-    def fit_model(self, train_data, **kwargs):
+    def fit_model(self, train_data, fit_params=None, **kwargs):
         """
         Fits current model with provided training data.
 
-        :param train_data: Training data.
-        :type train_data: Data structure containing training data, \
-        varied based on model types.
-        :param kwargs: Dictionary of model-specific arguments for fitting, \
-        e.g., hyperparameters for local training, information provided \
-        by aggregator, etc.
-        :type kwargs: `dict`
+        :param train_data: Training data a tuple given in the form \
+        (x_train, y_train).
+        :type train_data: `np.ndarray`
+        :param fit_params: Not used, present here for API consistency by \
+        convention.
         :return: None
         """
-        raise NotImplementedError
+        self.model.fit(train_data[0], train_data[1])
+        self.model_trained = True
 
-    @abc.abstractmethod
     def update_model(self, model_update, **kwargs):
         """
-        Updates model using provided `model_update`. Additional arguments
-        specific to the model can be added through `**kwargs`
+        Update GaussianNB model with provided model_update.
 
-        :param model_update: Model with update. This is specific to each model \
-        type e.g., `ModelUpdateSGD`. The specific type should be checked by \
-        the corresponding FLModel class.
+        :param model_update: `ModelUpdate` object that contains \
+        the required information to update the GaussianNB model.
         :type model_update: `ModelUpdate`
-        :param kwargs: Dictionary of model-specific arguments.
-        :type kwargs: `dict`
         :return: None
         """
-        raise NotImplementedError
+        if model_update.get("class_count") is None:
+            return
+
+        self.model.class_count_ = model_update.get("class_count")
+        self.model.theta_ = model_update.get("theta")
+
+        try:
+            self.model.sigma_ = model_update.get("var")
+        except AttributeError:
+            self.model.var_ = model_update.get("var")
+
+        self._store_old_values(model_update.get("theta"), model_update.get("var"), model_update.get("class_count"))
 
-    @abc.abstractmethod
     def get_model_update(self):
         """
-        Generate a `ModelUpdate` object specific to the FLModel being trained.
-        This object will be shared with other parties.
+        Generates a `ModelUpdate` object that will be sent to other entities.
 
-        :return: Model Update. Object specific to model being trained.
+        :return: ModelUpdate
         :rtype: `ModelUpdate`
         """
-        raise NotImplementedError
+        if (
+            not self.model_trained
+            or not hasattr(self.model, "class_count_")
+            or np.all(self.old_vals["class_count"] == self.model.class_count_)
+        ):
+            return ModelUpdate(theta=None, var=None, class_count=None)
 
-    @abc.abstractmethod
-    def predict(self, x, **kwargs):
-        """
-        Perform prediction for a batch of inputs.
+        try:
+            model_var = self.model.var_
+        except AttributeError:
+            model_var = self.model.sigma_
 
-        :param x: Samples with shape as expected by the model.
-        :type x: Data structure as expected by the model
-        :param kwargs: Dictionary of model-specific arguments.
-        :type kwargs: `dict`
+        if self.old_vals["class_count"] is None:
+            return ModelUpdate(theta=self.model.theta_, var=model_var, class_count=self.model.class_count_)
 
-        :return: Predictions
-        :rtype: Data structure the same as the type defines labels \
-        in testing data.
-        """
-        raise NotImplementedError
-
-    @abc.abstractmethod
-    def evaluate(self, test_dataset, batch_size=128, **kwargs):
-        """
-        Evaluates model given the test dataset.
-        Multiple evaluation metrics are returned in a dictionary
-
-        :param test_dataset: Provided test dataset to evalute the model.
-        :type test_dataset: `tuple` of `np.ndarray` or data generator.
-        :param batch_size: batch_size: Size of batches.
-        :type batch_size: `int`
-        :param kwargs: Dictionary of model-specific arguments.
-        :type kwargs: `dict`
-        :return: Dictionary with all evaluation metrics provided by specific \
-        implementation.
-        :rtype: `dict`
-        """
-        raise NotImplementedError
+        class_count_update = self.model.class_count_ - self.old_vals["class_count"]
 
-    @abc.abstractmethod
-    def save_model(self, filename, path=None):
-        """
-        Save a model to file in the format specific to the backend framework.
+        nm = self.model.class_count_[:, np.newaxis]
+        n = self.old_vals["class_count"][:, np.newaxis]
+        m = class_count_update[:, np.newaxis]
 
-        :param filename: Name of the file where to store the model.
-        :type filename: `str`
-        :param path: Path of the folder where to store the model. If no path \
-        is specified, the model will be stored in \
-                     the default data location of the library `DATA_PATH`.
-        :type path: `str`
-        :return: filename
-        """
-        raise NotImplementedError
+        theta_update = self.model.theta_ * nm - self.old_vals["theta"] * n
+        theta_update = theta_update / m
 
-    @abc.abstractmethod
-    def load_model(self, filename):
-        """
-        Load model from provided filename
+        var_update = (
+            nm * model_var - n * self.old_vals["var"] - (n * m) / nm * (self.old_vals["theta"] - theta_update) ** 2
+        )
 
-        :param filename: Name of the file where to store the model.
-        :type filename: `str`
-        :param path: Path of the folder where to store the model. If no path is specified, \
-        the model will be stored in the default data location of the library `DATA_PATH`.
-        :type path: `str`
-        :return: model
-        """
-        raise NotImplementedError
+        return ModelUpdate(theta=theta_update, var=var_update, class_count=class_count_update)
 
-    def is_fitted(self):
+    def evaluate(self, test_dataset, **kwargs):
         """
-        Return a boolean value indicating if the model is fitted or not.
+        Evaluates the model given testing data.
+        :param test_dataset: Testing data, a tuple given in the form \
+        (x_test, test) or a datagenerator of of type `keras.utils.Sequence`, 
+        `keras.preprocessing.image.ImageDataGenerator`
+        :type test_dataset: `np.ndarray`
 
-        :return: res
-        :rtype: `bool`
+        :param kwargs: Dictionary of metrics available for the model
+        :type kwargs: `dict`
         """
-        raise NotImplementedError
 
-    @staticmethod
-    def get_model_absolute_path(filename):
-        """Construct absolute path using MODEL_DIR env variable
+        if type(test_dataset) is tuple:
+            x_test = test_dataset[0]
+            y_test = test_dataset[1]
 
-        :param filename: Name of the file
-        :type filename: `str`
-        :return: absolute_path; constructed absolute path using model_dir
-        :rtype: `str``
+            return self.evaluate_model(x_test, y_test)
 
-        """
-        if "MODEL_DIR" in os.environ:
-            model_path = Path(os.environ["MODEL_DIR"])
         else:
-            model_path = Path(fl_envs.model_directory)
-
-        model_path.mkdir(parents=True, exist_ok=True)
-        absolute_path = model_path.joinpath(filename)
-        return str(absolute_path)
+            raise ModelException("Invalid test dataset!")
 
-    def get_loss(self, dataset):
+    def evaluate_model(self, x, y, **kwargs):
         """
-        Return the resulting loss computed based on the provided dataset.
+        Evaluates the model given test data x and the corresponding labels y.
 
-        :param dataset: Provided dataset, a tuple given in the form \
-        (x_test, y_test) or a datagenerator
-        :type dataset: `np.ndarray`
-        :return: The resulting loss.
-        :rtype: `float`
+        :param x: Samples with shape as expected by the model.
+        :type x: `np.ndarray`
+        :param y: Corresponding true labels to x
+        :type y: `np.ndarray`
+        :param kwargs: Dictionary of model-specific arguments for evaluating \
+        models. For example, sample weights accepted by model.score.
+        :return: Dictionary with all evaluation metrics provided by \
+        specific implementation.
+        :rtype: `dict`
         """
-        raise NotImplementedError
-
+        acc = {"score": self.model.score(x, y, **kwargs)}
+        return acc
 
-    def get_gpu_config_auto(self, gpu_config):
+    def _store_old_values(self, theta, var, class_count):
         """
-        Extract num of gpus and gpu ids automatically using gpu util.
-        :param gpu_config: Provided gpu config
-        :type gpu_config:  `dict`
+        Store old values of training parameters to allow reconstruction of
+        parameters for newly-trained examples when model update is sought.
         """
-        num_gpus = 0
-        gpu_ids = ''
+        self.old_vals = {"theta": theta, "var": var, "class_count": class_count}
 
-        import GPUtil
-        import math
+    @staticmethod
+    def load_model_from_spec(model_spec):
+        """
+        Loads model from provided model_spec, where model_spec is a `dict`
+        that contains the following items: model_spec['model_definition']
+        contains the model definition as type diffprivlib.GaussianNB.
+
+        :param model_spec: Model specification contains \
+        a complied sklearn model.
+        :param model_spec: `dict`
+        :return: model
+        :rtype: `sklearn.cluster`
+        """
+        model = None
         try:
-           _gpu_ids = GPUtil.getAvailable(order='memory', limit=math.inf)
-           num_gpus = gpu_config.get('num_gpus') if 'num_gpus' in gpu_config else len(_gpu_ids)
-           gpu_ids = str(_gpu_ids[0:num_gpus]).replace('[', '').replace(']', '')
-        except RuntimeError:
-           logger.error("No gpu was detected on the node.")
-        return num_gpus, gpu_ids
-
-
-    def get_gpu_config_manual(self, gpu_config):
-        """
-        Extract num of gpus and gpu ids from user provided gpu configuration.
-        :param gpu_config: Provided gpu config
-        :type gpu_config:  `dict`
-        """
-        num_gpus = 0
-        gpu_ids = ''
-        if not gpu_config:
-            logger.error("No gpu configuration provided for this job.")
-
-        if 'num_gpus' not in gpu_config and 'gpu_ids' not in gpu_config:
-            logger.error("No valid gpu configuration provided for this job.")
-
-        if 'gpu_ids' in gpu_config:
-            gpu_ids = gpu_config.get('gpu_ids')
-            num_gpus = len(gpu_ids.split(','))
-        
-        if 'num_gpus' in gpu_config:
-            if not isinstance(gpu_config.get('num_gpus'), int):
-                logger.error(
-                    'Provided number of gpu should be of type int. '
-                    'Instead it is of type ' +
-                    str(type(gpu_config.get('num_gpus'))) +
-                    'Set number of gpu to default value 1.')
-                num_gpus = 1
-            else:
-                num_gpus = gpu_config.get('num_gpus')
-            gpu_ids = ','.join(gpu_ids.split(',')[0:num_gpus])
-        
-
-        return num_gpus, gpu_ids
+            if "model_definition" in model_spec:
+                path = model_spec["model_definition"]
+                model = SKLearnPersistence.load_model(path)
+                
+                if not issubclass(type(model), GaussianNB):
+                    raise ValueError(
+                        "Provided complied model in model_spec "
+                        "should be of type GaussianNB."
+                        "Instead they are:" + str(type(model))
+                    )
+        except Exception as ex:
+            raise ModelInitializationException("Model specification was " "badly formed. ", str(ex))
+        return model
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/model/model_update.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_upload_content_output_entity.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,76 +1,107 @@
+# coding: utf-8
+
 #  -----------------------------------------------------------------------------------------
 #  (C) Copyright IBM Corp. 2023-2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
-import abc
-import numpy as np
-import logging
-from pickle import loads, dumps
-from ibmfl.exceptions import ModelUpdateException
-
-logger = logging.getLogger(__name__)
+from pprint import pformat
+from six import iteritems
 
 
-class ModelUpdate():
+class MlAssetsUploadContentOutputEntity(object):
     """
-    Class to hold model update dictionary. Dictionary can only be accessed
-    using inbuild methods like `get` and `add`
+    NOTE: This class is auto generated by the swagger code generator program.
+    Do not edit the class manually.
     """
+    def __init__(self):
+        """
+        MlAssetsUploadContentOutputEntity - a model defined in Swagger
 
-    def __init__(self, **kwargs):
+        :param dict swaggerTypes: The key is attribute name
+                                  and the value is attribute type.
+        :param dict attributeMap: The key is attribute name
+                                  and the value is json key in definition.
         """
-        Initialize the dictionary and add updates.
-        :param kwargs: Dictionary of model-update specific arguments.
-        :type kwargs: `dict`
+        self.swagger_types = {
+            'status_message': 'str'
+        }
+
+        self.attribute_map = {
+            'status_message': 'status_message'
+        }
+
+        self._status_message = None
+
+    @property
+    def status_message(self):
         """
-        self.__updates = {}
-        for key, value in kwargs.items():
-            self.add(key, value)
+        Gets the status_message of this MlAssetsUploadContentOutputEntity.
+
 
-    def add(self, key, value):
+        :return: The status_message of this MlAssetsUploadContentOutputEntity.
+        :rtype: str
         """
-        Add update `value` for `key`
+        return self._status_message
 
-        :param key: Identifier which represents the update
-        :type key: `str`
-        :param value: Content of the update
-        :type value: any ds/object which can be pickled
+    @status_message.setter
+    def status_message(self, status_message):
         """
-        try:
-            self.__updates[key] = dumps(value)
-        except Exception as ex:
-            logger.exception(ex)
+        Sets the status_message of this MlAssetsUploadContentOutputEntity.
 
-            logger.exception("Error occured while adding a update.\
-                                Make sure model update data structure is picklable")
 
-            raise ModelUpdateException("Error updating model update")
+        :param status_message: The status_message of this MlAssetsUploadContentOutputEntity.
+        :type: str
+        """
+        self._status_message = status_message
 
-    def get(self, key):
+    def to_dict(self):
+        """
+        Returns the model properties as a dict
         """
-        Get update value for given key from model update dictionary
+        result = {}
 
-        :param key: Identifier which represents the update
-        :type key: `str`
-        :return: content of the update
-        :rtype: any ds/object after its unpickled
+        for attr, _ in iteritems(self.swagger_types):
+            value = getattr(self, attr)
+            if isinstance(value, list):
+                result[attr] = list(map(
+                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
+                    value
+                ))
+            elif hasattr(value, "to_dict"):
+                result[attr] = value.to_dict()
+            elif isinstance(value, dict):
+                result[attr] = dict(map(
+                    lambda item: (item[0], item[1].to_dict())
+                    if hasattr(item[1], "to_dict") else item,
+                    value.items()
+                ))
+            else:
+                result[attr] = value
+
+        return result
+
+    def to_str(self):
         """
-        if key not in self.__updates:
-            logger.error("Key "+key+" not found in model updates")
-            raise ModelUpdateException(
-                "Invalid key was requested from model update")
+        Returns the string representation of the model
+        """
+        return pformat(self.to_dict())
 
-        ret_val = loads(self.__updates[key])
-        return ret_val
+    def __repr__(self):
+        """
+        For `print` and `pprint`
+        """
+        return self.to_str()
 
-    def exist_key(self, key):
+    def __eq__(self, other):
+        """
+        Returns true if both objects are equal
         """
-        Check if a specified key is used in model update dictionary
+        return self.__dict__ == other.__dict__
 
-        :param key: Identifier which represents the update
-        :type key: `str`
-        :return: check result of existing the key
-        :rtype: True or False
+    def __ne__(self, other):
         """
-        return True if key in self.__updates else False
+        Returns true if both objects are not equal
+        """
+        return not self == other
+
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/model/naive_bayes_fl_model.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/model/sklearn_fl_model.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,199 +1,196 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
-import joblib
-import numpy as np
 import logging
+import time
 
-from diffprivlib.models import GaussianNB
+from sklearn.exceptions import NotFittedError
+from sklearn.utils.validation import check_is_fitted
 
-from ibmfl.exceptions import ModelInitializationException, ModelException
-from ibmfl.model.model_update import ModelUpdate
-from ibmfl.model.sklearn_fl_model import SklearnFLModel
+from ibmfl.exceptions import ModelInitializationException
+from ibmfl.model.fl_model import FLModel
+from ibmfl.util import config
+from ibmfl.util.persistence.sklearn import SKLearnPersistence
 
 logger = logging.getLogger(__name__)
 
 
-class NaiveBayesFLModel(SklearnFLModel):
+class SklearnFLModel(FLModel):
     """
-    Wrapper class for diffprivlib.models.GaussianNB classifier, which itself
-    is an implementation of sklearn.naive_bayes.GaussianNB with differential
-    privacy.
+    Wrapper class for sklearn models.
     """
 
-    def __init__(self, model_name, model_spec, model=None, **kwargs):
+    def __init__(self, model_name, model_spec, sklearn_model=None, **kwargs):
         """
-        Create a `NaiveBayesFLModel` instance from a diffprivlib.models.GaussianNB
-        model. If model is provided, it will use it; otherwise it will take
+        Create a `SklearnFLModel` instance from a sklearn model.
+        If sklearn_model is provided, it will use it; otherwise it will take
         the model_spec to create the model.
 
         :param model_name: A name specifying the type of model, e.g., \
-        naive_bayes
+                linear_SVM
         :type model_name: `str`
         :param model_spec: A dictionary contains model specification
         :type model_spec: `dict`
-        :param model: Compiled GaussianNB model
-        :type model: `diffprivlib.models.GaussianNB`
+        :param sklearn_model: A compiled sklearn model
+        :type sklearn_model: `sklearn`
         :param kwargs: A dictionary contains other parameter settings on \
-         to initialize a Naive Bayes model.
+         to initialize a sklearn model.
         :type kwargs: `dict`
         """
-        if model:
-            if not issubclass(type(model), GaussianNB):
-                raise ValueError('Compiled GaussianNB needs to be provided'
-                                 '(diffprivlib.models.GaussianNB). '
-                                 'Type provided' + str(type(model)))
-
-        super().__init__(model_name,
-                         model_spec,
-                         sklearn_model=model,
-                         **kwargs)
-
-        self.old_vals = {"theta": None, "sigma": None, "class_count": None}
-        self.model_trained = False
-        self.model_type = 'Sklearn-NaiveBayes'
+        super().__init__(model_name, model_spec, **kwargs)
+
+        if sklearn_model is None:
+            if model_spec is None or (type(model_spec) is not dict):
+                raise ValueError(
+                    "Initializing model requires "
+                    "a model specification or "
+                    "compiled sklearn model. "
+                    "None was provided."
+                )
+            self.model = self.load_model_from_spec(model_spec)
+        else:
+            self.model = sklearn_model
+
+        self.is_classification = (
+            True if not (model_spec and model_spec.get("is_classification")) else model_spec.get("is_classification")
+        )
 
     def fit_model(self, train_data, fit_params=None, **kwargs):
         """
         Fits current model with provided training data.
 
-        :param train_data: Training data a tuple given in the form \
-        (x_train, y_train).
+        :param train_data: Training data a tuple
+        given in the form (x_train, y_train).
         :type train_data: `np.ndarray`
-        :param fit_params: Not used, present here for API consistency by \
-        convention.
+        :param fit_params: (Optional) Dictionary with hyperparameters that \
+        will be used to train the model. \
+        If the corresponding sklearn fit function is called, \
+        the provided hyperparameter should only contains parameters that \
+        match sklearn expected values, e.g., `learning_rate`, which provides \
+        the learning rate schedule
         :return: None
         """
-        self.model.fit(train_data[0], train_data[1])
-        self.model_trained = True
+        raise NotImplementedError
 
-    def update_model(self, model_update, **kwargs):
+    def update_model(self, model_update):
         """
-        Update GaussianNB model with provided model_update.
+        Update sklearn model with provided model_update.
 
         :param model_update: `ModelUpdate` object that contains \
-        the required information to update the GaussianNB model.
+        the required information to update the sklearn model.
         :type model_update: `ModelUpdate`
         :return: None
         """
-        if model_update.get("class_count") is None:
-            return
-
-        self.model.theta_ = model_update.get("theta")
-        self.model.sigma_ = model_update.get("sigma")
-        self.model.class_count_ = model_update.get("class_count")
-
-        self._store_old_values(self.model.theta_, self.model.sigma_,
-                               self.model.class_count_)
+        raise NotImplementedError
 
     def get_model_update(self):
         """
         Generates a `ModelUpdate` object that will be sent to other entities.
 
         :return: ModelUpdate
         :rtype: `ModelUpdate`
         """
-        if (not self.model_trained or
-                not hasattr(self.model, "class_count_") or
-                np.all(self.old_vals['class_count'] ==
-                       self.model.class_count_)):
-            return ModelUpdate(theta=None, sigma=None, class_count=None)
-        if self.old_vals['class_count'] is None:
-            return ModelUpdate(theta=self.model.theta_,
-                               sigma=self.model.sigma_,
-                               class_count=self.model.class_count_)
-
-        class_count_update = self.model.class_count_ - \
-            self.old_vals['class_count']
-
-        nm = self.model.class_count_[:, np.newaxis]
-        n = self.old_vals['class_count'][:, np.newaxis]
-        m = class_count_update[:, np.newaxis]
-
-        theta_update = self.model.theta_ * nm - self.old_vals['theta'] * n
-        theta_update = theta_update / m
-
-        sigma_update = nm * self.model.sigma_ - n * self.old_vals['sigma'] - \
-            (n * m) / nm * (self.old_vals['theta'] - theta_update) ** 2
-
-        return ModelUpdate(theta=theta_update,
-                           sigma=sigma_update,
-                           class_count=class_count_update)
-
-    def evaluate(self, test_dataset, **kwargs):
-        """
-        Evaluates the model given testing data.
-        :param test_dataset: Testing data, a tuple given in the form \
-        (x_test, test) or a datagenerator of of type `keras.utils.Sequence`, 
-        `keras.preprocessing.image.ImageDataGenerator`
-        :type test_dataset: `np.ndarray`
+        raise NotImplementedError
 
-        :param kwargs: Dictionary of metrics available for the model
-        :type kwargs: `dict`
+    def predict(self, x):
         """
+        Perform prediction for the given input.
 
-        if type(test_dataset) is tuple:
-            x_test = test_dataset[0]
-            y_test = test_dataset[1]
-
-            return self.evaluate_model(x_test, y_test)
-
-        else:
-            raise ModelException("Invalid test dataset!")
+        :param x: Samples with shape as expected by the model.
+        :type x: `np.ndarray`
+        :return: Array of predictions
+        :rtype: `np.ndarray`
+        """
+        return self.model.predict(x)
 
     def evaluate_model(self, x, y, **kwargs):
         """
         Evaluates the model given test data x and the corresponding labels y.
 
         :param x: Samples with shape as expected by the model.
         :type x: `np.ndarray`
         :param y: Corresponding true labels to x
         :type y: `np.ndarray`
-        :param kwargs: Dictionary of model-specific arguments for evaluating \
-        models. For example, sample weights accepted by model.score.
+        :param kwargs: Dictionary of model-specific arguments \
+        for evaluating models. For example, sample weights accepted \
+        by model.score.
         :return: Dictionary with all evaluation metrics provided by \
         specific implementation.
         :rtype: `dict`
         """
-        acc = {'score': self.model.score(x, y, **kwargs)}
-        return acc
+        raise NotImplementedError
 
-    def _store_old_values(self, theta, sigma, class_count):
+    def save_model(self, filename=None, path=None):
         """
-        Store old values of training parameters to allow reconstruction of
-        parameters for newly-trained examples when model update is sought.
+        Save a sklearn model to file in the format specific
+        to the framework requirement.
+
+        :param filename: Name of the file where to store the model.
+        :type filename: `str`
+        :param path: Path of the folder where to store the model. If no path \
+        is specified, the model will be stored in the default data location of \
+        the library `DATA_PATH`.
+        :type path: `str`
+        :return: filename
+        """
+        if filename is None:
+            file = self.model_name if self.model_name else self.model_type
+            filename = SKLearnPersistence.model_filename(file)
+
+        full_path = super().get_model_absolute_path(filename)
+        SKLearnPersistence.save_model(self.model, full_path)
+        logger.info("Model saved in path: %s.", full_path)
+        return filename
+
+    @staticmethod
+    def load_model(file_name):
         """
-        self.old_vals = {"theta": theta,
-                         "sigma": sigma,
-                         "class_count": class_count}
+        Load a sklearn from the disk given the filename.
+
+        :param file_name: Name of the file that contains the model to be loaded.
+        :type file_name: `str`
+        :return: A sklearn model
+        :rtype: `sklearn`
+        """
+        absolute_path = config.get_absolute_path(file_name)
+        model = SKLearnPersistence.load_model(absolute_path)
+        return model
 
     @staticmethod
     def load_model_from_spec(model_spec):
         """
         Loads model from provided model_spec, where model_spec is a `dict`
         that contains the following items: model_spec['model_definition']
-        contains the model definition as type diffprivlib.GaussianNB.
+        contains the model definition as a specific sklearn model type,
+        e.g., `sklearn.linear_model.SGDClassifier`.
 
         :param model_spec: Model specification contains \
-        a complied sklearn model.
-        :param model_spec: `dict`
+        a compiled sklearn model.
+        :type model_spec: `dict`
         :return: model
-        :rtype: `sklearn.cluster`
+        :rtype: `sklearn`
         """
         model = None
         try:
-            if 'model_definition' in model_spec:
-                path = model_spec['model_definition']
-                with open(path, 'rb') as f:
-                    model = joblib.load(f)
-
-                if not issubclass(type(model), GaussianNB):
-                    raise ValueError('Provided complied model in model_spec '
-                                     'should be of type GaussianNB.'
-                                     'Instead they are:' + str(type(model)))
+            if "model_definition" in model_spec:
+                path = model_spec["model_definition"]
+                model = SKLearnPersistence.load_model(path)
+                
         except Exception as ex:
-            raise ModelInitializationException('Model specification was '
-                                               'badly formed. ', str(ex))
+            raise ModelInitializationException("Model specification was " "badly formed. " + str(ex))
         return model
+
+    def is_fitted(self):
+        """
+        Return a boolean value indicating if the model is fitted or not.
+
+        :return: res
+        :rtype: `bool`
+        """
+        try:
+            check_is_fitted(self.model)
+        except NotFittedError:
+            return False
+        return True
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/model/sklearn_kmeans_fl_model.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/model/sklearn_kmeans_fl_model.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,23 +1,23 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
 import logging
-import joblib
+
 import numpy as np
 from sklearn.cluster import KMeans
 from sklearn.exceptions import NotFittedError
 
-from ibmfl.util import config
-from ibmfl.model.sklearn_fl_model import SklearnFLModel
+from ibmfl.exceptions import LocalTrainingException, ModelException, ModelInitializationException
 from ibmfl.model.model_update import ModelUpdate
-from ibmfl.exceptions import LocalTrainingException, \
-    ModelInitializationException, ModelException
+from ibmfl.model.sklearn_fl_model import SklearnFLModel
+from ibmfl.util import config
+from ibmfl.util.persistence.sklearn import SKLearnPersistence
 
 logger = logging.getLogger(__name__)
 
 
 class SklearnKMeansFLModel(SklearnFLModel):
     """
     Wrapper class for sklearn.cluster.KMeans
@@ -37,23 +37,23 @@
         :type model_spec: `dict`
         :param sklearn_model: Complied sklearn model
         :type sklearn_model: `sklearn.cluster.KMeans`
         :param kwargs: A dictionary contains other parameter settings on \
          to initialize a sklearn KMeans model.
         :type kwargs: `dict`
         """
-        super().__init__(model_name, model_spec,
-                         sklearn_model=sklearn_model,
-                         **kwargs)
-        self.model_type = 'Sklearn-KMeans'
+        super().__init__(model_name, model_spec, sklearn_model=sklearn_model, **kwargs)
+        self.model_type = "Sklearn-KMeans"
         if sklearn_model:
             if not issubclass(type(sklearn_model), KMeans):
-                raise ValueError('Compiled sklearn model needs to be provided'
-                                 '(sklearn.cluster.KMeans). '
-                                 'Type provided: ' + str(type(sklearn_model)))
+                raise ValueError(
+                    "Compiled sklearn model needs to be provided"
+                    "(sklearn.cluster.KMeans). "
+                    "Type provided: " + str(type(sklearn_model))
+                )
 
             self.model = sklearn_model
 
     def fit_model(self, train_data, fit_params=None, **kwargs):
         """
         Fits current model with provided training data.
 
@@ -70,61 +70,57 @@
         :return: None
         """
 
         # Extract x_train by default,
         # Only x_train is extracted since Clustering is unsupervised
 
         x_train = train_data[0]
-        
-        hyperparams = fit_params.get('hyperparams', {}) or {} if fit_params else {}
-        local_hp = hyperparams.get('local', {}) or {}
-        training_hp = local_hp.get('training', {}) or {}
+
+        hyperparams = fit_params.get("hyperparams", {}) or {} if fit_params else {}
+        local_hp = hyperparams.get("local", {}) or {}
+        training_hp = local_hp.get("training", {}) or {}
 
         try:
             self.model.set_params(**training_hp)
         except Exception as err:
             logger.exception(str(err))
-            raise LocalTrainingException(
-                'Error occurred while setting up model parameters')
+            raise LocalTrainingException("Error occurred while setting up model parameters")
 
         try:
             self.model.fit(x_train)
         except Exception as err:
             logger.info(str(err))
-            raise LocalTrainingException(
-                'Error occurred while performing model.fit'
-            )
+            raise LocalTrainingException("Error occurred while performing model.fit")
 
     def update_model(self, model_update):
         """
         Update sklearn model with provided model_update, where model_update
         should contain `cluster_centers_` having the same
         dimension as expected by the sklearn.cluster model.
         `cluster_centers_` : np.ndarray, shape (n_clusters, n_features)
 
         :param model_update: `ModelUpdate` object that contains the \
         cluster_centers vectors that will be used to update the model.
         :type model_update: `ModelUpdate`
         :return: None
         """
         if isinstance(model_update, ModelUpdate):
-            cluster_centers_ = model_update.get('weights')
+            cluster_centers_ = model_update.get("weights")
 
             try:
                 if cluster_centers_ is not None:
                     self.model.cluster_centers_ = np.array(cluster_centers_)
             except Exception as err:
-                raise LocalTrainingException('Error occurred during '
-                                             'updating the model weights. ' +
-                                             str(err))
+                raise LocalTrainingException("Error occurred during " "updating the model weights. " + str(err))
         else:
-            raise LocalTrainingException('Provided model_update should be of '
-                                         'type ModelUpdate. '
-                                         'Instead they are: ' +
-                                         str(type(model_update)))
+            raise LocalTrainingException(
+                "Provided model_update should be of "
+                "type ModelUpdate. "
+                "Instead they are: " + str(type(model_update))
+            )
 
     def get_model_update(self):
         """
         Generates a `ModelUpdate` object that will be sent to other entities.
 
         :return: ModelUpdate
         :rtype: `ModelUpdate`
@@ -171,19 +167,18 @@
         :return: Dictionary with all evaluation metrics provided by \
         specific implementation.
         :rtype: `dict`
         """
         acc = {}
 
         try:
-            acc['score'] = self.model.score(x, **kwargs)
+            acc["score"] = self.model.score(x, **kwargs)
         except NotFittedError:
-            logger.info('Model evaluated before fitted. '
-                        'Returning accuracy as 0')
-            acc['score'] = 0
+            logger.info("Model evaluated before fitted. " "Returning accuracy as 0")
+            acc["score"] = 0
 
         return acc
 
     def save_model(self, filename=None, path=None):
         """
         Save a sklearn.cluster.KMeans model to file in the format specific
         to the framework requirement. Meanwhile, initialize the attribute
@@ -193,17 +188,16 @@
         :type filename: `str`
         :param path: Path of the folder where to store the model. If no path \
         is specified, the model will be stored in the default data location of \
         the library `DATA_PATH`.
         :type path: `str`
         :return: filename
         """
-        if not hasattr(self.model, '_n_threads'):
-            logger.info("Attribute _n_threads does not exist. "
-                        "Setting it to default value.")
+        if not hasattr(self.model, "_n_threads"):
+            logger.info("Attribute _n_threads does not exist. " "Setting it to default value.")
             self.model._n_threads = 1
 
         return super().save_model(filename, path)
 
     @staticmethod
     def load_model_from_spec(model_spec):
         """
@@ -215,22 +209,22 @@
         a compiled sklearn model.
         :type model_spec: `dict`
         :return: model
         :rtype: `sklearn.cluster`
         """
         model = None
         try:
-            if 'model_definition' in model_spec:
-                model_file = model_spec['model_definition']
+            if "model_definition" in model_spec:
+                model_file = model_spec["model_definition"]
                 model_absolute_path = config.get_absolute_path(model_file)
 
-                with open(model_absolute_path, 'rb') as f:
-                    model = joblib.load(f)
-
+                model = SKLearnPersistence.load_model(model_absolute_path)
+                
                 if not issubclass(type(model), KMeans):
-                    raise ValueError('Provided complied model in model_spec '
-                                     'should be of type sklearn.cluster.'
-                                     'Instead they are: ' + str(type(model)))
+                    raise ValueError(
+                        "Provided complied model in model_spec "
+                        "should be of type sklearn.cluster."
+                        "Instead they are: " + str(type(model))
+                    )
         except Exception as ex:
-            raise ModelInitializationException('Model specification was '
-                                               'badly formed. '+ str(ex))
+            raise ModelInitializationException("Model specification was " "badly formed. " + str(ex))
         return model
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/model/tensorflow_fl_model.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/party_wrapper.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,650 +1,580 @@
+#!/usr/bin/env python3
+
 #  -----------------------------------------------------------------------------------------
 #  (C) Copyright IBM Corp. 2023-2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
-import logging
-import time
+from __future__ import annotations
+import importlib.util
 import json
-import numpy as np
-import tensorflow as tf
-tf.config.experimental.enable_tensor_float_32_execution(False)
-import inspect
-# if tf.__version__ != "2.1.0":
-# raise ImportError("This function requires TensorFlow v2.1.0.")
-
-from ibmfl.util import config
-from ibmfl.util import fl_metrics
-from ibmfl.model.fl_model import FLModel
-from ibmfl.model.model_update import ModelUpdate
-from ibmfl.exceptions import FLException, LocalTrainingException, ModelException
-from tensorflow.keras import backend as K
-from sklearn.model_selection import train_test_split
+import logging
+import os
+import platform
+import sys
+from pathlib import Path
+from typing import TYPE_CHECKING, Any
+
+import ibm_watsonx_ai._wrappers.requests as requests
+import requests as req
+from ibm_watsonx_ai.utils.utils import get_module_version
+from ibm_watsonx_ai.wml_client_error import ApiRequestFailure
+from ibm_watsonx_ai.wml_resource import WMLResource
+
+if TYPE_CHECKING:
+    from ibm_watsonx_ai import APIClient
+    from ibmfl.party.party import Party as IBMFL_Party
 
 logger = logging.getLogger(__name__)
 
+CRYPTO_LIBRARY = "pyhelayers"
 
-class TensorFlowFLModel(FLModel):
-    """
-    Wrapper class for importing tensorflow models.
-    """
 
-    def __init__(self, model_name, model_spec, tf_model=None, **kwargs):
-        """
-        Create a `TensorFlowFLModel` instance from a tensorflow model.\
-        If `tf_model` is provided, it will use it; otherwise it will take\
-        the model_spec to create the model.\
-        Assumes the `tf_model` passed as argument is compiled.
-
-        :param model_name: String specifying the type of model e.g., tf_CNN
-        :type model_name: `str`
-        :param model_spec: Specification of the `tf_model`
-        :type model_spec: `dict`
-        :param tf_model: Compiled TensorFlow model.
-        :type tf_model: `tf.keras.Model`
-        """
-
-        super().__init__(model_name, model_spec, **kwargs)
-        if tf_model is None:
-            if model_spec is None or (not isinstance(model_spec, dict)):
-                raise ValueError('Initializing model requires '
-                                 'a model specification or '
-                                 'compiled TensorFlow model. '
-                                 'None was provided')
-            # In this case we need to recreate the model from model_spec
-            self.model = self.load_model_from_spec(model_spec)
-        else:
-            if not issubclass(type(tf_model), tf.keras.Model):
-                raise ValueError('Compiled TensorFlow model needs to be '
-                                 'provided of type `tensorflow.keras.models`.'
-                                 ' Type provided: ' + str(type(tf_model)))
-
-            if self.use_gpu_for_training and self.num_gpus >= 1:
-                strategy = tf.distribute.MirroredStrategy()
-                with strategy.scope():
-                    self.model = tf_model
-            else:
-                self.model = tf_model
-        self.model_type = 'TensorFlow-2.1.0'
-        # Default values for local training
-        self.batch_size = 128
-        self.epochs = 1
-        self.validation_split = 0
-        self.steps_per_epoch = None
-        self.is_classification = True if not (model_spec and model_spec.get(
-            'is_classification')) else model_spec.get('is_classification')
-
-    def fit_model(self, train_data, fit_params=None, validation_data=None, **kwargs):
-        """
-        Fits current model with provided training data.
-
-        :param train_data: Training data, a tuple\
-        given in the form (x_train, y_train).
-        :type train_data: `np.ndarray`
-        :param fit_params: (optional) Dictionary with hyperparameters\
-        that will be used to call fit function.\
-        Hyperparameter parameters should match  expected values\
-        e.g., `epochs`, which specifies the number of epochs to be run.\
-        If no `epochs` or `batch_size` are provided, a default value\
-        will be used (1 and 128, respectively).
-        :type fit_params: `dict`
-        :return: None
-        """
-        fit_args = self.get_fit_args(fit_params, **kwargs)
-        if validation_data is not None:
-            fit_args.pop('validation_split', None)
-            fit_args['validation_data'] = validation_data
-
-        logger.info('Training hps for this round => '
-                    'batch_size: {}, epochs {}, steps_per_epoch {}'
-                    .format(fit_args.get('batch_size'), 
-                    fit_args.get('epochs'), fit_args.get('steps_per_epoch')))
+def is_crypto_supported() -> bool:
+    # pyhelayers is used for client side encryption
+    # Test is pyhelayers is installed, and if so we assume crypto is supported
+
+    pyhelayers_spec = importlib.util.find_spec(CRYPTO_LIBRARY)
+
+    return pyhelayers_spec is not None
+
+
+def import_diff(module_file_path):
+
+    if "runtime-23-1" in module_file_path:
+        return
+    pathlist = Path(module_file_path).rglob("*.py")
+    for path in pathlist:
+        # because path is object not string
+        path_in_str = str(path)
+        if not path_in_str.endswith("__init__.py"):
+            module_name = (
+                "ibmfl" + path_in_str.split("ibmfl")[2].replace(os.sep, ".")
+            )[:-3]
+            module_spec = importlib.util.spec_from_file_location(
+                module_name, path_in_str
+            )
+            loader = importlib.util.LazyLoader(module_spec.loader)  # type: ignore
+            module_spec.loader = loader  # type: ignore[union-attr]
+            module = importlib.util.module_from_spec(module_spec)  # type: ignore[arg-type]
+            sys.modules[module_name] = module
+            module_spec.loader.exec_module(module)  # type: ignore
+
+
+def check_python_framework_version(client_reqs: dict[str, Any]) -> None:
+    client_system = platform.system()
+    client_processor = platform.processor()
+
+    if "fl_extras" in client_reqs:
+        install_msg = (
+            "  You can install this and other required packages by running pip install --upgrade 'ibm-watsonx-ai["
+            + client_reqs["fl_extras"]
+            + "]'"
+        )
+    else:
+        install_msg = "  See documentation for more information."
+
+    py_version = platform.python_version()
+    logger.info("Detected Client Python Version: {}".format(py_version))
+    if not py_version.startswith(client_reqs["py_version"]):
+        raise Exception(
+            "The selected software spec requires python=={}.".format(
+                client_reqs["py_version"]
+            )
+        )
 
+    if client_system == "Darwin" and client_processor == "arm":
         try:
-            if type(train_data) is tuple and type(train_data[0]) is np.ndarray:
-                # Extract x_train and y_train, by default,
-                # label is stored in the last column
-                x = train_data[0]
-                y = train_data[1]
-                self.model.fit(x, y, **fit_args)
-            else:
-                if isinstance(train_data, (tf.keras.utils.Sequence)) and \
-                    hasattr(train_data, 'set_batch_size'):
-                    train_data.set_batch_size(fit_args.get('batch_size'))
-
-                fit_args.pop('batch_size', None)
-                fit_args.pop('validation_split', None)
-                self.model.fit(train_data, **fit_args)
-
-        except Exception as e:
-            logger.exception(str(e))
-            raise LocalTrainingException(
-                'Error occurred while performing model.fit')
-
-    def update_model(self, model_update):
-        """
-        Update TensorFlow model with provided model_update, where model_update \
-        should be generated according to \
-        `TensorFlowFLModel.get_model_update()`.
-
-        :param model_update: `ModelUpdate` object that contains the weights \
-        that will be used to update the model.
-        :type model_update: `ModelUpdate`
-        :return: None
-        """
-        if isinstance(model_update, ModelUpdate):
-            w = model_update.get("weights")
-            self.model.set_weights(w)
-        else:
-            raise LocalTrainingException('Provided model_update should be of '
-                                         'type ModelUpdate. '
-                                         'Instead they are: ' +
-                                         str(type(model_update)))
-
-    def get_model_update(self):
-        """
-        Generates a `ModelUpdate` object that will be sent to other entities.
-
-        :return: ModelUpdate
-        :rtype: `ModelUpdate`
-        """
-        w = self.model.get_weights()
-        return ModelUpdate(weights=w)
-
-    def predict(self, x, **kwargs):
-        """
-        Perform prediction for a batch of inputs. Note that for classification \
-        problems, it returns the resulting probabilities.
-
-        :param x: Samples with shape as expected by the model.
-        :type x: `np.ndarray`
-        :param kwargs: Dictionary of tf-specific arguments.
-        :type kwargs: `dict`
-
-        :return: Array of predictions
-        :rtype: `np.ndarray`
-        """
-        return self.model.predict(x, **kwargs)
-
-    def evaluate(self, test_dataset, **kwargs):
-        """
-        Evaluates the model given testing data.
-
-        :param test_dataset: Testing data, a tuple given in the form \
-        (x_test, y_test) or a datagenerator of type `keras.utils.Sequence`, \
-        `keras.preprocessing.image.ImageDataGenerator`
-        :type test_dataset: `np.ndarray`
-        :param kwargs: Dictionary of metrics available for the model
-        :type kwargs: `dict`
-        :return: metrics
-        :rtype: `dict`
-        """
-
-        if type(test_dataset) is tuple:
-            x_test = test_dataset[0]
-            y_test = test_dataset[1]
-
-            return self.evaluate_model(x_test, y_test, **kwargs)
-
-        else:
-            return self.evaluate_generator_model(
-                test_dataset, **kwargs)
-
-    def evaluate_model(self, x, y, batch_size=128, **kwargs):
-        """
-        Evaluates the model given x and y.
-
-        :param x: Samples with shape as expected by the model.
-        :type x: `np.ndarray`
-        :param y: Corresponding labels to x
-        :type y: `np.ndarray`
-        :param batch_size: Size of batches.
-        :type batch_size: `int`
-        :param kwargs: Dictionary of metrics available for the model
-        :type kwargs: `dict`
-        :return: metrics
-        :rtype: `dict`
-        """
+            tensorflowmacos_version = get_module_version(lib_name="tensorflow-macos")
+            logger.info(
+                "Detected tensorflow-macos Version: {}".format(tensorflowmacos_version)
+            )
+            if not tensorflowmacos_version.startswith(
+                client_reqs["tensorflow_version"]
+            ):
+                raise Exception("Incompatible tensorflow-macos version found")
+        except Exception as ex:
+            logger.warning("{}, this may cause unexpected errors.".format(ex))
+            logger.warning(
+                "The selected software spec requires tensorflow-macos=={}.".format(
+                    client_reqs["tensorflow_version"]
+                )
+            )
+    else:
+        try:
+            tensorflow_version = get_module_version(lib_name="tensorflow")
+            logger.info("Detected tensorflow Version: {}".format(tensorflow_version))
+            if not tensorflow_version.startswith(client_reqs["tensorflow_version"]):
+                raise Exception("Incompatible tensorflow version found")
+        except Exception as ex:
+            logger.warning("{}, this may cause unexpected errors.".format(ex))
+            logger.warning(
+                "The selected software spec requires tensorflow=={}.".format(
+                    client_reqs["tensorflow_version"]
+                )
+                + install_msg
+            )
+
+    try:
+        torch_version = get_module_version(lib_name="torch")
+        logger.info("Detected torch Version: {}".format(torch_version))
+        if not torch_version.startswith(client_reqs["torch_version"]):
+            raise Exception("Incompatible torch version found")
+    except Exception as ex:
+        logger.warning("{}, this may cause unexpected errors.".format(ex))
+        logger.warning(
+            "The selected software spec requires torch=={}.".format(
+                client_reqs["torch_version"]
+            )
+            + install_msg
+        )
+
+    try:
+        scikitlearn_version = get_module_version(lib_name="scikit-learn")
+        logger.info("Detected scikit-learn Version: {}".format(scikitlearn_version))
+        if not scikitlearn_version.startswith(client_reqs["scikitlearn_version"]):
+            raise Exception("Incompatible scikit-learn version found")
+    except Exception as ex:
+        logger.warning("{}, this may cause unexpected errors.".format(ex))
+        logger.warning(
+            "The selected software spec requires scikit-learn=={}.".format(
+                client_reqs["scikitlearn_version"]
+            )
+            + install_msg
+        )
+
+
+def choose_software_version(software_spec: str) -> str:
+    logger.info("Aggregator Software Spec: {}".format(software_spec))
+
+    if (
+        software_spec == "runtime-23.1-py3.10"
+        or software_spec == "336b29df-e0e1-5e7d-b6a5-f6ab722625b2"
+    ):
+        client_reqs = {
+            "py_version": "3.10",
+            "tensorflow_version": "2.12",
+            "torch_version": "2.0",
+            "scikitlearn_version": "1.1",
+            "fl_extras": "fl-rt23.1-py3.10",
+        }
+        check_python_framework_version(client_reqs)
+        return "runtime-23-1"
+    else:
+        client_reqs = {
+            "py_version": "3.10",
+            "tensorflow_version": "2.12",
+            "torch_version": "2.0",
+            "scikitlearn_version": "1.1",
+            "fl_extras": "fl-rt23.1-py3.10",
+        }
+        check_python_framework_version(client_reqs)
+        return "runtime-23-1"
 
-        metrics = self.model.evaluate(
-            x, y, batch_size=batch_size, **kwargs)
-        names = self.model.metrics_names
-        dict_metrics = {}
-        additional_metrics = {}
-        if type(metrics) == list:
-            for metric, name in zip(metrics, names):
-                # metric = metric.item()
-                if name == 'accuracy':
-                    dict_metrics['acc'] = round(metric, 2)
-                dict_metrics[name] = metric
-        else:
-            dict_metrics[names[0]] = metrics
-
-        y_pred = self.predict(x, batch_size=batch_size)
-        if self.is_classification:
-            additional_metrics = fl_metrics.get_eval_metrics_for_classificaton(
-                y, y_pred)
-        else:
-            additional_metrics = fl_metrics.get_eval_metrics_for_regression(
-                y, y_pred)
-
-        logger.info(additional_metrics)
-        dict_metrics = {**dict_metrics, **additional_metrics}
-        logger.info(dict_metrics)
 
-        return dict_metrics
+fl_path = os.path.abspath(".")
+if fl_path not in sys.path:
+    sys.path.append(fl_path)
 
-    def evaluate_generator_model(self, test_generator, **kwargs):
-        """
-        Evaluates the model based on the provided data generator.
 
-        :param test_generator: Testing datagenerator of type \
-        `keras.utils.Sequence`, or \
-        `keras.preprocessing.image.ImageDataGenerator`.
-        :type test_generator: `ImageDataGenerator` or `keras.utils.Sequence`
-        :return: metrics
-        :rtype: `dict`
-        """
+class Party(WMLResource):
+    """The Party class embodies a Federated Learning party, with methods to run, cancel, and query local training.
+    Refer to the ``client.remote_training_system.create_party()`` API for more information about creating an
+    instance of the Party class.
+    """
 
-        steps = self.steps_per_epoch
-        if steps in kwargs:
-            steps = kwargs.get('steps')
-
-        metrics = self.model.evaluate_generator(
-            test_generator, steps=steps)
-        names = self.model.metrics_names
-        dict_metrics = {}
-        additional_metrics = {}
-
-        if type(metrics) == list:
-            for metric, name in zip(metrics, names):
-                # metric = metric.item()
-                if name == 'accuracy':
-                    dict_metrics['acc'] = round(metric, 2)
-                dict_metrics[name] = metric
-        else:
-            dict_metrics[names[0]] = metrics
+    base_platform = "runtime-23-1"
+    default_software_spec = "runtime-23.1-py3.10"
+    SUPPORTED_PLATFORMS_MAP = {
+        base_platform: "/runtime-23-1/ibmfl",  # default
+    }
+
+    def __init__(self, client: APIClient | None = None, **kwargs: Any) -> None:
+
+        libs_module = sys.modules["ibm_watsonx_ai.libs"]
+        libs_location_list = libs_module.__path__
+
+        # base location string, default to cloud location
+        ibmfl_base_module_location = libs_location_list[0] + "/ibmfl"
+
+        # process location
+        ibmfl_module_location = (
+            ibmfl_base_module_location
+            + self.SUPPORTED_PLATFORMS_MAP.get(self.base_platform)
+        )
+
+        # check if using old connector script which is removed
+        if not client:
+            raise Exception(
+                "This version of the party connector script is outdated. "
+                "Please download the party connector script from your current Federated Learning experiment. "
+                "For more details, please refer to the documentation."
+            )
+
+        self.module_location = ibmfl_module_location
+        self.args = kwargs
+        self.Party: IBMFL_Party | None = None
+        self.connection = None
+        self.log_level = None
+        self.metrics_output: str | None = None
+
+        if "ibmfl" in sys.modules:
+            del sys.modules["ibmfl"]
+        if "ibmfl.party" in sys.modules:
+            del sys.modules["ibmfl.party"]
+        if "ibmfl.party.party" in sys.modules:
+            del sys.modules["ibmfl.party.party"]
+
+        # install the general lib
+        module_name = "ibmfl"
+        module_spec = importlib.util.spec_from_file_location(
+            module_name,
+            ibmfl_base_module_location
+            + "/"
+            + self.base_platform
+            + "/ibmfl/__init__.py",
+        )
+        module = importlib.util.module_from_spec(module_spec)  # type: ignore[arg-type]
+        sys.modules[module_name] = module
+        module_spec.loader.exec_module(module)  # type: ignore
+
+        WMLResource.__init__(self, __name__, client)
+        self._client = client
+        self.auth_token = "Bearer " + self._client.token
+        self.project_id = self._client.default_project_id
+        self.log_level = kwargs.get("log_level", "ERROR")
+
+    def start(self) -> None:
+        self.Party.start()  # type: ignore
+
+    def run(
+        self,
+        aggregator_id: str | None = None,
+        experiment_id: str | None = None,
+        asynchronous: bool = True,
+        verify: bool = True,
+        timeout: int = 60 * 10,
+    ) -> None:
+        """Connect to a Federated Learning aggregator and run local training.
+        Exactly one of `aggregator_id` and `experiment_id` must be supplied.
+
+        :param aggregator_id: aggregator identifier
+
+            * If aggregator_id is supplied, the party will connect to the given aggregator.
+
+        :type aggregator_id: str, optional
+        :param experiment_id: experiment identifier
+
+            * If experiment_id is supplied, the party will connect to the most recently created aggregator
+                for the experiment.
+
+        :type experiment_id: str, optional
+        :param asynchronous:
+            * `True`  - party starts to run the job in the background and progress can be checked later
+            * `False` - method will wait until training is complete and then print the job status
+        :type asynchronous: bool, optional
+        :param verify: verify certificate
+        :type verify: bool, optional
+        :param timeout: timeout in seconds
+
+            * If the aggregator is not ready within timeout seconds from now, exit.
+
+        :type timeout: int, or None for no timeout
+
+        **Examples**
+
+        .. code-block:: python
+
+            party.run( aggregator_id = "69500105-9fd2-4326-ad27-7231aeb37ac8", asynchronous = True, verify = True )
+            party.run( experiment_id = "2466fa06-1110-4169-a166-01959adec995", asynchronous = False )
+
+        """
+        import time
+        from datetime import datetime
+
+        from ibmfl.exceptions import FLException
+
+        timeout_time = None if timeout is None else timeout + time.time()
+        if (experiment_id is None and aggregator_id is None) or (
+            experiment_id is not None and aggregator_id is not None
+        ):
+            raise FLException(
+                "Exactly one of aggregator_id and experiment_id must be supplied"
+            )
 
-        return dict_metrics
+        if experiment_id is not None:
+            while True:
+                try:
+                    details = self._client.training.get_details(
+                        get_all=True,
+                        training_definition_id=experiment_id,
+                        _internal=True,
+                    )["resources"]
+                    details = [
+                        d
+                        for d in details
+                        if d["entity"]["status"]["state"]
+                        in ["accepting_parties", "pending", "running"]
+                    ]
+                    if not details:
+                        if timeout_time and timeout_time < time.time():
+                            raise FLException(
+                                "Cannot find an aggregator for experiment %s",
+                                experiment_id,
+                            )
+                        else:
+                            logger.info(
+                                "Cannot find an aggregator for experiment %s. Retrying.",
+                                experiment_id,
+                            )
+                            time.sleep(30)
+                            continue
+                    else:
+                        aggregator_id = max(
+                            [
+                                (
+                                    t["metadata"]["id"],
+                                    datetime.strptime(
+                                        t["metadata"]["created_at"],
+                                        "%Y-%m-%dT%H:%M:%S.%fZ",
+                                    ),
+                                )
+                                for t in details
+                            ],
+                            key=lambda d: d[1],
+                        )[0]
+                        logger.info("Using aggregator id %s", aggregator_id)
+                        break
+                except Exception as ex:
+                    logger.exception(ex)
+                    raise FLException(str(ex))
 
-    @staticmethod
-    def load_model(file_name, custom_objects={}):
-        """
-        Loads a model from disk given the specified file_name
+        # If crypto is enabled, test if this client supports it
+        # WML training service uses the fusion_type field to determine if encryption is required.
 
-        :param file_name: Name of the file that contains the model to be loaded.
-        :type file_name: `str`
-        :return: TensorFlow model loaded to memory
-        :rtype: `tf.keras.models.Model`
-        """
         try:
-            model = tf.keras.models.load_model(
-                file_name, custom_objects=custom_objects)
-        except Exception as ex:
-            logger.exception(str(ex))
-            logger.error(
-                'Loading model via tf.keras.models.load_model failed!')
-        return model
+            aggregator_details = self._client.training.get_details(
+                training_id=aggregator_id, _internal=True
+            )
+        except ApiRequestFailure as exc:
+            logger.error("Failed get training details")
+            raise FLException("Failed get training details") from exc
 
-    def save_model(self, filename=None):
-        """
-        Save a model to file in the format specific to the backend framework.
+        # Response might not be for a federated learning training
+        try:
+            fusion_type = aggregator_details["entity"]["federated_learning"][
+                "fusion_type"
+            ]
+        except (KeyError, TypeError) as exc:
+            logger.error("Failed to read fusion type from training details")
+            raise FLException(
+                "Failed to read fusion type from training details"
+            ) from exc
 
-        :param filename: Name of the file where to store the model.
-        :type filename: `str`
-        :return: filename
-        :rtype `string`
-        """
-        if filename is None:
-            file = self.model_name if self.model_name else self.model_type
-            filename = '{}'.format(file)
-
-        full_path = super().get_model_absolute_path(filename)
-        self.model.save(full_path)
-        logger.info('Model saved in path: %s.', full_path)
-        return filename
+        if fusion_type == "crypto_iter_avg":
+            logger.info("This training requires encryption")
+            if not is_crypto_supported():
+                logger.error(
+                    "Encryption is required, but the '%s' module required for encryption was not found",
+                    CRYPTO_LIBRARY,
+                )
+                raise FLException(f"'{CRYPTO_LIBRARY}' module not found")
+
+        # Import the changed files in the desired lib version
+
+        config_dict = self.args.get("config_dict", {})
+        metrics_config = {
+            "name": "WMLMetricsRecorder",
+            "path": "ibmfl.party.metrics.metrics_recorder",
+            "output_file": self.metrics_output,
+            "output_type": "json",
+            "compute_pre_train_eval": False,
+            "compute_post_train_eval": False,
+        }
+        if "metrics_recorder" not in config_dict:
+            config_dict["metrics_recorder"] = metrics_config
+        wml_services_url = config_dict.get("aggregator").get("ip").split("/")[0]
+        agg_info = wml_services_url + "/ml/v4/trainings/" + aggregator_id
+        config_dict["aggregator"]["ip"] = agg_info
+        self.args["config_dict"] = config_dict
 
-    @staticmethod
-    def model_from_json_via_tf_keras(json_file_name, custom_objects={}):
-        """
-        Loads a model architecture from disk via tf.keras \
-        given the specified json file name.
+        # Verify ssl context
+        if verify:
+            try:
+                req.get(
+                    "https://" + wml_services_url + "/wml_services/training/heartbeat",
+                    verify=verify,
+                )
+            except requests.exceptions.SSLError as ex:
+                logger.error(str(ex))
+                raise FLException(
+                    "No valid certificate detected. Please replace the default certificate with your own "
+                    "TLS certificate, or set verify to False at your own risk. For more details, please see "
+                    "https://www.ibm.com/docs/en/cloud-paks/cp-data/4.0?topic=client-using-custom-tls-certificate-connect-platform"
+                )
 
-        :param json_file_name: Name of the file that contains \
-        the model architecture to be loaded.
-        :type json_file_name: `str`
-        :param custom_objects: Dictionary of custom objects required for loading arch
-        :type custom_objects: `dict`
-        :return: tf.keras model with only model architecture loaded to memory
-        :rtype: `tf.keras.models.Model`
-        """
-        model = None
-        json_file = open(json_file_name, 'r')
-        f = json_file.read()
-        json_file.close()
+        # Check for aggregator state and start job
         try:
-            model = tf.keras.models.model_from_json(
-                f, custom_objects=custom_objects)
+            training_status = self._client.training.get_status(aggregator_id)
+            state = training_status["state"]
+            ready = False
+            if state == "pending":
+                while state == "pending" and (
+                    not timeout_time or timeout_time > time.time()
+                ):
+                    logger.info("Waiting for aggregator accepting parties state..")
+                    time.sleep(10)
+                    training_status = self._client.training.get_status(aggregator_id)
+                    state = training_status["state"]
+                if state != "accepting_parties":
+                    raise FLException(
+                        "The current state of training %s is %s, so the party is not able to start a job."
+                        % (aggregator_id, state)
+                    )
+                ready = True
+            elif state == "running" or state == "accepting_parties":
+                ready = True
+            else:
+                raise FLException(
+                    "The current state of training %s is %s, so the party is not able to start a job."
+                    % (aggregator_id, state)
+                )
+
+            if ready:
+
+                details = self._client.training.get_details(
+                    aggregator_id, _internal=True
+                )
+                fl_entity = details["entity"]["federated_learning"]
+                if "software_spec" in fl_entity:
+                    software_spec = (
+                        fl_entity["software_spec"]["name"]
+                        if "name" in fl_entity["software_spec"]
+                        else fl_entity["software_spec"]["id"]
+                    )
+                else:
+                    software_spec = self.default_software_spec
+                platform_env = choose_software_version(software_spec)
+                logger.info("Loading {} environment..".format(platform_env))
+                if "system" in details and "warnings" in details["system"]:
+                    for warning in details["system"]["warnings"]:
+                        logger.info("Warning: {}".format(warning["message"]))
+                self.module_location = "/".join(
+                    self.module_location.split("/")[0:-2]
+                ) + self.SUPPORTED_PLATFORMS_MAP.get(platform_env)
+
+                import_diff(self.module_location)
+                from ibmfl.party.party import Party
+
+                self.Party = Party(
+                    **self.args,
+                    token=self.auth_token,
+                    self_signed_cert=not verify,
+                    log_level=self.log_level,
+                )
+                self.connection = self.Party.connection  # type: ignore[attr-defined]
+                self.start()
+            else:
+                raise FLException(
+                    "The current state of training %s is %s, so the party is not able to start a job."
+                    % (aggregator_id, state)
+                )
+            # wait for the job to finish if synchrounous
+            if not asynchronous:
+                while (
+                    "completed" != state
+                    and "failed" != state
+                    and "canceled" != state
+                    and self.is_running()
+                ):
+                    training_status = self._client.training.get_status(aggregator_id)
+                    state = training_status["state"]
+                    time.sleep(10)
+                logger.info("The training finishes with %s status" % state)
+        except FLException as ex:
+            raise FLException(str(ex)) from ex
         except Exception as ex:
-            logger.error(
-                'Loading model via tf.keras.models.model_from_json failed! ')
+            logger.info("The party failed to start training")
+            logger.exception(ex)
 
-        return model
+    def monitor_logs(self, log_level: str = "INFO") -> None:
+        """Enable logging of the training job to standard output.
+        This method should be called before calling the ``run()`` method.
 
-    def load_model_from_spec(self, model_spec):
-        """
-        Loads model from provided model_spec, where model_spec is a `dict` \
-        that contains the following items: \
-            'model_definition': the path where the tf model is stored, \
-                usually in a `SavedModel` format.
-        :return: model
-        :rtype: `keras.models.Model`
-        """
-        custom_objects = {}
-        if 'custom_objects' in model_spec:
+        :param log_level: log level specified by user
+        :type log_level: str, optional
 
-            custom_objects_config = model_spec['custom_objects']
-            for custom_object in custom_objects_config:
-                key = custom_object['key']
-                value = custom_object['value']
-                path = custom_object['path']
-                custom_objects[key] = config.get_attr_from_path(
-                    path, value)
+        **Example**
 
-        if 'model_definition' in model_spec:
-            try:
-                model_file = model_spec['model_definition']
-                model_absolute_path = config.get_absolute_path(model_file)
-
-                if self.use_gpu_for_training:
-                    strategy = tf.distribute.MirroredStrategy()
-                    with strategy.scope():
-                        model = TensorFlowFLModel.load_model(
-                            model_absolute_path, custom_objects=custom_objects)
-                else:
-                    model = TensorFlowFLModel.load_model(
-                        model_absolute_path, custom_objects=custom_objects)
-
-            except Exception as ex:
-                logger.exception(str(ex))
-                raise FLException('Failed to load TensorFlow model!')
-        else:
-
-            if self.use_gpu_for_training:
-                strategy = tf.distribute.MirroredStrategy()
-                model = self.load_model_from_architecture(
-                    model_spec, custom_objects)
-
-            else:
-                model = self.load_model_from_architecture(
-                    model_spec, custom_objects)
+        .. code-block:: python
 
-        return model
+            party.monitor_logs()
 
-    def load_model_from_architecture(self, model_spec, custom_objects):
-        """
-        Loads model from provided model_spec, where model_spec is a `dict` \
-        that contains the following items: \
-            'model_architecture': the path where the tf model is stored, \
-                usually in a `SavedModel` format. \
-            'model_weights': the path to where the tf model weights are saved \
-            'compile_model_options': attributes used to compile model.
-        :param model_spec: Disctionary of spec provided by the user
-        :type model_spec: `dict`
-        :param custom_objects: Dictionary of custom objects required for loading arch
-        :type custom_objects: `dict`
-        :return: model
-        :rtype: `keras.models.Model`
         """
+        self.log_level = log_level
 
-        try:
-            model = TensorFlowFLModel.model_from_json_via_tf_keras(
-                model_spec['model_architecture'], custom_objects=custom_objects)
+        # Configure logging locally as well
+        from ibmfl.util.config import configure_logging_from_file
 
-            if model is None:
-                logger.error('An acceptable compiled model should be of type '
-                             'tensorflow.keras.models!')
-        except Exception as ex:
-            logger.error(repr(ex))
-            raise FLException('Unable to load the provided uncompiled model!')
+        configure_logging_from_file(log_level=log_level)
+        logger.setLevel(log_level)
 
-            # Load weights from provided path
-        if 'model_weights' in model_spec:
-            model.load_weights(model_spec['model_weights'])
+    def monitor_metrics(self, metrics_file: str = "-") -> None:
+        """Enable output of training metrics.
 
-        if 'compile_model_options' in model_spec:
-            # Load compile options:
-            try:
-                compiled_options = model_spec['compile_model_options']
-                optimizer = self.get_custom_attribute(
-                    compiled_options.get('optimizer'))
-                loss = self.get_custom_attribute(compiled_options.get('loss'))
-                metrics = self.get_custom_attribute(
-                    compiled_options.get('metrics'))
-                metrics = [metrics] if not isinstance(
-                    metrics, list) else metrics
-                model.compile(optimizer=optimizer,
-                              loss=loss,
-                              metrics=metrics)
-            except Exception as ex:
-                logger.exception(str(ex))
-                logger.exception(
-                    'Failed to compiled the TensorFlow.keras model.')
-        else:
-            raise ModelException('Failed to compile keras model, '
-                                 'no compile options provided.')
+        :param metrics_file: a filename specified by user to which the metrics should be written
+        :type metrics_file: str, optional
 
-        return model
+        .. note::
+            This method outputs the metrics to stdout if a filename is not specified
 
-    def get_gradient(self, train_data):
-        """
-        Compute the gradient with the provided dataset at the current local \
-        model's weights.
+        **Example**
 
-        :param train_data: Training data, a tuple \
-        given in the form (x_train, y_train).
-        :type train_data: `np.ndarray`
-        :return: gradients
-        :rtype: `list` of `tf.Tensor`
-        """
-        try:
-            x, y = train_data[0], train_data[1]
-        except Exception as ex:
-            logger.exception(str(ex))
-            raise FLException('Provided dataset has incorrect format. '
-                              'It should be a tuple in the form of '
-                              '(x_train, y_train).')
-        with tf.GradientTape() as tape:
-            predictions = self.model(x, training=True)
-            loss = self.model.loss(y, predictions)
+        .. code-block:: python
 
-        gradients = tape.gradient(loss, self.model.trainable_variables)
-        return gradients
+            party.monitor_metrics()
 
-    def expand_model_by_layer_name(self, new_dimension, layer_name="dense"):
-        """
-        Expand the current Keras model with provided dimension of
-        the hidden layers or model weights.
-        This method by default expands the dense layer of
-        the current neural network.
-        It can be extends to expand other layers specified by `layer_name`,
-        for example, it can be use to increase the number of CNN filters or
-        increase the hidden layer size inside LSTM.
-
-        :param new_dimension: New number of dimensions for \
-        the fully connected layers
-        :type new_dimension: `list`
-        :param layer_name: layer's name to be expanded
-        :type layer_name: `str`
-        :return: None
         """
-        if new_dimension is None:
-            raise FLException('No information is provided for '
-                              'the new expanded model. '
-                              'Please provide the new dimension of '
-                              'the resulting expanded model.')
-        try:
-            model_config = json.loads(self.model.to_json())
-        except NotImplementedError:
-            raise ModelException(
-                "Please construct the model config for models in "
-                "`SavedModel` format. "
-                "Details about how to construct the model config can be found"
-                " in TensorFlowFLModel tutorials.")
-        except Exception as ex:
-            logger.exception(str(ex))
-            raise FLException("Error occurred during extracting "
-                              "the model architecture.")
-        i = 0
-
-        for layer in model_config['config']['layers']:
-            # find the specified layers
-            if 'class_name' in layer and \
-                    layer['class_name'].strip().lower() == layer_name:
-                layer['config']['units'] = new_dimension[i]
-                i += 1
-
-        custom_obj = {
-            self.model.__class__.__name__: self.model.__class__
-        }
-
-        try:
-            new_model = tf.keras.models.model_from_json(
-                json.dumps(model_config), custom_objects=custom_obj)
-        except Exception as ex:
-            logger.exception(str(ex))
-            raise FLException("Error occurred during loading model from "
-                              "the new config.")
-
-        metrics = self.model.metrics_names
-        if 'loss' in metrics:
-            metrics.remove('loss')
-        if not self.use_gpu_for_training or self.num_gpus == 1:
-            new_model.compile(optimizer=self.model.optimizer,
-                              loss=self.model.loss,
-                              metrics=metrics)
-        else:
-            strategy = tf.distribute.MirroredStrategy()
-            with strategy.scope():
-                new_model.compile(optimizer=self.model.optimizer,
-                                  loss=self.model.loss,
-                                  metrics=metrics)
+        self.metrics_output = metrics_file
 
-        self.model = new_model
+    def is_running(self) -> bool:
+        """Check if the training job is running.
 
-    def is_fitted(self):
-        """
-        Return a boolean value indicating if the model is fitted or not. \
-        In particular, check if the tensorflow model has weights. \
-        If it has, return True; otherwise return false. 
+        :return: if the job is running
+        :rtype: bool
 
-        :return: res
-        :rtype: `bool`
-        """
-        try:
-            self.model.get_weights()
-        except Exception:
-            return False
-        return True
+        **Example**
 
-    def get_loss(self, dataset):
-        """
-        Return the resulting loss computed based on the provided dataset.
+        .. code-block:: python
 
-        :param dataset: Provided dataset, a tuple given in the form \
-        (x_test, y_test) or a datagenerator of type `keras.utils.Sequence`, \
-        `keras.preprocessing.image.ImageDataGenerator`.
-        :type dataset: `np.ndarray`
-        :return: The resulting loss.
-        :rtype: `float`
-        """
-        if 'loss' not in self.model.metrics_names:
-            self.model.metrics_names.append('loss')
-        res = self.evaluate(dataset)
-
-        if 'loss' in res:
-            loss = round(res['loss'], 2)
-            return loss
-        else:
-            raise FLException(
-                "Loss is not listed in the model's metrics_names.")
+            party.is_running()
 
-    def get_custom_attribute(self, attr):
         """
-        Load compiled options which are provided as config.
-        :param attr: Attribute config provided in config
-        :type attr: dict or key
-        :return: Attribute loaded and returned back for compilation
-        :rtype: `str` or python attr
+        return not self.connection.stopped  # type: ignore[attr-defined]
 
-        """
-        if attr is None:
-            raise ModelException("Invalid Model config exception")
+    def get_round(self) -> int:
+        """Get the current round number.
 
-        if isinstance(attr, dict):
-            try:
+        :return: the current round number
+        :rtype: int
 
-                value = attr.get('value')
-                path = attr.get('path')
-                args = attr.get('args') if 'args' in attr else {}
-                attribute = config.get_attr_from_path(
-                    path, value)
+        **Example**
 
-            except Exception as ex:
-                logger.error(
-                    "Error occurred while loading the custom attribute!")
-                logger.error("Custom attribute : " + attr)
-                logger.error()
+        .. code-block:: python
 
-            logger.debug(type(attribute))
+            party.get_round()
 
-            if inspect.isclass(attribute):
-                return attribute(**args)
-            else:
-                return attribute
+        """
+        return self.Party.proto_handler.metrics_recorder.get_round_no()  # type: ignore
 
-        else:
-            return attr
+    def cancel(self) -> None:
+        """Stop the local connection to the training on the party side.
 
-    def get_fit_args(self, global_params, **kwargs):
+        **Example**
 
-        fit_args = {}
-        local_params = kwargs.get('local_params', {}) or {} if global_params else {}
-        hyperparams = global_params.get('hyperparams', {}) or {} if global_params else {}
-        local_hp = hyperparams.get('local', {}) or {}
-        training_hp = local_hp.get('training', {}) or {}
-        optimizer_hp = local_hp.get('optimizer', {}) or {}
-
-        lr = optimizer_hp.get('lr', None)
-        if lr:
-            K.set_value(self.model.optimizer.learning_rate, lr)
-        logger.info("Learning rate of optimizer is set as {}".format(
-            self.model.optimizer.learning_rate))
+        .. code-block:: python
 
-        validation_split = training_hp.get('validation_split', self.validation_split)
-        try:
-            if float(validation_split) != 0:
-                fit_args['validation_split'] = float(validation_split)
-        except (TypeError, ValueError):
-            raise ValueError('Validation split cannot be a NoneType')
-        if 'validation_split' in local_params:
-            validation_split = local_params.get('validation_split')
-            try:
-                if float(validation_split) != 0:
-                    fit_args['validation_split'] = float(validation_split)
-            except (TypeError, ValueError):
-                raise ValueError('Validation split cannot be a NoneType')
-
-        fit_args['batch_size'] = training_hp.get('batch_size', self.batch_size)
-        if 'batch_size' in local_params:
-            fit_args['batch_size'] = local_params.get('batch_size')
-
-        fit_args['epochs'] = training_hp.get('epochs', self.epochs)
-        if 'epochs' in local_params:
-            fit_args['epochs'] = local_params.get('epochs')
-
-        fit_args['steps_per_epoch'] = training_hp.get('steps_per_epoch', self.steps_per_epoch)
-        if 'steps_per_epoch' in local_params:
-            fit_args['steps_per_epoch'] = local_params.get('steps_per_epoch')
+            party.cancel()
 
-        return fit_args
+        """
+        self.Party.stop_connection()  # type: ignore
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/party/training/fedavg_local_training_handler.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/party/training/fedavg_local_training_handler.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,44 +1,39 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
 import logging
-from ibmfl.party.training.local_training_handler import \
-    LocalTrainingHandler
+
+from ibmfl.party.training.local_training_handler import LocalTrainingHandler
 
 logger = logging.getLogger(__name__)
 
 
 class FedAvgLocalTrainingHandler(LocalTrainingHandler):
-
-    def train(self,  fit_params=None):
+    def train(self, fit_params=None):
         """
         Train locally using fl_model. At the end of training, a
         model_update with the new model information is generated and
         send through the connection.
 
         :param fit_params: (optional) Query instruction from aggregator
         :type fit_params: `dict`
         :return: ModelUpdate
         :rtype: `ModelUpdate`
         """
         train_data, (_) = self.data_handler.get_data()
-        _train_count = train_data[0].shape[0]
+        _train_count = self.data_handler.get_train_counts()
 
-        self.update_model(fit_params['model_update'])
+        self.update_model(fit_params["model_update"])
 
-        self.get_train_metrics_pre()
-
-        logger.info('Local training started...')
+        logger.info("Local training started...")
 
         self.fl_model.fit_model(train_data, fit_params, local_params=self.hyperparams)
 
         update = self.fl_model.get_model_update()
-        update.add('train_counts', _train_count)
-
-        logger.info('Local training done, generating model update...')
+        update["train_counts"] = _train_count
 
-        self.get_train_metrics_post()
+        logger.info("Local training done, generating model update...")
 
         return update
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/party/training/xgboost_local_training_handler.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/api_client.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,476 +1,619 @@
 #  -----------------------------------------------------------------------------------------
 #  (C) Copyright IBM Corp. 2023-2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
-from __future__ import print_function
+from __future__ import absolute_import
+from . import models
+from .rest import RESTClientObject
+from .rest import ApiException
+
+import os
+import re
 import sys
-import inspect
+import urllib
+import json
+import mimetypes
+import random
+import tempfile
+import threading
 import logging
-import operator
-import itertools
-import numpy as np
-from scipy import stats
-import scipy.stats.mstats as mstats
-from functools import partial
-from numcompress import compress
-from abc import ABC, abstractmethod
-from ddsketch.ddsketch import DDSketch
-from timeit import default_timer as time
-
-from sklearn.metrics import check_scoring
-from sklearn.model_selection import train_test_split
-from sklearn.preprocessing import OrdinalEncoder, LabelEncoder
-from sklearn.ensemble._hist_gradient_boosting.loss import _LOSSES
-from sklearn.utils.multiclass import check_classification_targets
-from sklearn.ensemble._hist_gradient_boosting.common import X_DTYPE, Y_DTYPE
-from sklearn.utils import check_X_y, check_random_state, check_array
-from sklearn.ensemble._hist_gradient_boosting.binning import _BinMapper
-
-from ibmfl.model.xgb_fl_model import XGBFLModel
-from ibmfl.util.xgboost.utils import is_classifier
-from ibmfl.util.xgboost.hyperparams import init_parameters
-
-from ibmfl.exceptions import HyperparamsException
-from ibmfl.exceptions import LocalTrainingException, ModelUpdateException
-from ibmfl.party.training.local_training_handler import LocalTrainingHandler
 
-logger = logging.getLogger(__name__)
+from random import choice
+import string
+import hashlib
+
+from datetime import datetime
+from datetime import date
+import uuid;
+
+# python 2 and python 3 compatibility library
+from six import iteritems
+
+try:
+    # for python3
+    from urllib.parse import quote
+except ImportError:
+    # for python2
+    from urllib import quote
+
+# special handling of `long` (python2 only)
+try:
+    # Python 2
+    long
+except NameError:
+    # Python 3
+    long = int
+
+from .configuration import Configuration
 
 
-class XGBoostBaseLocalTrainingHandler(LocalTrainingHandler, ABC):
+class ApiClient(object):
     """
-    Class implementation for XGBoost Base Local Training Handler
+    Generic API client for Swagger client library builds.
+
+    Swagger generic API client. This client handles the client-
+    server communication, and is invariant across implementations. Specifics of
+    the methods and models for each application are generated from the Swagger
+    templates.
+
+    NOTE: This class is auto generated by the swagger code generator program.
+    Ref: https://github.com/swagger-api/swagger-codegen
+    Do not edit the class manually.
+
+    :param host: The base path for the server to call.
+    :param header_name: a header to pass when making calls to the API.
+    :param header_value: a header value to pass when making calls to the API.
     """
+    def __init__(self, host=None, header_name=None, header_value=None, cookie=None):
 
-    def update_model(self, model_update):
         """
-        Update local model with model updates received from FusionHandler
-        :param model_update: ModelUpdate
-        :type model_update: `ModelUpdate`
-        :return: `None`
+        Constructor of the class.
         """
-        try:
-            # Check to see if worker's core model object has been initialized.
-            if self.fl_model is None:
-                self.fl_model = XGBFLModel('XGBFLModel', self.hyperparams)
-                logger.info(
-                    'No model update was provided, initialized new FL model object.')
-
-            # Update Model Object
-            if model_update is not None:
-                self.fl_model.update_model(model_update)
-                logger.info('Local model updated.')
-            else:
-                logger.info('No model update was provided.')
-        except Exception as ex:
-            raise LocalTrainingException(
-                'No query information is provided. '+ str(ex))
-
-    def train(self, fit_params=None):
-        """
-        Primary wrapper function used for routing internal remote function calls
-        within the Local Training Handler functions.
-        :param fit_params: A dictionary payload structure containing two key \
-        signatures, `func` and `args`, which respectively are the target \
-        function defined within the Local Training Handler and the arguments \
-        defined within the executing function, which is defined as a dictionary \
-        containing key-value pairs of matching arguments.
-        :type fit_params: `dict`
-        :return: Returns the corresponding values depending on the function \
-        remotely called from the aggregator.
+        self.rest_client = RESTClientObject()
+        self.default_headers = {}
+        if header_name is not None:
+            self.default_headers[header_name] = header_value
+        if host is None:
+            self.host = Configuration().host
+        else:
+            self.host = host
+        self.cookie = cookie
+        # Set default User-Agent.
+        self.user_agent = 'Swagger-Codegen/1.0.0/python'
+        #self.logger = logging.getLogger(__name__)
+        #self.logger.setLevel(logging.INFO)
+
+
+    @property
+    def user_agent(self):
+        """
+        Gets user agent.
+        """
+        return self.default_headers['User-Agent']
+
+    @user_agent.setter
+    def user_agent(self, value):
+        """
+        Sets user agent.
+        """
+        self.default_headers['User-Agent'] = value
+
+    def __get_next_salt(self, length):
+        """
+        This method returns a random  length String every time it is called.
+        It can be used as a generator of Salt
+           Convert UUID format to a Python string , remove "_" and
+           make all character uppercase
+        :return: Random 10 Char String as a Salt
+        """
+        random = uuid.uuid4().hex.upper()[0:length]
+        return random
+
+    def __generate_unique_id(self,id_length):
+        """
+        This method
+          - create random base input string with Salt and use it sha1 algorithm
+          -  generate random number of 16 char
+          -  combine all and return random set value
+        :param id_length: expected length of the unique id
+
+        :return: random unique id of size id_length
+        """
+        base_input = self.__get_next_salt(10)
+        hex_string = hashlib.sha1(base_input.encode('utf-8')).hexdigest()
+        random_uuid = uuid.uuid4().hex
+        return ''.join(choice(hex_string + string.ascii_lowercase + string.digits + random_uuid) for _ in range(id_length))
+
+    def set_default_header(self, header_name, header_value):
+        self.default_headers[header_name] = header_value
+
+    def __call_api(self, resource_path, method,
+                   path_params=None, query_params=None, header_params=None,
+                   body=None, post_params=None, files=None,
+                   response_type=None, auth_settings=None, callback=None, _return_http_data_only=None):
+
+        # headers parameters
+        header_params = header_params or {}
+        reqId = self.__generate_unique_id(36)
+        self.set_default_header('x-global-transaction-id', reqId)
+
+        header_params.update(self.default_headers)
+        if self.cookie:
+            header_params['Cookie'] = self.cookie
+        if header_params:
+            header_params = self.sanitize_for_serialization(header_params)
+
+        # path parameters
+        if path_params:
+            path_params = self.sanitize_for_serialization(path_params)
+            for k, v in iteritems(path_params):
+                replacement = quote(str(self.to_path_value(v)))
+                resource_path = resource_path.\
+                    replace('{' + k + '}', replacement)
+
+        # query parameters
+        if query_params:
+            query_params = self.sanitize_for_serialization(query_params)
+            query_params = {k: self.to_path_value(v)
+                            for k, v in iteritems(query_params)}
+
+        # post parameters
+        if post_params or files:
+            post_params = self.prepare_post_parameters(post_params, files)
+            post_params = self.sanitize_for_serialization(post_params)
+
+        # auth setting
+        self.update_params_for_auth(header_params, query_params, auth_settings)
+
+        # body
+        if body:
+            body = self.sanitize_for_serialization(body)
+
+        # request url
+        url = self.host + resource_path
+
+        # perform request and return response
+        #self.logger.info("Request to service with global transaction id: ", reqId)
+
+        response_data = self.request(method, url,
+                                     query_params=query_params,
+                                     headers=header_params,
+                                     post_params=post_params, body=body)
+
+        self.last_response = response_data
+        #self.logger.info("Service request completed for global transaction id: ", response_data.getheader('x-global-transaction-id'))
+
+        # deserialize response data
+        if response_type:
+            deserialized_data = self.deserialize(response_data, response_type)
+        else:
+            deserialized_data = None
+
+        if callback:
+            callback(deserialized_data) if _return_http_data_only else callback((deserialized_data, response_data.status, response_data.getheaders()))
+        elif _return_http_data_only:
+            return ( deserialized_data );
+        else:
+            return (deserialized_data, response_data.status, response_data.getheaders())
+
+
+    def to_path_value(self, obj):
+        """
+        Takes value and turn it into a string suitable for inclusion in
+        the path, by url-encoding.
+
+        :param obj: object or string value.
+
+        :return string: quoted value.
         """
-        result = None
+        if type(obj) == list:
+            return ','.join(obj)
+        else:
+            return str(obj)
+
+    def sanitize_for_serialization(self, obj):
+        """
+        Builds a JSON POST object.
+
+        If obj is None, return None.
+        If obj is str, int, long, float, bool, return directly.
+        If obj is datetime.datetime, datetime.date
+            convert to string in iso8601 format.
+        If obj is list, sanitize each element in the list.
+        If obj is dict, return the dict.
+        If obj is swagger model, return the properties dict.
+
+        :param obj: The data to serialize.
+        :return: The serialized form of data.
+        """
+        types = (str, int, long, float, bool, tuple)
+        if sys.version_info < (3, 0):
+            types = types + (unicode,)
+        if isinstance(obj, type(None)):
+            return None
+        elif isinstance(obj, types):
+            return obj
+        elif isinstance(obj, list):
+            return [self.sanitize_for_serialization(sub_obj)
+                    for sub_obj in obj]
+        elif isinstance(obj, (datetime, date)):
+            return obj.isoformat()
+        else:
+            if isinstance(obj, dict):
+                obj_dict = obj
+            else:
+                # Convert model obj to dict except
+                # attributes `swagger_types`, `attribute_map`
+                # and attributes which value is not None.
+                # Convert attribute name to json key in
+                # model definition for request.
+                obj_dict = {obj.attribute_map[attr]: getattr(obj, attr)
+                            for attr, _ in iteritems(obj.swagger_types)
+                            if getattr(obj, attr) is not None}
+
+            return {key: self.sanitize_for_serialization(val)
+                    for key, val in iteritems(obj_dict)}
+
+    def deserialize(self, response, response_type):
+        """
+        Deserializes response into an object.
+
+        :param response: RESTResponse object to be deserialized.
+        :param response_type: class literal for
+            deserialzied object, or string of class name.
+
+        :return: deserialized object.
+        """
+        # handle file downloading
+        # save response body into a tmp file and return the instance
+        if "file" == response_type:
+            return self.__deserialize_file(response)
+
+        # fetch data from response object
         try:
-            # Validate Incoming Payload Parameter
-            if fit_params is None:
-                raise LocalTrainingException('Provided fit_params is None, no '
-                                             'functions were executed.')
-
-            # Validate Payload Signature
-            if 'func' not in fit_params or 'args' not in fit_params:
-                raise LocalTrainingException('Malformed payload, must include '
-                                             'func and args in payload.')
-
-            # Validate Defined Function Header
-            if not (isinstance(fit_params['func'], str) and
-                    hasattr(self, fit_params['func'])):
-                raise LocalTrainingException('Function header is not valid or is '
-                                             'not defined within the scope of the '
-                                             'local training handler.')
-
-            # Validate Payload Argument Parameter Mappings Against Function
-            spec = inspect.getargspec(eval('self.'+fit_params['func']))
-            for k in fit_params['args'].keys():
-                if k not in spec.args:
-                    raise LocalTrainingException('Specified parameter argument is '
-                                                 'not defined in the function.')
-            self.get_train_metrics_pre()
-
-            # Construct Function Call Command
-            result = eval('self.'+fit_params['func'])(**fit_params['args'])
-
-            self.get_train_metrics_post()
-        except Exception as ex:
-            raise LocalTrainingException('Error processing remote function ' +
-                                         'call: ' + str(ex))
-
-        return result
-
-    def initialize(self, params):
-        """
-        A remote function call which performs the following procedures:
-        1. Obtains global hyperparameters from the aggregator and initializes
-        them for each of the local worker's parameters.
-        2. Performs the following set of preliminary validation checks
-        and processes:
-           a. Perform dataset encoding and validation checks.
-           b. Set local worker's seed and PRNG states.
-           c. Initialize key parameters from defined hyperparameters in 1.
-           d. Check for missing data within each local worker's dataset.
-           e. Returns various parameters needed for aggregator to perform
-           tree growth operation.
-        :param params: A hyperparameter dictionary from the aggregator.
-        :type params: `dict`
-        :return: Dictionary containing parameters necessary for tree growth at \
-        the aggregator.
-        :rtype: `dict`
-        """
-        # Initialize Hyperparameters
-        logger.info('Recieved and initializing local hyperparameters.')
-        self.hyperparams = params
-
-        init_parameters(self, params)
-
-        logger.info(
-            'Performing preliminary checks, validation, and initialization.')
-
-        # Data Validation and Encoding Checks
-        logger.info('[CHECK] Dataset Encoding and Validation Checks')
-        self.data_val_enc()
-
-        # Check PRNG State
-        logger.info('[CHECK] PRNG State and Seed Validation')
-        self.check_prng_state()
-
-        # Initialize Key Parameters
-        logger.info('[CHECK] Initialize Key Parameters')
-        self.loss_ = self.get_loss(sample_weight=self.sample_weight)
-
-        # Validate Dataset
-        self.X_train, self.y_train = self.X, self.Y
-        self.X_val, self.y_val = None, None
-
-        # Check for Missing Data
-        logger.info('[CHECK] Check for Missing Data')
-        self.has_missing_values = np.isnan(
-            self.X_train).any(axis=0).astype(np.uint8)
-
-        # Prepare Payload
-        params = {
-            'has_missing_values': self.has_missing_values,
-            'need_update_leaves_values': self.loss_.need_update_leaves_values,
-            'n_features_': self.X.shape[1]
-        }
-        if is_classifier(self):
-            params['classes_'] = self.classes_
+            data = json.loads(response.data)
+           # print "Reasponse data after json loads"
+           # print data
+        except ValueError:
+            data = response.data
+
+        return self.__deserialize(data, response_type)
+
+    def __deserialize(self, data, klass):
+        """
+        Deserializes dict, list, str into an object.
+
+        :param data: dict, list or str.
+        :param klass: class literal, or string of class name.
+
+        :return: object.
+        """
+        if data is None:
+            return None
+
+        if type(klass) == str:
+            if klass.startswith('list['):
+                sub_kls = re.match('list\[(.*)\]', klass).group(1)
+                return [self.__deserialize(sub_data, sub_kls)
+                        for sub_data in data]
+
+            if klass.startswith('dict('):
+                sub_kls = re.match('dict\(([^,]*), (.*)\)', klass).group(2)
+                return {k: self.__deserialize(v, sub_kls)
+                        for k, v in iteritems(data)}
+
+            # convert str to class
+            # for native types
+            if klass in ['int', 'long', 'float', 'str', 'bool',
+                         "date", 'datetime', "object"]:
+                klass = eval(klass)
+            # for model types
+            else:
+                klass = eval('models.' + klass)
+
+        if klass in [int, long, float, str, bool]:
+            return self.__deserialize_primitive(data, klass)
+        elif klass == object:
+            return self.__deserialize_object(data)
+        elif klass == date:
+            return self.__deserialize_date(data)
+        elif klass == datetime:
+            return self.__deserialize_datatime(data)
+        else:
+            return self.__deserialize_model(data, klass)
+
+    def call_api(self, resource_path, method,
+                 path_params=None, query_params=None, header_params=None,
+                 body=None, post_params=None, files=None,
+                 response_type=None, auth_settings=None, callback=None, _return_http_data_only=None):
+        """
+        Makes the HTTP request (synchronous) and return the deserialized data.
+        To make an async request, define a function for callback.
+
+        :param resource_path: Path to method endpoint.
+        :param method: Method to call.
+        :param path_params: Path parameters in the url.
+        :param query_params: Query parameters in the url.
+        :param header_params: Header parameters to be
+            placed in the request header.
+        :param body: Request body.
+        :param post_params dict: Request post form parameters,
+            for `application/x-www-form-urlencoded`, `multipart/form-data`.
+        :param auth_settings list: Auth Settings names for the request.
+        :param response: Response data type.
+        :param files dict: key -> filename, value -> filepath,
+            for `multipart/form-data`.
+        :param callback function: Callback function for asynchronous request.
+            If provide this parameter,
+            the request will be called asynchronously.
+        :param _return_http_data_only: response data without head status code and headers
+        :return:
+            If provide parameter callback,
+            the request will be called asynchronously.
+            The method will return the request thread.
+            If parameter callback is None,
+            then the method will return the response directly.
+        """
+        if callback is None:
+            return self.__call_api(resource_path, method,
+                                   path_params, query_params, header_params,
+                                   body, post_params, files,
+                                   response_type, auth_settings, callback, _return_http_data_only)
+        else:
+            thread = threading.Thread(target=self.__call_api,
+                                      args=(resource_path, method,
+                                            path_params, query_params,
+                                            header_params, body,
+                                            post_params, files,
+                                            response_type, auth_settings,
+                                            callback,_return_http_data_only))
+        thread.start()
+        return thread
+
+    def request(self, method, url, query_params=None, headers=None,
+                post_params=None, body=None):
+        """
+        Makes the HTTP request using RESTClient.
+        """
+        if method == "GET":
+            return self.rest_client.GET(url,
+                                        query_params=query_params,
+                                        headers=headers)
+        elif method == "HEAD":
+            return self.rest_client.HEAD(url,
+                                         query_params=query_params,
+                                         headers=headers)
+        elif method == "OPTIONS":
+            return self.rest_client.OPTIONS(url,
+                                            query_params=query_params,
+                                            headers=headers,
+                                            post_params=post_params,
+                                            body=body)
+        elif method == "POST":
+            return self.rest_client.POST(url,
+                                         query_params=query_params,
+                                         headers=headers,
+                                         post_params=post_params,
+                                         body=body)
+        elif method == "PUT":
+            return self.rest_client.PUT(url,
+                                        query_params=query_params,
+                                        headers=headers,
+                                        post_params=post_params,
+                                        body=body)
+        elif method == "PATCH":
+            return self.rest_client.PATCH(url,
+                                          query_params=query_params,
+                                          headers=headers,
+                                          post_params=post_params,
+                                          body=body)
+        elif method == "DELETE":
+            return self.rest_client.DELETE(url,
+                                           query_params=query_params,
+                                           headers=headers,
+                                           body=body)
+        else:
+            raise ValueError(
+                "http method must be `GET`, `HEAD`,"
+                " `POST`, `PATCH`, `PUT` or `DELETE`."
+            )
+
+    def prepare_post_parameters(self, post_params=None, files=None):
+        """
+        Builds form parameters.
+
+        :param post_params: Normal form parameters.
+        :param files: File parameters.
+        :return: Form parameters with files.
+        """
+        params = []
+
+        if post_params:
+            params = post_params
+
+        if files:
+            for k, v in iteritems(files):
+                if not v:
+                    continue
+                file_names = v if type(v) is list else [v]
+                for n in file_names:
+                    with open(n, 'rb') as f:
+                        filename = os.path.basename(f.name)
+                        filedata = f.read()
+                        mimetype = mimetypes.\
+                            guess_type(filename)[0] or 'application/octet-stream'
+                        params.append(tuple([k, tuple([filename, filedata, mimetype])]))
 
         return params
 
-    def check_prng_state(self):
+    def select_header_accept(self, accepts):
+        """
+        Returns `Accept` based on an array of accepts provided.
+
+        :param accepts: List of headers.
+        :return: Accept (e.g. application/json).
         """
-        Given the initialize `random_state` from the hyperparameter, we set the
-        random seed value of the local training handler.
-        :return: `None`
-        """
-        # Check PRNG Random State
-        rng = check_random_state(self.random_state)
-        self._random_seed = rng.randint(np.iinfo(np.uint32).max, dtype='u8')
-
-    def generate_sketch(self, party_id):
-        logger.info('Generate Local Party Data Sketch')
-
-        # Initialize Local Party Data Parameters
-        self.n_samples = self.X_train.shape[0]
-
-        # Initialize Sketch - TODO: Parameterize the sketch to choose sketch type.
-        logger.info('> Initialize Feature-Wise Sketchers')
-        self.sketchers = [DDSketch() for i in range(self.X_train.shape[1])]
-
-        # Append Data to Sketch - TODO: Parallelize this process if necessary.
-        logger.info('> Allocate Data to Sketch Objects')
-        for f_idx in range(self.X_train.shape[1]):
-            for x in self.X_train[:, f_idx]:
-                if x != np.NaN:
-                    # TODO: Add DP Noise in Data - Use epsilon parameter for noise.
-                    self.sketchers[f_idx].add(x)
-
-        # Obtain Percentile Data
-        logger.info('> Generate Percentile Representations of Training Dataset')
-        masked_x_train = np.ma.masked_invalid(self.X_train)
-        self.X_ptiles_train = np.transpose(np.array([
-            mstats.rankdata(masked_x_train[:, idx]) * 100 / masked_x_train.shape[0] for idx in range(masked_x_train.shape[1]) ]))
-        self.X_ptiles_train[self.X_ptiles_train == 0] = -1
-
-        # Perform Data Compression of Percentile Data (Experimental)
-        # TODO: Enable this in a future release.
-        if False:
-            logger.info('> Data Compression Enabled - Performing Percentile Data Compression')
-            X_ptile_train_compressed = compress(list(self.X_ptiles_train.flatten()), precision=4)
-            data_dim = self.X_ptiles_train.shape
-
-            # Compute Compression Ratio
-            orig_size = sum(sys.getsizeof(i) for i in self.X_ptiles_train.flatten())
-            comp_size = sys.getsizeof(X_ptile_train_compressed)
-            ratio = (orig_size - comp_size) / orig_size
-
-            logger.info('> Compression Ratio: ' + str(ratio))
-
-            # Transmit X_ptile_train_compressed and data_dim to Aggregator
-
-        return party_id, self.sketchers, self.X_ptiles_train
-
-    def init_null_preds(self, baseline):
-        """
-        Initialize null predictions from baseline models. These values are used
-        to construct the initial tree structure of the model. Note that
-        raw_predictions has shape (n_trees_per_iteration, n_samples) whereas
-        n_trees_per_iteration is n_classes in multiclass classification, else 1.
-        Furthermore, setups initial data structures for metrics and performs
-        early stop validation checks.
-        :return: `None`
-        """
-        # Generate Baseline Predictions
-        logger.info('Generating Initial Predictions from Null Model')
-        self.baseline = baseline
-        self.raw_predictions = np.zeros(shape=(self.n_trees, self.n_samples),
-                                        dtype=self.baseline.dtype) + self.baseline
-
-        # Initialize Relevant Datastructures for Metrics
-        self._predictors = predictors = []
-        self._scorer = None
-        self.raw_predictions_val = None
-        self.train_score_ = []
-        self.validation_score_ = []
-
-        self.begin_at_stage = 0
-
-        # Initialize Model Attributes
-        self.fl_model.n_trees = self.n_trees
-        self.fl_model._baseline_prediction = self.baseline
-        self.fl_model._raw_predictions = self.raw_predictions
-        self.fl_model.loss_ = self.loss_
-        self.fl_model.n_features_ = self.X.shape[1]
-        if is_classifier(self):
-            self.fl_model.classes_ = self.classes_
-        self.fl_model._in_fit = True
-
-    def init_grad(self):
-        """
-        Process to initialize gradients and hessians on the Local Training
-        Handler. This is an empty data structure used to contain and later
-        will be used to send over to aggregator.
-        Note: shape = (n_trees_per_iteration, n_samples)
-        :return: `None`
-        """
-        self.gradients, self.hessians = self.loss_.init_gradients_and_hessians(
-            n_samples=self.n_samples,
-            sample_weight=self.sample_weight,
-            prediction_dim=self.n_trees
-        )
-
-    def update_grad(self):
-        """
-        A wrapper function to update the corresponding gradient and hessian
-        statistics inplace based on the raw predictions from the previous
-        iteration of the model.
-        :return: `None`
-        """
-        self.loss_.update_gradients_and_hessians(self.gradients, self.hessians,
-                                                 self.y_train,
-                                                 self.raw_predictions,
-                                                 self.sample_weight)
-
-    def collect_hist(self, party_id, k):
-        """
-        We return values of the surrogate histogram values, gradients, hessians,
-        and the number of non-missing bin counts from the local worker.
-        :param k: The correseponding feature index of the dataset.
-        :type k: `int`
-        :return: Returns a tuple comprising of the histogram bin index values, \
-        gradient, hessian, missing value counts per feature, bin threshold value.
-        :rtype: (`np.array`, `np.array`, `np.array`, `list`, `list`)
-        """
-        return party_id, self.gradients[k, :], self.hessians[k, :]
-
-    def update_raw_preds(self, k, predictor, use_val):
-        """
-        Function which updates internal local trainer handler's raw_prediction
-        values given the intermediate state of the model.
-        NOTE: Because we merged the histogram, the histogram mappings no longer
-        apply in this context, so all predictions must be done entirely from
-        scratch again - cannot use the histogram subtraction technique from
-        previous implementation here.
-        :param k: The correseponding feature index of the dataset.
-        :type k: `int`
-        :param predictor: The predictor object state of the XGBoost tree.
-        :type  predictor: `TreePredictor`
-        :param use_val: Boolean indicator whether to use the validation set. \
-        (Currently this parameter is not supported at the moment.)
-        :type use_val: `Boolean`
-        :return: `None`
-        """
-        self.raw_predictions[k] += predictor.predict(self.X_train)
-
-    def data_val_enc(self):
-        """
-        Performs a validation procedure of the input dataset to correspondingly
-        check the data types as well as the data encoding.
-        :return: `None`
-        """
-        logging.info('Performing local worker data encoding and validation.')
-        (x, y), (_) = self.data_handler.get_data()
-
-        # Perform Data Encoding Validation
-        self.X, y = check_X_y(x, y, dtype=[X_DTYPE], force_all_finite=False)
-        self.Y = self.encode_target(y)
-
-        # TODO load sample_weight from config
-        self.sample_weight = None
-
-    def set_fit(self, in_fit):
-        """
-        Sets the state of the in fit function, which is used to determine
-        whether or not to used the binned functionality during inference.
-        :param in_fit: Attribute to set the in fit function to.
-        :type  in_fit: `Boolean`
-        :return: `None`
-        """
-        self.fl_model._in_fit = in_fit
-
-    @abstractmethod
-    def encode_target(self, y=None):
-        raise NotImplementedError
-
-
-class XGBRegressorLocalTrainingHandler(XGBoostBaseLocalTrainingHandler):
-    _VALID_LOSSES = ('least_squares')
-
-    def encode_target(self, y):
-        """
-        Converts the input y to the expected dtype.
-        :param y: The corresponding target data from the dataset to encode.
-        :type y: `np.array`
-        :return: Returns the corresponding encoded y values.
-        :rtype: `np.array`
-        """
-        self.n_trees = 1
-        return y.astype(Y_DTYPE, copy=False)
-
-    def get_loss(self, sample_weight):
-        """
-        Given the initialized loss type defined under the hyerparameters, we
-        return the corresponding loss function to dictate the corresponding
-        learning task of the model.
-        :param sample_weight: Weights of training data
-        :type sample_weight: `np.ndarray`
-        :return: Returns the respective loss object as defined in the FL \
-        hyperparameters.
-        :rtype: Derivation of `BaseLoss`
-        """
-        return _LOSSES[self.loss](sample_weight=sample_weight)
-
-    def get_avg_stats(self):
-        """
-        Helper process to compute necessary statistics for computing the global
-        average. Wrapper function calls respective function to derive statistics
-        based on the defined loss function.
-        :return: Returns y_hat * n and n.
-        :rtype: (`np.array`, `np.array`)
-        """
-        logger.info('Compute Local Party Average Statistics')
-
-        return (np.array(np.average(self.y_train, weights=self.sample_weight)), \
-                np.array([len(self.y_train)]))
-
-
-class XGBClassifierLocalTrainingHandler(XGBoostBaseLocalTrainingHandler):
-    _VALID_LOSSES = ('binary_crossentropy', 'categorical_crossentropy', 'auto')
-
-    def encode_target(self, y):
-        """
-        Converts the input y to the expected dtype and performs a label
-        encoding. Here, we assume that each party has at least one sample of
-        the corresponding class label type for each different classes.
-        :param y: The corresponding target data from the dataset to encode.
-        :type y: `np.array`
-        :return: Returns the corresponding encoded y values.
-        :rtype: `np.array`
-        """
-        # Validate Classification Target Values
-        check_classification_targets(y)
-
-        # Apply Label Encoder Transformation
-        lab_enc = LabelEncoder()
-        enc_y = lab_enc.fit_transform(y).astype(np.float64, copy=False)
-
-        # Extract Encoded Target Sizes
-        self.classes_ = lab_enc.classes_
-        if self.classes_.shape[0] != self.num_classes:
-            raise ValueError('Number of classes defined in configuration file '
-                             'and the classes derived from the data does not '
-                             'match. Found {0} classes, while config file '
-                             'is defined as {1} classes.'.format(
-                self.classes_.shape[0], self.num_classes))
+        if not accepts:
+            return
 
-        if self.loss == 'auto':
-            self.n_trees = 1 if self.classes_.shape[0] <= 2 else self.classes_.shape[0]
+        accepts = list(map(lambda x: x.lower(), accepts))
+
+        if 'application/json' in accepts:
+            return 'application/json'
         else:
-            self.n_trees = 1 if self.num_classes <= 2 else self.num_classes
+            return ', '.join(accepts)
 
-        return enc_y
+    def select_header_content_type(self, content_types):
+        """
+        Returns `Content-Type` based on an array of content_types provided.
 
-    def get_loss(self, sample_weight):
+        :param content_types: List of content-types.
+        :return: Content-Type (e.g. application/json).
         """
-        Given the initialized loss type defined under the hyerparameters, we
-        return the corresponding loss function to dictate the corresponding
-        learning task of the model. If auto is selected, then we will
-        automatically determine whether the classification task is binary or
-        multiclass given the label encoding cardinality.
-        :param sample_weight: Weights of training data
-        :type sample_weight: `np.ndarray`
-        :return: Returns the respective loss object as defined in the FL \
-        hyperparameters.
-        :rtype: Derivation of `BaseLoss`
-        """
-        if (self.loss == 'categorical_crossentropy' and self.n_trees == 1):
-            raise ValueError("Incompatible loss and target variable counts.")
-
-        if self.loss == 'auto':
-            return _LOSSES['binary_crossentropy'](sample_weight=sample_weight) \
-                if self.n_trees == 1 else \
-                _LOSSES['categorical_crossentropy'](sample_weight=sample_weight)
+        if not content_types:
+            return 'application/json'
+
+        content_types = list(map(lambda x: x.lower(), content_types))
+
+        if 'application/json' in content_types:
+            return 'application/json'
         else:
-            return _LOSSES[self.loss](sample_weight=sample_weight)
+            return content_types[0]
+
+    def update_params_for_auth(self, headers, querys, auth_settings):
+        """
+        Updates header and query params based on authentication setting.
+
+        :param headers: Header parameters dict to be updated.
+        :param querys: Query parameters dict to be updated.
+        :param auth_settings: Authentication setting identifiers list.
+        """
+        config = Configuration()
+
+        if not auth_settings:
+            return
 
-    def get_avg_stats(self):
+        for auth in auth_settings:
+            auth_setting = config.auth_settings().get(auth)
+            if auth_setting:
+                if not auth_setting['value']:
+                    continue
+                elif auth_setting['in'] == 'header':
+                    headers[auth_setting['key']] = auth_setting['value']
+                elif auth_setting['in'] == 'query':
+                    querys[auth_setting['key']] = auth_setting['value']
+                else:
+                    raise ValueError(
+                        'Authentication token must be in `query` or `header`'
+                    )
+
+    def __deserialize_file(self, response):
+        """
+        Saves response body into a file in a temporary folder,
+        using the filename from the `Content-Disposition` header if provided.
+
+        :param response:  RESTResponse.
+        :return: file path.
         """
-        Helper process to compute necessary statistics for computing the global
-        average. Wrapper function calls respective function to derive statistics
-        based on the defined loss function.
-        :return: Returns y_hat * n and n.
-        :rtype: (`np.array`, `np.array`)
-        """
-        logger.info('Compute Local Party Average Statistics')
-
-        # Compute Average Depending on the Value
-        if (self.n_trees == 1) or self.loss == 'binary_crossentropy':
-            return (np.array([np.average(self.y_train, weights=self.sample_weight)]), \
-                    np.array([len(self.y_train)]))
-        elif self.loss == 'categorical_crossentropy':
-            average, counts = [], []
-            for k in range(self.num_classes):
-                average.append(np.average(self.y_train == k, \
-                                          weights=self.sample_weight))
-                counts.append(len(self.y_train == k))
-            return (np.array(average), np.array(counts))
+        config = Configuration()
+
+        fd, path = tempfile.mkstemp(dir=config.temp_folder_path)
+        os.close(fd)
+        os.remove(path)
+
+        content_disposition = response.getheader("Content-Disposition")
+        if content_disposition:
+            filename = re.\
+                search(r'filename=[\'"]?([^\'"\s]+)[\'"]?', content_disposition).\
+                group(1)
+            path = os.path.join(os.path.dirname(path), filename)
+
+        with open(path, "w") as f:
+            f.write(response.data)
+
+        return path
+
+    def __deserialize_primitive(self, data, klass):
+        """
+        Deserializes string to primitive type.
+
+        :param data: str.
+        :param klass: class literal.
+
+        :return: int, long, float, str, bool.
+        """
+        try:
+            value = klass(data)
+        except UnicodeEncodeError:
+            value = unicode(data)
+        except TypeError:
+            value = data
+        return value
+
+    def __deserialize_object(self, value):
+        """
+        Return a original value.
+
+        :return: object.
+        """
+        return value
+
+    def __deserialize_date(self, string):
+        """
+        Deserializes string to date.
+
+        :param string: str.
+        :return: date.
+        """
+        try:
+            from dateutil.parser import parse
+            return parse(string).date()
+        except ImportError:
+            return string
+        except ValueError:
+            raise ApiException(
+                status=0,
+                reason="Failed to parse `{0}` into a date object"
+                .format(string)
+            )
+
+    def __deserialize_datatime(self, string):
+        """
+        Deserializes string to datetime.
+
+        The string should be in iso8601 datetime format.
+
+        :param string: str.
+        :return: datetime.
+        """
+        try:
+            from dateutil.parser import parse
+            return parse(string)
+        except ImportError:
+            return string
+        except ValueError:
+            raise ApiException(
+                status=0,
+                reason="Failed to parse `{0}` into a datetime object".
+                format(string)
+            )
+
+    def __deserialize_model(self, data, klass):
+        """
+        Deserializes list or dict to model.
+
+        :param data: dict, list.
+        :param klass: class literal.
+        :return: model object.
+        """
+        instance = klass()
+
+        for attr, attr_type in iteritems(instance.swagger_types):
+            if data is not None \
+               and instance.attribute_map[attr] in data\
+               and isinstance(data, (list, dict)):
+                value = data[instance.attribute_map[attr]]
+                setattr(instance, attr, self.__deserialize(value, attr_type))
+
+        return instance
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/util/fl_metrics.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/fl_metrics.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,21 +1,31 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
 import logging
 
 import numpy as np
-from sklearn.metrics import roc_auc_score, average_precision_score, log_loss
-from sklearn.metrics import confusion_matrix, f1_score, precision_score, \
-    recall_score, classification_report
-from sklearn.metrics import explained_variance_score, mean_squared_error, \
-    mean_squared_log_error, median_absolute_error, median_absolute_error, \
-    mean_absolute_error, r2_score
+from sklearn.metrics import (
+    average_precision_score,
+    classification_report,
+    confusion_matrix,
+    explained_variance_score,
+    f1_score,
+    log_loss,
+    mean_absolute_error,
+    mean_squared_error,
+    mean_squared_log_error,
+    median_absolute_error,
+    precision_score,
+    r2_score,
+    recall_score,
+    roc_auc_score,
+)
 from sklearn.utils.multiclass import unique_labels
 
 logger = logging.getLogger(__name__)
 
 
 def _f1_precision_recall(y_true, y_pred):
     """Compute the f1, precision, recall based on the average method. \
@@ -28,27 +38,23 @@
     :type average: `str`
     :return: f1, precision, recall or array of float
     :rtype: `float`, `float`, `float`
     """
     metrics = {}
     round_digits = 2
     try:
-        metrics['f1'] = round(
-            f1_score(y_true, y_pred, zero_division=0), round_digits)
-        metrics['precision'] = round(
-            precision_score(y_true, y_pred, zero_division=0), round_digits)
-        metrics['recall'] = round(recall_score(
-            y_true, y_pred, zero_division=0), round_digits)
+        metrics["f1"] = round(f1_score(y_true, y_pred, zero_division=0), round_digits)
+        metrics["precision"] = round(precision_score(y_true, y_pred, zero_division=0), round_digits)
+        metrics["recall"] = round(recall_score(y_true, y_pred, zero_division=0), round_digits)
 
     except ValueError as ve:
         logger.exception(ve)
-        logger.info(
-            "Error occurred while collecting binary classification metircs.")
+        logger.info("Error occurred while collecting binary classification metircs.")
         labels = unique_labels(y_true, y_pred)
-        if(len(labels) > 2):
+        if len(labels) > 2:
             logger.info(len(labels) + " labels given, expected number 2")
 
     except Exception as ex:
         logger.exception(ex)
 
     return metrics
 
@@ -66,27 +72,23 @@
     :rtype: `dict`
     """
     metrics = {}
     round_digits = 2
     metrics = _f1_precision_recall(y_true, y_pred)
     try:
         labels = unique_labels(y_true)
-        metrics['average precision'] = round(
-            average_precision_score(y_true, y_pred), round_digits)
+        metrics["average precision"] = round(average_precision_score(y_true, y_pred), round_digits)
         if len(labels) > 1:
-            metrics['roc auc'] = round(
-                roc_auc_score(y_true, y_pred), round_digits)
-            metrics['negative log loss'] = round(
-                log_loss(y_true, y_pred), round_digits)
+            metrics["roc auc"] = round(roc_auc_score(y_true, y_pred), round_digits)
+            metrics["negative log loss"] = round(log_loss(y_true, y_pred), round_digits)
     except ValueError as ve:
         logger.exception(ve)
-        logger.info(
-            "Error occurred while collecting binary classification metircs.")
+        logger.info("Error occurred while collecting binary classification metircs.")
         labels = unique_labels(y_true, y_pred)
-        if(len(labels) > 2):
+        if len(labels) > 2:
             logger.error(len(labels) + " labels given, expected number 2")
 
     except Exception as ex:
         logger.exception(ex)
 
     # for metric in eval_metrics:
     # process each requested metric and send back the results.
@@ -104,25 +106,23 @@
     :param metrics: metrics requested by the user
     :type metrics: list of metrics which needs to be sent back.
     :return: dictionary with metrics
     :rtype: `dict`
     """
     metrics = {}
     round_digits = 2
-    multilabel_average_options = ['micro', 'macro', 'weighted']
+    multilabel_average_options = ["micro", "macro", "weighted"]
 
     for avg in multilabel_average_options:
-
         try:
-            metrics['f1 ' +
-                    avg] = round(f1_score(y_true, y_pred, average=avg, zero_division=0), round_digits)
-            metrics['precision ' +
-                    avg] = round(precision_score(y_true, y_pred, average=avg, zero_division=0), round_digits)
-            metrics['recall ' +
-                    avg] = round(recall_score(y_true, y_pred, average=avg, zero_division=0), round_digits)
+            metrics["f1 " + avg] = round(f1_score(y_true, y_pred, average=avg, zero_division=0), round_digits)
+            metrics["precision " + avg] = round(
+                precision_score(y_true, y_pred, average=avg, zero_division=0), round_digits
+            )
+            metrics["recall " + avg] = round(recall_score(y_true, y_pred, average=avg, zero_division=0), round_digits)
         except Exception as ex:
             logger.exception(ex)
 
     return metrics
 
 
 def get_eval_metrics_for_classificaton(y_true, y_pred, eval_metrics={}):
@@ -147,19 +147,17 @@
             logger.info("reshaping y_pred")
             y_pred = list(np.argmax(y_pred, axis=-1))
 
         # TODO: handle mix types labels {'multilabel-indicator', 'multiclass'}
         labels = unique_labels(y_true, y_pred)
 
         if len(labels) <= 2:
-            metrics = get_binary_classification_metrics(
-                y_true, y_pred, eval_metrics)
+            metrics = get_binary_classification_metrics(y_true, y_pred, eval_metrics)
         else:
-            metrics = get_multi_label_classification_metrics(
-                y_true, y_pred, metrics)
+            metrics = get_multi_label_classification_metrics(y_true, y_pred, metrics)
 
     except Exception as ex:
         logger.exception(ex)
         logger.exception("Error occurred while evaluating metrics")
 
     return metrics
 
@@ -176,27 +174,20 @@
     :return: dictionary with metrics
     :rtype: `dict`
     """
 
     metrics = {}
 
     try:
-        metrics['Negative root mean squared error'] = round(mean_squared_error(
-            y_true, y_pred, squared=True), 2)
-        metrics['Negative mean absolute error'] = round(mean_absolute_error(
-            y_true, y_pred), 2)
-        metrics['Negative root mean squared log error'] = round(mean_squared_log_error(
-            y_true, y_pred), 2)
-        metrics['Explained Variance'] = round(
-            explained_variance_score(y_true, y_pred), 2)
-        metrics['Negative mean squared error'] = round(mean_squared_error(
-            y_true, y_pred, squared=False), 2)
-        metrics['Negative mean squared log error'] = round(np.sqrt(mean_squared_log_error(
-            y_true, y_pred)), 2)
-        metrics['Negative median absolute error'] = round(median_absolute_error(
-            y_true, y_pred), 2)
-        metrics['R2'] = round(r2_score(y_true, y_pred), 2)
+        metrics["Negative root mean squared error"] = round(mean_squared_error(y_true, y_pred, squared=False), 2)
+        metrics["Negative mean absolute error"] = round(mean_absolute_error(y_true, y_pred), 2)
+        metrics["Negative root mean squared log error"] = round(mean_squared_log_error(y_true, y_pred), 2)
+        metrics["Explained Variance"] = round(explained_variance_score(y_true, y_pred), 2)
+        metrics["Negative mean squared error"] = round(mean_squared_error(y_true, y_pred, squared=True), 2)
+        metrics["Negative mean squared log error"] = round(np.sqrt(mean_squared_log_error(y_true, y_pred)), 2)
+        metrics["Negative median absolute error"] = round(median_absolute_error(y_true, y_pred), 2)
+        metrics["R2"] = round(r2_score(y_true, y_pred), 2)
     except Exception as ex:
         logger.error("Exception occurred while calculating metrics.")
         logger.error(ex)
 
     return metrics
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/util/log_config.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/log_config.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 #  -----------------------------------------------------------------------------------------
-#  (C) Copyright IBM Corp. 2023-2024.
+#  (C) Copyright IBM Corp. 2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
-import os
 import logging
 import logging.config
+import os
 import time
 
 from ibmfl._version import __version__
 from ibmfl.exceptions import InvalidConfigurationException
 
 logger = logging.getLogger(__name__)
 
@@ -27,52 +27,42 @@
         else:
             t = time.strftime("%Y-%m-%dT%H:%M:%S.%F", ct)
             s = "%s.%03dZ" % (t, record.msecs)
         return s
 
 
 class FLVersionFilter(logging.Filter):
-
     def filter(self, record):
         record.version = __version__
         return True
 
 
-def configure_file_logging(filename, log_level='INFO'):
+def configure_file_logging(filename, log_level="INFO"):
     log_level = logging.getLevelName(log_level)
 
     dict_config = {
         "version": 1,
         "disable_existing_loggers": False,
         "formatters": {
             "fl_std": {
                 "()": UTCFormatter,
                 "format": "%(asctime)s | %(version)s | %(levelname)s | %(name)-50s | %(message)s",
-                "datefmt": "%Y-%m-%dT%H:%M:%S.%FZ"
+                "datefmt": "%Y-%m-%dT%H:%M:%S.%FZ",
             }
         },
         "handlers": {
             "WriteFile": {
                 "class": "logging.FileHandler",
-                "filters": ['version_filter'],
+                "filters": ["version_filter"],
                 "level": "DEBUG",
                 "formatter": "fl_std",
-                "filename": filename
+                "filename": filename,
             }
-
         },
-        "loggers": {
-            "ibmfl": {
-                "level": log_level,
-                "handlers": [
-                    "WriteFile"
-                ],
-                "propagate": False
-            }
-        }
+        "loggers": {"ibmfl": {"level": log_level, "handlers": ["WriteFile"], "propagate": False}},
     }
 
     result = configure_logging(dict_config, log_level)
     return result
 
 
 def configure_logging(config, log_level=logging.INFO):
@@ -91,63 +81,46 @@
     dict_config = {
         "version": 1,
         "disable_existing_loggers": False,
         "formatters": {
             "fl_std": {
                 "()": UTCFormatter,
                 "format": "%(asctime)s | %(version)s | %(levelname)s | %(name)-50s | %(message)s",
-                "datefmt": "%Y-%m-%dT%H:%M:%S.%FZ"
+                "datefmt": "%Y-%m-%dT%H:%M:%S.%FZ",
             }
         },
         "handlers": {
             "console": {
                 "class": "logging.StreamHandler",
-                "filters": ['version_filter'],
+                "filters": ["version_filter"],
                 "level": log_level,
                 "formatter": "fl_std",
-                "stream": "ext://sys.stdout"
-            }
-
-        },
-        "loggers": {
-            "ibmfl": {
-                "level": log_level,
-                "handlers": [
-                    "console"
-                ],
-                "propagate": False
+                "stream": "ext://sys.stdout",
             }
         },
-        "root": {
-            "handlers": [
-                "console"
-            ]
-        }
+        "loggers": {"ibmfl": {"level": log_level, "handlers": ["console"], "propagate": False}},
+        "root": {"level": log_level, "handlers": ["console"]},
     }
     if not config:
         config = dict_config
 
-    log_filters = {
-        'version_filter': {
-            '()': FLVersionFilter
-        }
-    }
-    if 'filters' in config:
-        config['filters']['version_filter'] = log_filters['version_filter']
+    log_filters = {"version_filter": {"()": FLVersionFilter}}
+    if "filters" in config:
+        config["filters"]["version_filter"] = log_filters["version_filter"]
     else:
-        config['filters'] = log_filters
+        config["filters"] = log_filters
 
     add_version_filter(config)
     result = False
     if config:
         try:
             logging.config.dictConfig(config)
             result = True
         except InvalidConfigurationException as ice:
-            logging.error('Failed to load log configuration %s', ice)
+            logging.error("Failed to load log configuration %s", ice)
             configure_basic_logging(log_level=log_level)
     else:
         configure_basic_logging(log_level=log_level)
 
     return result
 
 
@@ -155,38 +128,36 @@
     """
     Add versioning filter to log config if not provided
 
     :param config: yaml file containing the definitions of formatter and handler
     :type config: `dict`
     :return: None
     """
-    filters = ['version_filter']
-    if 'handlers' in config:
-
-        handlers = config['handlers']
+    filters = ["version_filter"]
+    if "handlers" in config:
+        handlers = config["handlers"]
 
         for key in handlers:
             handler = handlers.get(key)
 
-            if 'filters' in handler:
-                filt = handler.get('filters')
-                if 'version_filter' not in filt:
-                    filt.append('version_filter')
+            if "filters" in handler:
+                filt = handler.get("filters")
+                if "version_filter" not in filt:
+                    filt.append("version_filter")
             else:
-                handler['filters'] = filters
+                handler["filters"] = filters
 
 
 def configure_basic_logging(log_level=logging.INFO):
     """
     configures logging for the session based on pre defined format.
     default logging will be done on console.
 
     :param level: should be a value from [DEBUG, INFO, WARNING, ERROR, CRITICAL]
             based on the required granularity
     :type log_level: `int`
     :return: None
     """
 
-    logging.basicConfig(level=log_level,
-                        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
+    logging.basicConfig(level=log_level, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")
 
     # logging.config.dictConfig(default_log_config)
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/TestRepoSaveLoad.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/TestRepoSaveLoad.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/__init__.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/__init__.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/base_constants.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/base_constants.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/ml_api_client.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/ml_api_client.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/ml_authorization.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/ml_authorization.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepository/__init__.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepository/__init__.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepository/artifact.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepository/artifact.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepository/artifact_reader.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepository/artifact_reader.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepository/generic_archive_model_artifact.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepository/generic_archive_model_artifact.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepository/generic_archive_pipeline_artifact.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepository/generic_archive_pipeline_artifact.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepository/hybrid_model_artifact.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepository/hybrid_model_artifact.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepository/meta_names.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepository/meta_names.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepository/model_artifact.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepository/model_artifact.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepository/pipeline_artifact.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepository/pipeline_artifact.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepository/scikit_model_artifact.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepository/scikit_model_artifact.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepository/tensorflow_model_artifact.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepository/tensorflow_model_artifact.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepository/wml_experiment_artifact.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepository/wml_experiment_artifact.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepository/wml_function_artifact.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepository/wml_function_artifact.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepository/wml_libraries_artifact.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepository/wml_libraries_artifact.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepository/wml_runtimes_artifact.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepository/wml_runtimes_artifact.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepository/xgboost_model_artifact.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepository/xgboost_model_artifact.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/__init__.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/__init__.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/content_loaders.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/content_loaders.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/experiment_artifact.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/experiment_artifact.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/function_artifact.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/function_artifact.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/function_artifact_loader.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/function_artifact_loader.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/function_artifact_reader.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/function_artifact_reader.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/function_loader.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/function_loader.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/generic_archive_pipeline_model_artifact.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/generic_archive_pipeline_model_artifact.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/generic_file_artifact_loader.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/generic_file_artifact_loader.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/generic_file_pipeline_model_loader.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/generic_file_pipeline_model_loader.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/generic_file_reader.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/generic_file_reader.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/hybrid_artifact_loader.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/hybrid_artifact_loader.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/hybrid_pipeline_model_artifact.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/hybrid_pipeline_model_artifact.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/hybrid_pipeline_model_loader.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/hybrid_pipeline_model_loader.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/hybrid_pipeline_model_reader.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/hybrid_pipeline_model_reader.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/libraries_artifact.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/libraries_artifact.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/libraries_artifact_loader.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/libraries_artifact_loader.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/libraries_artifact_reader.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/libraries_artifact_reader.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/libraries_loader.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/libraries_loader.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/ml_repository_artifact.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/ml_repository_artifact.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/runtimes_artifact.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/runtimes_artifact.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/runtimes_artifact_loader.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/runtimes_artifact_loader.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/runtimes_artifact_reader.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/runtimes_artifact_reader.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/scikit_artifact_loader.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/scikit_artifact_loader.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/scikit_pipeline_model_artifact.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/scikit_pipeline_model_artifact.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/scikit_pipeline_model_loader.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/scikit_pipeline_model_loader.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/scikit_pipeline_reader.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/scikit_pipeline_reader.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/spark_artifact_loader.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/spark_artifact_loader.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/spark_pipeline_artifact.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/spark_pipeline_artifact.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/spark_pipeline_loader.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/spark_pipeline_loader.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/spark_pipeline_model_artifact.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/spark_pipeline_model_artifact.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/spark_pipeline_model_loader.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/spark_pipeline_model_loader.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/spark_pipeline_reader.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/spark_pipeline_reader.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/spark_version.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/spark_version.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/tensorflow_artifact_loader.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/tensorflow_artifact_loader.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/tensorflow_pipeline_model_artifact.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/tensorflow_pipeline_model_artifact.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/tensorflow_pipeline_model_loader.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/tensorflow_pipeline_model_loader.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/tensorflow_pipeline_reader.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/tensorflow_pipeline_reader.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/version_helper.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/version_helper.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/xgboost_model_reader.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryartifact/xgboost_model_reader.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryclient/__init__.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryclient/__init__.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryclient/content_reader.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryclient/content_reader.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryclient/experiment_adapter.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryclient/experiment_adapter.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryclient/experiment_collection.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryclient/experiment_collection.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryclient/function_adapter.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryclient/function_adapter.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryclient/function_collection.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryclient/function_collection.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryclient/libraries_adapter.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryclient/libraries_adapter.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryclient/libraries_collection.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryclient/libraries_collection.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryclient/ml_repository_api.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryclient/ml_repository_api.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryclient/ml_repository_client.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryclient/ml_repository_client.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryclient/model_adapter.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryclient/model_adapter.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryclient/model_collection.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryclient/model_collection.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryclient/runtimes_adapter.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryclient/runtimes_adapter.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryclient/runtimes_collection.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryclient/runtimes_collection.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryclient/wml_experiment_adapter.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryclient/wml_experiment_adapter.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/mlrepositoryclient/wml_experiment_collection.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/mlrepositoryclient/wml_experiment_collection.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/__init__.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/__init__.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/apis/repository_api.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/apis/repository_api.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/apis/token_api.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/apis/token_api.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/configuration.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/configuration.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/__init__.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/__init__.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/array_data_input_repository.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/array_data_input_repository.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/array_model_metrics_output.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/array_model_metrics_output.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/array_model_output.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/array_model_output.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/array_model_version_metrics_experiments.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/array_model_version_metrics_experiments.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/array_model_version_output.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/array_model_version_output.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/array_pipeline_output.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/array_pipeline_output.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/array_pipeline_version_output.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/array_pipeline_version_output.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/array_training_output_experiments.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/array_training_output_experiments.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/artifact_author.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/artifact_author.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/artifact_version_metadata.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/artifact_version_metadata.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/artifact_version_repository.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/artifact_version_repository.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/artifact_version_short_metadata.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/artifact_version_short_metadata.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/author_experiments.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/author_experiments.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/author_repository.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/author_repository.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/batch_deploy_output.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/batch_deploy_output.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/batch_deploy_output_entity.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/batch_deploy_output_entity.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/batch_deploy_output_entity_execution.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/batch_deploy_output_entity_execution.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/batch_deploy_output_meta.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/batch_deploy_output_meta.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/batch_input.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/batch_input.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/batch_output.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/batch_output.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/batch_output_array.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/batch_output_array.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/cols_output.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/cols_output.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/compute_configuration_experiments.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/compute_configuration_experiments.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/connection.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/connection.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/connection_object_source_experiments.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/connection_object_source_experiments.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/connection_object_target_experiments.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/connection_object_target_experiments.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/connection_object_with_name_repository.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/connection_object_with_name_repository.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/content_location.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/content_location.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/content_status.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/content_status.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/custom_models.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/custom_models.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/deploy_input.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/deploy_input.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/error_bad_request_libraries_target.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/error_bad_request_libraries_target.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/error_experiments.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/error_experiments.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/error_experiments_target.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/error_experiments_target.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/error_message.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/error_message.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/error_repository.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/error_repository.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/error_repository_target.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/error_repository_target.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/error_schema_experiments.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/error_schema_experiments.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/error_schema_repository.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/error_schema_repository.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/evaluation_definition.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/evaluation_definition.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/evaluation_definition_experiments.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/evaluation_definition_experiments.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/evaluation_definition_metrics.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/evaluation_definition_metrics.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/evaluation_definition_repository.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/evaluation_definition_repository.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/evaluation_definition_repository_metrics.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/evaluation_definition_repository_metrics.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/experiment_input.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/experiment_input.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/experiment_input_settings.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/experiment_input_settings.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/experiment_output.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/experiment_output.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/experiment_output_array.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/experiment_output_array.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/experiment_output_array_first.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/experiment_output_array_first.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/experiment_patch.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/experiment_patch.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/experiment_status_experiments.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/experiment_status_experiments.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/framework_output_repository.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/framework_output_repository.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/framework_output_repository_libraries.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/framework_output_repository_libraries.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/framework_output_repository_runtimes.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/framework_output_repository_runtimes.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters_experiments.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters_experiments.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters_experiments_double_range.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters_experiments_double_range.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters_experiments_inner_double_range.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters_experiments_inner_double_range.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters_experiments_inner_int_range.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters_experiments_inner_int_range.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters_experiments_inner_values_range.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters_experiments_inner_values_range.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters_experiments_int_range.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters_experiments_int_range.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters_for_status_experiments.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters_for_status_experiments.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters_for_status_experiments_inner.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters_for_status_experiments_inner.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters_optimization_experiments.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters_optimization_experiments.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters_optimization_experiments_method.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters_optimization_experiments_method.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters_optimization_experiments_method_parameters.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/hyper_parameters_optimization_experiments_method_parameters.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/input_data_schema.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/input_data_schema.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/internal_input_batch.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/internal_input_batch.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/internal_output_batch.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/internal_output_batch.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/json_patch_array.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/json_patch_array.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/json_patch_entity.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/json_patch_entity.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/libraries_definition_input.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/libraries_definition_input.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/libraries_definition_input_platform.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/libraries_definition_input_platform.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/meta_object.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/meta_object.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/meta_object_experiments.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/meta_object_experiments.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/meta_object_experiments_metadata.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/meta_object_experiments_metadata.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/meta_object_functions_metadata.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/meta_object_functions_metadata.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/meta_object_metadata.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/meta_object_metadata.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/meta_object_repository.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/meta_object_repository.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/meta_object_repository_metadata.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/meta_object_repository_metadata.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/metric_object_experiments.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/metric_object_experiments.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/metrics_models.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/metrics_models.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_experiment_input.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_experiment_input.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_experiment_output.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_experiment_output.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_experiment_output_array.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_experiment_output_array.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_function_input.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_function_input.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_functions_output.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_functions_output.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_functions_output_array.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_functions_output_array.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_functions_output_array_first.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_functions_output_array_first.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_libraries_output.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_libraries_output.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_libraries_output_array.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_libraries_output_array.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_libraries_output_array_first.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_libraries_output_array_first.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_model_input.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_model_input.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_model_output.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_model_output.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_model_output_array.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_model_output_array.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_patch_libraries_output.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_patch_libraries_output.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_runtime_spec_output.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_runtime_spec_output.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_runtime_spec_output_array.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_runtime_spec_output_array.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_runtime_spec_output_array_first.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_create_runtime_spec_output_array_first.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_get_presigned_url_output.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_get_presigned_url_output.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_model_size_output.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_model_size_output.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_patch_function.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_patch_function.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_patch_libraries_input.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_patch_libraries_input.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_patch_runtime_spec_input.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_patch_runtime_spec_input.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_patch_runtime_spec_output.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_patch_runtime_spec_output.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_upload_content_output.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_upload_content_output.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_upload_content_output_entity.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/output_data_schema.py`

 * *Files 18% similar despite different names*

```diff
@@ -3,61 +3,38 @@
 #  -----------------------------------------------------------------------------------------
 #  (C) Copyright IBM Corp. 2023-2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
 from pprint import pformat
 from six import iteritems
+import re
 
 
-class MlAssetsUploadContentOutputEntity(object):
+class OutputDataSchema(object):
     """
     NOTE: This class is auto generated by the swagger code generator program.
     Do not edit the class manually.
     """
     def __init__(self):
         """
-        MlAssetsUploadContentOutputEntity - a model defined in Swagger
+        outputDataSchema - a model defined in Swagger
 
         :param dict swaggerTypes: The key is attribute name
                                   and the value is attribute type.
         :param dict attributeMap: The key is attribute name
                                   and the value is json key in definition.
         """
         self.swagger_types = {
-            'status_message': 'str'
-        }
 
-        self.attribute_map = {
-            'status_message': 'status_message'
         }
 
-        self._status_message = None
-
-    @property
-    def status_message(self):
-        """
-        Gets the status_message of this MlAssetsUploadContentOutputEntity.
-
-
-        :return: The status_message of this MlAssetsUploadContentOutputEntity.
-        :rtype: str
-        """
-        return self._status_message
-
-    @status_message.setter
-    def status_message(self, status_message):
-        """
-        Sets the status_message of this MlAssetsUploadContentOutputEntity.
-
+        self.attribute_map = {
 
-        :param status_message: The status_message of this MlAssetsUploadContentOutputEntity.
-        :type: str
-        """
-        self._status_message = status_message
+        }
 
     def to_dict(self):
         """
         Returns the model properties as a dict
         """
         result = {}
 
@@ -100,8 +77,7 @@
         return self.__dict__ == other.__dict__
 
     def __ne__(self, other):
         """
         Returns true if both objects are not equal
         """
         return not self == other
-
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_upload_content_output_metadata.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/ml_assets_upload_content_output_metadata.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/model_content_location.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/model_content_location.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/model_definition_models.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/model_definition_models.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/model_input.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/model_input.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/model_metrics.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/model_metrics.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/model_metrics_values.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/model_metrics_values.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/model_output.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/model_output.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/model_output_entity.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/model_output_entity.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/model_output_entity_pipeline_version.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/model_output_entity_pipeline_version.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/model_schemas.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/model_schemas.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/model_training_data_ref.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/model_training_data_ref.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/model_type.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/model_type.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/model_version_input.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/model_version_input.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/model_version_metrics_experiments.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/model_version_metrics_experiments.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/model_version_output.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/model_version_output.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/model_version_output_entity.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/model_version_output_entity.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/model_version_output_entity_model.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/model_version_output_entity_model.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/online_deploy_output.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/online_deploy_output.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/online_output.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/online_output.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/online_output_array.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/online_output_array.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/output_data_schema.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/training_data_schema.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,41 +1,42 @@
 # coding: utf-8
 
+
 #  -----------------------------------------------------------------------------------------
 #  (C) Copyright IBM Corp. 2023-2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
 from pprint import pformat
 from six import iteritems
-import re
 
 
-class OutputDataSchema(object):
+class TrainingDataSchema(object):
     """
     NOTE: This class is auto generated by the swagger code generator program.
     Do not edit the class manually.
     """
     def __init__(self):
         """
-        outputDataSchema - a model defined in Swagger
+        TrainingDataSchema - a model defined in Swagger
 
         :param dict swaggerTypes: The key is attribute name
                                   and the value is attribute type.
         :param dict attributeMap: The key is attribute name
                                   and the value is json key in definition.
         """
         self.swagger_types = {
-
+            
         }
 
         self.attribute_map = {
-
+            
         }
 
+
     def to_dict(self):
         """
         Returns the model properties as a dict
         """
         result = {}
 
         for attr, _ in iteritems(self.swagger_types):
@@ -77,7 +78,8 @@
         return self.__dict__ == other.__dict__
 
     def __ne__(self, other):
         """
         Returns true if both objects are not equal
         """
         return not self == other
+
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/patch_operation_experiments.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/patch_operation_experiments.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/patch_operation_functions.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/patch_operation_functions.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/patch_operation_libraries.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/patch_operation_libraries.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/patch_operation_runtime_spec.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/patch_operation_runtime_spec.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/pipeline_input.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/pipeline_input.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/pipeline_models.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/pipeline_models.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/pipeline_output.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/pipeline_output.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/pipeline_output_entity.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/pipeline_output_entity.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/pipeline_type.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/pipeline_type.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/pipeline_version_input.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/pipeline_version_input.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/pipeline_version_output.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/pipeline_version_output.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/pipeline_version_output_entity.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/pipeline_version_output_entity.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/pipeline_version_output_entity_parent.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/pipeline_version_output_entity_parent.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/runtime_environment.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/runtime_environment.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/runtime_models.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/runtime_models.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/runtime_output_repository.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/runtime_output_repository.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/runtime_spec_definition_input.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/runtime_spec_definition_input.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/runtime_spec_definition_input_custom_libraries.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/runtime_spec_definition_input_custom_libraries.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/runtime_spec_definition_input_platform.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/runtime_spec_definition_input_platform.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/runtime_spec_definition_input_public_libraries.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/runtime_spec_definition_input_public_libraries.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/runtime_spec_definition_input_repository.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/runtime_spec_definition_input_repository.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/sample_scoring_input.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/sample_scoring_input.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/schemas.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/schemas.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/score_input.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/score_input.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/score_output.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/score_output.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/size_models.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/size_models.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/software_spec_models.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/software_spec_models.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/space_models.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/space_models.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/spark_service.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/spark_service.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/stream_input_internal.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/stream_input_internal.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/stream_internal.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/stream_internal.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/stream_output.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/stream_output.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/stream_output_array.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/stream_output_array.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/stream_output_internal.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/stream_output_internal.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/tag_repository.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/tag_repository.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/token_response.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/token_response.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/training_data_schema.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/training_models.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,41 +1,65 @@
 # coding: utf-8
 
-
 #  -----------------------------------------------------------------------------------------
 #  (C) Copyright IBM Corp. 2023-2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
 from pprint import pformat
 from six import iteritems
+import re
 
 
-class TrainingDataSchema(object):
+class TrainingModels(object):
     """
     NOTE: This class is auto generated by the swagger code generator program.
     Do not edit the class manually.
     """
-    def __init__(self):
+    def __init__(self, href=None):
         """
-        TrainingDataSchema - a model defined in Swagger
+        TrainingModels - a model defined in Swagger
 
-        :param dict swaggerTypes: The key is attribute name
+        :param dict swaggerTypes: The key is attribute href
                                   and the value is attribute type.
-        :param dict attributeMap: The key is attribute name
+        :param dict attributeMap: The key is attribute href
                                   and the value is json key in definition.
         """
         self.swagger_types = {
-            
+            'href': 'str'
         }
 
         self.attribute_map = {
-            
+            'href': 'href'
         }
 
+        self._href = href
+
+    @property
+    def href(self):
+        """
+        Gets the href of this TrainingModels.
+
+
+        :return: The href of this TrainingModels.
+        :rtype: str
+        """
+        return self._href
+
+    @href.setter
+    def href(self, href):
+        """
+        Sets the href of this TrainingModels.
+
+
+        :param href: The href of this TrainingModels.
+        :type: str
+        """
+
+        self._href = href
 
     def to_dict(self):
         """
         Returns the model properties as a dict
         """
         result = {}
 
@@ -78,8 +102,7 @@
         return self.__dict__ == other.__dict__
 
     def __ne__(self, other):
         """
         Returns true if both objects are not equal
         """
         return not self == other
-
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/training_output_experiments.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/training_output_experiments.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/training_reference_experiments.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/training_reference_experiments.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/training_status_experiments.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/training_status_experiments.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/models/training_status_experiments_result.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/models/training_status_experiments_result.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/swagger_client/rest.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/swagger_client/rest.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/util/base_singleton.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/util/base_singleton.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/util/compression_util.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/util/compression_util.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/util/file_system_ops.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/util/file_system_ops.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/util/generic_archive_file_check.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/util/generic_archive_file_check.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/util/json_2_object_mapper.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/util/json_2_object_mapper.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/util/library_imports.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/util/library_imports.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/libs/repo/util/spark_util.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/libs/repo/util/spark_util.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/lifecycle.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/lifecycle.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/messages/messages.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/messages/messages.py`

 * *Files 0% similar despite different names*

```diff
@@ -51,15 +51,14 @@
             message = message.format(*varss)
         else:
             message = message % tuple(varss)
     return message
 
 
 class Messages:
-
     @classmethod
     def get_message(cls, *args: Any, message_id: str) -> str:
         message = MESSAGE_DICT.get(message_id)
 
         if args and message:
             message = replace_args_in_message(message, *args)
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/messages/messages_en.json` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/messages/messages_en.json`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/metanames.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/metanames.py`

 * *Files 0% similar despite different names*

```diff
@@ -2563,14 +2563,15 @@
     REPETITION_PENALTY = "repetition_penalty"
     MIN_NEW_TOKENS = "min_new_tokens"
     MAX_NEW_TOKENS = "max_new_tokens"
     STOP_SEQUENCES = "stop_sequences"
     TIME_LIMIT = " time_limit"
     TRUNCATE_INPUT_TOKENS = "truncate_input_tokens"
     RETURN_OPTIONS = "return_options"
+    PROMPT_VARIABLES = "prompt_variables"
 
     _meta_props_definitions = [
         MetaProp("DECODING_METHOD", DECODING_METHOD, str, False, "sample"),
         MetaProp(
             "LENGTH_PENALTY",
             LENGTH_PENALTY,
             dict,
@@ -2584,14 +2585,17 @@
         MetaProp("REPETITION_PENALTY", REPETITION_PENALTY, float, False, 2),
         MetaProp("MIN_NEW_TOKENS", MIN_NEW_TOKENS, int, False, 50),
         MetaProp("MAX_NEW_TOKENS", MAX_NEW_TOKENS, int, False, 200),
         MetaProp("STOP_SEQUENCES", STOP_SEQUENCES, list, False, ["fail"]),
         MetaProp("TIME_LIMIT", TIME_LIMIT, int, False, 600000),
         MetaProp("TRUNCATE_INPUT_TOKENS", TRUNCATE_INPUT_TOKENS, int, False, 200),
         MetaProp(
+            "PROMPT_VARIABLES", PROMPT_VARIABLES, dict, False, {"object": "brain"}
+        ),
+        MetaProp(
             "RETURN_OPTIONS",
             RETURN_OPTIONS,
             dict,
             False,
             {
                 "input_text": True,
                 "generated_tokens": True,
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/model_definition.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/model_definition.py`

 * *Files 1% similar despite different names*

```diff
@@ -17,27 +17,30 @@
 
 if TYPE_CHECKING:
     from ibm_watsonx_ai import APIClient
     import pandas
 
 _DEFAULT_LIST_LENGTH = 50
 
+
 class ModelDefinition(WMLResource):
     """Store and manage model definitions."""
 
     ConfigurationMetaNames = ModelDefinitionMetaNames()
 
     """MetaNames for model definition creation."""
 
     def __init__(self, client: APIClient) -> None:
         WMLResource.__init__(self, __name__, client)
         self._ICP_PLATFORM_SPACES = client.ICP_PLATFORM_SPACES
         self.default_space_id = client.default_space_id
 
-    def _generate_model_definition_document(self, meta_props: dict[str, str | dict]) -> dict:
+    def _generate_model_definition_document(
+        self, meta_props: dict[str, str | dict]
+    ) -> dict:
         doc: dict = {
             "metadata": {
                 "name": "generated_name_" + str(uuid.uuid4()),
                 "tags": ["generated_tag_" + str(uuid.uuid4())],
                 "asset_type": "wml_model_definition",
                 "origin_country": "us",
                 "rov": {"mode": 0},
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/models.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/models.py`

 * *Files 0% similar despite different names*

```diff
@@ -4054,4734 +4054,4707 @@
 0000fd50: 696e 675f 726f 6c65 2229 203d 3d20 2274  ing_role") == "t
 0000fd60: 6172 6765 7422 0a20 2020 2020 2020 2020  arget".         
 0000fd70: 2020 2020 2020 2020 2020 205d 0a20 2020             ].   
 0000fd80: 2020 2020 2020 2020 2020 2020 2020 2020                  
 0000fd90: 2069 6620 7461 7267 6574 5f66 6965 6c64   if target_field
 0000fda0: 733a 0a20 2020 2020 2020 2020 2020 2020  s:.             
 0000fdb0: 2020 2020 2020 2020 2020 206d 6574 615f             meta_
-0000fdc0: 7072 6f70 735b 0a20 2020 2020 2020 2020  props[.         
-0000fdd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000fde0: 2020 2073 656c 662e 436f 6e66 6967 7572     self.Configur
-0000fdf0: 6174 696f 6e4d 6574 614e 616d 6573 2e4c  ationMetaNames.L
-0000fe00: 4142 454c 5f46 4945 4c44 0a20 2020 2020  ABEL_FIELD.     
-0000fe10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000fe20: 2020 205d 203d 2074 6172 6765 745f 6669     ] = target_fi
-0000fe30: 656c 6473 5b30 5d0a 0a20 2020 2020 2020  elds[0]..       
-0000fe40: 2020 2020 2020 2020 2064 6566 2069 735f           def is_
-0000fe50: 6669 656c 645f 6e6f 6e5f 6c61 6265 6c28  field_non_label(
-0000fe60: 663a 2064 6963 745b 7374 722c 2041 6e79  f: dict[str, Any
-0000fe70: 5d29 202d 3e20 626f 6f6c 3a0a 2020 2020  ]) -> bool:.    
+0000fdc0: 7072 6f70 735b 7365 6c66 2e43 6f6e 6669  props[self.Confi
+0000fdd0: 6775 7261 7469 6f6e 4d65 7461 4e61 6d65  gurationMetaName
+0000fde0: 732e 4c41 4245 4c5f 4649 454c 445d 203d  s.LABEL_FIELD] =
+0000fdf0: 2028 0a20 2020 2020 2020 2020 2020 2020   (.             
+0000fe00: 2020 2020 2020 2020 2020 2020 2020 2074                 t
+0000fe10: 6172 6765 745f 6669 656c 6473 5b30 5d0a  arget_fields[0].
+0000fe20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000fe30: 2020 2020 2020 2020 290a 0a20 2020 2020          )..     
+0000fe40: 2020 2020 2020 2020 2020 2064 6566 2069             def i
+0000fe50: 735f 6669 656c 645f 6e6f 6e5f 6c61 6265  s_field_non_labe
+0000fe60: 6c28 663a 2064 6963 745b 7374 722c 2041  l(f: dict[str, A
+0000fe70: 6e79 5d29 202d 3e20 626f 6f6c 3a0a 2020  ny]) -> bool:.  
 0000fe80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000fe90: 6966 206d 6574 615f 7072 6f70 732e 6765  if meta_props.ge
-0000fea0: 7428 7365 6c66 2e43 6f6e 6669 6775 7261  t(self.Configura
-0000feb0: 7469 6f6e 4d65 7461 4e61 6d65 732e 4c41  tionMetaNames.LA
-0000fec0: 4245 4c5f 4649 454c 4429 3a0a 2020 2020  BEL_FIELD):.    
+0000fe90: 2020 6966 206d 6574 615f 7072 6f70 732e    if meta_props.
+0000fea0: 6765 7428 7365 6c66 2e43 6f6e 6669 6775  get(self.Configu
+0000feb0: 7261 7469 6f6e 4d65 7461 4e61 6d65 732e  rationMetaNames.
+0000fec0: 4c41 4245 4c5f 4649 454c 4429 3a0a 2020  LABEL_FIELD):.  
 0000fed0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000fee0: 2020 2020 7265 7475 726e 2028 0a20 2020      return (.   
+0000fee0: 2020 2020 2020 7265 7475 726e 2028 0a20        return (. 
 0000fef0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000ff00: 2020 2020 2020 2020 2066 5b22 6e61 6d65           f["name
-0000ff10: 225d 0a20 2020 2020 2020 2020 2020 2020  "].             
-0000ff20: 2020 2020 2020 2020 2020 2020 2020 2021                 !
-0000ff30: 3d20 6d65 7461 5f70 726f 7073 5b73 656c  = meta_props[sel
-0000ff40: 662e 436f 6e66 6967 7572 6174 696f 6e4d  f.ConfigurationM
-0000ff50: 6574 614e 616d 6573 2e4c 4142 454c 5f46  etaNames.LABEL_F
-0000ff60: 4945 4c44 5d0a 2020 2020 2020 2020 2020  IELD].          
-0000ff70: 2020 2020 2020 2020 2020 2020 2020 290a                ).
-0000ff80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000ff90: 2020 2020 656c 7365 3a0a 2020 2020 2020      else:.      
+0000ff00: 2020 2020 2020 2020 2020 2066 5b22 6e61             f["na
+0000ff10: 6d65 225d 0a20 2020 2020 2020 2020 2020  me"].           
+0000ff20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000ff30: 2021 3d20 6d65 7461 5f70 726f 7073 5b73   != meta_props[s
+0000ff40: 656c 662e 436f 6e66 6967 7572 6174 696f  elf.Configuratio
+0000ff50: 6e4d 6574 614e 616d 6573 2e4c 4142 454c  nMetaNames.LABEL
+0000ff60: 5f46 4945 4c44 5d0a 2020 2020 2020 2020  _FIELD].        
+0000ff70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000ff80: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
+0000ff90: 2020 2020 2020 656c 7365 3a0a 2020 2020        else:.    
 0000ffa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000ffb0: 2020 7265 7475 726e 2054 7275 650a 0a20    return True.. 
-0000ffc0: 2020 2020 2020 2020 2020 2020 2020 2069                 i
-0000ffd0: 6620 6d65 7461 5f70 726f 7073 2e67 6574  f meta_props.get
-0000ffe0: 2873 656c 662e 436f 6e66 6967 7572 6174  (self.Configurat
-0000fff0: 696f 6e4d 6574 614e 616d 6573 2e4c 4142  ionMetaNames.LAB
-00010000: 454c 5f46 4945 4c44 293a 0a20 2020 2020  EL_FIELD):.     
-00010010: 2020 2020 2020 2020 2020 2020 2020 2069                 i
-00010020: 6e70 7574 5f64 6174 615f 7363 6865 6d61  nput_data_schema
-00010030: 203d 207b 0a20 2020 2020 2020 2020 2020   = {.           
-00010040: 2020 2020 2020 2020 2020 2020 2022 6669               "fi
-00010050: 656c 6473 223a 205b 0a20 2020 2020 2020  elds": [.       
+0000ffb0: 2020 2020 7265 7475 726e 2054 7275 650a      return True.
+0000ffc0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0000ffd0: 2069 6620 6d65 7461 5f70 726f 7073 2e67   if meta_props.g
+0000ffe0: 6574 2873 656c 662e 436f 6e66 6967 7572  et(self.Configur
+0000fff0: 6174 696f 6e4d 6574 614e 616d 6573 2e4c  ationMetaNames.L
+00010000: 4142 454c 5f46 4945 4c44 293a 0a20 2020  ABEL_FIELD):.   
+00010010: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010020: 2069 6e70 7574 5f64 6174 615f 7363 6865   input_data_sche
+00010030: 6d61 203d 207b 0a20 2020 2020 2020 2020  ma = {.         
+00010040: 2020 2020 2020 2020 2020 2020 2020 2022                 "
+00010050: 6669 656c 6473 223a 205b 0a20 2020 2020  fields": [.     
 00010060: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010070: 2020 2020 2066 0a20 2020 2020 2020 2020       f.         
+00010070: 2020 2020 2020 2066 0a20 2020 2020 2020         f.       
 00010080: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010090: 2020 2066 6f72 2066 2069 6e20 6d65 7461     for f in meta
-000100a0: 5f70 726f 7073 5b0a 2020 2020 2020 2020  _props[.        
+00010090: 2020 2020 2066 6f72 2066 2069 6e20 6d65       for f in me
+000100a0: 7461 5f70 726f 7073 5b0a 2020 2020 2020  ta_props[.      
 000100b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000100c0: 2020 2020 2020 2020 7365 6c66 2e43 6f6e          self.Con
-000100d0: 6669 6775 7261 7469 6f6e 4d65 7461 4e61  figurationMetaNa
-000100e0: 6d65 732e 5452 4149 4e49 4e47 5f44 4154  mes.TRAINING_DAT
-000100f0: 415f 5245 4645 5245 4e43 4553 0a20 2020  A_REFERENCES.   
+000100c0: 2020 2020 2020 2020 2020 7365 6c66 2e43            self.C
+000100d0: 6f6e 6669 6775 7261 7469 6f6e 4d65 7461  onfigurationMeta
+000100e0: 4e61 6d65 732e 5452 4149 4e49 4e47 5f44  Names.TRAINING_D
+000100f0: 4154 415f 5245 4645 5245 4e43 4553 0a20  ATA_REFERENCES. 
 00010100: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010110: 2020 2020 2020 2020 205d 5b30 5d2e 6765           ][0].ge
-00010120: 7428 2273 6368 656d 6122 295b 2266 6965  t("schema")["fie
-00010130: 6c64 7322 5d0a 2020 2020 2020 2020 2020  lds"].          
+00010110: 2020 2020 2020 2020 2020 205d 5b30 5d2e             ][0].
+00010120: 6765 7428 2273 6368 656d 6122 295b 2266  get("schema")["f
+00010130: 6965 6c64 7322 5d0a 2020 2020 2020 2020  ields"].        
 00010140: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010150: 2020 6966 2069 735f 6669 656c 645f 6e6f    if is_field_no
-00010160: 6e5f 6c61 6265 6c28 6629 0a20 2020 2020  n_label(f).     
+00010150: 2020 2020 6966 2069 735f 6669 656c 645f      if is_field_
+00010160: 6e6f 6e5f 6c61 6265 6c28 6629 0a20 2020  non_label(f).   
 00010170: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010180: 2020 205d 2c0a 2020 2020 2020 2020 2020     ],.          
-00010190: 2020 2020 2020 2020 2020 2020 2020 2274                "t
-000101a0: 7970 6522 3a20 2273 7472 7563 7422 2c0a  ype": "struct",.
-000101b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000101c0: 2020 2020 2020 2020 2269 6422 3a20 2231          "id": "1
-000101d0: 222c 0a20 2020 2020 2020 2020 2020 2020  ",.             
-000101e0: 2020 2020 2020 207d 0a0a 2020 2020 2020         }..      
-000101f0: 2020 2020 2020 2020 2020 2020 2020 6d65                me
-00010200: 7461 5f70 726f 7073 5b0a 2020 2020 2020  ta_props[.      
-00010210: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010220: 2020 7365 6c66 2e43 6f6e 6669 6775 7261    self.Configura
-00010230: 7469 6f6e 4d65 7461 4e61 6d65 732e 494e  tionMetaNames.IN
-00010240: 5055 545f 4441 5441 5f53 4348 454d 410a  PUT_DATA_SCHEMA.
-00010250: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010260: 2020 2020 5d20 3d20 696e 7075 745f 6461      ] = input_da
-00010270: 7461 5f73 6368 656d 610a 2020 2020 2020  ta_schema.      
-00010280: 2020 2020 2020 6578 6365 7074 2045 7863        except Exc
-00010290: 6570 7469 6f6e 3a0a 2020 2020 2020 2020  eption:.        
-000102a0: 2020 2020 2020 2020 7061 7373 0a0a 2020          pass..  
-000102b0: 2020 2020 2020 2320 6e6f 7465 3a20 646f        # note: do
-000102c0: 206e 6f74 2076 616c 6964 6174 6520 6d65   not validate me
-000102d0: 7461 7072 6f70 7320 7768 656e 2077 6520  taprops when we 
-000102e0: 6861 7665 2074 6865 6d20 6672 6f6d 2074  have them from t
-000102f0: 7261 696e 696e 6720 6d69 6372 6f73 6572  raining microser
-00010300: 7669 6365 2028 616c 7761 7973 2063 6f72  vice (always cor
-00010310: 7265 6374 290a 2020 2020 2020 2020 6966  rect).        if
-00010320: 2028 6d6f 6465 6c20 6973 204e 6f6e 6529   (model is None)
-00010330: 206f 7220 2869 7369 6e73 7461 6e63 6528   or (isinstance(
-00010340: 6d6f 6465 6c2c 2073 7472 2920 616e 6420  model, str) and 
-00010350: 2261 7574 6f61 695f 7364 6b22 2069 6e20  "autoai_sdk" in 
-00010360: 6d6f 6465 6c29 3a0a 2020 2020 2020 2020  model):.        
-00010370: 2020 2020 7061 7373 0a20 2020 2020 2020      pass.       
-00010380: 2065 6c69 6620 6578 7065 7269 6d65 6e74   elif experiment
-00010390: 5f6d 6574 6164 6174 6120 6f72 2074 7261  _metadata or tra
-000103a0: 696e 696e 675f 6964 3a0a 2020 2020 2020  ining_id:.      
-000103b0: 2020 2020 2020 2320 6e6f 7465 3a20 6966        # note: if
-000103c0: 2065 7870 6572 696d 656e 745f 6d65 7461   experiment_meta
-000103d0: 6461 7461 2061 7265 206e 6f74 204e 6f6e  data are not Non
-000103e0: 6520 6974 206d 6561 6e73 2074 6861 7420  e it means that 
-000103f0: 7468 6520 6d6f 6465 6c20 6973 2063 7265  the model is cre
-00010400: 6174 6564 2066 726f 6d20 6578 7065 7269  ated from experi
-00010410: 6d65 6e74 2c0a 2020 2020 2020 2020 2020  ment,.          
-00010420: 2020 2320 616e 6420 616c 6c20 7265 7175    # and all requ
-00010430: 6972 6564 2069 6e66 6f72 6d61 7469 6f6e  ired information
-00010440: 2061 7265 206b 6e6f 776e 2066 726f 6d20   are known from 
-00010450: 7468 6520 6578 7065 7269 6d65 6e74 206d  the experiment m
-00010460: 6574 6164 6174 6120 616e 6420 7468 6520  etadata and the 
-00010470: 6f72 6967 696e 0a20 2020 2020 2020 2020  origin.         
-00010480: 2020 204d 6f64 656c 732e 5f76 616c 6964     Models._valid
-00010490: 6174 655f 7479 7065 286d 6574 615f 7072  ate_type(meta_pr
-000104a0: 6f70 732c 2022 6d65 7461 5f70 726f 7073  ops, "meta_props
-000104b0: 222c 2064 6963 742c 2054 7275 6529 0a20  ", dict, True). 
-000104c0: 2020 2020 2020 2020 2020 204d 6f64 656c             Model
-000104d0: 732e 5f76 616c 6964 6174 655f 7479 7065  s._validate_type
-000104e0: 286d 6574 615f 7072 6f70 735b 226e 616d  (meta_props["nam
-000104f0: 6522 5d2c 2022 6d65 7461 5f70 726f 7073  e"], "meta_props
-00010500: 2e6e 616d 6522 2c20 7374 722c 2054 7275  .name", str, Tru
-00010510: 6529 0a20 2020 2020 2020 2065 6c73 653a  e).        else:
-00010520: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-00010530: 662e 436f 6e66 6967 7572 6174 696f 6e4d  f.ConfigurationM
-00010540: 6574 614e 616d 6573 2e5f 7661 6c69 6461  etaNames._valida
-00010550: 7465 286d 6574 615f 7072 6f70 7329 0a0a  te(meta_props)..
-00010560: 2020 2020 2020 2020 6966 2022 6672 616d          if "fram
-00010570: 6577 6f72 6b4e 616d 6522 2069 6e20 6d65  eworkName" in me
-00010580: 7461 5f70 726f 7073 3a0a 2020 2020 2020  ta_props:.      
-00010590: 2020 2020 2020 6672 616d 6577 6f72 6b5f        framework_
-000105a0: 6e61 6d65 203d 206d 6574 615f 7072 6f70  name = meta_prop
-000105b0: 735b 2266 7261 6d65 776f 726b 4e61 6d65  s["frameworkName
-000105c0: 225d 2e6c 6f77 6572 2829 0a20 2020 2020  "].lower().     
-000105d0: 2020 2020 2020 2069 6620 7665 7273 696f         if versio
-000105e0: 6e20 3d3d 2054 7275 6520 616e 6420 280a  n == True and (.
-000105f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010600: 6672 616d 6577 6f72 6b5f 6e61 6d65 203d  framework_name =
-00010610: 3d20 226d 6c6c 6962 2220 6f72 2066 7261  = "mllib" or fra
-00010620: 6d65 776f 726b 5f6e 616d 6520 3d3d 2022  mework_name == "
-00010630: 776d 6c22 0a20 2020 2020 2020 2020 2020  wml".           
-00010640: 2029 3a0a 2020 2020 2020 2020 2020 2020   ):.            
-00010650: 2020 2020 7261 6973 6520 574d 4c43 6c69      raise WMLCli
-00010660: 656e 7445 7272 6f72 280a 2020 2020 2020  entError(.      
-00010670: 2020 2020 2020 2020 2020 2020 2020 2255                "U
-00010680: 6e73 7570 706f 7274 6564 2066 7261 6d65  nsupported frame
-00010690: 776f 726b 206e 616d 653a 2027 7b7d 2720  work name: '{}' 
-000106a0: 666f 7220 6372 6561 7469 6e67 2061 206d  for creating a m
-000106b0: 6f64 656c 2076 6572 7369 6f6e 222e 666f  odel version".fo
-000106c0: 726d 6174 280a 2020 2020 2020 2020 2020  rmat(.          
-000106d0: 2020 2020 2020 2020 2020 2020 2020 6672                fr
-000106e0: 616d 6577 6f72 6b5f 6e61 6d65 0a20 2020  amework_name.   
-000106f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010700: 2029 0a20 2020 2020 2020 2020 2020 2020   ).             
-00010710: 2020 2029 0a0a 2020 2020 2020 2020 6966     )..        if
-00010720: 2074 7261 696e 696e 675f 6964 2061 6e64   training_id and
-00010730: 2069 735f 7472 6169 6e69 6e67 5f70 726f   is_training_pro
-00010740: 6d70 745f 7475 6e69 6e67 2874 7261 696e  mpt_tuning(train
-00010750: 696e 675f 6964 2c20 7365 6c66 2e5f 636c  ing_id, self._cl
-00010760: 6965 6e74 293a 0a20 2020 2020 2020 2020  ient):.         
-00010770: 2020 2023 2069 6d70 6f72 7420 6865 7265     # import here
-00010780: 2074 6f20 6176 6f69 6420 6369 7263 756c   to avoid circul
-00010790: 6172 2069 6d70 6f72 740a 2020 2020 2020  ar import.      
-000107a0: 2020 2020 2020 6672 6f6d 2069 626d 5f77        from ibm_w
-000107b0: 6174 736f 6e78 5f61 692e 666f 756e 6461  atsonx_ai.founda
-000107c0: 7469 6f6e 5f6d 6f64 656c 732e 7574 696c  tion_models.util
-000107d0: 732e 7574 696c 7320 696d 706f 7274 206c  s.utils import l
-000107e0: 6f61 645f 7265 7175 6573 745f 6a73 6f6e  oad_request_json
-000107f0: 0a0a 2020 2020 2020 2020 2020 2020 6d6f  ..            mo
-00010800: 6465 6c5f 7265 7175 6573 745f 6a73 6f6e  del_request_json
-00010810: 203d 206c 6f61 645f 7265 7175 6573 745f   = load_request_
-00010820: 6a73 6f6e 280a 2020 2020 2020 2020 2020  json(.          
-00010830: 2020 2020 2020 7275 6e5f 6964 3d74 7261        run_id=tra
-00010840: 696e 696e 675f 6964 2c20 6170 695f 636c  ining_id, api_cl
-00010850: 6965 6e74 3d73 656c 662e 5f63 6c69 656e  ient=self._clien
-00010860: 740a 2020 2020 2020 2020 2020 2020 290a  t.            ).
-00010870: 2020 2020 2020 2020 2020 2020 6966 206d              if m
-00010880: 6574 615f 7072 6f70 733a 0a20 2020 2020  eta_props:.     
-00010890: 2020 2020 2020 2020 2020 206d 6f64 656c             model
-000108a0: 5f72 6571 7565 7374 5f6a 736f 6e2e 7570  _request_json.up
-000108b0: 6461 7465 286d 6574 615f 7072 6f70 7329  date(meta_props)
-000108c0: 0a0a 2020 2020 2020 2020 2020 2020 6372  ..            cr
-000108d0: 6561 7469 6f6e 5f72 6573 706f 6e73 6520  eation_response 
-000108e0: 3d20 7265 7175 6573 7473 2e70 6f73 7428  = requests.post(
-000108f0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00010900: 2073 656c 662e 5f63 6c69 656e 742e 7365   self._client.se
-00010910: 7276 6963 655f 696e 7374 616e 6365 2e5f  rvice_instance._
-00010920: 6872 6566 5f64 6566 696e 6974 696f 6e73  href_definitions
-00010930: 2e67 6574 5f70 7562 6c69 7368 6564 5f6d  .get_published_m
-00010940: 6f64 656c 735f 6872 6566 2829 2c0a 2020  odels_href(),.  
-00010950: 2020 2020 2020 2020 2020 2020 2020 6865                he
-00010960: 6164 6572 733d 7365 6c66 2e5f 636c 6965  aders=self._clie
-00010970: 6e74 2e5f 6765 745f 6865 6164 6572 7328  nt._get_headers(
-00010980: 292c 0a20 2020 2020 2020 2020 2020 2020  ),.             
-00010990: 2020 2070 6172 616d 733d 7365 6c66 2e5f     params=self._
-000109a0: 636c 6965 6e74 2e5f 7061 7261 6d73 2829  client._params()
-000109b0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-000109c0: 2020 6a73 6f6e 3d6d 6f64 656c 5f72 6571    json=model_req
-000109d0: 7565 7374 5f6a 736f 6e2c 0a20 2020 2020  uest_json,.     
-000109e0: 2020 2020 2020 2029 0a20 2020 2020 2020         ).       
-000109f0: 2020 2020 206d 6f64 656c 5f64 6574 6169       model_detai
-00010a00: 6c73 203d 2073 656c 662e 5f68 616e 646c  ls = self._handl
-00010a10: 655f 7265 7370 6f6e 7365 280a 2020 2020  e_response(.    
-00010a20: 2020 2020 2020 2020 2020 2020 3230 322c              202,
-00010a30: 2022 6372 6561 7469 6e67 206e 6577 206d   "creating new m
-00010a40: 6f64 656c 222c 2063 7265 6174 696f 6e5f  odel", creation_
-00010a50: 7265 7370 6f6e 7365 0a20 2020 2020 2020  response.       
-00010a60: 2020 2020 2029 0a20 2020 2020 2020 2020       ).         
-00010a70: 2020 206d 6f64 656c 5f69 6420 3d20 6d6f     model_id = mo
-00010a80: 6465 6c5f 6465 7461 696c 735b 226d 6574  del_details["met
-00010a90: 6164 6174 6122 5d5b 2269 6422 5d0a 0a20  adata"]["id"].. 
-00010aa0: 2020 2020 2020 2020 2020 2023 206e 6f74             # not
-00010ab0: 653a 2077 6169 7420 7469 6c6c 2063 6f6e  e: wait till con
-00010ac0: 7465 6e74 5f69 6d70 6f72 745f 7374 6174  tent_import_stat
-00010ad0: 6520 6973 2064 6f6e 650a 2020 2020 2020  e is done.      
-00010ae0: 2020 2020 2020 6966 2022 656e 7469 7479        if "entity
-00010af0: 2220 696e 206d 6f64 656c 5f64 6574 6169  " in model_detai
-00010b00: 6c73 3a0a 2020 2020 2020 2020 2020 2020  ls:.            
-00010b10: 2020 2020 7374 6172 745f 7469 6d65 203d      start_time =
-00010b20: 2074 696d 652e 7469 6d65 2829 0a20 2020   time.time().   
-00010b30: 2020 2020 2020 2020 2020 2020 2065 6c61               ela
-00010b40: 7073 6564 5f74 696d 6520 3d20 302e 300a  psed_time = 0.0.
-00010b50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010b60: 7768 696c 6520 280a 2020 2020 2020 2020  while (.        
-00010b70: 2020 2020 2020 2020 2020 2020 6d6f 6465              mode
-00010b80: 6c5f 6465 7461 696c 735b 2265 6e74 6974  l_details["entit
-00010b90: 7922 5d2e 6765 7428 2263 6f6e 7465 6e74  y"].get("content
-00010ba0: 5f69 6d70 6f72 745f 7374 6174 6522 2920  _import_state") 
-00010bb0: 3d3d 2022 7275 6e6e 696e 6722 0a20 2020  == "running".   
-00010bc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010bd0: 2061 6e64 2065 6c61 7073 6564 5f74 696d   and elapsed_tim
-00010be0: 6520 3c20 3630 0a20 2020 2020 2020 2020  e < 60.         
-00010bf0: 2020 2020 2020 2029 3a0a 2020 2020 2020         ):.      
-00010c00: 2020 2020 2020 2020 2020 2020 2020 7469                ti
-00010c10: 6d65 2e73 6c65 6570 2832 290a 2020 2020  me.sleep(2).    
+00010180: 2020 2020 205d 2c0a 2020 2020 2020 2020       ],.        
+00010190: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000101a0: 2274 7970 6522 3a20 2273 7472 7563 7422  "type": "struct"
+000101b0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+000101c0: 2020 2020 2020 2020 2020 2269 6422 3a20            "id": 
+000101d0: 2231 222c 0a20 2020 2020 2020 2020 2020  "1",.           
+000101e0: 2020 2020 2020 2020 207d 0a0a 2020 2020           }..    
+000101f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010200: 6d65 7461 5f70 726f 7073 5b73 656c 662e  meta_props[self.
+00010210: 436f 6e66 6967 7572 6174 696f 6e4d 6574  ConfigurationMet
+00010220: 614e 616d 6573 2e49 4e50 5554 5f44 4154  aNames.INPUT_DAT
+00010230: 415f 5343 4845 4d41 5d20 3d20 280a 2020  A_SCHEMA] = (.  
+00010240: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010250: 2020 2020 2020 696e 7075 745f 6461 7461        input_data
+00010260: 5f73 6368 656d 610a 2020 2020 2020 2020  _schema.        
+00010270: 2020 2020 2020 2020 2020 2020 290a 2020              ).  
+00010280: 2020 2020 2020 2020 2020 6578 6365 7074            except
+00010290: 2045 7863 6570 7469 6f6e 3a0a 2020 2020   Exception:.    
+000102a0: 2020 2020 2020 2020 2020 2020 7061 7373              pass
+000102b0: 0a0a 2020 2020 2020 2020 2320 6e6f 7465  ..        # note
+000102c0: 3a20 646f 206e 6f74 2076 616c 6964 6174  : do not validat
+000102d0: 6520 6d65 7461 7072 6f70 7320 7768 656e  e metaprops when
+000102e0: 2077 6520 6861 7665 2074 6865 6d20 6672   we have them fr
+000102f0: 6f6d 2074 7261 696e 696e 6720 6d69 6372  om training micr
+00010300: 6f73 6572 7669 6365 2028 616c 7761 7973  oservice (always
+00010310: 2063 6f72 7265 6374 290a 2020 2020 2020   correct).      
+00010320: 2020 6966 2028 6d6f 6465 6c20 6973 204e    if (model is N
+00010330: 6f6e 6529 206f 7220 2869 7369 6e73 7461  one) or (isinsta
+00010340: 6e63 6528 6d6f 6465 6c2c 2073 7472 2920  nce(model, str) 
+00010350: 616e 6420 2261 7574 6f61 695f 7364 6b22  and "autoai_sdk"
+00010360: 2069 6e20 6d6f 6465 6c29 3a0a 2020 2020   in model):.    
+00010370: 2020 2020 2020 2020 7061 7373 0a20 2020          pass.   
+00010380: 2020 2020 2065 6c69 6620 6578 7065 7269       elif experi
+00010390: 6d65 6e74 5f6d 6574 6164 6174 6120 6f72  ment_metadata or
+000103a0: 2074 7261 696e 696e 675f 6964 3a0a 2020   training_id:.  
+000103b0: 2020 2020 2020 2020 2020 2320 6e6f 7465            # note
+000103c0: 3a20 6966 2065 7870 6572 696d 656e 745f  : if experiment_
+000103d0: 6d65 7461 6461 7461 2061 7265 206e 6f74  metadata are not
+000103e0: 204e 6f6e 6520 6974 206d 6561 6e73 2074   None it means t
+000103f0: 6861 7420 7468 6520 6d6f 6465 6c20 6973  hat the model is
+00010400: 2063 7265 6174 6564 2066 726f 6d20 6578   created from ex
+00010410: 7065 7269 6d65 6e74 2c0a 2020 2020 2020  periment,.      
+00010420: 2020 2020 2020 2320 616e 6420 616c 6c20        # and all 
+00010430: 7265 7175 6972 6564 2069 6e66 6f72 6d61  required informa
+00010440: 7469 6f6e 2061 7265 206b 6e6f 776e 2066  tion are known f
+00010450: 726f 6d20 7468 6520 6578 7065 7269 6d65  rom the experime
+00010460: 6e74 206d 6574 6164 6174 6120 616e 6420  nt metadata and 
+00010470: 7468 6520 6f72 6967 696e 0a20 2020 2020  the origin.     
+00010480: 2020 2020 2020 204d 6f64 656c 732e 5f76         Models._v
+00010490: 616c 6964 6174 655f 7479 7065 286d 6574  alidate_type(met
+000104a0: 615f 7072 6f70 732c 2022 6d65 7461 5f70  a_props, "meta_p
+000104b0: 726f 7073 222c 2064 6963 742c 2054 7275  rops", dict, Tru
+000104c0: 6529 0a20 2020 2020 2020 2020 2020 204d  e).            M
+000104d0: 6f64 656c 732e 5f76 616c 6964 6174 655f  odels._validate_
+000104e0: 7479 7065 286d 6574 615f 7072 6f70 735b  type(meta_props[
+000104f0: 226e 616d 6522 5d2c 2022 6d65 7461 5f70  "name"], "meta_p
+00010500: 726f 7073 2e6e 616d 6522 2c20 7374 722c  rops.name", str,
+00010510: 2054 7275 6529 0a20 2020 2020 2020 2065   True).        e
+00010520: 6c73 653a 0a20 2020 2020 2020 2020 2020  lse:.           
+00010530: 2073 656c 662e 436f 6e66 6967 7572 6174   self.Configurat
+00010540: 696f 6e4d 6574 614e 616d 6573 2e5f 7661  ionMetaNames._va
+00010550: 6c69 6461 7465 286d 6574 615f 7072 6f70  lidate(meta_prop
+00010560: 7329 0a0a 2020 2020 2020 2020 6966 2022  s)..        if "
+00010570: 6672 616d 6577 6f72 6b4e 616d 6522 2069  frameworkName" i
+00010580: 6e20 6d65 7461 5f70 726f 7073 3a0a 2020  n meta_props:.  
+00010590: 2020 2020 2020 2020 2020 6672 616d 6577            framew
+000105a0: 6f72 6b5f 6e61 6d65 203d 206d 6574 615f  ork_name = meta_
+000105b0: 7072 6f70 735b 2266 7261 6d65 776f 726b  props["framework
+000105c0: 4e61 6d65 225d 2e6c 6f77 6572 2829 0a20  Name"].lower(). 
+000105d0: 2020 2020 2020 2020 2020 2069 6620 7665             if ve
+000105e0: 7273 696f 6e20 3d3d 2054 7275 6520 616e  rsion == True an
+000105f0: 6420 280a 2020 2020 2020 2020 2020 2020  d (.            
+00010600: 2020 2020 6672 616d 6577 6f72 6b5f 6e61      framework_na
+00010610: 6d65 203d 3d20 226d 6c6c 6962 2220 6f72  me == "mllib" or
+00010620: 2066 7261 6d65 776f 726b 5f6e 616d 6520   framework_name 
+00010630: 3d3d 2022 776d 6c22 0a20 2020 2020 2020  == "wml".       
+00010640: 2020 2020 2029 3a0a 2020 2020 2020 2020       ):.        
+00010650: 2020 2020 2020 2020 7261 6973 6520 574d          raise WM
+00010660: 4c43 6c69 656e 7445 7272 6f72 280a 2020  LClientError(.  
+00010670: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010680: 2020 2255 6e73 7570 706f 7274 6564 2066    "Unsupported f
+00010690: 7261 6d65 776f 726b 206e 616d 653a 2027  ramework name: '
+000106a0: 7b7d 2720 666f 7220 6372 6561 7469 6e67  {}' for creating
+000106b0: 2061 206d 6f64 656c 2076 6572 7369 6f6e   a model version
+000106c0: 222e 666f 726d 6174 280a 2020 2020 2020  ".format(.      
+000106d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000106e0: 2020 6672 616d 6577 6f72 6b5f 6e61 6d65    framework_name
+000106f0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00010700: 2020 2020 2029 0a20 2020 2020 2020 2020       ).         
+00010710: 2020 2020 2020 2029 0a0a 2020 2020 2020         )..      
+00010720: 2020 6966 2074 7261 696e 696e 675f 6964    if training_id
+00010730: 2061 6e64 2069 735f 7472 6169 6e69 6e67   and is_training
+00010740: 5f70 726f 6d70 745f 7475 6e69 6e67 2874  _prompt_tuning(t
+00010750: 7261 696e 696e 675f 6964 2c20 7365 6c66  raining_id, self
+00010760: 2e5f 636c 6965 6e74 293a 0a20 2020 2020  ._client):.     
+00010770: 2020 2020 2020 2023 2069 6d70 6f72 7420         # import 
+00010780: 6865 7265 2074 6f20 6176 6f69 6420 6369  here to avoid ci
+00010790: 7263 756c 6172 2069 6d70 6f72 740a 2020  rcular import.  
+000107a0: 2020 2020 2020 2020 2020 6672 6f6d 2069            from i
+000107b0: 626d 5f77 6174 736f 6e78 5f61 692e 666f  bm_watsonx_ai.fo
+000107c0: 756e 6461 7469 6f6e 5f6d 6f64 656c 732e  undation_models.
+000107d0: 7574 696c 732e 7574 696c 7320 696d 706f  utils.utils impo
+000107e0: 7274 206c 6f61 645f 7265 7175 6573 745f  rt load_request_
+000107f0: 6a73 6f6e 0a0a 2020 2020 2020 2020 2020  json..          
+00010800: 2020 6d6f 6465 6c5f 7265 7175 6573 745f    model_request_
+00010810: 6a73 6f6e 203d 206c 6f61 645f 7265 7175  json = load_requ
+00010820: 6573 745f 6a73 6f6e 280a 2020 2020 2020  est_json(.      
+00010830: 2020 2020 2020 2020 2020 7275 6e5f 6964            run_id
+00010840: 3d74 7261 696e 696e 675f 6964 2c20 6170  =training_id, ap
+00010850: 695f 636c 6965 6e74 3d73 656c 662e 5f63  i_client=self._c
+00010860: 6c69 656e 740a 2020 2020 2020 2020 2020  lient.          
+00010870: 2020 290a 2020 2020 2020 2020 2020 2020    ).            
+00010880: 6966 206d 6574 615f 7072 6f70 733a 0a20  if meta_props:. 
+00010890: 2020 2020 2020 2020 2020 2020 2020 206d                 m
+000108a0: 6f64 656c 5f72 6571 7565 7374 5f6a 736f  odel_request_jso
+000108b0: 6e2e 7570 6461 7465 286d 6574 615f 7072  n.update(meta_pr
+000108c0: 6f70 7329 0a0a 2020 2020 2020 2020 2020  ops)..          
+000108d0: 2020 6372 6561 7469 6f6e 5f72 6573 706f    creation_respo
+000108e0: 6e73 6520 3d20 7265 7175 6573 7473 2e70  nse = requests.p
+000108f0: 6f73 7428 0a20 2020 2020 2020 2020 2020  ost(.           
+00010900: 2020 2020 2073 656c 662e 5f63 6c69 656e       self._clien
+00010910: 742e 7365 7276 6963 655f 696e 7374 616e  t.service_instan
+00010920: 6365 2e5f 6872 6566 5f64 6566 696e 6974  ce._href_definit
+00010930: 696f 6e73 2e67 6574 5f70 7562 6c69 7368  ions.get_publish
+00010940: 6564 5f6d 6f64 656c 735f 6872 6566 2829  ed_models_href()
+00010950: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00010960: 2020 6865 6164 6572 733d 7365 6c66 2e5f    headers=self._
+00010970: 636c 6965 6e74 2e5f 6765 745f 6865 6164  client._get_head
+00010980: 6572 7328 292c 0a20 2020 2020 2020 2020  ers(),.         
+00010990: 2020 2020 2020 2070 6172 616d 733d 7365         params=se
+000109a0: 6c66 2e5f 636c 6965 6e74 2e5f 7061 7261  lf._client._para
+000109b0: 6d73 2829 2c0a 2020 2020 2020 2020 2020  ms(),.          
+000109c0: 2020 2020 2020 6a73 6f6e 3d6d 6f64 656c        json=model
+000109d0: 5f72 6571 7565 7374 5f6a 736f 6e2c 0a20  _request_json,. 
+000109e0: 2020 2020 2020 2020 2020 2029 0a20 2020             ).   
+000109f0: 2020 2020 2020 2020 206d 6f64 656c 5f64           model_d
+00010a00: 6574 6169 6c73 203d 2073 656c 662e 5f68  etails = self._h
+00010a10: 616e 646c 655f 7265 7370 6f6e 7365 280a  andle_response(.
+00010a20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010a30: 3230 322c 2022 6372 6561 7469 6e67 206e  202, "creating n
+00010a40: 6577 206d 6f64 656c 222c 2063 7265 6174  ew model", creat
+00010a50: 696f 6e5f 7265 7370 6f6e 7365 0a20 2020  ion_response.   
+00010a60: 2020 2020 2020 2020 2029 0a20 2020 2020           ).     
+00010a70: 2020 2020 2020 206d 6f64 656c 5f69 6420         model_id 
+00010a80: 3d20 6d6f 6465 6c5f 6465 7461 696c 735b  = model_details[
+00010a90: 226d 6574 6164 6174 6122 5d5b 2269 6422  "metadata"]["id"
+00010aa0: 5d0a 0a20 2020 2020 2020 2020 2020 2023  ]..            #
+00010ab0: 206e 6f74 653a 2077 6169 7420 7469 6c6c   note: wait till
+00010ac0: 2063 6f6e 7465 6e74 5f69 6d70 6f72 745f   content_import_
+00010ad0: 7374 6174 6520 6973 2064 6f6e 650a 2020  state is done.  
+00010ae0: 2020 2020 2020 2020 2020 6966 2022 656e            if "en
+00010af0: 7469 7479 2220 696e 206d 6f64 656c 5f64  tity" in model_d
+00010b00: 6574 6169 6c73 3a0a 2020 2020 2020 2020  etails:.        
+00010b10: 2020 2020 2020 2020 7374 6172 745f 7469          start_ti
+00010b20: 6d65 203d 2074 696d 652e 7469 6d65 2829  me = time.time()
+00010b30: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00010b40: 2065 6c61 7073 6564 5f74 696d 6520 3d20   elapsed_time = 
+00010b50: 302e 300a 2020 2020 2020 2020 2020 2020  0.0.            
+00010b60: 2020 2020 7768 696c 6520 280a 2020 2020      while (.    
+00010b70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010b80: 6d6f 6465 6c5f 6465 7461 696c 735b 2265  model_details["e
+00010b90: 6e74 6974 7922 5d2e 6765 7428 2263 6f6e  ntity"].get("con
+00010ba0: 7465 6e74 5f69 6d70 6f72 745f 7374 6174  tent_import_stat
+00010bb0: 6522 2920 3d3d 2022 7275 6e6e 696e 6722  e") == "running"
+00010bc0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00010bd0: 2020 2020 2061 6e64 2065 6c61 7073 6564       and elapsed
+00010be0: 5f74 696d 6520 3c20 3630 0a20 2020 2020  _time < 60.     
+00010bf0: 2020 2020 2020 2020 2020 2029 3a0a 2020             ):.  
+00010c00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010c10: 2020 7469 6d65 2e73 6c65 6570 2832 290a    time.sleep(2).
 00010c20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010c30: 656c 6170 7365 645f 7469 6d65 203d 2074  elapsed_time = t
-00010c40: 696d 652e 7469 6d65 2829 202d 2073 7461  ime.time() - sta
-00010c50: 7274 5f74 696d 650a 2020 2020 2020 2020  rt_time.        
-00010c60: 2020 2020 2020 2020 2020 2020 6d6f 6465              mode
-00010c70: 6c5f 6465 7461 696c 7320 3d20 7365 6c66  l_details = self
-00010c80: 2e67 6574 5f64 6574 6169 6c73 286d 6f64  .get_details(mod
-00010c90: 656c 5f69 6429 0a20 2020 2020 2020 2020  el_id).         
-00010ca0: 2020 2023 202d 2d2d 2065 6e64 206e 6f74     # --- end not
-00010cb0: 650a 2020 2020 2020 2020 2020 2020 7265  e.            re
-00010cc0: 7475 726e 2073 656c 662e 6765 745f 6465  turn self.get_de
-00010cd0: 7461 696c 7328 6d6f 6465 6c5f 6964 290a  tails(model_id).
-00010ce0: 0a20 2020 2020 2020 2069 6620 6d6f 6465  .        if mode
-00010cf0: 6c20 6973 206e 6f74 204e 6f6e 6520 616e  l is not None an
-00010d00: 6420 6375 7374 6f6d 5f6d 6f64 656c 3a0a  d custom_model:.
-00010d10: 2020 2020 2020 2020 2020 2020 7361 7665              save
-00010d20: 645f 6d6f 6465 6c20 3d20 7365 6c66 2e5f  d_model = self._
-00010d30: 7374 6f72 655f 6375 7374 6f6d 5f66 6f75  store_custom_fou
-00010d40: 6e64 6174 696f 6e5f 6d6f 6465 6c28 6d6f  ndation_model(mo
-00010d50: 6465 6c2c 206d 6574 615f 7072 6f70 7329  del, meta_props)
-00010d60: 0a20 2020 2020 2020 2065 6c69 6620 6e6f  .        elif no
-00010d70: 7420 6973 696e 7374 616e 6365 286d 6f64  t isinstance(mod
-00010d80: 656c 2c20 7374 7229 3a0a 2020 2020 2020  el, str):.      
-00010d90: 2020 2020 2020 6966 2076 6572 7369 6f6e        if version
-00010da0: 203d 3d20 5472 7565 3a0a 2020 2020 2020   == True:.      
-00010db0: 2020 2020 2020 2020 2020 7261 6973 6520            raise 
-00010dc0: 574d 4c43 6c69 656e 7445 7272 6f72 280a  WMLClientError(.
-00010dd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010de0: 2020 2020 2255 6e73 7570 706f 7274 6564      "Unsupported
-00010df0: 2074 7970 653a 206f 626a 6563 7420 666f   type: object fo
-00010e00: 7220 7061 7261 6d20 6d6f 6465 6c2e 2053  r param model. S
-00010e10: 7570 706f 7274 6564 2074 7970 6573 3a20  upported types: 
-00010e20: 7061 7468 2074 6f20 7361 7665 6420 6d6f  path to saved mo
-00010e30: 6465 6c2c 2074 7261 696e 696e 6720 4944  del, training ID
-00010e40: 220a 2020 2020 2020 2020 2020 2020 2020  ".              
-00010e50: 2020 290a 2020 2020 2020 2020 2020 2020    ).            
-00010e60: 656c 7365 3a0a 2020 2020 2020 2020 2020  else:.          
-00010e70: 2020 2020 2020 6966 2065 7870 6572 696d        if experim
-00010e80: 656e 745f 6d65 7461 6461 7461 206f 7220  ent_metadata or 
-00010e90: 7472 6169 6e69 6e67 5f69 643a 0a0a 2020  training_id:..  
-00010ea0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010eb0: 2020 6966 2065 7870 6572 696d 656e 745f    if experiment_
-00010ec0: 6d65 7461 6461 7461 3a0a 0a20 2020 2020  metadata:..     
+00010c30: 2020 2020 656c 6170 7365 645f 7469 6d65      elapsed_time
+00010c40: 203d 2074 696d 652e 7469 6d65 2829 202d   = time.time() -
+00010c50: 2073 7461 7274 5f74 696d 650a 2020 2020   start_time.    
+00010c60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010c70: 6d6f 6465 6c5f 6465 7461 696c 7320 3d20  model_details = 
+00010c80: 7365 6c66 2e67 6574 5f64 6574 6169 6c73  self.get_details
+00010c90: 286d 6f64 656c 5f69 6429 0a20 2020 2020  (model_id).     
+00010ca0: 2020 2020 2020 2023 202d 2d2d 2065 6e64         # --- end
+00010cb0: 206e 6f74 650a 2020 2020 2020 2020 2020   note.          
+00010cc0: 2020 7265 7475 726e 2073 656c 662e 6765    return self.ge
+00010cd0: 745f 6465 7461 696c 7328 6d6f 6465 6c5f  t_details(model_
+00010ce0: 6964 290a 0a20 2020 2020 2020 2069 6620  id)..        if 
+00010cf0: 6d6f 6465 6c20 6973 206e 6f74 204e 6f6e  model is not Non
+00010d00: 6520 616e 6420 6375 7374 6f6d 5f6d 6f64  e and custom_mod
+00010d10: 656c 3a0a 2020 2020 2020 2020 2020 2020  el:.            
+00010d20: 7361 7665 645f 6d6f 6465 6c20 3d20 7365  saved_model = se
+00010d30: 6c66 2e5f 7374 6f72 655f 6375 7374 6f6d  lf._store_custom
+00010d40: 5f66 6f75 6e64 6174 696f 6e5f 6d6f 6465  _foundation_mode
+00010d50: 6c28 6d6f 6465 6c2c 206d 6574 615f 7072  l(model, meta_pr
+00010d60: 6f70 7329 0a20 2020 2020 2020 2065 6c69  ops).        eli
+00010d70: 6620 6e6f 7420 6973 696e 7374 616e 6365  f not isinstance
+00010d80: 286d 6f64 656c 2c20 7374 7229 3a0a 2020  (model, str):.  
+00010d90: 2020 2020 2020 2020 2020 6966 2076 6572            if ver
+00010da0: 7369 6f6e 203d 3d20 5472 7565 3a0a 2020  sion == True:.  
+00010db0: 2020 2020 2020 2020 2020 2020 2020 7261                ra
+00010dc0: 6973 6520 574d 4c43 6c69 656e 7445 7272  ise WMLClientErr
+00010dd0: 6f72 280a 2020 2020 2020 2020 2020 2020  or(.            
+00010de0: 2020 2020 2020 2020 2255 6e73 7570 706f          "Unsuppo
+00010df0: 7274 6564 2074 7970 653a 206f 626a 6563  rted type: objec
+00010e00: 7420 666f 7220 7061 7261 6d20 6d6f 6465  t for param mode
+00010e10: 6c2e 2053 7570 706f 7274 6564 2074 7970  l. Supported typ
+00010e20: 6573 3a20 7061 7468 2074 6f20 7361 7665  es: path to save
+00010e30: 6420 6d6f 6465 6c2c 2074 7261 696e 696e  d model, trainin
+00010e40: 6720 4944 220a 2020 2020 2020 2020 2020  g ID".          
+00010e50: 2020 2020 2020 290a 2020 2020 2020 2020        ).        
+00010e60: 2020 2020 656c 7365 3a0a 2020 2020 2020      else:.      
+00010e70: 2020 2020 2020 2020 2020 6966 2065 7870            if exp
+00010e80: 6572 696d 656e 745f 6d65 7461 6461 7461  eriment_metadata
+00010e90: 206f 7220 7472 6169 6e69 6e67 5f69 643a   or training_id:
+00010ea0: 0a0a 2020 2020 2020 2020 2020 2020 2020  ..              
+00010eb0: 2020 2020 2020 6966 2065 7870 6572 696d        if experim
+00010ec0: 656e 745f 6d65 7461 6461 7461 3a0a 0a20  ent_metadata:.. 
 00010ed0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010ee0: 2020 2074 7261 696e 696e 675f 6964 203d     training_id =
-00010ef0: 2067 6574 5f61 7574 6f61 695f 7275 6e5f   get_autoai_run_
-00010f00: 6964 5f66 726f 6d5f 6578 7065 7269 6d65  id_from_experime
-00010f10: 6e74 5f6d 6574 6164 6174 6128 0a20 2020  nt_metadata(.   
-00010f20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010f30: 2020 2020 2020 2020 2065 7870 6572 696d           experim
-00010f40: 656e 745f 6d65 7461 6461 7461 0a20 2020  ent_metadata.   
-00010f50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010f60: 2020 2020 2029 0a0a 2020 2020 2020 2020       )..        
-00010f70: 2020 2020 2020 2020 2020 2020 2320 4e6f              # No
-00010f80: 7465 3a20 7661 6c69 6461 7465 2069 6620  te: validate if 
-00010f90: 7472 6169 6e69 6e67 5f69 6420 6973 2066  training_id is f
-00010fa0: 726f 6d20 4175 746f 4149 2065 7870 6572  rom AutoAI exper
-00010fb0: 696d 656e 740a 2020 2020 2020 2020 2020  iment.          
-00010fc0: 2020 2020 2020 2020 2020 7275 6e5f 7061            run_pa
-00010fd0: 7261 6d73 203d 2073 656c 662e 5f63 6c69  rams = self._cli
-00010fe0: 656e 742e 7472 6169 6e69 6e67 2e67 6574  ent.training.get
-00010ff0: 5f64 6574 6169 6c73 280a 2020 2020 2020  _details(.      
+00010ee0: 2020 2020 2020 2074 7261 696e 696e 675f         training_
+00010ef0: 6964 203d 2067 6574 5f61 7574 6f61 695f  id = get_autoai_
+00010f00: 7275 6e5f 6964 5f66 726f 6d5f 6578 7065  run_id_from_expe
+00010f10: 7269 6d65 6e74 5f6d 6574 6164 6174 6128  riment_metadata(
+00010f20: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00010f30: 2020 2020 2020 2020 2020 2020 2065 7870               exp
+00010f40: 6572 696d 656e 745f 6d65 7461 6461 7461  eriment_metadata
+00010f50: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00010f60: 2020 2020 2020 2020 2029 0a0a 2020 2020           )..    
+00010f70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010f80: 2320 4e6f 7465 3a20 7661 6c69 6461 7465  # Note: validate
+00010f90: 2069 6620 7472 6169 6e69 6e67 5f69 6420   if training_id 
+00010fa0: 6973 2066 726f 6d20 4175 746f 4149 2065  is from AutoAI e
+00010fb0: 7870 6572 696d 656e 740a 2020 2020 2020  xperiment.      
+00010fc0: 2020 2020 2020 2020 2020 2020 2020 7275                ru
+00010fd0: 6e5f 7061 7261 6d73 203d 2073 656c 662e  n_params = self.
+00010fe0: 5f63 6c69 656e 742e 7472 6169 6e69 6e67  _client.training
+00010ff0: 2e67 6574 5f64 6574 6169 6c73 280a 2020  .get_details(.  
 00011000: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011010: 2020 7472 6169 6e69 6e67 5f69 643d 7472    training_id=tr
-00011020: 6169 6e69 6e67 5f69 642c 205f 696e 7465  aining_id, _inte
-00011030: 726e 616c 3d54 7275 650a 2020 2020 2020  rnal=True.      
-00011040: 2020 2020 2020 2020 2020 2020 2020 290a                ).
-00011050: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011060: 2020 2020 7069 7065 6c69 6e65 5f69 6420      pipeline_id 
-00011070: 3d20 7275 6e5f 7061 7261 6d73 5b22 656e  = run_params["en
-00011080: 7469 7479 225d 2e67 6574 2822 7069 7065  tity"].get("pipe
-00011090: 6c69 6e65 222c 207b 7d29 2e67 6574 2822  line", {}).get("
-000110a0: 6964 2229 0a20 2020 2020 2020 2020 2020  id").           
-000110b0: 2020 2020 2020 2020 2070 6970 656c 696e           pipelin
-000110c0: 655f 6e6f 6465 735f 6c69 7374 203d 2028  e_nodes_list = (
-000110d0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-000110e0: 2020 2020 2020 2020 2073 656c 662e 5f63           self._c
-000110f0: 6c69 656e 742e 7069 7065 6c69 6e65 732e  lient.pipelines.
-00011100: 6765 745f 6465 7461 696c 7328 7069 7065  get_details(pipe
-00011110: 6c69 6e65 5f69 6429 5b22 656e 7469 7479  line_id)["entity
-00011120: 225d 0a20 2020 2020 2020 2020 2020 2020  "].             
-00011130: 2020 2020 2020 2020 2020 202e 6765 7428             .get(
-00011140: 2264 6f63 756d 656e 7422 2c20 7b7d 290a  "document", {}).
-00011150: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011160: 2020 2020 2020 2020 2e67 6574 2822 7069          .get("pi
-00011170: 7065 6c69 6e65 7322 2c20 5b5d 290a 2020  pelines", []).  
-00011180: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011190: 2020 2020 2020 6966 2070 6970 656c 696e        if pipelin
-000111a0: 655f 6964 0a20 2020 2020 2020 2020 2020  e_id.           
-000111b0: 2020 2020 2020 2020 2020 2020 2065 6c73               els
-000111c0: 6520 5b5d 0a20 2020 2020 2020 2020 2020  e [].           
-000111d0: 2020 2020 2020 2020 2029 0a20 2020 2020           ).     
-000111e0: 2020 2020 2020 2020 2020 2020 2020 2069                 i
-000111f0: 6620 280a 2020 2020 2020 2020 2020 2020  f (.            
-00011200: 2020 2020 2020 2020 2020 2020 6c65 6e28              len(
-00011210: 7069 7065 6c69 6e65 5f6e 6f64 6573 5f6c  pipeline_nodes_l
-00011220: 6973 7429 203d 3d20 300a 2020 2020 2020  ist) == 0.      
+00011010: 2020 2020 2020 7472 6169 6e69 6e67 5f69        training_i
+00011020: 643d 7472 6169 6e69 6e67 5f69 642c 205f  d=training_id, _
+00011030: 696e 7465 726e 616c 3d54 7275 650a 2020  internal=True.  
+00011040: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011050: 2020 290a 2020 2020 2020 2020 2020 2020    ).            
+00011060: 2020 2020 2020 2020 7069 7065 6c69 6e65          pipeline
+00011070: 5f69 6420 3d20 7275 6e5f 7061 7261 6d73  _id = run_params
+00011080: 5b22 656e 7469 7479 225d 2e67 6574 2822  ["entity"].get("
+00011090: 7069 7065 6c69 6e65 222c 207b 7d29 2e67  pipeline", {}).g
+000110a0: 6574 2822 6964 2229 0a20 2020 2020 2020  et("id").       
+000110b0: 2020 2020 2020 2020 2020 2020 2070 6970               pip
+000110c0: 656c 696e 655f 6e6f 6465 735f 6c69 7374  eline_nodes_list
+000110d0: 203d 2028 0a20 2020 2020 2020 2020 2020   = (.           
+000110e0: 2020 2020 2020 2020 2020 2020 2073 656c               sel
+000110f0: 662e 5f63 6c69 656e 742e 7069 7065 6c69  f._client.pipeli
+00011100: 6e65 732e 6765 745f 6465 7461 696c 7328  nes.get_details(
+00011110: 7069 7065 6c69 6e65 5f69 6429 5b22 656e  pipeline_id)["en
+00011120: 7469 7479 225d 0a20 2020 2020 2020 2020  tity"].         
+00011130: 2020 2020 2020 2020 2020 2020 2020 202e                 .
+00011140: 6765 7428 2264 6f63 756d 656e 7422 2c20  get("document", 
+00011150: 7b7d 290a 2020 2020 2020 2020 2020 2020  {}).            
+00011160: 2020 2020 2020 2020 2020 2020 2e67 6574              .get
+00011170: 2822 7069 7065 6c69 6e65 7322 2c20 5b5d  ("pipelines", []
+00011180: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
+00011190: 2020 2020 2020 2020 2020 6966 2070 6970            if pip
+000111a0: 656c 696e 655f 6964 0a20 2020 2020 2020  eline_id.       
+000111b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000111c0: 2065 6c73 6520 5b5d 0a20 2020 2020 2020   else [].       
+000111d0: 2020 2020 2020 2020 2020 2020 2029 0a20               ). 
+000111e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000111f0: 2020 2069 6620 280a 2020 2020 2020 2020     if (.        
+00011200: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011210: 6c65 6e28 7069 7065 6c69 6e65 5f6e 6f64  len(pipeline_nod
+00011220: 6573 5f6c 6973 7429 203d 3d20 300a 2020  es_list) == 0.  
 00011230: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011240: 2020 6f72 2070 6970 656c 696e 655f 6e6f    or pipeline_no
-00011250: 6465 735f 6c69 7374 5b30 5d5b 2269 6422  des_list[0]["id"
-00011260: 5d20 213d 2022 6175 746f 6169 220a 2020  ] != "autoai".  
-00011270: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011280: 2020 293a 0a20 2020 2020 2020 2020 2020    ):.           
-00011290: 2020 2020 2020 2020 2020 2020 2072 6169               rai
-000112a0: 7365 2057 4d4c 436c 6965 6e74 4572 726f  se WMLClientErro
-000112b0: 7228 0a20 2020 2020 2020 2020 2020 2020  r(.             
-000112c0: 2020 2020 2020 2020 2020 2020 2020 2022                 "
-000112d0: 5061 7261 6d65 7465 7220 7472 6169 6e69  Parameter traini
-000112e0: 6e67 5f69 6420 6f72 2065 7870 6572 696d  ng_id or experim
-000112f0: 656e 745f 6d65 7461 6461 7461 2069 7320  ent_metadata is 
-00011300: 6e6f 7420 636f 6e6e 6563 7465 6420 746f  not connected to
-00011310: 2041 7574 6f41 4920 7472 6169 6e69 6e67   AutoAI training
-00011320: 220a 2020 2020 2020 2020 2020 2020 2020  ".              
-00011330: 2020 2020 2020 2020 2020 290a 0a20 2020            )..   
-00011340: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011350: 2069 6620 6973 5f6c 616c 655f 7069 7065   if is_lale_pipe
-00011360: 6c69 6e65 286d 6f64 656c 293a 0a20 2020  line(model):.   
-00011370: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011380: 2020 2020 206d 6f64 656c 203d 206d 6f64       model = mod
-00011390: 656c 2e65 7870 6f72 745f 746f 5f73 6b6c  el.export_to_skl
-000113a0: 6561 726e 5f70 6970 656c 696e 6528 290a  earn_pipeline().
-000113b0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-000113c0: 2020 2020 2077 6974 6820 7761 726e 696e       with warnin
-000113d0: 6773 2e63 6174 6368 5f77 6172 6e69 6e67  gs.catch_warning
-000113e0: 7328 293a 0a20 2020 2020 2020 2020 2020  s():.           
-000113f0: 2020 2020 2020 2020 2020 2020 2077 6172               war
-00011400: 6e69 6e67 732e 7369 6d70 6c65 6669 6c74  nings.simplefilt
-00011410: 6572 2822 6967 6e6f 7265 222c 2063 6174  er("ignore", cat
-00011420: 6567 6f72 793d 4465 7072 6563 6174 696f  egory=Deprecatio
-00011430: 6e57 6172 6e69 6e67 290a 2020 2020 2020  nWarning).      
+00011240: 2020 2020 2020 6f72 2070 6970 656c 696e        or pipelin
+00011250: 655f 6e6f 6465 735f 6c69 7374 5b30 5d5b  e_nodes_list[0][
+00011260: 2269 6422 5d20 213d 2022 6175 746f 6169  "id"] != "autoai
+00011270: 220a 2020 2020 2020 2020 2020 2020 2020  ".              
+00011280: 2020 2020 2020 293a 0a20 2020 2020 2020        ):.       
+00011290: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000112a0: 2072 6169 7365 2057 4d4c 436c 6965 6e74   raise WMLClient
+000112b0: 4572 726f 7228 0a20 2020 2020 2020 2020  Error(.         
+000112c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000112d0: 2020 2022 5061 7261 6d65 7465 7220 7472     "Parameter tr
+000112e0: 6169 6e69 6e67 5f69 6420 6f72 2065 7870  aining_id or exp
+000112f0: 6572 696d 656e 745f 6d65 7461 6461 7461  eriment_metadata
+00011300: 2069 7320 6e6f 7420 636f 6e6e 6563 7465   is not connecte
+00011310: 6420 746f 2041 7574 6f41 4920 7472 6169  d to AutoAI trai
+00011320: 6e69 6e67 220a 2020 2020 2020 2020 2020  ning".          
+00011330: 2020 2020 2020 2020 2020 2020 2020 290a                ).
+00011340: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00011350: 2020 2020 2069 6620 6973 5f6c 616c 655f       if is_lale_
+00011360: 7069 7065 6c69 6e65 286d 6f64 656c 293a  pipeline(model):
+00011370: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00011380: 2020 2020 2020 2020 206d 6f64 656c 203d           model =
+00011390: 206d 6f64 656c 2e65 7870 6f72 745f 746f   model.export_to
+000113a0: 5f73 6b6c 6561 726e 5f70 6970 656c 696e  _sklearn_pipelin
+000113b0: 6528 290a 0a20 2020 2020 2020 2020 2020  e()..           
+000113c0: 2020 2020 2020 2020 2077 6974 6820 7761           with wa
+000113d0: 726e 696e 6773 2e63 6174 6368 5f77 6172  rnings.catch_war
+000113e0: 6e69 6e67 7328 293a 0a20 2020 2020 2020  nings():.       
+000113f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011400: 2077 6172 6e69 6e67 732e 7369 6d70 6c65   warnings.simple
+00011410: 6669 6c74 6572 2822 6967 6e6f 7265 222c  filter("ignore",
+00011420: 2063 6174 6567 6f72 793d 4465 7072 6563   category=Deprec
+00011430: 6174 696f 6e57 6172 6e69 6e67 290a 2020  ationWarning).  
 00011440: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011450: 2020 7363 6865 6d61 2c20 6172 7469 6661    schema, artifa
-00011460: 6374 5f6e 616d 6520 3d20 7072 6570 6172  ct_name = prepar
-00011470: 655f 6175 746f 5f61 695f 6d6f 6465 6c5f  e_auto_ai_model_
-00011480: 746f 5f70 7562 6c69 7368 280a 2020 2020  to_publish(.    
+00011450: 2020 2020 2020 7363 6865 6d61 2c20 6172        schema, ar
+00011460: 7469 6661 6374 5f6e 616d 6520 3d20 7072  tifact_name = pr
+00011470: 6570 6172 655f 6175 746f 5f61 695f 6d6f  epare_auto_ai_mo
+00011480: 6465 6c5f 746f 5f70 7562 6c69 7368 280a  del_to_publish(.
 00011490: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000114a0: 2020 2020 2020 2020 7069 7065 6c69 6e65          pipeline
-000114b0: 5f6d 6f64 656c 3d6d 6f64 656c 2c0a 2020  _model=model,.  
-000114c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000114d0: 2020 2020 2020 2020 2020 7275 6e5f 7061            run_pa
-000114e0: 7261 6d73 3d72 756e 5f70 6172 616d 732c  rams=run_params,
-000114f0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00011500: 2020 2020 2020 2020 2020 2020 2072 756e               run
-00011510: 5f69 643d 7472 6169 6e69 6e67 5f69 642c  _id=training_id,
-00011520: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00011530: 2020 2020 2020 2020 2020 2020 2061 7069               api
-00011540: 5f63 6c69 656e 743d 7365 6c66 2e5f 636c  _client=self._cl
-00011550: 6965 6e74 2c0a 2020 2020 2020 2020 2020  ient,.          
-00011560: 2020 2020 2020 2020 2020 2020 2020 290a                ).
-00011570: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00011580: 2020 2020 206e 6577 5f6d 6574 615f 7072       new_meta_pr
-00011590: 6f70 7320 3d20 7b0a 2020 2020 2020 2020  ops = {.        
+000114a0: 2020 2020 2020 2020 2020 2020 7069 7065              pipe
+000114b0: 6c69 6e65 5f6d 6f64 656c 3d6d 6f64 656c  line_model=model
+000114c0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+000114d0: 2020 2020 2020 2020 2020 2020 2020 7275                ru
+000114e0: 6e5f 7061 7261 6d73 3d72 756e 5f70 6172  n_params=run_par
+000114f0: 616d 732c 0a20 2020 2020 2020 2020 2020  ams,.           
+00011500: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011510: 2072 756e 5f69 643d 7472 6169 6e69 6e67   run_id=training
+00011520: 5f69 642c 0a20 2020 2020 2020 2020 2020  _id,.           
+00011530: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011540: 2061 7069 5f63 6c69 656e 743d 7365 6c66   api_client=self
+00011550: 2e5f 636c 6965 6e74 2c0a 2020 2020 2020  ._client,.      
+00011560: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011570: 2020 290a 0a20 2020 2020 2020 2020 2020    )..           
+00011580: 2020 2020 2020 2020 206e 6577 5f6d 6574           new_met
+00011590: 615f 7072 6f70 7320 3d20 7b0a 2020 2020  a_props = {.    
 000115a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000115b0: 7365 6c66 2e5f 636c 6965 6e74 2e72 6570  self._client.rep
-000115c0: 6f73 6974 6f72 792e 4d6f 6465 6c4d 6574  ository.ModelMet
-000115d0: 614e 616d 6573 2e54 5950 453a 2022 776d  aNames.TYPE: "wm
-000115e0: 6c2d 6879 6272 6964 5f30 2e31 222c 0a20  l-hybrid_0.1",. 
-000115f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011600: 2020 2020 2020 2073 656c 662e 5f63 6c69         self._cli
-00011610: 656e 742e 7265 706f 7369 746f 7279 2e4d  ent.repository.M
-00011620: 6f64 656c 4d65 7461 4e61 6d65 732e 534f  odelMetaNames.SO
-00011630: 4654 5741 5245 5f53 5045 435f 4944 3a20  FTWARE_SPEC_ID: 
-00011640: 7365 6c66 2e5f 636c 6965 6e74 2e73 6f66  self._client.sof
-00011650: 7477 6172 655f 7370 6563 6966 6963 6174  tware_specificat
-00011660: 696f 6e73 2e67 6574 5f69 645f 6279 5f6e  ions.get_id_by_n
-00011670: 616d 6528 0a20 2020 2020 2020 2020 2020  ame(.           
+000115b0: 2020 2020 7365 6c66 2e5f 636c 6965 6e74      self._client
+000115c0: 2e72 6570 6f73 6974 6f72 792e 4d6f 6465  .repository.Mode
+000115d0: 6c4d 6574 614e 616d 6573 2e54 5950 453a  lMetaNames.TYPE:
+000115e0: 2022 776d 6c2d 6879 6272 6964 5f30 2e31   "wml-hybrid_0.1
+000115f0: 222c 0a20 2020 2020 2020 2020 2020 2020  ",.             
+00011600: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+00011610: 5f63 6c69 656e 742e 7265 706f 7369 746f  _client.reposito
+00011620: 7279 2e4d 6f64 656c 4d65 7461 4e61 6d65  ry.ModelMetaName
+00011630: 732e 534f 4654 5741 5245 5f53 5045 435f  s.SOFTWARE_SPEC_
+00011640: 4944 3a20 7365 6c66 2e5f 636c 6965 6e74  ID: self._client
+00011650: 2e73 6f66 7477 6172 655f 7370 6563 6966  .software_specif
+00011660: 6963 6174 696f 6e73 2e67 6574 5f69 645f  ications.get_id_
+00011670: 6279 5f6e 616d 6528 0a20 2020 2020 2020  by_name(.       
 00011680: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011690: 2022 6879 6272 6964 5f30 2e31 220a 2020   "hybrid_0.1".  
-000116a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000116b0: 2020 2020 2020 292c 0a20 2020 2020 2020        ),.       
-000116c0: 2020 2020 2020 2020 2020 2020 207d 0a0a               }..
-000116d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000116e0: 2020 2020 7265 7375 6c74 735f 7265 6665      results_refe
-000116f0: 7265 6e63 6520 3d20 4461 7461 436f 6e6e  rence = DataConn
-00011700: 6563 7469 6f6e 2e5f 6672 6f6d 5f64 6963  ection._from_dic
-00011710: 7428 0a20 2020 2020 2020 2020 2020 2020  t(.             
-00011720: 2020 2020 2020 2020 2020 2072 756e 5f70             run_p
-00011730: 6172 616d 735b 2265 6e74 6974 7922 5d5b  arams["entity"][
-00011740: 2272 6573 756c 7473 5f72 6566 6572 656e  "results_referen
-00011750: 6365 225d 0a20 2020 2020 2020 2020 2020  ce"].           
-00011760: 2020 2020 2020 2020 2029 0a20 2020 2020           ).     
-00011770: 2020 2020 2020 2020 2020 2020 2020 2072                 r
-00011780: 6573 756c 7473 5f72 6566 6572 656e 6365  esults_reference
-00011790: 2e73 6574 5f63 6c69 656e 7428 7365 6c66  .set_client(self
-000117a0: 2e5f 636c 6965 6e74 290a 0a20 2020 2020  ._client)..     
-000117b0: 2020 2020 2020 2020 2020 2020 2020 2072                 r
-000117c0: 6571 7565 7374 5f6a 736f 6e20 3d20 646f  equest_json = do
-000117d0: 776e 6c6f 6164 5f72 6571 7565 7374 5f6a  wnload_request_j
-000117e0: 736f 6e28 0a20 2020 2020 2020 2020 2020  son(.           
-000117f0: 2020 2020 2020 2020 2020 2020 2072 756e               run
-00011800: 5f70 6172 616d 733d 7275 6e5f 7061 7261  _params=run_para
-00011810: 6d73 2c0a 2020 2020 2020 2020 2020 2020  ms,.            
-00011820: 2020 2020 2020 2020 2020 2020 6d6f 6465              mode
-00011830: 6c5f 6e61 6d65 3d73 656c 662e 5f67 6574  l_name=self._get
-00011840: 5f6c 6173 745f 7275 6e5f 6d65 7472 6963  _last_run_metric
-00011850: 735f 6e61 6d65 280a 2020 2020 2020 2020  s_name(.        
+00011690: 2020 2020 2022 6879 6272 6964 5f30 2e31       "hybrid_0.1
+000116a0: 220a 2020 2020 2020 2020 2020 2020 2020  ".              
+000116b0: 2020 2020 2020 2020 2020 292c 0a20 2020            ),.   
+000116c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000116d0: 207d 0a0a 2020 2020 2020 2020 2020 2020   }..            
+000116e0: 2020 2020 2020 2020 7265 7375 6c74 735f          results_
+000116f0: 7265 6665 7265 6e63 6520 3d20 4461 7461  reference = Data
+00011700: 436f 6e6e 6563 7469 6f6e 2e5f 6672 6f6d  Connection._from
+00011710: 5f64 6963 7428 0a20 2020 2020 2020 2020  _dict(.         
+00011720: 2020 2020 2020 2020 2020 2020 2020 2072                 r
+00011730: 756e 5f70 6172 616d 735b 2265 6e74 6974  un_params["entit
+00011740: 7922 5d5b 2272 6573 756c 7473 5f72 6566  y"]["results_ref
+00011750: 6572 656e 6365 225d 0a20 2020 2020 2020  erence"].       
+00011760: 2020 2020 2020 2020 2020 2020 2029 0a20               ). 
+00011770: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011780: 2020 2072 6573 756c 7473 5f72 6566 6572     results_refer
+00011790: 656e 6365 2e73 6574 5f63 6c69 656e 7428  ence.set_client(
+000117a0: 7365 6c66 2e5f 636c 6965 6e74 290a 0a20  self._client).. 
+000117b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000117c0: 2020 2072 6571 7565 7374 5f6a 736f 6e20     request_json 
+000117d0: 3d20 646f 776e 6c6f 6164 5f72 6571 7565  = download_reque
+000117e0: 7374 5f6a 736f 6e28 0a20 2020 2020 2020  st_json(.       
+000117f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011800: 2072 756e 5f70 6172 616d 733d 7275 6e5f   run_params=run_
+00011810: 7061 7261 6d73 2c0a 2020 2020 2020 2020  params,.        
+00011820: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011830: 6d6f 6465 6c5f 6e61 6d65 3d73 656c 662e  model_name=self.
+00011840: 5f67 6574 5f6c 6173 745f 7275 6e5f 6d65  _get_last_run_me
+00011850: 7472 6963 735f 6e61 6d65 280a 2020 2020  trics_name(.    
 00011860: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011870: 2020 2020 7472 6169 6e69 6e67 5f69 643d      training_id=
-00011880: 6361 7374 2873 7472 2c20 7472 6169 6e69  cast(str, traini
-00011890: 6e67 5f69 6429 0a20 2020 2020 2020 2020  ng_id).         
-000118a0: 2020 2020 2020 2020 2020 2020 2020 2029                 )
-000118b0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-000118c0: 2020 2020 2020 2020 2020 6170 695f 636c            api_cl
-000118d0: 6965 6e74 3d73 656c 662e 5f63 6c69 656e  ient=self._clien
-000118e0: 742c 0a20 2020 2020 2020 2020 2020 2020  t,.             
-000118f0: 2020 2020 2020 2020 2020 2072 6573 756c             resul
-00011900: 7473 5f72 6566 6572 656e 6365 3d72 6573  ts_reference=res
-00011910: 756c 7473 5f72 6566 6572 656e 6365 2c0a  ults_reference,.
-00011920: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011930: 2020 2020 290a 2020 2020 2020 2020 2020      ).          
-00011940: 2020 2020 2020 2020 2020 6966 2072 6571            if req
-00011950: 7565 7374 5f6a 736f 6e3a 0a20 2020 2020  uest_json:.     
+00011870: 2020 2020 2020 2020 7472 6169 6e69 6e67          training
+00011880: 5f69 643d 6361 7374 2873 7472 2c20 7472  _id=cast(str, tr
+00011890: 6169 6e69 6e67 5f69 6429 0a20 2020 2020  aining_id).     
+000118a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000118b0: 2020 2029 2c0a 2020 2020 2020 2020 2020     ),.          
+000118c0: 2020 2020 2020 2020 2020 2020 2020 6170                ap
+000118d0: 695f 636c 6965 6e74 3d73 656c 662e 5f63  i_client=self._c
+000118e0: 6c69 656e 742c 0a20 2020 2020 2020 2020  lient,.         
+000118f0: 2020 2020 2020 2020 2020 2020 2020 2072                 r
+00011900: 6573 756c 7473 5f72 6566 6572 656e 6365  esults_reference
+00011910: 3d72 6573 756c 7473 5f72 6566 6572 656e  =results_referen
+00011920: 6365 2c0a 2020 2020 2020 2020 2020 2020  ce,.            
+00011930: 2020 2020 2020 2020 290a 2020 2020 2020          ).      
+00011940: 2020 2020 2020 2020 2020 2020 2020 6966                if
+00011950: 2072 6571 7565 7374 5f6a 736f 6e3a 0a20   request_json:. 
 00011960: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011970: 2020 2069 6620 2273 6368 656d 6173 2220     if "schemas" 
-00011980: 696e 2072 6571 7565 7374 5f6a 736f 6e3a  in request_json:
-00011990: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-000119a0: 2020 2020 2020 2020 2020 2020 206e 6577               new
-000119b0: 5f6d 6574 615f 7072 6f70 735b 2273 6368  _meta_props["sch
-000119c0: 656d 6173 225d 203d 2072 6571 7565 7374  emas"] = request
-000119d0: 5f6a 736f 6e5b 2273 6368 656d 6173 225d  _json["schemas"]
-000119e0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-000119f0: 2020 2020 2020 2020 2069 6620 2268 7962           if "hyb
-00011a00: 7269 645f 7069 7065 6c69 6e65 5f73 6f66  rid_pipeline_sof
-00011a10: 7477 6172 655f 7370 6563 7322 2069 6e20  tware_specs" in 
-00011a20: 7265 7175 6573 745f 6a73 6f6e 3a0a 2020  request_json:.  
-00011a30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011a40: 2020 2020 2020 2020 2020 6e65 775f 6d65            new_me
-00011a50: 7461 5f70 726f 7073 5b0a 2020 2020 2020  ta_props[.      
-00011a60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011a70: 2020 2020 2020 2020 2020 2268 7962 7269            "hybri
-00011a80: 645f 7069 7065 6c69 6e65 5f73 6f66 7477  d_pipeline_softw
-00011a90: 6172 655f 7370 6563 7322 0a20 2020 2020  are_specs".     
-00011aa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011ab0: 2020 2020 2020 205d 203d 2072 6571 7565         ] = reque
-00011ac0: 7374 5f6a 736f 6e5b 2268 7962 7269 645f  st_json["hybrid_
-00011ad0: 7069 7065 6c69 6e65 5f73 6f66 7477 6172  pipeline_softwar
-00011ae0: 655f 7370 6563 7322 5d0a 0a20 2020 2020  e_specs"]..     
-00011af0: 2020 2020 2020 2020 2020 2020 2020 2069                 i
-00011b00: 6620 6578 7065 7269 6d65 6e74 5f6d 6574  f experiment_met
-00011b10: 6164 6174 613a 0a20 2020 2020 2020 2020  adata:.         
-00011b20: 2020 2020 2020 2020 2020 2020 2020 2069                 i
-00011b30: 6620 2270 7265 6469 6374 696f 6e5f 636f  f "prediction_co
-00011b40: 6c75 6d6e 2220 696e 2065 7870 6572 696d  lumn" in experim
-00011b50: 656e 745f 6d65 7461 6461 7461 3a0a 2020  ent_metadata:.  
-00011b60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011b70: 2020 2020 2020 2020 2020 6e65 775f 6d65            new_me
-00011b80: 7461 5f70 726f 7073 5b0a 2020 2020 2020  ta_props[.      
+00011970: 2020 2020 2020 2069 6620 2273 6368 656d         if "schem
+00011980: 6173 2220 696e 2072 6571 7565 7374 5f6a  as" in request_j
+00011990: 736f 6e3a 0a20 2020 2020 2020 2020 2020  son:.           
+000119a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000119b0: 206e 6577 5f6d 6574 615f 7072 6f70 735b   new_meta_props[
+000119c0: 2273 6368 656d 6173 225d 203d 2072 6571  "schemas"] = req
+000119d0: 7565 7374 5f6a 736f 6e5b 2273 6368 656d  uest_json["schem
+000119e0: 6173 225d 0a20 2020 2020 2020 2020 2020  as"].           
+000119f0: 2020 2020 2020 2020 2020 2020 2069 6620               if 
+00011a00: 2268 7962 7269 645f 7069 7065 6c69 6e65  "hybrid_pipeline
+00011a10: 5f73 6f66 7477 6172 655f 7370 6563 7322  _software_specs"
+00011a20: 2069 6e20 7265 7175 6573 745f 6a73 6f6e   in request_json
+00011a30: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
+00011a40: 2020 2020 2020 2020 2020 2020 2020 6e65                ne
+00011a50: 775f 6d65 7461 5f70 726f 7073 5b22 6879  w_meta_props["hy
+00011a60: 6272 6964 5f70 6970 656c 696e 655f 736f  brid_pipeline_so
+00011a70: 6674 7761 7265 5f73 7065 6373 225d 203d  ftware_specs"] =
+00011a80: 2028 0a20 2020 2020 2020 2020 2020 2020   (.             
+00011a90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011aa0: 2020 2072 6571 7565 7374 5f6a 736f 6e5b     request_json[
+00011ab0: 2268 7962 7269 645f 7069 7065 6c69 6e65  "hybrid_pipeline
+00011ac0: 5f73 6f66 7477 6172 655f 7370 6563 7322  _software_specs"
+00011ad0: 5d0a 2020 2020 2020 2020 2020 2020 2020  ].              
+00011ae0: 2020 2020 2020 2020 2020 2020 2020 290a                ).
+00011af0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00011b00: 2020 2020 2069 6620 6578 7065 7269 6d65       if experime
+00011b10: 6e74 5f6d 6574 6164 6174 613a 0a20 2020  nt_metadata:.   
+00011b20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011b30: 2020 2020 2069 6620 2270 7265 6469 6374       if "predict
+00011b40: 696f 6e5f 636f 6c75 6d6e 2220 696e 2065  ion_column" in e
+00011b50: 7870 6572 696d 656e 745f 6d65 7461 6461  xperiment_metada
+00011b60: 7461 3a0a 2020 2020 2020 2020 2020 2020  ta:.            
+00011b70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011b80: 6e65 775f 6d65 7461 5f70 726f 7073 5b0a  new_meta_props[.
 00011b90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011ba0: 2020 2020 2020 2020 2020 7365 6c66 2e5f            self._
-00011bb0: 636c 6965 6e74 2e72 6570 6f73 6974 6f72  client.repositor
-00011bc0: 792e 4d6f 6465 6c4d 6574 614e 616d 6573  y.ModelMetaNames
-00011bd0: 2e4c 4142 454c 5f46 4945 4c44 0a20 2020  .LABEL_FIELD.   
-00011be0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011bf0: 2020 2020 2020 2020 205d 203d 2065 7870           ] = exp
-00011c00: 6572 696d 656e 745f 6d65 7461 6461 7461  eriment_metadata
-00011c10: 2e67 6574 2822 7072 6564 6963 7469 6f6e  .get("prediction
-00011c20: 5f63 6f6c 756d 6e22 290a 0a20 2020 2020  _column")..     
-00011c30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011c40: 2020 2069 6620 2274 7261 696e 696e 675f     if "training_
-00011c50: 6461 7461 5f72 6566 6572 656e 6365 7322  data_references"
-00011c60: 2069 6e20 6578 7065 7269 6d65 6e74 5f6d   in experiment_m
-00011c70: 6574 6164 6174 613a 0a20 2020 2020 2020  etadata:.       
+00011ba0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011bb0: 7365 6c66 2e5f 636c 6965 6e74 2e72 6570  self._client.rep
+00011bc0: 6f73 6974 6f72 792e 4d6f 6465 6c4d 6574  ository.ModelMet
+00011bd0: 614e 616d 6573 2e4c 4142 454c 5f46 4945  aNames.LABEL_FIE
+00011be0: 4c44 0a20 2020 2020 2020 2020 2020 2020  LD.             
+00011bf0: 2020 2020 2020 2020 2020 2020 2020 205d                 ]
+00011c00: 203d 2065 7870 6572 696d 656e 745f 6d65   = experiment_me
+00011c10: 7461 6461 7461 2e67 6574 2822 7072 6564  tadata.get("pred
+00011c20: 6963 7469 6f6e 5f63 6f6c 756d 6e22 290a  iction_column").
+00011c30: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00011c40: 2020 2020 2020 2020 2069 6620 2274 7261           if "tra
+00011c50: 696e 696e 675f 6461 7461 5f72 6566 6572  ining_data_refer
+00011c60: 656e 6365 7322 2069 6e20 6578 7065 7269  ences" in experi
+00011c70: 6d65 6e74 5f6d 6574 6164 6174 613a 0a20  ment_metadata:. 
 00011c80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011c90: 2020 2020 206e 6577 5f6d 6574 615f 7072       new_meta_pr
-00011ca0: 6f70 735b 0a20 2020 2020 2020 2020 2020  ops[.           
+00011c90: 2020 2020 2020 2020 2020 206e 6577 5f6d             new_m
+00011ca0: 6574 615f 7072 6f70 735b 0a20 2020 2020  eta_props[.     
 00011cb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011cc0: 2020 2020 2073 656c 662e 5f63 6c69 656e       self._clien
-00011cd0: 742e 7265 706f 7369 746f 7279 2e4d 6f64  t.repository.Mod
-00011ce0: 656c 4d65 7461 4e61 6d65 732e 5452 4149  elMetaNames.TRAI
-00011cf0: 4e49 4e47 5f44 4154 415f 5245 4645 5245  NING_DATA_REFERE
-00011d00: 4e43 4553 0a20 2020 2020 2020 2020 2020  NCES.           
+00011cc0: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+00011cd0: 5f63 6c69 656e 742e 7265 706f 7369 746f  _client.reposito
+00011ce0: 7279 2e4d 6f64 656c 4d65 7461 4e61 6d65  ry.ModelMetaName
+00011cf0: 732e 5452 4149 4e49 4e47 5f44 4154 415f  s.TRAINING_DATA_
+00011d00: 5245 4645 5245 4e43 4553 0a20 2020 2020  REFERENCES.     
 00011d10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011d20: 205d 203d 205b 0a20 2020 2020 2020 2020   ] = [.         
+00011d20: 2020 2020 2020 205d 203d 205b 0a20 2020         ] = [.   
 00011d30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011d40: 2020 2020 2020 2065 2e5f 746f 5f64 6963         e._to_dic
-00011d50: 7428 2920 6966 2069 7369 6e73 7461 6e63  t() if isinstanc
-00011d60: 6528 652c 2044 6174 6143 6f6e 6e65 6374  e(e, DataConnect
-00011d70: 696f 6e29 2065 6c73 6520 650a 2020 2020  ion) else e.    
-00011d80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011d90: 2020 2020 2020 2020 2020 2020 666f 7220              for 
-00011da0: 6520 696e 2065 7870 6572 696d 656e 745f  e in experiment_
-00011db0: 6d65 7461 6461 7461 2e67 6574 280a 2020  metadata.get(.  
-00011dc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011d40: 2020 2020 2020 2020 2020 2020 2065 2e5f               e._
+00011d50: 746f 5f64 6963 7428 2920 6966 2069 7369  to_dict() if isi
+00011d60: 6e73 7461 6e63 6528 652c 2044 6174 6143  nstance(e, DataC
+00011d70: 6f6e 6e65 6374 696f 6e29 2065 6c73 6520  onnection) else 
+00011d80: 650a 2020 2020 2020 2020 2020 2020 2020  e.              
+00011d90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011da0: 2020 666f 7220 6520 696e 2065 7870 6572    for e in exper
+00011db0: 696d 656e 745f 6d65 7461 6461 7461 2e67  iment_metadata.g
+00011dc0: 6574 280a 2020 2020 2020 2020 2020 2020  et(.            
 00011dd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011de0: 2020 2274 7261 696e 696e 675f 6461 7461    "training_data
-00011df0: 5f72 6566 6572 656e 6365 7322 2c20 5b5d  _references", []
-00011e00: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00011de0: 2020 2020 2020 2020 2274 7261 696e 696e          "trainin
+00011df0: 675f 6461 7461 5f72 6566 6572 656e 6365  g_data_reference
+00011e00: 7322 2c20 5b5d 0a20 2020 2020 2020 2020  s", [].         
 00011e10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011e20: 2029 0a20 2020 2020 2020 2020 2020 2020   ).             
-00011e30: 2020 2020 2020 2020 2020 2020 2020 205d                 ]
-00011e40: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00011e50: 2020 2020 2020 2020 2020 2020 2069 6620               if 
-00011e60: 280a 2020 2020 2020 2020 2020 2020 2020  (.              
+00011e20: 2020 2020 2020 2029 0a20 2020 2020 2020         ).       
+00011e30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011e40: 2020 2020 205d 0a20 2020 2020 2020 2020       ].         
+00011e50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011e60: 2020 2069 6620 280a 2020 2020 2020 2020     if (.        
 00011e70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011e80: 2020 6c65 6e28 0a20 2020 2020 2020 2020    len(.         
+00011e80: 2020 2020 2020 2020 6c65 6e28 0a20 2020          len(.   
 00011e90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011ea0: 2020 2020 2020 2020 2020 206e 6577 5f6d             new_m
-00011eb0: 6574 615f 7072 6f70 735b 0a20 2020 2020  eta_props[.     
-00011ec0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011ea0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011eb0: 206e 6577 5f6d 6574 615f 7072 6f70 735b   new_meta_props[
+00011ec0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
 00011ed0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011ee0: 2020 2073 656c 662e 5f63 6c69 656e 742e     self._client.
-00011ef0: 7265 706f 7369 746f 7279 2e4d 6f64 656c  repository.Model
-00011f00: 4d65 7461 4e61 6d65 732e 5452 4149 4e49  MetaNames.TRAINI
-00011f10: 4e47 5f44 4154 415f 5245 4645 5245 4e43  NG_DATA_REFERENC
-00011f20: 4553 0a20 2020 2020 2020 2020 2020 2020  ES.             
+00011ee0: 2020 2020 2020 2020 2073 656c 662e 5f63           self._c
+00011ef0: 6c69 656e 742e 7265 706f 7369 746f 7279  lient.repository
+00011f00: 2e4d 6f64 656c 4d65 7461 4e61 6d65 732e  .ModelMetaNames.
+00011f10: 5452 4149 4e49 4e47 5f44 4154 415f 5245  TRAINING_DATA_RE
+00011f20: 4645 5245 4e43 4553 0a20 2020 2020 2020  FERENCES.       
 00011f30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011f40: 2020 2020 2020 205d 0a20 2020 2020 2020         ].       
+00011f40: 2020 2020 2020 2020 2020 2020 205d 0a20               ]. 
 00011f50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011f60: 2020 2020 2020 2020 2029 0a20 2020 2020           ).     
-00011f70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011f80: 2020 2020 2020 2020 2020 203e 2030 0a20             > 0. 
-00011f90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011fa0: 2020 2020 2020 2020 2020 2029 3a0a 2020             ):.  
-00011fb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011fc0: 2020 2020 2020 2020 2020 2020 2020 6e65                ne
-00011fd0: 775f 6d65 7461 5f70 726f 7073 5b0a 2020  w_meta_props[.  
-00011fe0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011f60: 2020 2020 2020 2020 2020 2020 2020 2029                 )
+00011f70: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00011f80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011f90: 203e 2030 0a20 2020 2020 2020 2020 2020   > 0.           
+00011fa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011fb0: 2029 3a0a 2020 2020 2020 2020 2020 2020   ):.            
+00011fc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011fd0: 2020 2020 6e65 775f 6d65 7461 5f70 726f      new_meta_pro
+00011fe0: 7073 5b0a 2020 2020 2020 2020 2020 2020  ps[.            
 00011ff0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012000: 2020 7365 6c66 2e5f 636c 6965 6e74 2e72    self._client.r
-00012010: 6570 6f73 6974 6f72 792e 4d6f 6465 6c4d  epository.ModelM
-00012020: 6574 614e 616d 6573 2e54 5241 494e 494e  etaNames.TRAININ
-00012030: 475f 4441 5441 5f52 4546 4552 454e 4345  G_DATA_REFERENCE
-00012040: 530a 2020 2020 2020 2020 2020 2020 2020  S.              
+00012000: 2020 2020 2020 2020 7365 6c66 2e5f 636c          self._cl
+00012010: 6965 6e74 2e72 6570 6f73 6974 6f72 792e  ient.repository.
+00012020: 4d6f 6465 6c4d 6574 614e 616d 6573 2e54  ModelMetaNames.T
+00012030: 5241 494e 494e 475f 4441 5441 5f52 4546  RAINING_DATA_REF
+00012040: 4552 454e 4345 530a 2020 2020 2020 2020  ERENCES.        
 00012050: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012060: 2020 5d5b 305d 5b22 7363 6865 6d61 225d    ][0]["schema"]
-00012070: 203d 2073 6368 656d 610a 0a20 2020 2020   = schema..     
-00012080: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012090: 2020 2069 6620 2274 6573 745f 6461 7461     if "test_data
-000120a0: 5f72 6566 6572 656e 6365 7322 2069 6e20  _references" in 
-000120b0: 6578 7065 7269 6d65 6e74 5f6d 6574 6164  experiment_metad
-000120c0: 6174 613a 0a20 2020 2020 2020 2020 2020  ata:.           
+00012060: 2020 2020 2020 2020 5d5b 305d 5b22 7363          ][0]["sc
+00012070: 6865 6d61 225d 203d 2073 6368 656d 610a  hema"] = schema.
+00012080: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00012090: 2020 2020 2020 2020 2069 6620 2274 6573           if "tes
+000120a0: 745f 6461 7461 5f72 6566 6572 656e 6365  t_data_reference
+000120b0: 7322 2069 6e20 6578 7065 7269 6d65 6e74  s" in experiment
+000120c0: 5f6d 6574 6164 6174 613a 0a20 2020 2020  _metadata:.     
 000120d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000120e0: 206e 6577 5f6d 6574 615f 7072 6f70 735b   new_meta_props[
-000120f0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+000120e0: 2020 2020 2020 206e 6577 5f6d 6574 615f         new_meta_
+000120f0: 7072 6f70 735b 0a20 2020 2020 2020 2020  props[.         
 00012100: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012110: 2073 656c 662e 5f63 6c69 656e 742e 7265   self._client.re
-00012120: 706f 7369 746f 7279 2e4d 6f64 656c 4d65  pository.ModelMe
-00012130: 7461 4e61 6d65 732e 5445 5354 5f44 4154  taNames.TEST_DAT
-00012140: 415f 5245 4645 5245 4e43 4553 0a20 2020  A_REFERENCES.   
-00012150: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012160: 2020 2020 2020 2020 205d 203d 205b 0a20           ] = [. 
-00012170: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012180: 2020 2020 2020 2020 2020 2020 2020 2065                 e
-00012190: 2e5f 746f 5f64 6963 7428 2920 6966 2069  ._to_dict() if i
-000121a0: 7369 6e73 7461 6e63 6528 652c 2044 6174  sinstance(e, Dat
-000121b0: 6143 6f6e 6e65 6374 696f 6e29 2065 6c73  aConnection) els
-000121c0: 6520 650a 2020 2020 2020 2020 2020 2020  e e.            
+00012110: 2020 2020 2020 2073 656c 662e 5f63 6c69         self._cli
+00012120: 656e 742e 7265 706f 7369 746f 7279 2e4d  ent.repository.M
+00012130: 6f64 656c 4d65 7461 4e61 6d65 732e 5445  odelMetaNames.TE
+00012140: 5354 5f44 4154 415f 5245 4645 5245 4e43  ST_DATA_REFERENC
+00012150: 4553 0a20 2020 2020 2020 2020 2020 2020  ES.             
+00012160: 2020 2020 2020 2020 2020 2020 2020 205d                 ]
+00012170: 203d 205b 0a20 2020 2020 2020 2020 2020   = [.           
+00012180: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012190: 2020 2020 2065 2e5f 746f 5f64 6963 7428       e._to_dict(
+000121a0: 2920 6966 2069 7369 6e73 7461 6e63 6528  ) if isinstance(
+000121b0: 652c 2044 6174 6143 6f6e 6e65 6374 696f  e, DataConnectio
+000121c0: 6e29 2065 6c73 6520 650a 2020 2020 2020  n) else e.      
 000121d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000121e0: 2020 2020 666f 7220 6520 696e 2065 7870      for e in exp
-000121f0: 6572 696d 656e 745f 6d65 7461 6461 7461  eriment_metadata
-00012200: 2e67 6574 280a 2020 2020 2020 2020 2020  .get(.          
+000121e0: 2020 2020 2020 2020 2020 666f 7220 6520            for e 
+000121f0: 696e 2065 7870 6572 696d 656e 745f 6d65  in experiment_me
+00012200: 7461 6461 7461 2e67 6574 280a 2020 2020  tadata.get(.    
 00012210: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012220: 2020 2020 2020 2020 2020 2274 6573 745f            "test_
-00012230: 6461 7461 5f72 6566 6572 656e 6365 7322  data_references"
-00012240: 2c20 5b5d 0a20 2020 2020 2020 2020 2020  , [].           
+00012220: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012230: 2274 6573 745f 6461 7461 5f72 6566 6572  "test_data_refer
+00012240: 656e 6365 7322 2c20 5b5d 0a20 2020 2020  ences", [].     
 00012250: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012260: 2020 2020 2029 0a20 2020 2020 2020 2020       ).         
+00012260: 2020 2020 2020 2020 2020 2029 0a20 2020             ).   
 00012270: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012280: 2020 205d 0a20 2020 2020 2020 2020 2020     ].           
-00012290: 2020 2020 2020 2020 2065 6c73 653a 2020           else:  
-000122a0: 2320 6966 2074 7261 696e 696e 675f 6964  # if training_id
-000122b0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-000122c0: 2020 2020 2020 2020 206c 6162 656c 5f63           label_c
-000122d0: 6f6c 756d 6e20 3d20 4e6f 6e65 0a20 2020  olumn = None.   
-000122e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000122f0: 2020 2020 2070 6970 656c 696e 655f 6465       pipeline_de
-00012300: 7461 696c 7320 3d20 7365 6c66 2e5f 636c  tails = self._cl
-00012310: 6965 6e74 2e70 6970 656c 696e 6573 2e67  ient.pipelines.g
-00012320: 6574 5f64 6574 6169 6c73 280a 2020 2020  et_details(.    
-00012330: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012340: 2020 2020 2020 2020 7275 6e5f 7061 7261          run_para
-00012350: 6d73 5b22 656e 7469 7479 225d 5b22 7069  ms["entity"]["pi
-00012360: 7065 6c69 6e65 225d 5b22 6964 225d 0a20  peline"]["id"]. 
-00012370: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012380: 2020 2020 2020 2029 0a20 2020 2020 2020         ).       
+00012280: 2020 2020 2020 2020 205d 0a20 2020 2020           ].     
+00012290: 2020 2020 2020 2020 2020 2020 2020 2065                 e
+000122a0: 6c73 653a 2020 2320 6966 2074 7261 696e  lse:  # if train
+000122b0: 696e 675f 6964 0a20 2020 2020 2020 2020  ing_id.         
+000122c0: 2020 2020 2020 2020 2020 2020 2020 206c                 l
+000122d0: 6162 656c 5f63 6f6c 756d 6e20 3d20 4e6f  abel_column = No
+000122e0: 6e65 0a20 2020 2020 2020 2020 2020 2020  ne.             
+000122f0: 2020 2020 2020 2020 2020 2070 6970 656c             pipel
+00012300: 696e 655f 6465 7461 696c 7320 3d20 7365  ine_details = se
+00012310: 6c66 2e5f 636c 6965 6e74 2e70 6970 656c  lf._client.pipel
+00012320: 696e 6573 2e67 6574 5f64 6574 6169 6c73  ines.get_details
+00012330: 280a 2020 2020 2020 2020 2020 2020 2020  (.              
+00012340: 2020 2020 2020 2020 2020 2020 2020 7275                ru
+00012350: 6e5f 7061 7261 6d73 5b22 656e 7469 7479  n_params["entity
+00012360: 225d 5b22 7069 7065 6c69 6e65 225d 5b22  "]["pipeline"]["
+00012370: 6964 225d 0a20 2020 2020 2020 2020 2020  id"].           
+00012380: 2020 2020 2020 2020 2020 2020 2029 0a20               ). 
 00012390: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000123a0: 2066 6f72 206e 6f64 6520 696e 2070 6970   for node in pip
-000123b0: 656c 696e 655f 6465 7461 696c 735b 2265  eline_details["e
-000123c0: 6e74 6974 7922 5d5b 2264 6f63 756d 656e  ntity"]["documen
-000123d0: 7422 5d5b 2270 6970 656c 696e 6573 225d  t"]["pipelines"]
-000123e0: 5b0a 2020 2020 2020 2020 2020 2020 2020  [.              
-000123f0: 2020 2020 2020 2020 2020 2020 2020 300a                0.
-00012400: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012410: 2020 2020 2020 2020 5d5b 226e 6f64 6573          ]["nodes
-00012420: 225d 3a0a 2020 2020 2020 2020 2020 2020  "]:.            
+000123a0: 2020 2020 2020 2066 6f72 206e 6f64 6520         for node 
+000123b0: 696e 2070 6970 656c 696e 655f 6465 7461  in pipeline_deta
+000123c0: 696c 735b 2265 6e74 6974 7922 5d5b 2264  ils["entity"]["d
+000123d0: 6f63 756d 656e 7422 5d5b 2270 6970 656c  ocument"]["pipel
+000123e0: 696e 6573 225d 5b0a 2020 2020 2020 2020  ines"][.        
+000123f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012400: 2020 2020 300a 2020 2020 2020 2020 2020      0.          
+00012410: 2020 2020 2020 2020 2020 2020 2020 5d5b                ][
+00012420: 226e 6f64 6573 225d 3a0a 2020 2020 2020  "nodes"]:.      
 00012430: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012440: 6966 2022 6175 746f 6d6c 2220 696e 206e  if "automl" in n
-00012450: 6f64 655b 2269 6422 5d20 6f72 2022 6175  ode["id"] or "au
-00012460: 746f 6169 2220 696e 206e 6f64 655b 2269  toai" in node["i
-00012470: 6422 5d3a 0a20 2020 2020 2020 2020 2020  d"]:.           
+00012440: 2020 2020 2020 6966 2022 6175 746f 6d6c        if "automl
+00012450: 2220 696e 206e 6f64 655b 2269 6422 5d20  " in node["id"] 
+00012460: 6f72 2022 6175 746f 6169 2220 696e 206e  or "autoai" in n
+00012470: 6f64 655b 2269 6422 5d3a 0a20 2020 2020  ode["id"]:.     
 00012480: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012490: 2020 2020 206c 6162 656c 5f63 6f6c 756d       label_colum
-000124a0: 6e20 3d20 280a 2020 2020 2020 2020 2020  n = (.          
+00012490: 2020 2020 2020 2020 2020 206c 6162 656c             label
+000124a0: 5f63 6f6c 756d 6e20 3d20 280a 2020 2020  _column = (.    
 000124b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000124c0: 2020 2020 2020 2020 2020 6e6f 6465 2e67            node.g
-000124d0: 6574 2822 7061 7261 6d65 7465 7273 222c  et("parameters",
-000124e0: 207b 7d29 0a20 2020 2020 2020 2020 2020   {}).           
+000124c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000124d0: 6e6f 6465 2e67 6574 2822 7061 7261 6d65  node.get("parame
+000124e0: 7465 7273 222c 207b 7d29 0a20 2020 2020  ters", {}).     
 000124f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012500: 2020 2020 2020 2020 202e 6765 7428 226f           .get("o
-00012510: 7074 696d 697a 6174 696f 6e22 2c20 7b7d  ptimization", {}
-00012520: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
+00012500: 2020 2020 2020 2020 2020 2020 2020 202e                 .
+00012510: 6765 7428 226f 7074 696d 697a 6174 696f  get("optimizatio
+00012520: 6e22 2c20 7b7d 290a 2020 2020 2020 2020  n", {}).        
 00012530: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012540: 2020 2020 2020 2e67 6574 2822 6c61 6265        .get("labe
-00012550: 6c22 2c20 4e6f 6e65 290a 2020 2020 2020  l", None).      
+00012540: 2020 2020 2020 2020 2020 2020 2e67 6574              .get
+00012550: 2822 6c61 6265 6c22 2c20 4e6f 6e65 290a  ("label", None).
 00012560: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012570: 2020 2020 2020 2020 2020 290a 0a20 2020            )..   
-00012580: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012590: 2020 2020 2069 6620 6c61 6265 6c5f 636f       if label_co
-000125a0: 6c75 6d6e 2069 7320 6e6f 7420 4e6f 6e65  lumn is not None
-000125b0: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
-000125c0: 2020 2020 2020 2020 2020 2020 2020 6e65                ne
-000125d0: 775f 6d65 7461 5f70 726f 7073 5b0a 2020  w_meta_props[.  
-000125e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000125f0: 2020 2020 2020 2020 2020 2020 2020 7365                se
-00012600: 6c66 2e5f 636c 6965 6e74 2e72 6570 6f73  lf._client.repos
-00012610: 6974 6f72 792e 4d6f 6465 6c4d 6574 614e  itory.ModelMetaN
-00012620: 616d 6573 2e4c 4142 454c 5f46 4945 4c44  ames.LABEL_FIELD
-00012630: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00012640: 2020 2020 2020 2020 2020 2020 205d 203d               ] =
-00012650: 206c 6162 656c 5f63 6f6c 756d 6e0a 0a20   label_column.. 
-00012660: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012670: 2020 2020 2020 2023 2054 4f44 4f20 4973         # TODO Is
-00012680: 2074 7261 696e 696e 675f 6461 7461 5f72   training_data_r
-00012690: 6566 6572 656e 6365 7320 616e 6420 7465  eferences and te
-000126a0: 7374 5f64 6174 615f 7265 6665 7265 6e63  st_data_referenc
-000126b0: 6573 206e 6565 6465 6420 696e 206d 6574  es needed in met
-000126c0: 6120 7072 6f70 733f 3f0a 2020 2020 2020  a props??.      
+00012570: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012580: 290a 0a20 2020 2020 2020 2020 2020 2020  )..             
+00012590: 2020 2020 2020 2020 2020 2069 6620 6c61             if la
+000125a0: 6265 6c5f 636f 6c75 6d6e 2069 7320 6e6f  bel_column is no
+000125b0: 7420 4e6f 6e65 3a0a 2020 2020 2020 2020  t None:.        
+000125c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000125d0: 2020 2020 6e65 775f 6d65 7461 5f70 726f      new_meta_pro
+000125e0: 7073 5b0a 2020 2020 2020 2020 2020 2020  ps[.            
+000125f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012600: 2020 2020 7365 6c66 2e5f 636c 6965 6e74      self._client
+00012610: 2e72 6570 6f73 6974 6f72 792e 4d6f 6465  .repository.Mode
+00012620: 6c4d 6574 614e 616d 6573 2e4c 4142 454c  lMetaNames.LABEL
+00012630: 5f46 4945 4c44 0a20 2020 2020 2020 2020  _FIELD.         
+00012640: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012650: 2020 205d 203d 206c 6162 656c 5f63 6f6c     ] = label_col
+00012660: 756d 6e0a 0a20 2020 2020 2020 2020 2020  umn..           
+00012670: 2020 2020 2020 2020 2020 2020 2023 2054               # T
+00012680: 4f44 4f20 4973 2074 7261 696e 696e 675f  ODO Is training_
+00012690: 6461 7461 5f72 6566 6572 656e 6365 7320  data_references 
+000126a0: 616e 6420 7465 7374 5f64 6174 615f 7265  and test_data_re
+000126b0: 6665 7265 6e63 6573 206e 6565 6465 6420  ferences needed 
+000126c0: 696e 206d 6574 6120 7072 6f70 733f 3f0a  in meta props??.
 000126d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000126e0: 2020 6966 2022 7472 6169 6e69 6e67 5f64    if "training_d
-000126f0: 6174 615f 7265 6665 7265 6e63 6573 2220  ata_references" 
-00012700: 696e 2072 756e 5f70 6172 616d 735b 2265  in run_params["e
-00012710: 6e74 6974 7922 5d3a 0a20 2020 2020 2020  ntity"]:.       
+000126e0: 2020 2020 2020 2020 6966 2022 7472 6169          if "trai
+000126f0: 6e69 6e67 5f64 6174 615f 7265 6665 7265  ning_data_refere
+00012700: 6e63 6573 2220 696e 2072 756e 5f70 6172  nces" in run_par
+00012710: 616d 735b 2265 6e74 6974 7922 5d3a 0a20  ams["entity"]:. 
 00012720: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012730: 2020 2020 206e 6577 5f6d 6574 615f 7072       new_meta_pr
-00012740: 6f70 735b 0a20 2020 2020 2020 2020 2020  ops[.           
+00012730: 2020 2020 2020 2020 2020 206e 6577 5f6d             new_m
+00012740: 6574 615f 7072 6f70 735b 0a20 2020 2020  eta_props[.     
 00012750: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012760: 2020 2020 2073 656c 662e 5f63 6c69 656e       self._clien
-00012770: 742e 7265 706f 7369 746f 7279 2e4d 6f64  t.repository.Mod
-00012780: 656c 4d65 7461 4e61 6d65 732e 5452 4149  elMetaNames.TRAI
-00012790: 4e49 4e47 5f44 4154 415f 5245 4645 5245  NING_DATA_REFERE
-000127a0: 4e43 4553 0a20 2020 2020 2020 2020 2020  NCES.           
+00012760: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+00012770: 5f63 6c69 656e 742e 7265 706f 7369 746f  _client.reposito
+00012780: 7279 2e4d 6f64 656c 4d65 7461 4e61 6d65  ry.ModelMetaName
+00012790: 732e 5452 4149 4e49 4e47 5f44 4154 415f  s.TRAINING_DATA_
+000127a0: 5245 4645 5245 4e43 4553 0a20 2020 2020  REFERENCES.     
 000127b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000127c0: 205d 203d 2072 756e 5f70 6172 616d 735b   ] = run_params[
-000127d0: 2265 6e74 6974 7922 5d5b 2274 7261 696e  "entity"]["train
-000127e0: 696e 675f 6461 7461 5f72 6566 6572 656e  ing_data_referen
-000127f0: 6365 7322 5d0a 0a20 2020 2020 2020 2020  ces"]..         
-00012800: 2020 2020 2020 2020 2020 2020 2020 2069                 i
-00012810: 6620 2274 6573 745f 6461 7461 5f72 6566  f "test_data_ref
-00012820: 6572 656e 6365 7322 2069 6e20 7275 6e5f  erences" in run_
-00012830: 7061 7261 6d73 5b22 656e 7469 7479 225d  params["entity"]
-00012840: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
-00012850: 2020 2020 2020 2020 2020 2020 2020 6e65                ne
-00012860: 775f 6d65 7461 5f70 726f 7073 5b0a 2020  w_meta_props[.  
-00012870: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012880: 2020 2020 2020 2020 2020 2020 2020 7365                se
-00012890: 6c66 2e5f 636c 6965 6e74 2e72 6570 6f73  lf._client.repos
-000128a0: 6974 6f72 792e 4d6f 6465 6c4d 6574 614e  itory.ModelMetaN
-000128b0: 616d 6573 2e54 5241 494e 494e 475f 4441  ames.TRAINING_DA
-000128c0: 5441 5f52 4546 4552 454e 4345 530a 2020  TA_REFERENCES.  
-000128d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000128e0: 2020 2020 2020 2020 2020 5d20 3d20 7275            ] = ru
-000128f0: 6e5f 7061 7261 6d73 5b22 656e 7469 7479  n_params["entity
-00012900: 225d 5b22 7465 7374 5f64 6174 615f 7265  "]["test_data_re
-00012910: 6665 7265 6e63 6573 225d 0a0a 2020 2020  ferences"]..    
-00012920: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012930: 6966 2072 756e 5f70 6172 616d 735b 2265  if run_params["e
-00012940: 6e74 6974 7922 5d2e 6765 7428 2270 6970  ntity"].get("pip
-00012950: 656c 696e 6522 2c20 7b7d 292e 6765 7428  eline", {}).get(
-00012960: 2269 6422 293a 0a20 2020 2020 2020 2020  "id"):.         
-00012970: 2020 2020 2020 2020 2020 2020 2020 206e                 n
-00012980: 6577 5f6d 6574 615f 7072 6f70 735b 0a20  ew_meta_props[. 
-00012990: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000129a0: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-000129b0: 5f63 6c69 656e 742e 7265 706f 7369 746f  _client.reposito
-000129c0: 7279 2e4d 6f64 656c 4d65 7461 4e61 6d65  ry.ModelMetaName
-000129d0: 732e 5049 5045 4c49 4e45 5f49 440a 2020  s.PIPELINE_ID.  
-000129e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000129f0: 2020 2020 2020 5d20 3d20 7275 6e5f 7061        ] = run_pa
-00012a00: 7261 6d73 5b22 656e 7469 7479 225d 5b22  rams["entity"]["
-00012a10: 7069 7065 6c69 6e65 225d 5b22 6964 225d  pipeline"]["id"]
-00012a20: 0a0a 2020 2020 2020 2020 2020 2020 2020  ..              
-00012a30: 2020 2020 2020 6e65 775f 6d65 7461 5f70        new_meta_p
-00012a40: 726f 7073 2e75 7064 6174 6528 6d65 7461  rops.update(meta
-00012a50: 5f70 726f 7073 290a 0a20 2020 2020 2020  _props)..       
-00012a60: 2020 2020 2020 2020 2020 2020 2073 6176               sav
-00012a70: 6564 5f6d 6f64 656c 203d 2073 656c 662e  ed_model = self.
-00012a80: 5f63 6c69 656e 742e 7265 706f 7369 746f  _client.reposito
-00012a90: 7279 2e73 746f 7265 5f6d 6f64 656c 280a  ry.store_model(.
-00012aa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012ab0: 2020 2020 2020 2020 6d6f 6465 6c3d 6172          model=ar
-00012ac0: 7469 6661 6374 5f6e 616d 652c 206d 6574  tifact_name, met
-00012ad0: 615f 7072 6f70 733d 6e65 775f 6d65 7461  a_props=new_meta
-00012ae0: 5f70 726f 7073 0a20 2020 2020 2020 2020  _props.         
-00012af0: 2020 2020 2020 2020 2020 2029 0a20 2020             ).   
-00012b00: 2020 2020 2020 2020 2020 2020 2065 6c73               els
-00012b10: 653a 0a20 2020 2020 2020 2020 2020 2020  e:.             
-00012b20: 2020 2020 2020 2073 6176 6564 5f6d 6f64         saved_mod
-00012b30: 656c 203d 2073 656c 662e 5f70 7562 6c69  el = self._publi
-00012b40: 7368 5f66 726f 6d5f 6f62 6a65 6374 5f63  sh_from_object_c
-00012b50: 6c6f 7564 280a 2020 2020 2020 2020 2020  loud(.          
-00012b60: 2020 2020 2020 2020 2020 2020 2020 6d6f                mo
-00012b70: 6465 6c3d 6d6f 6465 6c2c 0a20 2020 2020  del=model,.     
-00012b80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012b90: 2020 206d 6574 615f 7072 6f70 733d 6d65     meta_props=me
-00012ba0: 7461 5f70 726f 7073 2c0a 2020 2020 2020  ta_props,.      
+000127c0: 2020 2020 2020 205d 203d 2072 756e 5f70         ] = run_p
+000127d0: 6172 616d 735b 2265 6e74 6974 7922 5d5b  arams["entity"][
+000127e0: 2274 7261 696e 696e 675f 6461 7461 5f72  "training_data_r
+000127f0: 6566 6572 656e 6365 7322 5d0a 0a20 2020  eferences"]..   
+00012800: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012810: 2020 2020 2069 6620 2274 6573 745f 6461       if "test_da
+00012820: 7461 5f72 6566 6572 656e 6365 7322 2069  ta_references" i
+00012830: 6e20 7275 6e5f 7061 7261 6d73 5b22 656e  n run_params["en
+00012840: 7469 7479 225d 3a0a 2020 2020 2020 2020  tity"]:.        
+00012850: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012860: 2020 2020 6e65 775f 6d65 7461 5f70 726f      new_meta_pro
+00012870: 7073 5b0a 2020 2020 2020 2020 2020 2020  ps[.            
+00012880: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012890: 2020 2020 7365 6c66 2e5f 636c 6965 6e74      self._client
+000128a0: 2e72 6570 6f73 6974 6f72 792e 4d6f 6465  .repository.Mode
+000128b0: 6c4d 6574 614e 616d 6573 2e54 5241 494e  lMetaNames.TRAIN
+000128c0: 494e 475f 4441 5441 5f52 4546 4552 454e  ING_DATA_REFEREN
+000128d0: 4345 530a 2020 2020 2020 2020 2020 2020  CES.            
+000128e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000128f0: 5d20 3d20 7275 6e5f 7061 7261 6d73 5b22  ] = run_params["
+00012900: 656e 7469 7479 225d 5b22 7465 7374 5f64  entity"]["test_d
+00012910: 6174 615f 7265 6665 7265 6e63 6573 225d  ata_references"]
+00012920: 0a0a 2020 2020 2020 2020 2020 2020 2020  ..              
+00012930: 2020 2020 2020 6966 2072 756e 5f70 6172        if run_par
+00012940: 616d 735b 2265 6e74 6974 7922 5d2e 6765  ams["entity"].ge
+00012950: 7428 2270 6970 656c 696e 6522 2c20 7b7d  t("pipeline", {}
+00012960: 292e 6765 7428 2269 6422 293a 0a20 2020  ).get("id"):.   
+00012970: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012980: 2020 2020 206e 6577 5f6d 6574 615f 7072       new_meta_pr
+00012990: 6f70 735b 0a20 2020 2020 2020 2020 2020  ops[.           
+000129a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000129b0: 2073 656c 662e 5f63 6c69 656e 742e 7265   self._client.re
+000129c0: 706f 7369 746f 7279 2e4d 6f64 656c 4d65  pository.ModelMe
+000129d0: 7461 4e61 6d65 732e 5049 5045 4c49 4e45  taNames.PIPELINE
+000129e0: 5f49 440a 2020 2020 2020 2020 2020 2020  _ID.            
+000129f0: 2020 2020 2020 2020 2020 2020 5d20 3d20              ] = 
+00012a00: 7275 6e5f 7061 7261 6d73 5b22 656e 7469  run_params["enti
+00012a10: 7479 225d 5b22 7069 7065 6c69 6e65 225d  ty"]["pipeline"]
+00012a20: 5b22 6964 225d 0a0a 2020 2020 2020 2020  ["id"]..        
+00012a30: 2020 2020 2020 2020 2020 2020 6e65 775f              new_
+00012a40: 6d65 7461 5f70 726f 7073 2e75 7064 6174  meta_props.updat
+00012a50: 6528 6d65 7461 5f70 726f 7073 290a 0a20  e(meta_props).. 
+00012a60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012a70: 2020 2073 6176 6564 5f6d 6f64 656c 203d     saved_model =
+00012a80: 2073 656c 662e 5f63 6c69 656e 742e 7265   self._client.re
+00012a90: 706f 7369 746f 7279 2e73 746f 7265 5f6d  pository.store_m
+00012aa0: 6f64 656c 280a 2020 2020 2020 2020 2020  odel(.          
+00012ab0: 2020 2020 2020 2020 2020 2020 2020 6d6f                mo
+00012ac0: 6465 6c3d 6172 7469 6661 6374 5f6e 616d  del=artifact_nam
+00012ad0: 652c 206d 6574 615f 7072 6f70 733d 6e65  e, meta_props=ne
+00012ae0: 775f 6d65 7461 5f70 726f 7073 0a20 2020  w_meta_props.   
+00012af0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012b00: 2029 0a20 2020 2020 2020 2020 2020 2020   ).             
+00012b10: 2020 2065 6c73 653a 0a20 2020 2020 2020     else:.       
+00012b20: 2020 2020 2020 2020 2020 2020 2073 6176               sav
+00012b30: 6564 5f6d 6f64 656c 203d 2073 656c 662e  ed_model = self.
+00012b40: 5f70 7562 6c69 7368 5f66 726f 6d5f 6f62  _publish_from_ob
+00012b50: 6a65 6374 5f63 6c6f 7564 280a 2020 2020  ject_cloud(.    
+00012b60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012b70: 2020 2020 6d6f 6465 6c3d 6d6f 6465 6c2c      model=model,
+00012b80: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00012b90: 2020 2020 2020 2020 206d 6574 615f 7072           meta_pr
+00012ba0: 6f70 733d 6d65 7461 5f70 726f 7073 2c0a  ops=meta_props,.
 00012bb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012bc0: 2020 7472 6169 6e69 6e67 5f64 6174 613d    training_data=
-00012bd0: 7472 6169 6e69 6e67 5f64 6174 612c 0a20  training_data,. 
-00012be0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012bf0: 2020 2020 2020 2074 7261 696e 696e 675f         training_
-00012c00: 7461 7267 6574 3d74 7261 696e 696e 675f  target=training_
-00012c10: 7461 7267 6574 2c0a 2020 2020 2020 2020  target,.        
+00012bc0: 2020 2020 2020 2020 7472 6169 6e69 6e67          training
+00012bd0: 5f64 6174 613d 7472 6169 6e69 6e67 5f64  _data=training_d
+00012be0: 6174 612c 0a20 2020 2020 2020 2020 2020  ata,.           
+00012bf0: 2020 2020 2020 2020 2020 2020 2074 7261               tra
+00012c00: 696e 696e 675f 7461 7267 6574 3d74 7261  ining_target=tra
+00012c10: 696e 696e 675f 7461 7267 6574 2c0a 2020  ining_target,.  
 00012c20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012c30: 7069 7065 6c69 6e65 3d70 6970 656c 696e  pipeline=pipelin
-00012c40: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
-00012c50: 2020 2020 2020 2020 2020 2066 6561 7475             featu
-00012c60: 7265 5f6e 616d 6573 3d66 6561 7475 7265  re_names=feature
-00012c70: 5f6e 616d 6573 2c0a 2020 2020 2020 2020  _names,.        
+00012c30: 2020 2020 2020 7069 7065 6c69 6e65 3d70        pipeline=p
+00012c40: 6970 656c 696e 652c 0a20 2020 2020 2020  ipeline,.       
+00012c50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012c60: 2066 6561 7475 7265 5f6e 616d 6573 3d66   feature_names=f
+00012c70: 6561 7475 7265 5f6e 616d 6573 2c0a 2020  eature_names,.  
 00012c80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012c90: 6c61 6265 6c5f 636f 6c75 6d6e 5f6e 616d  label_column_nam
-00012ca0: 6573 3d6c 6162 656c 5f63 6f6c 756d 6e5f  es=label_column_
-00012cb0: 6e61 6d65 732c 0a20 2020 2020 2020 2020  names,.         
-00012cc0: 2020 2020 2020 2020 2020 2029 0a0a 2020             )..  
-00012cd0: 2020 2020 2020 656c 7365 3a0a 2020 2020        else:.    
-00012ce0: 2020 2020 2020 2020 6966 2028 0a20 2020          if (.   
-00012cf0: 2020 2020 2020 2020 2020 2020 206d 6f64               mod
-00012d00: 656c 2e65 6e64 7377 6974 6828 222e 7069  el.endswith(".pi
-00012d10: 636b 6c65 2229 206f 7220 6d6f 6465 6c2e  ckle") or model.
-00012d20: 656e 6473 7769 7468 2822 7069 7065 6c69  endswith("pipeli
-00012d30: 6e65 2d6d 6f64 656c 2e6a 736f 6e22 290a  ne-model.json").
-00012d40: 2020 2020 2020 2020 2020 2020 2920 616e              ) an
-00012d50: 6420 6f73 2e70 6174 682e 7365 7020 696e  d os.path.sep in
-00012d60: 206d 6f64 656c 3a0a 2020 2020 2020 2020   model:.        
-00012d70: 2020 2020 2020 2020 2320 4155 544f 2041          # AUTO A
-00012d80: 4920 5472 6169 6e65 6420 6d6f 6465 6c0a  I Trained model.
-00012d90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012da0: 2320 7069 7065 6c69 6e65 2d6d 6f64 656c  # pipeline-model
-00012db0: 2e6a 736f 6e20 6973 206e 6565 6465 6420  .json is needed 
-00012dc0: 666f 7220 4f42 4d20 2b20 4b42 0a20 2020  for OBM + KB.   
-00012dd0: 2020 2020 2020 2020 2020 2020 2073 6176               sav
-00012de0: 6564 5f6d 6f64 656c 203d 2073 656c 662e  ed_model = self.
-00012df0: 5f73 746f 7265 5f61 7574 6f41 495f 6d6f  _store_autoAI_mo
-00012e00: 6465 6c28 0a20 2020 2020 2020 2020 2020  del(.           
-00012e10: 2020 2020 2020 2020 206d 6f64 656c 5f70           model_p
-00012e20: 6174 683d 6d6f 6465 6c2c 0a20 2020 2020  ath=model,.     
-00012e30: 2020 2020 2020 2020 2020 2020 2020 206d                 m
-00012e40: 6574 615f 7072 6f70 733d 6d65 7461 5f70  eta_props=meta_p
-00012e50: 726f 7073 2c0a 2020 2020 2020 2020 2020  rops,.          
-00012e60: 2020 2020 2020 2020 2020 6665 6174 7572            featur
-00012e70: 655f 6e61 6d65 733d 6665 6174 7572 655f  e_names=feature_
-00012e80: 6e61 6d65 732c 0a20 2020 2020 2020 2020  names,.         
-00012e90: 2020 2020 2020 2020 2020 206c 6162 656c             label
-00012ea0: 5f63 6f6c 756d 6e5f 6e61 6d65 733d 6c61  _column_names=la
-00012eb0: 6265 6c5f 636f 6c75 6d6e 5f6e 616d 6573  bel_column_names
-00012ec0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00012ed0: 2020 290a 2020 2020 2020 2020 2020 2020    ).            
-00012ee0: 656c 6966 206d 6f64 656c 2e73 7461 7274  elif model.start
-00012ef0: 7377 6974 6828 2250 6970 656c 696e 655f  swith("Pipeline_
-00012f00: 2229 2061 6e64 2028 6578 7065 7269 6d65  ") and (experime
-00012f10: 6e74 5f6d 6574 6164 6174 6120 6f72 2074  nt_metadata or t
-00012f20: 7261 696e 696e 675f 6964 293a 0a20 2020  raining_id):.   
-00012f30: 2020 2020 2020 2020 2020 2020 2069 6620               if 
-00012f40: 6578 7065 7269 6d65 6e74 5f6d 6574 6164  experiment_metad
-00012f50: 6174 613a 0a0a 2020 2020 2020 2020 2020  ata:..          
-00012f60: 2020 2020 2020 2020 2020 7472 6169 6e69            traini
-00012f70: 6e67 5f69 6420 3d20 6765 745f 6175 746f  ng_id = get_auto
-00012f80: 6169 5f72 756e 5f69 645f 6672 6f6d 5f65  ai_run_id_from_e
-00012f90: 7870 6572 696d 656e 745f 6d65 7461 6461  xperiment_metada
-00012fa0: 7461 280a 2020 2020 2020 2020 2020 2020  ta(.            
-00012fb0: 2020 2020 2020 2020 2020 2020 6578 7065              expe
-00012fc0: 7269 6d65 6e74 5f6d 6574 6164 6174 610a  riment_metadata.
-00012fd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012fe0: 2020 2020 290a 0a20 2020 2020 2020 2020      )..         
-00012ff0: 2020 2020 2020 2023 204e 6f74 653a 2076         # Note: v
-00013000: 616c 6964 6174 6520 6966 2074 7261 696e  alidate if train
-00013010: 696e 675f 6964 2069 7320 6672 6f6d 2041  ing_id is from A
-00013020: 7574 6f41 4920 6578 7065 7269 6d65 6e74  utoAI experiment
-00013030: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00013040: 2072 756e 5f70 6172 616d 7320 3d20 7365   run_params = se
-00013050: 6c66 2e5f 636c 6965 6e74 2e74 7261 696e  lf._client.train
-00013060: 696e 672e 6765 745f 6465 7461 696c 7328  ing.get_details(
-00013070: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00013080: 2020 2020 2074 7261 696e 696e 675f 6964       training_id
-00013090: 3d74 7261 696e 696e 675f 6964 2c20 5f69  =training_id, _i
-000130a0: 6e74 6572 6e61 6c3d 5472 7565 0a20 2020  nternal=True.   
-000130b0: 2020 2020 2020 2020 2020 2020 2029 0a0a               )..
-000130c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000130d0: 2320 7261 6973 6520 616e 2065 7272 6f72  # raise an error
-000130e0: 2077 6865 6e20 5453 2070 6970 656c 696e   when TS pipelin
-000130f0: 6520 6973 2064 6973 6361 7264 6564 206f  e is discarded o
-00013100: 6e65 0a20 2020 2020 2020 2020 2020 2020  ne.             
-00013110: 2020 2063 6865 636b 5f69 665f 7473 5f70     check_if_ts_p
-00013120: 6970 656c 696e 655f 6973 5f77 696e 6e65  ipeline_is_winne
-00013130: 7228 6465 7461 696c 733d 7275 6e5f 7061  r(details=run_pa
-00013140: 7261 6d73 2c20 6d6f 6465 6c5f 6e61 6d65  rams, model_name
-00013150: 3d6d 6f64 656c 290a 0a20 2020 2020 2020  =model)..       
-00013160: 2020 2020 2020 2020 2023 204e 6f74 653a           # Note:
-00013170: 2057 6520 6e65 6564 2074 6f20 6665 7463   We need to fetc
-00013180: 6820 6372 6564 656e 7469 616c 7320 7768  h credentials wh
-00013190: 656e 2027 636f 6e74 6169 6e65 7227 2069  en 'container' i
-000131a0: 7320 7468 6520 7479 7065 0a20 2020 2020  s the type.     
-000131b0: 2020 2020 2020 2020 2020 2069 6620 7275             if ru
-000131c0: 6e5f 7061 7261 6d73 5b22 656e 7469 7479  n_params["entity
-000131d0: 225d 5b22 7265 7375 6c74 735f 7265 6665  "]["results_refe
-000131e0: 7265 6e63 6522 5d5b 2274 7970 6522 5d20  rence"]["type"] 
-000131f0: 3d3d 2022 636f 6e74 6169 6e65 7222 3a0a  == "container":.
-00013200: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00013210: 2020 2020 6461 7461 5f63 6f6e 6e65 6374      data_connect
-00013220: 696f 6e20 3d20 4461 7461 436f 6e6e 6563  ion = DataConnec
-00013230: 7469 6f6e 2e5f 6672 6f6d 5f64 6963 7428  tion._from_dict(
-00013240: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00013250: 2020 2020 2020 2020 205f 6469 6374 3d72           _dict=r
-00013260: 756e 5f70 6172 616d 735b 2265 6e74 6974  un_params["entit
-00013270: 7922 5d5b 2272 6573 756c 7473 5f72 6566  y"]["results_ref
-00013280: 6572 656e 6365 225d 0a20 2020 2020 2020  erence"].       
-00013290: 2020 2020 2020 2020 2020 2020 2029 0a20               ). 
-000132a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000132b0: 2020 2064 6174 615f 636f 6e6e 6563 7469     data_connecti
-000132c0: 6f6e 2e73 6574 5f63 6c69 656e 7428 7365  on.set_client(se
-000132d0: 6c66 2e5f 636c 6965 6e74 290a 2020 2020  lf._client).    
-000132e0: 2020 2020 2020 2020 2020 2020 656c 7365              else
-000132f0: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
-00013300: 2020 2020 2020 6461 7461 5f63 6f6e 6e65        data_conne
-00013310: 6374 696f 6e20 3d20 4e6f 6e65 0a20 2020  ction = None.   
-00013320: 2020 2020 2020 2020 2020 2020 2023 202d               # -
-00013330: 2d2d 2065 6e64 206e 6f74 650a 0a20 2020  -- end note..   
-00013340: 2020 2020 2020 2020 2020 2020 2028 0a20               (. 
-00013350: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00013360: 2020 2061 7274 6966 6163 745f 6e61 6d65     artifact_name
-00013370: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00013380: 2020 2020 2020 6d6f 6465 6c5f 7072 6f70        model_prop
-00013390: 732c 0a20 2020 2020 2020 2020 2020 2020  s,.             
-000133a0: 2020 2029 203d 2070 7265 7061 7265 5f61     ) = prepare_a
-000133b0: 7574 6f5f 6169 5f6d 6f64 656c 5f74 6f5f  uto_ai_model_to_
-000133c0: 7075 626c 6973 685f 6e6f 726d 616c 5f73  publish_normal_s
-000133d0: 6365 6e61 7269 6f28 0a20 2020 2020 2020  cenario(.       
-000133e0: 2020 2020 2020 2020 2020 2020 2070 6970               pip
-000133f0: 656c 696e 655f 6d6f 6465 6c3d 6d6f 6465  eline_model=mode
-00013400: 6c2c 0a20 2020 2020 2020 2020 2020 2020  l,.             
-00013410: 2020 2020 2020 2072 756e 5f70 6172 616d         run_param
-00013420: 733d 7275 6e5f 7061 7261 6d73 2c0a 2020  s=run_params,.  
-00013430: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00013440: 2020 7275 6e5f 6964 3d74 7261 696e 696e    run_id=trainin
-00013450: 675f 6964 2c0a 2020 2020 2020 2020 2020  g_id,.          
-00013460: 2020 2020 2020 2020 2020 6170 695f 636c            api_cl
-00013470: 6965 6e74 3d73 656c 662e 5f63 6c69 656e  ient=self._clien
-00013480: 742c 0a20 2020 2020 2020 2020 2020 2020  t,.             
-00013490: 2020 2020 2020 2072 6573 756c 745f 7265         result_re
-000134a0: 6665 7265 6e63 653d 6461 7461 5f63 6f6e  ference=data_con
-000134b0: 6e65 6374 696f 6e2c 0a20 2020 2020 2020  nection,.       
-000134c0: 2020 2020 2020 2020 2029 0a20 2020 2020           ).     
-000134d0: 2020 2020 2020 2020 2020 206d 6f64 656c             model
-000134e0: 5f70 726f 7073 2e75 7064 6174 6528 6d65  _props.update(me
-000134f0: 7461 5f70 726f 7073 290a 0a20 2020 2020  ta_props)..     
-00013500: 2020 2020 2020 2020 2020 2073 6176 6564             saved
-00013510: 5f6d 6f64 656c 203d 2073 656c 662e 5f63  _model = self._c
-00013520: 6c69 656e 742e 7265 706f 7369 746f 7279  lient.repository
-00013530: 2e73 746f 7265 5f6d 6f64 656c 280a 2020  .store_model(.  
-00013540: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00013550: 2020 6d6f 6465 6c3d 6172 7469 6661 6374    model=artifact
-00013560: 5f6e 616d 652c 206d 6574 615f 7072 6f70  _name, meta_prop
-00013570: 733d 6d6f 6465 6c5f 7072 6f70 730a 2020  s=model_props.  
-00013580: 2020 2020 2020 2020 2020 2020 2020 290a                ).
-00013590: 0a20 2020 2020 2020 2020 2020 2065 6c69  .            eli
-000135a0: 6620 280a 2020 2020 2020 2020 2020 2020  f (.            
-000135b0: 2020 2020 286f 732e 7061 7468 2e73 6570      (os.path.sep
-000135c0: 2069 6e20 6d6f 6465 6c29 206f 7220 6f73   in model) or os
-000135d0: 2e70 6174 682e 6973 6669 6c65 286d 6f64  .path.isfile(mod
-000135e0: 656c 2920 6f72 206f 732e 7061 7468 2e69  el) or os.path.i
-000135f0: 7364 6972 286d 6f64 656c 290a 2020 2020  sdir(model).    
-00013600: 2020 2020 2020 2020 293a 0a20 2020 2020          ):.     
-00013610: 2020 2020 2020 2020 2020 2069 6620 6e6f             if no
-00013620: 7420 6f73 2e70 6174 682e 6973 6669 6c65  t os.path.isfile
-00013630: 286d 6f64 656c 2920 616e 6420 6e6f 7420  (model) and not 
-00013640: 6f73 2e70 6174 682e 6973 6469 7228 6d6f  os.path.isdir(mo
-00013650: 6465 6c29 3a0a 2020 2020 2020 2020 2020  del):.          
-00013660: 2020 2020 2020 2020 2020 7261 6973 6520            raise 
-00013670: 574d 4c43 6c69 656e 7445 7272 6f72 280a  WMLClientError(.
-00013680: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00013690: 2020 2020 2020 2020 2249 6e76 616c 6964          "Invalid
-000136a0: 2070 6174 683a 206e 6569 7468 6572 2066   path: neither f
-000136b0: 696c 6520 6e6f 7220 6469 7265 6374 6f72  ile nor director
-000136c0: 7920 6578 6973 7473 2075 6e64 6572 2074  y exists under t
-000136d0: 6869 7320 7061 7468 3a20 277b 7d27 2e22  his path: '{}'."
-000136e0: 2e66 6f72 6d61 7428 0a20 2020 2020 2020  .format(.       
+00012c90: 2020 2020 2020 6c61 6265 6c5f 636f 6c75        label_colu
+00012ca0: 6d6e 5f6e 616d 6573 3d6c 6162 656c 5f63  mn_names=label_c
+00012cb0: 6f6c 756d 6e5f 6e61 6d65 732c 0a20 2020  olumn_names,.   
+00012cc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012cd0: 2029 0a0a 2020 2020 2020 2020 656c 7365   )..        else
+00012ce0: 3a0a 2020 2020 2020 2020 2020 2020 6966  :.            if
+00012cf0: 2028 0a20 2020 2020 2020 2020 2020 2020   (.             
+00012d00: 2020 206d 6f64 656c 2e65 6e64 7377 6974     model.endswit
+00012d10: 6828 222e 7069 636b 6c65 2229 206f 7220  h(".pickle") or 
+00012d20: 6d6f 6465 6c2e 656e 6473 7769 7468 2822  model.endswith("
+00012d30: 7069 7065 6c69 6e65 2d6d 6f64 656c 2e6a  pipeline-model.j
+00012d40: 736f 6e22 290a 2020 2020 2020 2020 2020  son").          
+00012d50: 2020 2920 616e 6420 6f73 2e70 6174 682e    ) and os.path.
+00012d60: 7365 7020 696e 206d 6f64 656c 3a0a 2020  sep in model:.  
+00012d70: 2020 2020 2020 2020 2020 2020 2020 2320                # 
+00012d80: 4155 544f 2041 4920 5472 6169 6e65 6420  AUTO AI Trained 
+00012d90: 6d6f 6465 6c0a 2020 2020 2020 2020 2020  model.          
+00012da0: 2020 2020 2020 2320 7069 7065 6c69 6e65        # pipeline
+00012db0: 2d6d 6f64 656c 2e6a 736f 6e20 6973 206e  -model.json is n
+00012dc0: 6565 6465 6420 666f 7220 4f42 4d20 2b20  eeded for OBM + 
+00012dd0: 4b42 0a20 2020 2020 2020 2020 2020 2020  KB.             
+00012de0: 2020 2073 6176 6564 5f6d 6f64 656c 203d     saved_model =
+00012df0: 2073 656c 662e 5f73 746f 7265 5f61 7574   self._store_aut
+00012e00: 6f41 495f 6d6f 6465 6c28 0a20 2020 2020  oAI_model(.     
+00012e10: 2020 2020 2020 2020 2020 2020 2020 206d                 m
+00012e20: 6f64 656c 5f70 6174 683d 6d6f 6465 6c2c  odel_path=model,
+00012e30: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00012e40: 2020 2020 206d 6574 615f 7072 6f70 733d       meta_props=
+00012e50: 6d65 7461 5f70 726f 7073 2c0a 2020 2020  meta_props,.    
+00012e60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012e70: 6665 6174 7572 655f 6e61 6d65 733d 6665  feature_names=fe
+00012e80: 6174 7572 655f 6e61 6d65 732c 0a20 2020  ature_names,.   
+00012e90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012ea0: 206c 6162 656c 5f63 6f6c 756d 6e5f 6e61   label_column_na
+00012eb0: 6d65 733d 6c61 6265 6c5f 636f 6c75 6d6e  mes=label_column
+00012ec0: 5f6e 616d 6573 2c0a 2020 2020 2020 2020  _names,.        
+00012ed0: 2020 2020 2020 2020 290a 2020 2020 2020          ).      
+00012ee0: 2020 2020 2020 656c 6966 206d 6f64 656c        elif model
+00012ef0: 2e73 7461 7274 7377 6974 6828 2250 6970  .startswith("Pip
+00012f00: 656c 696e 655f 2229 2061 6e64 2028 6578  eline_") and (ex
+00012f10: 7065 7269 6d65 6e74 5f6d 6574 6164 6174  periment_metadat
+00012f20: 6120 6f72 2074 7261 696e 696e 675f 6964  a or training_id
+00012f30: 293a 0a20 2020 2020 2020 2020 2020 2020  ):.             
+00012f40: 2020 2069 6620 6578 7065 7269 6d65 6e74     if experiment
+00012f50: 5f6d 6574 6164 6174 613a 0a0a 2020 2020  _metadata:..    
+00012f60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012f70: 7472 6169 6e69 6e67 5f69 6420 3d20 6765  training_id = ge
+00012f80: 745f 6175 746f 6169 5f72 756e 5f69 645f  t_autoai_run_id_
+00012f90: 6672 6f6d 5f65 7870 6572 696d 656e 745f  from_experiment_
+00012fa0: 6d65 7461 6461 7461 280a 2020 2020 2020  metadata(.      
+00012fb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012fc0: 2020 6578 7065 7269 6d65 6e74 5f6d 6574    experiment_met
+00012fd0: 6164 6174 610a 2020 2020 2020 2020 2020  adata.          
+00012fe0: 2020 2020 2020 2020 2020 290a 0a20 2020            )..   
+00012ff0: 2020 2020 2020 2020 2020 2020 2023 204e               # N
+00013000: 6f74 653a 2076 616c 6964 6174 6520 6966  ote: validate if
+00013010: 2074 7261 696e 696e 675f 6964 2069 7320   training_id is 
+00013020: 6672 6f6d 2041 7574 6f41 4920 6578 7065  from AutoAI expe
+00013030: 7269 6d65 6e74 0a20 2020 2020 2020 2020  riment.         
+00013040: 2020 2020 2020 2072 756e 5f70 6172 616d         run_param
+00013050: 7320 3d20 7365 6c66 2e5f 636c 6965 6e74  s = self._client
+00013060: 2e74 7261 696e 696e 672e 6765 745f 6465  .training.get_de
+00013070: 7461 696c 7328 0a20 2020 2020 2020 2020  tails(.         
+00013080: 2020 2020 2020 2020 2020 2074 7261 696e             train
+00013090: 696e 675f 6964 3d74 7261 696e 696e 675f  ing_id=training_
+000130a0: 6964 2c20 5f69 6e74 6572 6e61 6c3d 5472  id, _internal=Tr
+000130b0: 7565 0a20 2020 2020 2020 2020 2020 2020  ue.             
+000130c0: 2020 2029 0a0a 2020 2020 2020 2020 2020     )..          
+000130d0: 2020 2020 2020 2320 7261 6973 6520 616e        # raise an
+000130e0: 2065 7272 6f72 2077 6865 6e20 5453 2070   error when TS p
+000130f0: 6970 656c 696e 6520 6973 2064 6973 6361  ipeline is disca
+00013100: 7264 6564 206f 6e65 0a20 2020 2020 2020  rded one.       
+00013110: 2020 2020 2020 2020 2063 6865 636b 5f69           check_i
+00013120: 665f 7473 5f70 6970 656c 696e 655f 6973  f_ts_pipeline_is
+00013130: 5f77 696e 6e65 7228 6465 7461 696c 733d  _winner(details=
+00013140: 7275 6e5f 7061 7261 6d73 2c20 6d6f 6465  run_params, mode
+00013150: 6c5f 6e61 6d65 3d6d 6f64 656c 290a 0a20  l_name=model).. 
+00013160: 2020 2020 2020 2020 2020 2020 2020 2023                 #
+00013170: 204e 6f74 653a 2057 6520 6e65 6564 2074   Note: We need t
+00013180: 6f20 6665 7463 6820 6372 6564 656e 7469  o fetch credenti
+00013190: 616c 7320 7768 656e 2027 636f 6e74 6169  als when 'contai
+000131a0: 6e65 7227 2069 7320 7468 6520 7479 7065  ner' is the type
+000131b0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+000131c0: 2069 6620 7275 6e5f 7061 7261 6d73 5b22   if run_params["
+000131d0: 656e 7469 7479 225d 5b22 7265 7375 6c74  entity"]["result
+000131e0: 735f 7265 6665 7265 6e63 6522 5d5b 2274  s_reference"]["t
+000131f0: 7970 6522 5d20 3d3d 2022 636f 6e74 6169  ype"] == "contai
+00013200: 6e65 7222 3a0a 2020 2020 2020 2020 2020  ner":.          
+00013210: 2020 2020 2020 2020 2020 6461 7461 5f63            data_c
+00013220: 6f6e 6e65 6374 696f 6e20 3d20 4461 7461  onnection = Data
+00013230: 436f 6e6e 6563 7469 6f6e 2e5f 6672 6f6d  Connection._from
+00013240: 5f64 6963 7428 0a20 2020 2020 2020 2020  _dict(.         
+00013250: 2020 2020 2020 2020 2020 2020 2020 205f                 _
+00013260: 6469 6374 3d72 756e 5f70 6172 616d 735b  dict=run_params[
+00013270: 2265 6e74 6974 7922 5d5b 2272 6573 756c  "entity"]["resul
+00013280: 7473 5f72 6566 6572 656e 6365 225d 0a20  ts_reference"]. 
+00013290: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000132a0: 2020 2029 0a20 2020 2020 2020 2020 2020     ).           
+000132b0: 2020 2020 2020 2020 2064 6174 615f 636f           data_co
+000132c0: 6e6e 6563 7469 6f6e 2e73 6574 5f63 6c69  nnection.set_cli
+000132d0: 656e 7428 7365 6c66 2e5f 636c 6965 6e74  ent(self._client
+000132e0: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
+000132f0: 2020 656c 7365 3a0a 2020 2020 2020 2020    else:.        
+00013300: 2020 2020 2020 2020 2020 2020 6461 7461              data
+00013310: 5f63 6f6e 6e65 6374 696f 6e20 3d20 4e6f  _connection = No
+00013320: 6e65 0a20 2020 2020 2020 2020 2020 2020  ne.             
+00013330: 2020 2023 202d 2d2d 2065 6e64 206e 6f74     # --- end not
+00013340: 650a 0a20 2020 2020 2020 2020 2020 2020  e..             
+00013350: 2020 2028 0a20 2020 2020 2020 2020 2020     (.           
+00013360: 2020 2020 2020 2020 2061 7274 6966 6163           artifac
+00013370: 745f 6e61 6d65 2c0a 2020 2020 2020 2020  t_name,.        
+00013380: 2020 2020 2020 2020 2020 2020 6d6f 6465              mode
+00013390: 6c5f 7072 6f70 732c 0a20 2020 2020 2020  l_props,.       
+000133a0: 2020 2020 2020 2020 2029 203d 2070 7265           ) = pre
+000133b0: 7061 7265 5f61 7574 6f5f 6169 5f6d 6f64  pare_auto_ai_mod
+000133c0: 656c 5f74 6f5f 7075 626c 6973 685f 6e6f  el_to_publish_no
+000133d0: 726d 616c 5f73 6365 6e61 7269 6f28 0a20  rmal_scenario(. 
+000133e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000133f0: 2020 2070 6970 656c 696e 655f 6d6f 6465     pipeline_mode
+00013400: 6c3d 6d6f 6465 6c2c 0a20 2020 2020 2020  l=model,.       
+00013410: 2020 2020 2020 2020 2020 2020 2072 756e               run
+00013420: 5f70 6172 616d 733d 7275 6e5f 7061 7261  _params=run_para
+00013430: 6d73 2c0a 2020 2020 2020 2020 2020 2020  ms,.            
+00013440: 2020 2020 2020 2020 7275 6e5f 6964 3d74          run_id=t
+00013450: 7261 696e 696e 675f 6964 2c0a 2020 2020  raining_id,.    
+00013460: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00013470: 6170 695f 636c 6965 6e74 3d73 656c 662e  api_client=self.
+00013480: 5f63 6c69 656e 742c 0a20 2020 2020 2020  _client,.       
+00013490: 2020 2020 2020 2020 2020 2020 2072 6573               res
+000134a0: 756c 745f 7265 6665 7265 6e63 653d 6461  ult_reference=da
+000134b0: 7461 5f63 6f6e 6e65 6374 696f 6e2c 0a20  ta_connection,. 
+000134c0: 2020 2020 2020 2020 2020 2020 2020 2029                 )
+000134d0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+000134e0: 206d 6f64 656c 5f70 726f 7073 2e75 7064   model_props.upd
+000134f0: 6174 6528 6d65 7461 5f70 726f 7073 290a  ate(meta_props).
+00013500: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00013510: 2073 6176 6564 5f6d 6f64 656c 203d 2073   saved_model = s
+00013520: 656c 662e 5f63 6c69 656e 742e 7265 706f  elf._client.repo
+00013530: 7369 746f 7279 2e73 746f 7265 5f6d 6f64  sitory.store_mod
+00013540: 656c 280a 2020 2020 2020 2020 2020 2020  el(.            
+00013550: 2020 2020 2020 2020 6d6f 6465 6c3d 6172          model=ar
+00013560: 7469 6661 6374 5f6e 616d 652c 206d 6574  tifact_name, met
+00013570: 615f 7072 6f70 733d 6d6f 6465 6c5f 7072  a_props=model_pr
+00013580: 6f70 730a 2020 2020 2020 2020 2020 2020  ops.            
+00013590: 2020 2020 290a 0a20 2020 2020 2020 2020      )..         
+000135a0: 2020 2065 6c69 6620 280a 2020 2020 2020     elif (.      
+000135b0: 2020 2020 2020 2020 2020 286f 732e 7061            (os.pa
+000135c0: 7468 2e73 6570 2069 6e20 6d6f 6465 6c29  th.sep in model)
+000135d0: 206f 7220 6f73 2e70 6174 682e 6973 6669   or os.path.isfi
+000135e0: 6c65 286d 6f64 656c 2920 6f72 206f 732e  le(model) or os.
+000135f0: 7061 7468 2e69 7364 6972 286d 6f64 656c  path.isdir(model
+00013600: 290a 2020 2020 2020 2020 2020 2020 293a  ).            ):
+00013610: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00013620: 2069 6620 6e6f 7420 6f73 2e70 6174 682e   if not os.path.
+00013630: 6973 6669 6c65 286d 6f64 656c 2920 616e  isfile(model) an
+00013640: 6420 6e6f 7420 6f73 2e70 6174 682e 6973  d not os.path.is
+00013650: 6469 7228 6d6f 6465 6c29 3a0a 2020 2020  dir(model):.    
+00013660: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00013670: 7261 6973 6520 574d 4c43 6c69 656e 7445  raise WMLClientE
+00013680: 7272 6f72 280a 2020 2020 2020 2020 2020  rror(.          
+00013690: 2020 2020 2020 2020 2020 2020 2020 2249                "I
+000136a0: 6e76 616c 6964 2070 6174 683a 206e 6569  nvalid path: nei
+000136b0: 7468 6572 2066 696c 6520 6e6f 7220 6469  ther file nor di
+000136c0: 7265 6374 6f72 7920 6578 6973 7473 2075  rectory exists u
+000136d0: 6e64 6572 2074 6869 7320 7061 7468 3a20  nder this path: 
+000136e0: 277b 7d27 2e22 2e66 6f72 6d61 7428 0a20  '{}'.".format(. 
 000136f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00013700: 2020 2020 206d 6f64 656c 0a20 2020 2020       model.     
-00013710: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00013720: 2020 2029 0a20 2020 2020 2020 2020 2020     ).           
-00013730: 2020 2020 2020 2020 2029 0a20 2020 2020           ).     
-00013740: 2020 2020 2020 2020 2020 2073 6176 6564             saved
-00013750: 5f6d 6f64 656c 203d 2073 656c 662e 5f70  _model = self._p
-00013760: 7562 6c69 7368 5f66 726f 6d5f 6669 6c65  ublish_from_file
-00013770: 280a 2020 2020 2020 2020 2020 2020 2020  (.              
-00013780: 2020 2020 2020 6d6f 6465 6c3d 6d6f 6465        model=mode
-00013790: 6c2c 0a20 2020 2020 2020 2020 2020 2020  l,.             
-000137a0: 2020 2020 2020 206d 6574 615f 7072 6f70         meta_prop
-000137b0: 733d 6d65 7461 5f70 726f 7073 2c0a 2020  s=meta_props,.  
-000137c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000137d0: 2020 7472 6169 6e69 6e67 5f64 6174 613d    training_data=
-000137e0: 7472 6169 6e69 6e67 5f64 6174 612c 0a20  training_data,. 
-000137f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00013800: 2020 2074 7261 696e 696e 675f 7461 7267     training_targ
-00013810: 6574 3d74 7261 696e 696e 675f 7461 7267  et=training_targ
-00013820: 6574 2c0a 2020 2020 2020 2020 2020 2020  et,.            
-00013830: 2020 2020 2020 2020 7665 723d 7665 7273          ver=vers
-00013840: 696f 6e2c 0a20 2020 2020 2020 2020 2020  ion,.           
-00013850: 2020 2020 2020 2020 2061 7274 6966 6163           artifac
-00013860: 7469 643d 6172 7469 6661 6374 6964 2c0a  tid=artifactid,.
-00013870: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00013880: 2020 2020 6665 6174 7572 655f 6e61 6d65      feature_name
-00013890: 733d 6665 6174 7572 655f 6e61 6d65 732c  s=feature_names,
-000138a0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-000138b0: 2020 2020 206c 6162 656c 5f63 6f6c 756d       label_colum
-000138c0: 6e5f 6e61 6d65 733d 6c61 6265 6c5f 636f  n_names=label_co
-000138d0: 6c75 6d6e 5f6e 616d 6573 2c0a 2020 2020  lumn_names,.    
-000138e0: 2020 2020 2020 2020 2020 2020 290a 2020              ).  
-000138f0: 2020 2020 2020 2020 2020 656c 7365 3a0a            else:.
-00013900: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00013910: 7361 7665 645f 6d6f 6465 6c20 3d20 7365  saved_model = se
-00013920: 6c66 2e5f 7075 626c 6973 685f 6672 6f6d  lf._publish_from
-00013930: 5f74 7261 696e 696e 6728 0a20 2020 2020  _training(.     
-00013940: 2020 2020 2020 2020 2020 2020 2020 206d                 m
-00013950: 6f64 656c 5f69 643d 6d6f 6465 6c2c 0a20  odel_id=model,. 
-00013960: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00013970: 2020 206d 6574 615f 7072 6f70 733d 6d65     meta_props=me
-00013980: 7461 5f70 726f 7073 2c0a 2020 2020 2020  ta_props,.      
-00013990: 2020 2020 2020 2020 2020 2020 2020 7375                su
-000139a0: 6274 7261 696e 696e 6749 643d 7375 6274  btrainingId=subt
-000139b0: 7261 696e 696e 6749 642c 2020 2320 7479  rainingId,  # ty
-000139c0: 7065 3a20 6967 6e6f 7265 5b61 7267 2d74  pe: ignore[arg-t
-000139d0: 7970 655d 0a20 2020 2020 2020 2020 2020  ype].           
-000139e0: 2020 2020 2020 2020 2066 6561 7475 7265           feature
-000139f0: 5f6e 616d 6573 3d66 6561 7475 7265 5f6e  _names=feature_n
-00013a00: 616d 6573 2c0a 2020 2020 2020 2020 2020  ames,.          
-00013a10: 2020 2020 2020 2020 2020 6c61 6265 6c5f            label_
-00013a20: 636f 6c75 6d6e 5f6e 616d 6573 3d6c 6162  column_names=lab
-00013a30: 656c 5f63 6f6c 756d 6e5f 6e61 6d65 732c  el_column_names,
-00013a40: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00013a50: 2020 2020 2072 6f75 6e64 5f6e 756d 6265       round_numbe
-00013a60: 723d 726f 756e 645f 6e75 6d62 6572 2c0a  r=round_number,.
-00013a70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00013a80: 290a 2020 2020 2020 2020 6966 2028 0a20  ).        if (. 
-00013a90: 2020 2020 2020 2020 2020 2022 7379 7374             "syst
-00013aa0: 656d 2220 696e 2073 6176 6564 5f6d 6f64  em" in saved_mod
-00013ab0: 656c 0a20 2020 2020 2020 2020 2020 2061  el.            a
-00013ac0: 6e64 2022 7761 726e 696e 6773 2220 696e  nd "warnings" in
-00013ad0: 2073 6176 6564 5f6d 6f64 656c 5b22 7379   saved_model["sy
-00013ae0: 7374 656d 225d 0a20 2020 2020 2020 2020  stem"].         
-00013af0: 2020 2061 6e64 2073 6176 6564 5f6d 6f64     and saved_mod
-00013b00: 656c 5b22 7379 7374 656d 225d 5b22 7761  el["system"]["wa
-00013b10: 726e 696e 6773 225d 0a20 2020 2020 2020  rnings"].       
-00013b20: 2029 3a0a 2020 2020 2020 2020 2020 2020   ):.            
-00013b30: 6966 2073 6176 6564 5f6d 6f64 656c 5b22  if saved_model["
-00013b40: 7379 7374 656d 225d 5b22 7761 726e 696e  system"]["warnin
-00013b50: 6773 225d 2069 7320 6e6f 7420 4e6f 6e65  gs"] is not None
-00013b60: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
-00013b70: 2020 6d65 7373 6167 6520 3d20 7361 7665    message = save
-00013b80: 645f 6d6f 6465 6c5b 2273 7973 7465 6d22  d_model["system"
-00013b90: 5d5b 2277 6172 6e69 6e67 7322 5d5b 305d  ]["warnings"][0]
-00013ba0: 2e67 6574 2822 6d65 7373 6167 6522 2c20  .get("message", 
-00013bb0: 4e6f 6e65 290a 2020 2020 2020 2020 2020  None).          
-00013bc0: 2020 2020 2020 7072 696e 7428 224e 6f74        print("Not
-00013bd0: 653a 2057 6172 6e69 6e67 7321 2120 3a20  e: Warnings!! : 
-00013be0: 222c 206d 6573 7361 6765 290a 2020 2020  ", message).    
-00013bf0: 2020 2020 7265 7475 726e 2073 6176 6564      return saved
-00013c00: 5f6d 6f64 656c 0a0a 2020 2020 6465 6620  _model..    def 
-00013c10: 7570 6461 7465 280a 2020 2020 2020 2020  update(.        
-00013c20: 7365 6c66 2c0a 2020 2020 2020 2020 6d6f  self,.        mo
-00013c30: 6465 6c5f 6964 3a20 7374 7220 7c20 4e6f  del_id: str | No
-00013c40: 6e65 203d 204e 6f6e 652c 0a20 2020 2020  ne = None,.     
-00013c50: 2020 206d 6574 615f 7072 6f70 733a 2064     meta_props: d
-00013c60: 6963 7420 7c20 4e6f 6e65 203d 204e 6f6e  ict | None = Non
-00013c70: 652c 0a20 2020 2020 2020 2075 7064 6174  e,.        updat
-00013c80: 655f 6d6f 6465 6c3a 204d 4c4d 6f64 656c  e_model: MLModel
-00013c90: 5479 7065 203d 204e 6f6e 652c 0a20 2020  Type = None,.   
-00013ca0: 2020 2020 202a 2a6b 7761 7267 733a 2041       **kwargs: A
-00013cb0: 6e79 2c0a 2020 2020 2920 2d3e 2064 6963  ny,.    ) -> dic
-00013cc0: 745b 7374 722c 2041 6e79 5d3a 0a20 2020  t[str, Any]:.   
-00013cd0: 2020 2020 2022 2222 5570 6461 7465 2065       """Update e
-00013ce0: 7869 7374 696e 6720 6d6f 6465 6c2e 0a0a  xisting model...
-00013cf0: 2020 2020 2020 2020 3a70 6172 616d 206d          :param m
-00013d00: 6f64 656c 5f69 643a 2049 4420 6f66 206d  odel_id: ID of m
-00013d10: 6f64 656c 2077 6869 6368 2064 6566 696e  odel which defin
-00013d20: 6520 7768 6174 2073 686f 756c 6420 6265  e what should be
-00013d30: 2075 7064 6174 6564 0a20 2020 2020 2020   updated.       
-00013d40: 203a 7479 7065 206d 6f64 656c 5f69 643a   :type model_id:
-00013d50: 2073 7472 0a0a 2020 2020 2020 2020 3a70   str..        :p
-00013d60: 6172 616d 206d 6574 615f 7072 6f70 733a  aram meta_props:
-00013d70: 206e 6577 2073 6574 206f 6620 6d65 7461   new set of meta
-00013d80: 5f70 726f 7073 2074 6861 7420 6e65 6564  _props that need
-00013d90: 7320 746f 2075 7064 6174 6564 0a20 2020  s to updated.   
-00013da0: 2020 2020 203a 7479 7065 206d 6574 615f       :type meta_
-00013db0: 7072 6f70 733a 2064 6963 742c 206f 7074  props: dict, opt
-00013dc0: 696f 6e61 6c0a 0a20 2020 2020 2020 203a  ional..        :
-00013dd0: 7061 7261 6d20 7570 6461 7465 5f6d 6f64  param update_mod
-00013de0: 656c 3a20 6172 6368 6976 6564 206d 6f64  el: archived mod
-00013df0: 656c 2063 6f6e 7465 6e74 2066 696c 6520  el content file 
-00013e00: 6f72 2070 6174 6820 746f 2064 6972 6563  or path to direc
-00013e10: 746f 7279 2063 6f6e 7461 696e 696e 6720  tory containing 
-00013e20: 6172 6368 6976 6564 206d 6f64 656c 2066  archived model f
-00013e30: 696c 650a 2020 2020 2020 2020 2020 2020  ile.            
-00013e40: 7768 6963 6820 7368 6f75 6c64 2062 6520  which should be 
-00013e50: 6368 616e 6765 6420 666f 7220 7370 6563  changed for spec
-00013e60: 6966 6963 206d 6f64 656c 5f69 640a 2020  ific model_id.  
-00013e70: 2020 2020 2020 3a74 7970 6520 7570 6461        :type upda
-00013e80: 7465 5f6d 6f64 656c 3a20 6f62 6a65 6374  te_model: object
-00013e90: 206f 7220 6d6f 6465 6c2c 206f 7074 696f   or model, optio
-00013ea0: 6e61 6c0a 0a20 2020 2020 2020 203a 7265  nal..        :re
-00013eb0: 7475 726e 3a20 7570 6461 7465 6420 6d65  turn: updated me
-00013ec0: 7461 6461 7461 206f 6620 6d6f 6465 6c0a  tadata of model.
-00013ed0: 2020 2020 2020 2020 3a72 7479 7065 3a20          :rtype: 
-00013ee0: 6469 6374 0a0a 2020 2020 2020 2020 2a2a  dict..        **
-00013ef0: 4578 616d 706c 652a 2a0a 0a20 2020 2020  Example**..     
-00013f00: 2020 202e 2e20 636f 6465 2d62 6c6f 636b     .. code-block
-00013f10: 3a3a 2070 7974 686f 6e0a 0a20 2020 2020  :: python..     
-00013f20: 2020 2020 2020 206d 6f64 656c 5f64 6574         model_det
-00013f30: 6169 6c73 203d 2063 6c69 656e 742e 6d6f  ails = client.mo
-00013f40: 6465 6c73 2e75 7064 6174 6528 6d6f 6465  dels.update(mode
-00013f50: 6c5f 6964 2c20 7570 6461 7465 5f6d 6f64  l_id, update_mod
-00013f60: 656c 3d75 7064 6174 6564 5f63 6f6e 7465  el=updated_conte
-00013f70: 6e74 290a 2020 2020 2020 2020 2222 220a  nt).        """.
-00013f80: 2020 2020 2020 2020 6d6f 6465 6c5f 6964          model_id
-00013f90: 203d 205f 6765 745f 6964 5f66 726f 6d5f   = _get_id_from_
-00013fa0: 6465 7072 6563 6174 6564 5f75 6964 286b  deprecated_uid(k
-00013fb0: 7761 7267 732c 206d 6f64 656c 5f69 642c  wargs, model_id,
-00013fc0: 2022 6d6f 6465 6c22 290a 2020 2020 2020   "model").      
-00013fd0: 2020 4d6f 6465 6c73 2e5f 7661 6c69 6461    Models._valida
-00013fe0: 7465 5f74 7970 6528 6d6f 6465 6c5f 6964  te_type(model_id
-00013ff0: 2c20 226d 6f64 656c 5f69 6422 2c20 7374  , "model_id", st
-00014000: 722c 2046 616c 7365 290a 2020 2020 2020  r, False).      
-00014010: 2020 6d6f 6465 6c5f 6964 203d 2063 6173    model_id = cas
-00014020: 7428 7374 722c 206d 6f64 656c 5f69 6429  t(str, model_id)
-00014030: 0a20 2020 2020 2020 204d 6f64 656c 732e  .        Models.
-00014040: 5f76 616c 6964 6174 655f 7479 7065 286d  _validate_type(m
-00014050: 6574 615f 7072 6f70 732c 2022 6d65 7461  eta_props, "meta
-00014060: 5f70 726f 7073 222c 2064 6963 742c 2054  _props", dict, T
-00014070: 7275 6529 0a0a 2020 2020 2020 2020 6966  rue)..        if
-00014080: 206d 6574 615f 7072 6f70 7320 6973 206e   meta_props is n
-00014090: 6f74 204e 6f6e 653a 2020 2320 544f 444f  ot None:  # TODO
-000140a0: 0a20 2020 2020 2020 2020 2020 2023 2072  .            # r
-000140b0: 6169 7365 2057 4d4c 436c 6965 6e74 4572  aise WMLClientEr
-000140c0: 726f 7228 274d 6574 615f 7072 6f70 7320  ror('Meta_props 
-000140d0: 7570 6461 7465 2075 6e73 7570 706f 7274  update unsupport
-000140e0: 6564 2e27 290a 2020 2020 2020 2020 2020  ed.').          
-000140f0: 2020 7365 6c66 2e5f 7661 6c69 6461 7465    self._validate
-00014100: 5f74 7970 6528 6d65 7461 5f70 726f 7073  _type(meta_props
-00014110: 2c20 226d 6574 615f 7072 6f70 7322 2c20  , "meta_props", 
-00014120: 6469 6374 2c20 5472 7565 290a 0a20 2020  dict, True)..   
-00014130: 2020 2020 2020 2020 2075 726c 203d 2073           url = s
-00014140: 656c 662e 5f63 6c69 656e 742e 7365 7276  elf._client.serv
-00014150: 6963 655f 696e 7374 616e 6365 2e5f 6872  ice_instance._hr
-00014160: 6566 5f64 6566 696e 6974 696f 6e73 2e67  ef_definitions.g
-00014170: 6574 5f70 7562 6c69 7368 6564 5f6d 6f64  et_published_mod
-00014180: 656c 5f68 7265 6628 0a20 2020 2020 2020  el_href(.       
-00014190: 2020 2020 2020 2020 206d 6f64 656c 5f69           model_i
-000141a0: 640a 2020 2020 2020 2020 2020 2020 290a  d.            ).
-000141b0: 0a20 2020 2020 2020 2020 2020 2072 6573  .            res
-000141c0: 706f 6e73 6520 3d20 7265 7175 6573 7473  ponse = requests
-000141d0: 2e67 6574 280a 2020 2020 2020 2020 2020  .get(.          
-000141e0: 2020 2020 2020 7572 6c2c 2070 6172 616d        url, param
-000141f0: 733d 7365 6c66 2e5f 636c 6965 6e74 2e5f  s=self._client._
-00014200: 7061 7261 6d73 2829 2c20 6865 6164 6572  params(), header
-00014210: 733d 7365 6c66 2e5f 636c 6965 6e74 2e5f  s=self._client._
-00014220: 6765 745f 6865 6164 6572 7328 290a 2020  get_headers().  
-00014230: 2020 2020 2020 2020 2020 290a 0a20 2020            )..   
-00014240: 2020 2020 2020 2020 2069 6620 7265 7370           if resp
-00014250: 6f6e 7365 2e73 7461 7475 735f 636f 6465  onse.status_code
-00014260: 2021 3d20 3230 303a 0a20 2020 2020 2020   != 200:.       
-00014270: 2020 2020 2020 2020 2069 6620 7265 7370           if resp
-00014280: 6f6e 7365 2e73 7461 7475 735f 636f 6465  onse.status_code
-00014290: 203d 3d20 3430 343a 0a20 2020 2020 2020   == 404:.       
-000142a0: 2020 2020 2020 2020 2020 2020 2072 6169               rai
-000142b0: 7365 2057 4d4c 436c 6965 6e74 4572 726f  se WMLClientErro
-000142c0: 7228 0a20 2020 2020 2020 2020 2020 2020  r(.             
-000142d0: 2020 2020 2020 2020 2020 2022 496e 7661             "Inva
-000142e0: 6c69 6420 696e 7075 742e 2055 6e61 626c  lid input. Unabl
-000142f0: 6520 746f 2067 6574 2074 6865 2064 6574  e to get the det
-00014300: 6169 6c73 206f 6620 6d6f 6465 6c5f 6964  ails of model_id
-00014310: 2070 726f 7669 6465 642e 220a 2020 2020   provided.".    
-00014320: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00014330: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
-00014340: 2020 656c 7365 3a0a 2020 2020 2020 2020    else:.        
-00014350: 2020 2020 2020 2020 2020 2020 7261 6973              rais
-00014360: 6520 4170 6952 6571 7565 7374 4661 696c  e ApiRequestFail
-00014370: 7572 6528 0a20 2020 2020 2020 2020 2020  ure(.           
-00014380: 2020 2020 2020 2020 2020 2020 2022 4661               "Fa
-00014390: 696c 7572 6520 6475 7269 6e67 207b 7d2e  ilure during {}.
-000143a0: 222e 666f 726d 6174 2822 6765 7474 696e  ".format("gettin
-000143b0: 6720 6d6f 6465 6c20 746f 2075 7064 6174  g model to updat
-000143c0: 6522 292c 2072 6573 706f 6e73 650a 2020  e"), response.  
-000143d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000143e0: 2020 290a 0a20 2020 2020 2020 2020 2020    )..           
-000143f0: 2064 6574 6169 6c73 203d 2073 656c 662e   details = self.
-00014400: 5f68 616e 646c 655f 7265 7370 6f6e 7365  _handle_response
-00014410: 2832 3030 2c20 2247 6574 206d 6f64 656c  (200, "Get model
-00014420: 2064 6574 6169 6c73 222c 2072 6573 706f   details", respo
-00014430: 6e73 6529 0a20 2020 2020 2020 2020 2020  nse).           
-00014440: 206d 6f64 656c 5f74 7970 6520 3d20 6465   model_type = de
-00014450: 7461 696c 735b 2265 6e74 6974 7922 5d5b  tails["entity"][
-00014460: 2274 7970 6522 5d0a 2020 2020 2020 2020  "type"].        
-00014470: 2020 2020 2320 7570 6461 7465 2074 6865      # update the
-00014480: 2063 6f6e 7465 6e74 2070 6174 6820 666f   content path fo
-00014490: 7220 7468 6520 4175 746f 2d61 6920 6d6f  r the Auto-ai mo
-000144a0: 6465 6c2e 0a20 2020 2020 2020 2020 2020  del..           
-000144b0: 2069 6620 6d6f 6465 6c5f 7479 7065 203d   if model_type =
-000144c0: 3d20 2277 6d6c 2d68 7962 7269 645f 302e  = "wml-hybrid_0.
-000144d0: 3122 2061 6e64 2075 7064 6174 655f 6d6f  1" and update_mo
-000144e0: 6465 6c20 6973 206e 6f74 204e 6f6e 653a  del is not None:
-000144f0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00014500: 2023 2054 6865 206f 6e6c 7920 7375 7070   # The only supp
-00014510: 6f72 7465 6420 666f 726d 6174 2069 7320  orted format is 
-00014520: 6120 7a69 7020 6669 6c65 2063 6f6e 7461  a zip file conta
-00014530: 696e 696e 6720 6070 6970 656c 696e 652d  ining `pipeline-
-00014540: 6d6f 6465 6c2e 6a73 6f6e 6020 616e 6420  model.json` and 
-00014550: 7069 636b 6c65 6420 6d6f 6465 6c20 636f  pickled model co
-00014560: 6d70 7265 7373 6564 0a20 2020 2020 2020  mpressed.       
-00014570: 2020 2020 2020 2020 2023 2074 6f20 7461           # to ta
-00014580: 722e 677a 2066 6f72 6d61 742e 0a20 2020  r.gz format..   
-00014590: 2020 2020 2020 2020 2020 2020 2069 6620               if 
-000145a0: 6e6f 7420 7570 6461 7465 5f6d 6f64 656c  not update_model
-000145b0: 2e65 6e64 7377 6974 6828 222e 7a69 7022  .endswith(".zip"
-000145c0: 293a 0a20 2020 2020 2020 2020 2020 2020  ):.             
-000145d0: 2020 2020 2020 2072 6169 7365 2057 4d4c         raise WML
-000145e0: 436c 6965 6e74 4572 726f 7228 0a20 2020  ClientError(.   
-000145f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00014600: 2020 2020 2022 496e 7661 6c69 6420 6d6f       "Invalid mo
-00014610: 6465 6c20 636f 6e74 656e 742e 2054 6865  del content. The
-00014620: 206d 6f64 656c 2063 6f6e 7465 6e74 2066   model content f
-00014630: 696c 6520 7368 6f75 6c64 2062 7265 207a  ile should bre z
-00014640: 6970 2061 7263 6869 7665 2063 6f6e 7461  ip archive conta
-00014650: 696e 696e 6722 0a20 2020 2020 2020 2020  ining".         
-00014660: 2020 2020 2020 2020 2020 2020 2020 2027                 '
-00014670: 2022 2e70 6963 6b6c 652e 7461 722e 677a   ".pickle.tar.gz
-00014680: 2220 6669 6c65 206f 7220 2270 6970 6c69  " file or "pipli
-00014690: 6e65 2d6d 6f64 656c 2e6a 736f 6e22 2c20  ne-model.json", 
-000146a0: 666f 7220 7468 6520 6d6f 6465 6c20 7479  for the model ty
-000146b0: 7065 5c27 7b7d 5c27 2e27 2e66 6f72 6d61  pe\'{}\'.'.forma
-000146c0: 7428 0a20 2020 2020 2020 2020 2020 2020  t(.             
-000146d0: 2020 2020 2020 2020 2020 2020 2020 206d                 m
-000146e0: 6f64 656c 5f74 7970 650a 2020 2020 2020  odel_type.      
+00013700: 2020 2020 2020 2020 2020 206d 6f64 656c             model
+00013710: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00013720: 2020 2020 2020 2020 2029 0a20 2020 2020           ).     
+00013730: 2020 2020 2020 2020 2020 2020 2020 2029                 )
+00013740: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00013750: 2073 6176 6564 5f6d 6f64 656c 203d 2073   saved_model = s
+00013760: 656c 662e 5f70 7562 6c69 7368 5f66 726f  elf._publish_fro
+00013770: 6d5f 6669 6c65 280a 2020 2020 2020 2020  m_file(.        
+00013780: 2020 2020 2020 2020 2020 2020 6d6f 6465              mode
+00013790: 6c3d 6d6f 6465 6c2c 0a20 2020 2020 2020  l=model,.       
+000137a0: 2020 2020 2020 2020 2020 2020 206d 6574               met
+000137b0: 615f 7072 6f70 733d 6d65 7461 5f70 726f  a_props=meta_pro
+000137c0: 7073 2c0a 2020 2020 2020 2020 2020 2020  ps,.            
+000137d0: 2020 2020 2020 2020 7472 6169 6e69 6e67          training
+000137e0: 5f64 6174 613d 7472 6169 6e69 6e67 5f64  _data=training_d
+000137f0: 6174 612c 0a20 2020 2020 2020 2020 2020  ata,.           
+00013800: 2020 2020 2020 2020 2074 7261 696e 696e           trainin
+00013810: 675f 7461 7267 6574 3d74 7261 696e 696e  g_target=trainin
+00013820: 675f 7461 7267 6574 2c0a 2020 2020 2020  g_target,.      
+00013830: 2020 2020 2020 2020 2020 2020 2020 7665                ve
+00013840: 723d 7665 7273 696f 6e2c 0a20 2020 2020  r=version,.     
+00013850: 2020 2020 2020 2020 2020 2020 2020 2061                 a
+00013860: 7274 6966 6163 7469 643d 6172 7469 6661  rtifactid=artifa
+00013870: 6374 6964 2c0a 2020 2020 2020 2020 2020  ctid,.          
+00013880: 2020 2020 2020 2020 2020 6665 6174 7572            featur
+00013890: 655f 6e61 6d65 733d 6665 6174 7572 655f  e_names=feature_
+000138a0: 6e61 6d65 732c 0a20 2020 2020 2020 2020  names,.         
+000138b0: 2020 2020 2020 2020 2020 206c 6162 656c             label
+000138c0: 5f63 6f6c 756d 6e5f 6e61 6d65 733d 6c61  _column_names=la
+000138d0: 6265 6c5f 636f 6c75 6d6e 5f6e 616d 6573  bel_column_names
+000138e0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+000138f0: 2020 290a 2020 2020 2020 2020 2020 2020    ).            
+00013900: 656c 7365 3a0a 2020 2020 2020 2020 2020  else:.          
+00013910: 2020 2020 2020 7361 7665 645f 6d6f 6465        saved_mode
+00013920: 6c20 3d20 7365 6c66 2e5f 7075 626c 6973  l = self._publis
+00013930: 685f 6672 6f6d 5f74 7261 696e 696e 6728  h_from_training(
+00013940: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00013950: 2020 2020 206d 6f64 656c 5f69 643d 6d6f       model_id=mo
+00013960: 6465 6c2c 0a20 2020 2020 2020 2020 2020  del,.           
+00013970: 2020 2020 2020 2020 206d 6574 615f 7072           meta_pr
+00013980: 6f70 733d 6d65 7461 5f70 726f 7073 2c0a  ops=meta_props,.
+00013990: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000139a0: 2020 2020 7375 6274 7261 696e 696e 6749      subtrainingI
+000139b0: 643d 7375 6274 7261 696e 696e 6749 642c  d=subtrainingId,
+000139c0: 2020 2320 7479 7065 3a20 6967 6e6f 7265    # type: ignore
+000139d0: 5b61 7267 2d74 7970 655d 0a20 2020 2020  [arg-type].     
+000139e0: 2020 2020 2020 2020 2020 2020 2020 2066                 f
+000139f0: 6561 7475 7265 5f6e 616d 6573 3d66 6561  eature_names=fea
+00013a00: 7475 7265 5f6e 616d 6573 2c0a 2020 2020  ture_names,.    
+00013a10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00013a20: 6c61 6265 6c5f 636f 6c75 6d6e 5f6e 616d  label_column_nam
+00013a30: 6573 3d6c 6162 656c 5f63 6f6c 756d 6e5f  es=label_column_
+00013a40: 6e61 6d65 732c 0a20 2020 2020 2020 2020  names,.         
+00013a50: 2020 2020 2020 2020 2020 2072 6f75 6e64             round
+00013a60: 5f6e 756d 6265 723d 726f 756e 645f 6e75  _number=round_nu
+00013a70: 6d62 6572 2c0a 2020 2020 2020 2020 2020  mber,.          
+00013a80: 2020 2020 2020 290a 2020 2020 2020 2020        ).        
+00013a90: 6966 2028 0a20 2020 2020 2020 2020 2020  if (.           
+00013aa0: 2022 7379 7374 656d 2220 696e 2073 6176   "system" in sav
+00013ab0: 6564 5f6d 6f64 656c 0a20 2020 2020 2020  ed_model.       
+00013ac0: 2020 2020 2061 6e64 2022 7761 726e 696e       and "warnin
+00013ad0: 6773 2220 696e 2073 6176 6564 5f6d 6f64  gs" in saved_mod
+00013ae0: 656c 5b22 7379 7374 656d 225d 0a20 2020  el["system"].   
+00013af0: 2020 2020 2020 2020 2061 6e64 2073 6176           and sav
+00013b00: 6564 5f6d 6f64 656c 5b22 7379 7374 656d  ed_model["system
+00013b10: 225d 5b22 7761 726e 696e 6773 225d 0a20  "]["warnings"]. 
+00013b20: 2020 2020 2020 2029 3a0a 2020 2020 2020         ):.      
+00013b30: 2020 2020 2020 6966 2073 6176 6564 5f6d        if saved_m
+00013b40: 6f64 656c 5b22 7379 7374 656d 225d 5b22  odel["system"]["
+00013b50: 7761 726e 696e 6773 225d 2069 7320 6e6f  warnings"] is no
+00013b60: 7420 4e6f 6e65 3a0a 2020 2020 2020 2020  t None:.        
+00013b70: 2020 2020 2020 2020 6d65 7373 6167 6520          message 
+00013b80: 3d20 7361 7665 645f 6d6f 6465 6c5b 2273  = saved_model["s
+00013b90: 7973 7465 6d22 5d5b 2277 6172 6e69 6e67  ystem"]["warning
+00013ba0: 7322 5d5b 305d 2e67 6574 2822 6d65 7373  s"][0].get("mess
+00013bb0: 6167 6522 2c20 4e6f 6e65 290a 2020 2020  age", None).    
+00013bc0: 2020 2020 2020 2020 2020 2020 7072 696e              prin
+00013bd0: 7428 224e 6f74 653a 2057 6172 6e69 6e67  t("Note: Warning
+00013be0: 7321 2120 3a20 222c 206d 6573 7361 6765  s!! : ", message
+00013bf0: 290a 2020 2020 2020 2020 7265 7475 726e  ).        return
+00013c00: 2073 6176 6564 5f6d 6f64 656c 0a0a 2020   saved_model..  
+00013c10: 2020 6465 6620 7570 6461 7465 280a 2020    def update(.  
+00013c20: 2020 2020 2020 7365 6c66 2c0a 2020 2020        self,.    
+00013c30: 2020 2020 6d6f 6465 6c5f 6964 3a20 7374      model_id: st
+00013c40: 7220 7c20 4e6f 6e65 203d 204e 6f6e 652c  r | None = None,
+00013c50: 0a20 2020 2020 2020 206d 6574 615f 7072  .        meta_pr
+00013c60: 6f70 733a 2064 6963 7420 7c20 4e6f 6e65  ops: dict | None
+00013c70: 203d 204e 6f6e 652c 0a20 2020 2020 2020   = None,.       
+00013c80: 2075 7064 6174 655f 6d6f 6465 6c3a 204d   update_model: M
+00013c90: 4c4d 6f64 656c 5479 7065 203d 204e 6f6e  LModelType = Non
+00013ca0: 652c 0a20 2020 2020 2020 202a 2a6b 7761  e,.        **kwa
+00013cb0: 7267 733a 2041 6e79 2c0a 2020 2020 2920  rgs: Any,.    ) 
+00013cc0: 2d3e 2064 6963 745b 7374 722c 2041 6e79  -> dict[str, Any
+00013cd0: 5d3a 0a20 2020 2020 2020 2022 2222 5570  ]:.        """Up
+00013ce0: 6461 7465 2065 7869 7374 696e 6720 6d6f  date existing mo
+00013cf0: 6465 6c2e 0a0a 2020 2020 2020 2020 3a70  del...        :p
+00013d00: 6172 616d 206d 6f64 656c 5f69 643a 2049  aram model_id: I
+00013d10: 4420 6f66 206d 6f64 656c 2077 6869 6368  D of model which
+00013d20: 2064 6566 696e 6520 7768 6174 2073 686f   define what sho
+00013d30: 756c 6420 6265 2075 7064 6174 6564 0a20  uld be updated. 
+00013d40: 2020 2020 2020 203a 7479 7065 206d 6f64         :type mod
+00013d50: 656c 5f69 643a 2073 7472 0a0a 2020 2020  el_id: str..    
+00013d60: 2020 2020 3a70 6172 616d 206d 6574 615f      :param meta_
+00013d70: 7072 6f70 733a 206e 6577 2073 6574 206f  props: new set o
+00013d80: 6620 6d65 7461 5f70 726f 7073 2074 6861  f meta_props tha
+00013d90: 7420 6e65 6564 7320 746f 2075 7064 6174  t needs to updat
+00013da0: 6564 0a20 2020 2020 2020 203a 7479 7065  ed.        :type
+00013db0: 206d 6574 615f 7072 6f70 733a 2064 6963   meta_props: dic
+00013dc0: 742c 206f 7074 696f 6e61 6c0a 0a20 2020  t, optional..   
+00013dd0: 2020 2020 203a 7061 7261 6d20 7570 6461       :param upda
+00013de0: 7465 5f6d 6f64 656c 3a20 6172 6368 6976  te_model: archiv
+00013df0: 6564 206d 6f64 656c 2063 6f6e 7465 6e74  ed model content
+00013e00: 2066 696c 6520 6f72 2070 6174 6820 746f   file or path to
+00013e10: 2064 6972 6563 746f 7279 2063 6f6e 7461   directory conta
+00013e20: 696e 696e 6720 6172 6368 6976 6564 206d  ining archived m
+00013e30: 6f64 656c 2066 696c 650a 2020 2020 2020  odel file.      
+00013e40: 2020 2020 2020 7768 6963 6820 7368 6f75        which shou
+00013e50: 6c64 2062 6520 6368 616e 6765 6420 666f  ld be changed fo
+00013e60: 7220 7370 6563 6966 6963 206d 6f64 656c  r specific model
+00013e70: 5f69 640a 2020 2020 2020 2020 3a74 7970  _id.        :typ
+00013e80: 6520 7570 6461 7465 5f6d 6f64 656c 3a20  e update_model: 
+00013e90: 6f62 6a65 6374 206f 7220 6d6f 6465 6c2c  object or model,
+00013ea0: 206f 7074 696f 6e61 6c0a 0a20 2020 2020   optional..     
+00013eb0: 2020 203a 7265 7475 726e 3a20 7570 6461     :return: upda
+00013ec0: 7465 6420 6d65 7461 6461 7461 206f 6620  ted metadata of 
+00013ed0: 6d6f 6465 6c0a 2020 2020 2020 2020 3a72  model.        :r
+00013ee0: 7479 7065 3a20 6469 6374 0a0a 2020 2020  type: dict..    
+00013ef0: 2020 2020 2a2a 4578 616d 706c 652a 2a0a      **Example**.
+00013f00: 0a20 2020 2020 2020 202e 2e20 636f 6465  .        .. code
+00013f10: 2d62 6c6f 636b 3a3a 2070 7974 686f 6e0a  -block:: python.
+00013f20: 0a20 2020 2020 2020 2020 2020 206d 6f64  .            mod
+00013f30: 656c 5f64 6574 6169 6c73 203d 2063 6c69  el_details = cli
+00013f40: 656e 742e 6d6f 6465 6c73 2e75 7064 6174  ent.models.updat
+00013f50: 6528 6d6f 6465 6c5f 6964 2c20 7570 6461  e(model_id, upda
+00013f60: 7465 5f6d 6f64 656c 3d75 7064 6174 6564  te_model=updated
+00013f70: 5f63 6f6e 7465 6e74 290a 2020 2020 2020  _content).      
+00013f80: 2020 2222 220a 2020 2020 2020 2020 6d6f    """.        mo
+00013f90: 6465 6c5f 6964 203d 205f 6765 745f 6964  del_id = _get_id
+00013fa0: 5f66 726f 6d5f 6465 7072 6563 6174 6564  _from_deprecated
+00013fb0: 5f75 6964 286b 7761 7267 732c 206d 6f64  _uid(kwargs, mod
+00013fc0: 656c 5f69 642c 2022 6d6f 6465 6c22 290a  el_id, "model").
+00013fd0: 2020 2020 2020 2020 4d6f 6465 6c73 2e5f          Models._
+00013fe0: 7661 6c69 6461 7465 5f74 7970 6528 6d6f  validate_type(mo
+00013ff0: 6465 6c5f 6964 2c20 226d 6f64 656c 5f69  del_id, "model_i
+00014000: 6422 2c20 7374 722c 2046 616c 7365 290a  d", str, False).
+00014010: 2020 2020 2020 2020 6d6f 6465 6c5f 6964          model_id
+00014020: 203d 2063 6173 7428 7374 722c 206d 6f64   = cast(str, mod
+00014030: 656c 5f69 6429 0a20 2020 2020 2020 204d  el_id).        M
+00014040: 6f64 656c 732e 5f76 616c 6964 6174 655f  odels._validate_
+00014050: 7479 7065 286d 6574 615f 7072 6f70 732c  type(meta_props,
+00014060: 2022 6d65 7461 5f70 726f 7073 222c 2064   "meta_props", d
+00014070: 6963 742c 2054 7275 6529 0a0a 2020 2020  ict, True)..    
+00014080: 2020 2020 6966 206d 6574 615f 7072 6f70      if meta_prop
+00014090: 7320 6973 206e 6f74 204e 6f6e 653a 2020  s is not None:  
+000140a0: 2320 544f 444f 0a20 2020 2020 2020 2020  # TODO.         
+000140b0: 2020 2023 2072 6169 7365 2057 4d4c 436c     # raise WMLCl
+000140c0: 6965 6e74 4572 726f 7228 274d 6574 615f  ientError('Meta_
+000140d0: 7072 6f70 7320 7570 6461 7465 2075 6e73  props update uns
+000140e0: 7570 706f 7274 6564 2e27 290a 2020 2020  upported.').    
+000140f0: 2020 2020 2020 2020 7365 6c66 2e5f 7661          self._va
+00014100: 6c69 6461 7465 5f74 7970 6528 6d65 7461  lidate_type(meta
+00014110: 5f70 726f 7073 2c20 226d 6574 615f 7072  _props, "meta_pr
+00014120: 6f70 7322 2c20 6469 6374 2c20 5472 7565  ops", dict, True
+00014130: 290a 0a20 2020 2020 2020 2020 2020 2075  )..            u
+00014140: 726c 203d 2073 656c 662e 5f63 6c69 656e  rl = self._clien
+00014150: 742e 7365 7276 6963 655f 696e 7374 616e  t.service_instan
+00014160: 6365 2e5f 6872 6566 5f64 6566 696e 6974  ce._href_definit
+00014170: 696f 6e73 2e67 6574 5f70 7562 6c69 7368  ions.get_publish
+00014180: 6564 5f6d 6f64 656c 5f68 7265 6628 0a20  ed_model_href(. 
+00014190: 2020 2020 2020 2020 2020 2020 2020 206d                 m
+000141a0: 6f64 656c 5f69 640a 2020 2020 2020 2020  odel_id.        
+000141b0: 2020 2020 290a 0a20 2020 2020 2020 2020      )..         
+000141c0: 2020 2072 6573 706f 6e73 6520 3d20 7265     response = re
+000141d0: 7175 6573 7473 2e67 6574 280a 2020 2020  quests.get(.    
+000141e0: 2020 2020 2020 2020 2020 2020 7572 6c2c              url,
+000141f0: 2070 6172 616d 733d 7365 6c66 2e5f 636c   params=self._cl
+00014200: 6965 6e74 2e5f 7061 7261 6d73 2829 2c20  ient._params(), 
+00014210: 6865 6164 6572 733d 7365 6c66 2e5f 636c  headers=self._cl
+00014220: 6965 6e74 2e5f 6765 745f 6865 6164 6572  ient._get_header
+00014230: 7328 290a 2020 2020 2020 2020 2020 2020  s().            
+00014240: 290a 0a20 2020 2020 2020 2020 2020 2069  )..            i
+00014250: 6620 7265 7370 6f6e 7365 2e73 7461 7475  f response.statu
+00014260: 735f 636f 6465 2021 3d20 3230 303a 0a20  s_code != 200:. 
+00014270: 2020 2020 2020 2020 2020 2020 2020 2069                 i
+00014280: 6620 7265 7370 6f6e 7365 2e73 7461 7475  f response.statu
+00014290: 735f 636f 6465 203d 3d20 3430 343a 0a20  s_code == 404:. 
+000142a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000142b0: 2020 2072 6169 7365 2057 4d4c 436c 6965     raise WMLClie
+000142c0: 6e74 4572 726f 7228 0a20 2020 2020 2020  ntError(.       
+000142d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000142e0: 2022 496e 7661 6c69 6420 696e 7075 742e   "Invalid input.
+000142f0: 2055 6e61 626c 6520 746f 2067 6574 2074   Unable to get t
+00014300: 6865 2064 6574 6169 6c73 206f 6620 6d6f  he details of mo
+00014310: 6465 6c5f 6964 2070 726f 7669 6465 642e  del_id provided.
+00014320: 220a 2020 2020 2020 2020 2020 2020 2020  ".              
+00014330: 2020 2020 2020 290a 2020 2020 2020 2020        ).        
+00014340: 2020 2020 2020 2020 656c 7365 3a0a 2020          else:.  
+00014350: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00014360: 2020 7261 6973 6520 4170 6952 6571 7565    raise ApiReque
+00014370: 7374 4661 696c 7572 6528 0a20 2020 2020  stFailure(.     
+00014380: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00014390: 2020 2022 4661 696c 7572 6520 6475 7269     "Failure duri
+000143a0: 6e67 207b 7d2e 222e 666f 726d 6174 2822  ng {}.".format("
+000143b0: 6765 7474 696e 6720 6d6f 6465 6c20 746f  getting model to
+000143c0: 2075 7064 6174 6522 292c 2072 6573 706f   update"), respo
+000143d0: 6e73 650a 2020 2020 2020 2020 2020 2020  nse.            
+000143e0: 2020 2020 2020 2020 290a 0a20 2020 2020          )..     
+000143f0: 2020 2020 2020 2064 6574 6169 6c73 203d         details =
+00014400: 2073 656c 662e 5f68 616e 646c 655f 7265   self._handle_re
+00014410: 7370 6f6e 7365 2832 3030 2c20 2247 6574  sponse(200, "Get
+00014420: 206d 6f64 656c 2064 6574 6169 6c73 222c   model details",
+00014430: 2072 6573 706f 6e73 6529 0a20 2020 2020   response).     
+00014440: 2020 2020 2020 206d 6f64 656c 5f74 7970         model_typ
+00014450: 6520 3d20 6465 7461 696c 735b 2265 6e74  e = details["ent
+00014460: 6974 7922 5d5b 2274 7970 6522 5d0a 2020  ity"]["type"].  
+00014470: 2020 2020 2020 2020 2020 2320 7570 6461            # upda
+00014480: 7465 2074 6865 2063 6f6e 7465 6e74 2070  te the content p
+00014490: 6174 6820 666f 7220 7468 6520 4175 746f  ath for the Auto
+000144a0: 2d61 6920 6d6f 6465 6c2e 0a20 2020 2020  -ai model..     
+000144b0: 2020 2020 2020 2069 6620 6d6f 6465 6c5f         if model_
+000144c0: 7479 7065 203d 3d20 2277 6d6c 2d68 7962  type == "wml-hyb
+000144d0: 7269 645f 302e 3122 2061 6e64 2075 7064  rid_0.1" and upd
+000144e0: 6174 655f 6d6f 6465 6c20 6973 206e 6f74  ate_model is not
+000144f0: 204e 6f6e 653a 0a20 2020 2020 2020 2020   None:.         
+00014500: 2020 2020 2020 2023 2054 6865 206f 6e6c         # The onl
+00014510: 7920 7375 7070 6f72 7465 6420 666f 726d  y supported form
+00014520: 6174 2069 7320 6120 7a69 7020 6669 6c65  at is a zip file
+00014530: 2063 6f6e 7461 696e 696e 6720 6070 6970   containing `pip
+00014540: 656c 696e 652d 6d6f 6465 6c2e 6a73 6f6e  eline-model.json
+00014550: 6020 616e 6420 7069 636b 6c65 6420 6d6f  ` and pickled mo
+00014560: 6465 6c20 636f 6d70 7265 7373 6564 0a20  del compressed. 
+00014570: 2020 2020 2020 2020 2020 2020 2020 2023                 #
+00014580: 2074 6f20 7461 722e 677a 2066 6f72 6d61   to tar.gz forma
+00014590: 742e 0a20 2020 2020 2020 2020 2020 2020  t..             
+000145a0: 2020 2069 6620 6e6f 7420 7570 6461 7465     if not update
+000145b0: 5f6d 6f64 656c 2e65 6e64 7377 6974 6828  _model.endswith(
+000145c0: 222e 7a69 7022 293a 0a20 2020 2020 2020  ".zip"):.       
+000145d0: 2020 2020 2020 2020 2020 2020 2072 6169               rai
+000145e0: 7365 2057 4d4c 436c 6965 6e74 4572 726f  se WMLClientErro
+000145f0: 7228 0a20 2020 2020 2020 2020 2020 2020  r(.             
+00014600: 2020 2020 2020 2020 2020 2022 496e 7661             "Inva
+00014610: 6c69 6420 6d6f 6465 6c20 636f 6e74 656e  lid model conten
+00014620: 742e 2054 6865 206d 6f64 656c 2063 6f6e  t. The model con
+00014630: 7465 6e74 2066 696c 6520 7368 6f75 6c64  tent file should
+00014640: 2062 7265 207a 6970 2061 7263 6869 7665   bre zip archive
+00014650: 2063 6f6e 7461 696e 696e 6722 0a20 2020   containing".   
+00014660: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00014670: 2020 2020 2027 2022 2e70 6963 6b6c 652e       ' ".pickle.
+00014680: 7461 722e 677a 2220 6669 6c65 206f 7220  tar.gz" file or 
+00014690: 2270 6970 6c69 6e65 2d6d 6f64 656c 2e6a  "pipline-model.j
+000146a0: 736f 6e22 2c20 666f 7220 7468 6520 6d6f  son", for the mo
+000146b0: 6465 6c20 7479 7065 5c27 7b7d 5c27 2e27  del type\'{}\'.'
+000146c0: 2e66 6f72 6d61 7428 0a20 2020 2020 2020  .format(.       
+000146d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000146e0: 2020 2020 206d 6f64 656c 5f74 7970 650a       model_type.
 000146f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00014700: 2020 290a 2020 2020 2020 2020 2020 2020    ).            
-00014710: 2020 2020 2020 2020 290a 0a20 2020 2020          )..     
-00014720: 2020 2020 2020 2023 2077 6974 6820 7661         # with va
-00014730: 6c69 6461 7469 6f6e 2073 686f 756c 6420  lidation should 
-00014740: 6265 2073 6f6d 6577 6865 7265 2065 6c73  be somewhere els
-00014750: 652c 206f 6e20 7468 6520 6265 6769 6e69  e, on the begini
-00014760: 6e67 2c20 6275 7420 7768 656e 2070 6174  ng, but when pat
-00014770: 6368 2077 696c 6c20 6265 2070 6f73 7369  ch will be possi
-00014780: 626c 650a 2020 2020 2020 2020 2020 2020  ble.            
-00014790: 7061 7463 685f 7061 796c 6f61 6420 3d20  patch_payload = 
-000147a0: 7365 6c66 2e43 6f6e 6669 6775 7261 7469  self.Configurati
-000147b0: 6f6e 4d65 7461 4e61 6d65 732e 5f67 656e  onMetaNames._gen
-000147c0: 6572 6174 655f 7061 7463 685f 7061 796c  erate_patch_payl
-000147d0: 6f61 6428 0a20 2020 2020 2020 2020 2020  oad(.           
-000147e0: 2020 2020 2064 6574 6169 6c73 5b22 656e       details["en
-000147f0: 7469 7479 225d 2c20 6d65 7461 5f70 726f  tity"], meta_pro
-00014800: 7073 2c20 7769 7468 5f76 616c 6964 6174  ps, with_validat
-00014810: 696f 6e3d 5472 7565 0a20 2020 2020 2020  ion=True.       
-00014820: 2020 2020 2029 0a20 2020 2020 2020 2020       ).         
-00014830: 2020 2072 6573 706f 6e73 655f 7061 7463     response_patc
-00014840: 6820 3d20 7265 7175 6573 7473 2e70 6174  h = requests.pat
-00014850: 6368 280a 2020 2020 2020 2020 2020 2020  ch(.            
-00014860: 2020 2020 7572 6c2c 0a20 2020 2020 2020      url,.       
-00014870: 2020 2020 2020 2020 206a 736f 6e3d 7061           json=pa
-00014880: 7463 685f 7061 796c 6f61 642c 0a20 2020  tch_payload,.   
-00014890: 2020 2020 2020 2020 2020 2020 2070 6172               par
-000148a0: 616d 733d 7365 6c66 2e5f 636c 6965 6e74  ams=self._client
-000148b0: 2e5f 7061 7261 6d73 2829 2c0a 2020 2020  ._params(),.    
-000148c0: 2020 2020 2020 2020 2020 2020 6865 6164              head
-000148d0: 6572 733d 7365 6c66 2e5f 636c 6965 6e74  ers=self._client
-000148e0: 2e5f 6765 745f 6865 6164 6572 7328 292c  ._get_headers(),
-000148f0: 0a20 2020 2020 2020 2020 2020 2029 0a20  .            ). 
-00014900: 2020 2020 2020 2020 2020 2075 7064 6174             updat
-00014910: 6564 5f64 6574 6169 6c73 203d 2073 656c  ed_details = sel
-00014920: 662e 5f68 616e 646c 655f 7265 7370 6f6e  f._handle_respon
-00014930: 7365 280a 2020 2020 2020 2020 2020 2020  se(.            
-00014940: 2020 2020 3230 302c 2022 6d6f 6465 6c20      200, "model 
-00014950: 7665 7273 696f 6e20 7061 7463 6822 2c20  version patch", 
-00014960: 7265 7370 6f6e 7365 5f70 6174 6368 0a20  response_patch. 
-00014970: 2020 2020 2020 2020 2020 2029 0a20 2020             ).   
-00014980: 2020 2020 2020 2020 2069 6620 7570 6461           if upda
-00014990: 7465 5f6d 6f64 656c 2069 7320 6e6f 7420  te_model is not 
-000149a0: 4e6f 6e65 3a0a 2020 2020 2020 2020 2020  None:.          
-000149b0: 2020 2020 2020 7365 6c66 2e5f 7570 6461        self._upda
-000149c0: 7465 5f6d 6f64 656c 5f63 6f6e 7465 6e74  te_model_content
-000149d0: 286d 6f64 656c 5f69 642c 2064 6574 6169  (model_id, detai
-000149e0: 6c73 2c20 7570 6461 7465 5f6d 6f64 656c  ls, update_model
-000149f0: 290a 2020 2020 2020 2020 2020 2020 7265  ).            re
-00014a00: 7475 726e 2075 7064 6174 6564 5f64 6574  turn updated_det
-00014a10: 6169 6c73 0a0a 2020 2020 2020 2020 7265  ails..        re
-00014a20: 7475 726e 2073 656c 662e 6765 745f 6465  turn self.get_de
-00014a30: 7461 696c 7328 6d6f 6465 6c5f 6964 290a  tails(model_id).
-00014a40: 0a20 2020 2064 6566 206c 6f61 6428 7365  .    def load(se
-00014a50: 6c66 2c20 6172 7469 6661 6374 5f69 643a  lf, artifact_id:
-00014a60: 2073 7472 207c 204e 6f6e 652c 202a 2a6b   str | None, **k
-00014a70: 7761 7267 733a 2041 6e79 2920 2d3e 2041  wargs: Any) -> A
-00014a80: 6e79 3a0a 2020 2020 2020 2020 2222 224c  ny:.        """L
-00014a90: 6f61 6420 6d6f 6465 6c20 6672 6f6d 2072  oad model from r
-00014aa0: 6570 6f73 6974 6f72 7920 746f 206f 626a  epository to obj
-00014ab0: 6563 7420 696e 206c 6f63 616c 2065 6e76  ect in local env
-00014ac0: 6972 6f6e 6d65 6e74 2e0a 0a20 2020 2020  ironment...     
-00014ad0: 2020 203a 7061 7261 6d20 6172 7469 6661     :param artifa
-00014ae0: 6374 5f69 643a 2073 746f 7265 6420 6d6f  ct_id: stored mo
-00014af0: 6465 6c20 4944 0a20 2020 2020 2020 203a  del ID.        :
-00014b00: 7479 7065 2061 7274 6966 6163 745f 6964  type artifact_id
-00014b10: 3a20 7374 720a 0a20 2020 2020 2020 203a  : str..        :
-00014b20: 7265 7475 726e 3a20 7472 6169 6e65 6420  return: trained 
-00014b30: 6d6f 6465 6c0a 2020 2020 2020 2020 3a72  model.        :r
-00014b40: 7479 7065 3a20 6f62 6a65 6374 0a0a 2020  type: object..  
-00014b50: 2020 2020 2020 2a2a 4578 616d 706c 652a        **Example*
-00014b60: 2a0a 0a20 2020 2020 2020 202e 2e20 636f  *..        .. co
-00014b70: 6465 2d62 6c6f 636b 3a3a 2070 7974 686f  de-block:: pytho
-00014b80: 6e0a 0a20 2020 2020 2020 2020 2020 206d  n..            m
-00014b90: 6f64 656c 203d 2063 6c69 656e 742e 6d6f  odel = client.mo
-00014ba0: 6465 6c73 2e6c 6f61 6428 6d6f 6465 6c5f  dels.load(model_
-00014bb0: 6964 290a 2020 2020 2020 2020 2222 220a  id).        """.
-00014bc0: 2020 2020 2020 2020 6172 7469 6661 6374          artifact
-00014bd0: 5f69 6420 3d20 5f67 6574 5f69 645f 6672  _id = _get_id_fr
-00014be0: 6f6d 5f64 6570 7265 6361 7465 645f 7569  om_deprecated_ui
-00014bf0: 6428 6b77 6172 6773 2c20 6172 7469 6661  d(kwargs, artifa
-00014c00: 6374 5f69 642c 2022 6172 7469 6661 6374  ct_id, "artifact
-00014c10: 2229 0a20 2020 2020 2020 2061 7274 6966  ").        artif
-00014c20: 6163 745f 6964 203d 2063 6173 7428 7374  act_id = cast(st
-00014c30: 722c 2061 7274 6966 6163 745f 6964 290a  r, artifact_id).
-00014c40: 2020 2020 2020 2020 4d6f 6465 6c73 2e5f          Models._
-00014c50: 7661 6c69 6461 7465 5f74 7970 6528 6172  validate_type(ar
-00014c60: 7469 6661 6374 5f69 642c 2022 6172 7469  tifact_id, "arti
-00014c70: 6661 6374 5f69 6422 2c20 7374 722c 2046  fact_id", str, F
-00014c80: 616c 7365 290a 2020 2020 2020 2020 2320  alse).        # 
-00014c90: 6368 6563 6b20 6966 2074 6869 7320 6973  check if this is
-00014ca0: 2074 656e 736f 7266 6c6f 7720 322e 7820   tensorflow 2.x 
-00014cb0: 6d6f 6465 6c20 7479 7065 0a20 2020 2020  model type.     
-00014cc0: 2020 206d 6f64 656c 5f64 6574 6169 6c73     model_details
-00014cd0: 203d 2073 656c 662e 6765 745f 6465 7461   = self.get_deta
-00014ce0: 696c 7328 6172 7469 6661 6374 5f69 6429  ils(artifact_id)
-00014cf0: 0a20 2020 2020 2020 2069 6620 6d6f 6465  .        if mode
-00014d00: 6c5f 6465 7461 696c 732e 6765 7428 2265  l_details.get("e
-00014d10: 6e74 6974 7922 2c20 7b7d 292e 6765 7428  ntity", {}).get(
-00014d20: 2274 7970 6522 2c20 2222 292e 7374 6172  "type", "").star
-00014d30: 7473 7769 7468 2822 7465 6e73 6f72 666c  tswith("tensorfl
-00014d40: 6f77 5f32 2e22 293a 0a20 2020 2020 2020  ow_2."):.       
-00014d50: 2020 2020 2072 6574 7572 6e20 7365 6c66       return self
-00014d60: 2e5f 7466 3278 5f6c 6f61 645f 6d6f 6465  ._tf2x_load_mode
-00014d70: 6c5f 696e 7374 616e 6365 2861 7274 6966  l_instance(artif
-00014d80: 6163 745f 6964 290a 2020 2020 2020 2020  act_id).        
-00014d90: 7472 793a 0a20 2020 2020 2020 2020 2020  try:.           
-00014da0: 2023 2043 6c6f 7564 2043 6f6e 7665 7267   # Cloud Converg
-00014db0: 656e 6365 3a20 4348 4b20 4946 2054 4849  ence: CHK IF THI
-00014dc0: 5320 434f 4e44 4954 494f 4e20 4953 2043  S CONDITION IS C
-00014dd0: 4f52 5245 4354 2073 696e 6365 206c 6f61  ORRECT since loa
-00014de0: 6465 645f 6d6f 6465 6c0a 2020 2020 2020  ded_model.      
-00014df0: 2020 2020 2020 2320 6675 6e63 7469 6f6e        # function
-00014e00: 616c 6974 7920 6265 6c6f 770a 2020 2020  ality below.    
-00014e10: 2020 2020 2020 2020 6966 2028 0a20 2020          if (.   
-00014e20: 2020 2020 2020 2020 2020 2020 2073 656c               sel
-00014e30: 662e 5f63 6c69 656e 742e 6465 6661 756c  f._client.defaul
-00014e40: 745f 7370 6163 655f 6964 2069 7320 4e6f  t_space_id is No
-00014e50: 6e65 0a20 2020 2020 2020 2020 2020 2020  ne.             
-00014e60: 2020 2061 6e64 2073 656c 662e 5f63 6c69     and self._cli
-00014e70: 656e 742e 6465 6661 756c 745f 7072 6f6a  ent.default_proj
-00014e80: 6563 745f 6964 2069 7320 4e6f 6e65 0a20  ect_id is None. 
-00014e90: 2020 2020 2020 2020 2020 2029 3a0a 2020             ):.  
-00014ea0: 2020 2020 2020 2020 2020 2020 2020 7261                ra
-00014eb0: 6973 6520 574d 4c43 6c69 656e 7445 7272  ise WMLClientErr
-00014ec0: 6f72 280a 2020 2020 2020 2020 2020 2020  or(.            
-00014ed0: 2020 2020 2020 2020 2249 7420 6973 206d          "It is m
-00014ee0: 616e 6461 746f 7279 2069 7320 7365 7420  andatory is set 
-00014ef0: 7468 6520 7370 6163 6520 6f72 2070 726f  the space or pro
-00014f00: 6a65 6374 2e20 5c0a 2020 2020 2020 2020  ject. \.        
-00014f10: 2020 2020 2020 2020 2020 2020 5573 6520              Use 
-00014f20: 636c 6965 6e74 2e73 6574 2e64 6566 6175  client.set.defau
-00014f30: 6c74 5f73 7061 6365 283c 5350 4143 455f  lt_space(<SPACE_
-00014f40: 4944 3e29 2074 6f20 7365 7420 7468 6520  ID>) to set the 
-00014f50: 7370 6163 6520 6f72 2063 6c69 656e 742e  space or client.
-00014f60: 7365 742e 6465 6661 756c 745f 7072 6f6a  set.default_proj
-00014f70: 6563 7428 3c50 524f 4a45 4354 5f49 443e  ect(<PROJECT_ID>
-00014f80: 292e 220a 2020 2020 2020 2020 2020 2020  ).".            
-00014f90: 2020 2020 290a 2020 2020 2020 2020 2020      ).          
-00014fa0: 2020 656c 7365 3a0a 2020 2020 2020 2020    else:.        
-00014fb0: 2020 2020 2020 2020 7175 6572 795f 7061          query_pa
-00014fc0: 7261 6d20 3d20 7365 6c66 2e5f 636c 6965  ram = self._clie
-00014fd0: 6e74 2e5f 7061 7261 6d73 2829 0a20 2020  nt._params().   
-00014fe0: 2020 2020 2020 2020 2020 2020 206c 6f61               loa
-00014ff0: 6465 645f 6d6f 6465 6c20 3d20 7365 6c66  ded_model = self
-00015000: 2e5f 636c 6965 6e74 2e72 6570 6f73 6974  ._client.reposit
-00015010: 6f72 792e 5f6d 6c5f 7265 706f 7369 746f  ory._ml_reposito
-00015020: 7279 5f63 6c69 656e 742e 6d6f 6465 6c73  ry_client.models
-00015030: 2e5f 6765 745f 7634 5f63 6c6f 7564 5f6d  ._get_v4_cloud_m
-00015040: 6f64 656c 280a 2020 2020 2020 2020 2020  odel(.          
-00015050: 2020 2020 2020 2020 2020 6172 7469 6661            artifa
-00015060: 6374 5f69 642c 2071 7565 7279 5f70 6172  ct_id, query_par
-00015070: 616d 3d71 7565 7279 5f70 6172 616d 0a20  am=query_param. 
-00015080: 2020 2020 2020 2020 2020 2020 2020 2029                 )
-00015090: 0a0a 2020 2020 2020 2020 2020 2020 6c6f  ..            lo
-000150a0: 6164 6564 5f6d 6f64 656c 203d 206c 6f61  aded_model = loa
-000150b0: 6465 645f 6d6f 6465 6c2e 6d6f 6465 6c5f  ded_model.model_
-000150c0: 696e 7374 616e 6365 2829 0a20 2020 2020  instance().     
-000150d0: 2020 2020 2020 2073 656c 662e 5f6c 6f67         self._log
-000150e0: 6765 722e 696e 666f 280a 2020 2020 2020  ger.info(.      
-000150f0: 2020 2020 2020 2020 2020 2253 7563 6365            "Succe
-00015100: 7373 6675 6c6c 7920 6c6f 6164 6564 2061  ssfully loaded a
-00015110: 7274 6966 6163 7420 7769 7468 2061 7274  rtifact with art
-00015120: 6966 6163 745f 6964 3a20 7b7d 222e 666f  ifact_id: {}".fo
-00015130: 726d 6174 2861 7274 6966 6163 745f 6964  rmat(artifact_id
-00015140: 290a 2020 2020 2020 2020 2020 2020 290a  ).            ).
-00015150: 2020 2020 2020 2020 2020 2020 7265 7475              retu
-00015160: 726e 206c 6f61 6465 645f 6d6f 6465 6c0a  rn loaded_model.
-00015170: 2020 2020 2020 2020 6578 6365 7074 2045          except E
-00015180: 7863 6570 7469 6f6e 2061 7320 653a 0a20  xception as e:. 
-00015190: 2020 2020 2020 2020 2020 2072 6169 7365             raise
-000151a0: 2057 4d4c 436c 6965 6e74 4572 726f 7228   WMLClientError(
-000151b0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-000151c0: 2022 4c6f 6164 696e 6720 6d6f 6465 6c20   "Loading model 
-000151d0: 7769 7468 2061 7274 6966 6163 745f 6964  with artifact_id
-000151e0: 3a20 277b 7d27 2066 6169 6c65 642e 222e  : '{}' failed.".
-000151f0: 666f 726d 6174 2861 7274 6966 6163 745f  format(artifact_
-00015200: 6964 292c 2065 0a20 2020 2020 2020 2020  id), e.         
-00015210: 2020 2029 0a0a 2020 2020 6465 6620 646f     )..    def do
-00015220: 776e 6c6f 6164 280a 2020 2020 2020 2020  wnload(.        
-00015230: 7365 6c66 2c0a 2020 2020 2020 2020 6d6f  self,.        mo
-00015240: 6465 6c5f 6964 3a20 7374 7220 7c20 4e6f  del_id: str | No
-00015250: 6e65 2c0a 2020 2020 2020 2020 6669 6c65  ne,.        file
-00015260: 6e61 6d65 3a20 7374 7220 3d20 2264 6f77  name: str = "dow
-00015270: 6e6c 6f61 6465 645f 6d6f 6465 6c2e 7461  nloaded_model.ta
-00015280: 722e 677a 222c 0a20 2020 2020 2020 2072  r.gz",.        r
-00015290: 6576 5f69 643a 2073 7472 207c 204e 6f6e  ev_id: str | Non
-000152a0: 6520 3d20 4e6f 6e65 2c0a 2020 2020 2020  e = None,.      
-000152b0: 2020 666f 726d 6174 3a20 7374 7220 7c20    format: str | 
-000152c0: 4e6f 6e65 203d 204e 6f6e 652c 0a20 2020  None = None,.   
-000152d0: 2020 2020 202a 2a6b 7761 7267 733a 2041       **kwargs: A
-000152e0: 6e79 2c0a 2020 2020 2920 2d3e 2073 7472  ny,.    ) -> str
-000152f0: 207c 204e 6f6e 653a 0a20 2020 2020 2020   | None:.       
-00015300: 2022 2222 446f 776e 6c6f 6164 206d 6f64   """Download mod
-00015310: 656c 2066 726f 6d20 7265 706f 7369 746f  el from reposito
-00015320: 7279 2074 6f20 6c6f 6361 6c20 6669 6c65  ry to local file
-00015330: 2e0a 0a20 2020 2020 2020 203a 7061 7261  ...        :para
-00015340: 6d20 6d6f 6465 6c5f 6964 3a20 7374 6f72  m model_id: stor
-00015350: 6564 206d 6f64 656c 2049 440a 2020 2020  ed model ID.    
-00015360: 2020 2020 3a74 7970 6520 6d6f 6465 6c5f      :type model_
-00015370: 6964 3a20 7374 720a 0a20 2020 2020 2020  id: str..       
-00015380: 203a 7061 7261 6d20 6669 6c65 6e61 6d65   :param filename
-00015390: 3a20 6e61 6d65 206f 6620 6c6f 6361 6c20  : name of local 
-000153a0: 6669 6c65 2074 6f20 6372 6561 7465 0a20  file to create. 
-000153b0: 2020 2020 2020 203a 7479 7065 2066 696c         :type fil
-000153c0: 656e 616d 653a 2073 7472 2c20 6f70 7469  ename: str, opti
-000153d0: 6f6e 616c 0a0a 2020 2020 2020 2020 3a70  onal..        :p
-000153e0: 6172 616d 2072 6576 5f69 643a 2072 6576  aram rev_id: rev
-000153f0: 6973 696f 6e20 6964 0a20 2020 2020 2020  ision id.       
-00015400: 203a 7479 7065 2072 6576 5f69 643a 2073   :type rev_id: s
-00015410: 7472 2c20 6f70 7469 6f6e 616c 0a0a 2020  tr, optional..  
-00015420: 2020 2020 2020 3a70 6172 616d 2066 6f72        :param for
-00015430: 6d61 743a 2066 6f72 6d61 7420 6f66 2074  mat: format of t
-00015440: 6865 2063 6f6e 7465 6e74 0a20 2020 2020  he content.     
-00015450: 2020 203a 7479 7065 2066 6f72 6d61 743a     :type format:
-00015460: 2073 7472 2c20 6f70 7469 6f6e 616c 0a0a   str, optional..
-00015470: 2020 2020 2020 2020 2a2a 4578 616d 706c          **Exampl
-00015480: 652a 2a0a 0a20 2020 2020 2020 202e 2e20  e**..        .. 
-00015490: 636f 6465 2d62 6c6f 636b 3a3a 2070 7974  code-block:: pyt
-000154a0: 686f 6e0a 0a20 2020 2020 2020 2020 2020  hon..           
-000154b0: 2063 6c69 656e 742e 6d6f 6465 6c73 2e64   client.models.d
-000154c0: 6f77 6e6c 6f61 6428 6d6f 6465 6c5f 6964  ownload(model_id
-000154d0: 2c20 276d 795f 6d6f 6465 6c2e 7461 722e  , 'my_model.tar.
-000154e0: 677a 2729 0a20 2020 2020 2020 2022 2222  gz').        """
-000154f0: 0a20 2020 2020 2020 206d 6f64 656c 5f69  .        model_i
-00015500: 6420 3d20 5f67 6574 5f69 645f 6672 6f6d  d = _get_id_from
-00015510: 5f64 6570 7265 6361 7465 645f 7569 6428  _deprecated_uid(
-00015520: 6b77 6172 6773 2c20 6d6f 6465 6c5f 6964  kwargs, model_id
-00015530: 2c20 226d 6f64 656c 2229 0a20 2020 2020  , "model").     
-00015540: 2020 206d 6f64 656c 5f69 6420 3d20 6361     model_id = ca
-00015550: 7374 2873 7472 2c20 6d6f 6465 6c5f 6964  st(str, model_id
-00015560: 290a 2020 2020 2020 2020 7265 765f 6964  ).        rev_id
-00015570: 203d 205f 6765 745f 6964 5f66 726f 6d5f   = _get_id_from_
-00015580: 6465 7072 6563 6174 6564 5f75 6964 286b  deprecated_uid(k
-00015590: 7761 7267 732c 2072 6576 5f69 642c 2022  wargs, rev_id, "
-000155a0: 7265 7622 2c20 5472 7565 290a 2020 2020  rev", True).    
-000155b0: 2020 2020 6966 206f 732e 7061 7468 2e69      if os.path.i
-000155c0: 7366 696c 6528 6669 6c65 6e61 6d65 293a  sfile(filename):
-000155d0: 0a20 2020 2020 2020 2020 2020 2072 6169  .            rai
-000155e0: 7365 2057 4d4c 436c 6965 6e74 4572 726f  se WMLClientErro
-000155f0: 7228 0a20 2020 2020 2020 2020 2020 2020  r(.             
-00015600: 2020 2022 4669 6c65 2077 6974 6820 6e61     "File with na
-00015610: 6d65 3a20 277b 7d27 2061 6c72 6561 6479  me: '{}' already
-00015620: 2065 7869 7374 732e 222e 666f 726d 6174   exists.".format
-00015630: 2866 696c 656e 616d 6529 0a20 2020 2020  (filename).     
-00015640: 2020 2020 2020 2029 0a0a 2020 2020 2020         )..      
-00015650: 2020 4d6f 6465 6c73 2e5f 7661 6c69 6461    Models._valida
-00015660: 7465 5f74 7970 6528 6d6f 6465 6c5f 6964  te_type(model_id
-00015670: 2c20 226d 6f64 656c 5f69 6422 2c20 7374  , "model_id", st
-00015680: 722c 2054 7275 6529 0a20 2020 2020 2020  r, True).       
-00015690: 204d 6f64 656c 732e 5f76 616c 6964 6174   Models._validat
-000156a0: 655f 7479 7065 2866 696c 656e 616d 652c  e_type(filename,
-000156b0: 2022 6669 6c65 6e61 6d65 222c 2073 7472   "filename", str
-000156c0: 2c20 5472 7565 290a 0a20 2020 2020 2020  , True)..       
-000156d0: 2069 6620 6669 6c65 6e61 6d65 2e65 6e64   if filename.end
-000156e0: 7377 6974 6828 222e 6a73 6f6e 2229 3a0a  swith(".json"):.
-000156f0: 2020 2020 2020 2020 2020 2020 6973 5f6a              is_j
-00015700: 736f 6e20 3d20 5472 7565 0a20 2020 2020  son = True.     
-00015710: 2020 2020 2020 206a 736f 6e5f 6669 6c65         json_file
-00015720: 6e61 6d65 203d 2066 696c 656e 616d 650a  name = filename.
-00015730: 2020 2020 2020 2020 2020 2020 696d 706f              impo
-00015740: 7274 2075 7569 640a 0a20 2020 2020 2020  rt uuid..       
-00015750: 2020 2020 2066 696c 656e 616d 6520 3d20       filename = 
-00015760: 6622 746d 705f 7b75 7569 642e 7575 6964  f"tmp_{uuid.uuid
-00015770: 3428 297d 2e74 6172 2e67 7a22 0a20 2020  4()}.tar.gz".   
-00015780: 2020 2020 2065 6c73 653a 0a20 2020 2020       else:.     
-00015790: 2020 2020 2020 2069 735f 6a73 6f6e 203d         is_json =
-000157a0: 2046 616c 7365 0a0a 2020 2020 2020 2020   False..        
-000157b0: 6172 7469 6661 6374 5f75 726c 203d 2028  artifact_url = (
-000157c0: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-000157d0: 662e 5f63 6c69 656e 742e 7365 7276 6963  f._client.servic
-000157e0: 655f 696e 7374 616e 6365 2e5f 6872 6566  e_instance._href
-000157f0: 5f64 6566 696e 6974 696f 6e73 2e67 6574  _definitions.get
-00015800: 5f6d 6f64 656c 5f6c 6173 745f 7665 7273  _model_last_vers
-00015810: 696f 6e5f 6872 6566 280a 2020 2020 2020  ion_href(.      
-00015820: 2020 2020 2020 2020 2020 6d6f 6465 6c5f            model_
-00015830: 6964 0a20 2020 2020 2020 2020 2020 2029  id.            )
-00015840: 0a20 2020 2020 2020 2029 0a20 2020 2020  .        ).     
-00015850: 2020 2070 6172 616d 7320 3d20 7365 6c66     params = self
-00015860: 2e5f 636c 6965 6e74 2e5f 7061 7261 6d73  ._client._params
-00015870: 2829 0a20 2020 2020 2020 2074 7279 3a0a  ().        try:.
-00015880: 2020 2020 2020 2020 2020 2020 7572 6c20              url 
-00015890: 3d20 7365 6c66 2e5f 636c 6965 6e74 2e73  = self._client.s
-000158a0: 6572 7669 6365 5f69 6e73 7461 6e63 652e  ervice_instance.
-000158b0: 5f68 7265 665f 6465 6669 6e69 7469 6f6e  _href_definition
-000158c0: 732e 6765 745f 7075 626c 6973 6865 645f  s.get_published_
-000158d0: 6d6f 6465 6c5f 6872 6566 280a 2020 2020  model_href(.    
-000158e0: 2020 2020 2020 2020 2020 2020 6d6f 6465              mode
-000158f0: 6c5f 6964 0a20 2020 2020 2020 2020 2020  l_id.           
-00015900: 2029 0a20 2020 2020 2020 2020 2020 206d   ).            m
-00015910: 6f64 656c 5f67 6574 5f72 6573 706f 6e73  odel_get_respons
-00015920: 6520 3d20 7265 7175 6573 7473 2e67 6574  e = requests.get
-00015930: 280a 2020 2020 2020 2020 2020 2020 2020  (.              
-00015940: 2020 7572 6c2c 2070 6172 616d 733d 7365    url, params=se
-00015950: 6c66 2e5f 636c 6965 6e74 2e5f 7061 7261  lf._client._para
-00015960: 6d73 2829 2c20 6865 6164 6572 733d 7365  ms(), headers=se
-00015970: 6c66 2e5f 636c 6965 6e74 2e5f 6765 745f  lf._client._get_
-00015980: 6865 6164 6572 7328 290a 2020 2020 2020  headers().      
-00015990: 2020 2020 2020 290a 0a20 2020 2020 2020        )..       
-000159a0: 2020 2020 206d 6f64 656c 5f64 6574 6169       model_detai
-000159b0: 6c73 203d 2073 656c 662e 5f68 616e 646c  ls = self._handl
-000159c0: 655f 7265 7370 6f6e 7365 2832 3030 2c20  e_response(200, 
-000159d0: 2267 6574 206d 6f64 656c 222c 206d 6f64  "get model", mod
-000159e0: 656c 5f67 6574 5f72 6573 706f 6e73 6529  el_get_response)
-000159f0: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if 
-00015a00: 7265 765f 6964 2069 7320 6e6f 7420 4e6f  rev_id is not No
-00015a10: 6e65 3a0a 2020 2020 2020 2020 2020 2020  ne:.            
-00015a20: 2020 2020 7061 7261 6d73 2e75 7064 6174      params.updat
-00015a30: 6528 7b22 7265 7669 7369 6f6e 5f69 6422  e({"revision_id"
-00015a40: 3a20 7265 765f 6964 7d29 0a0a 2020 2020  : rev_id})..    
-00015a50: 2020 2020 2020 2020 6d6f 6465 6c5f 7479          model_ty
-00015a60: 7065 203d 206d 6f64 656c 5f64 6574 6169  pe = model_detai
-00015a70: 6c73 5b22 656e 7469 7479 225d 5b22 7479  ls["entity"]["ty
-00015a80: 7065 225d 0a20 2020 2020 2020 2020 2020  pe"].           
-00015a90: 2069 6620 280a 2020 2020 2020 2020 2020   if (.          
-00015aa0: 2020 2020 2020 6d6f 6465 6c5f 7479 7065        model_type
-00015ab0: 2e73 7461 7274 7377 6974 6828 226b 6572  .startswith("ker
-00015ac0: 6173 5f22 290a 2020 2020 2020 2020 2020  as_").          
-00015ad0: 2020 2020 2020 6f72 206d 6f64 656c 5f74        or model_t
-00015ae0: 7970 652e 7374 6172 7473 7769 7468 2822  ype.startswith("
-00015af0: 7363 696b 6974 2d6c 6561 726e 5f22 290a  scikit-learn_").
-00015b00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00015b10: 6f72 206d 6f64 656c 5f74 7970 652e 7374  or model_type.st
-00015b20: 6172 7473 7769 7468 2822 7867 626f 6f73  artswith("xgboos
-00015b30: 745f 2229 0a20 2020 2020 2020 2020 2020  t_").           
-00015b40: 2029 2061 6e64 2066 6f72 6d61 7420 6973   ) and format is
-00015b50: 206e 6f74 204e 6f6e 653a 0a20 2020 2020   not None:.     
-00015b60: 2020 2020 2020 2020 2020 204d 6f64 656c             Model
-00015b70: 732e 5f76 616c 6964 6174 655f 7479 7065  s._validate_type
-00015b80: 2866 6f72 6d61 742c 2022 666f 726d 6174  (format, "format
-00015b90: 222c 2073 7472 2c20 4661 6c73 6529 0a20  ", str, False). 
-00015ba0: 2020 2020 2020 2020 2020 2020 2020 2069                 i
-00015bb0: 6620 7374 7228 666f 726d 6174 292e 7570  f str(format).up
-00015bc0: 7065 7228 2920 3d3d 2022 434f 5245 4d4c  per() == "COREML
-00015bd0: 223a 0a20 2020 2020 2020 2020 2020 2020  ":.             
-00015be0: 2020 2020 2020 2070 6172 616d 732e 7570         params.up
-00015bf0: 6461 7465 287b 2263 6f6e 7465 6e74 5f66  date({"content_f
-00015c00: 6f72 6d61 7422 3a20 2263 6f72 656d 6c22  ormat": "coreml"
-00015c10: 7d29 0a20 2020 2020 2020 2020 2020 2020  }).             
-00015c20: 2020 2065 6c73 653a 0a20 2020 2020 2020     else:.       
-00015c30: 2020 2020 2020 2020 2020 2020 2070 6172               par
-00015c40: 616d 732e 7570 6461 7465 287b 2263 6f6e  ams.update({"con
-00015c50: 7465 6e74 5f66 6f72 6d61 7422 3a20 226e  tent_format": "n
-00015c60: 6174 6976 6522 7d29 0a20 2020 2020 2020  ative"}).       
-00015c70: 2020 2020 2065 6c73 653a 0a20 2020 2020       else:.     
-00015c80: 2020 2020 2020 2020 2020 2070 6172 616d             param
-00015c90: 732e 7570 6461 7465 287b 2263 6f6e 7465  s.update({"conte
-00015ca0: 6e74 5f66 6f72 6d61 7422 3a20 226e 6174  nt_format": "nat
-00015cb0: 6976 6522 7d29 0a20 2020 2020 2020 2020  ive"}).         
-00015cc0: 2020 2061 7274 6966 6163 745f 636f 6e74     artifact_cont
-00015cd0: 656e 745f 7572 6c20 3d20 7374 7228 6172  ent_url = str(ar
-00015ce0: 7469 6661 6374 5f75 726c 202b 2022 2f64  tifact_url + "/d
-00015cf0: 6f77 6e6c 6f61 6422 290a 2020 2020 2020  ownload").      
-00015d00: 2020 2020 2020 6966 206d 6f64 656c 5f64        if model_d
-00015d10: 6574 6169 6c73 5b22 656e 7469 7479 225d  etails["entity"]
-00015d20: 5b22 7479 7065 225d 203d 3d20 2277 6d6c  ["type"] == "wml
-00015d30: 2d68 7962 7269 645f 302e 3122 3a0a 2020  -hybrid_0.1":.  
-00015d40: 2020 2020 2020 2020 2020 2020 2020 7365                se
-00015d50: 6c66 2e5f 646f 776e 6c6f 6164 5f61 7574  lf._download_aut
-00015d60: 6f5f 6169 5f6d 6f64 656c 5f63 6f6e 7465  o_ai_model_conte
-00015d70: 6e74 280a 2020 2020 2020 2020 2020 2020  nt(.            
-00015d80: 2020 2020 2020 2020 6d6f 6465 6c5f 6964          model_id
-00015d90: 2c20 6172 7469 6661 6374 5f63 6f6e 7465  , artifact_conte
-00015da0: 6e74 5f75 726c 2c20 6669 6c65 6e61 6d65  nt_url, filename
-00015db0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00015dc0: 2029 0a20 2020 2020 2020 2020 2020 2020   ).             
-00015dd0: 2020 2070 7269 6e74 2822 5375 6363 6573     print("Succes
-00015de0: 7366 756c 6c79 2073 6176 6564 206d 6f64  sfully saved mod
-00015df0: 656c 2063 6f6e 7465 6e74 2074 6f20 6669  el content to fi
-00015e00: 6c65 3a20 277b 7d27 222e 666f 726d 6174  le: '{}'".format
-00015e10: 2866 696c 656e 616d 6529 290a 2020 2020  (filename)).    
-00015e20: 2020 2020 2020 2020 2020 2020 7265 7475              retu
-00015e30: 726e 206f 732e 6765 7463 7764 2829 202b  rn os.getcwd() +
-00015e40: 2022 2f22 202b 2066 696c 656e 616d 650a   "/" + filename.
-00015e50: 2020 2020 2020 2020 2020 2020 656c 7365              else
-00015e60: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
-00015e70: 2020 7220 3d20 7265 7175 6573 7473 2e67    r = requests.g
-00015e80: 6574 280a 2020 2020 2020 2020 2020 2020  et(.            
-00015e90: 2020 2020 2020 2020 6172 7469 6661 6374          artifact
-00015ea0: 5f63 6f6e 7465 6e74 5f75 726c 2c0a 2020  _content_url,.  
-00015eb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00015ec0: 2020 7061 7261 6d73 3d70 6172 616d 732c    params=params,
-00015ed0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00015ee0: 2020 2020 2068 6561 6465 7273 3d73 656c       headers=sel
-00015ef0: 662e 5f63 6c69 656e 742e 5f67 6574 5f68  f._client._get_h
-00015f00: 6561 6465 7273 2829 2c0a 2020 2020 2020  eaders(),.      
-00015f10: 2020 2020 2020 2020 2020 2020 2020 7374                st
-00015f20: 7265 616d 3d54 7275 652c 0a20 2020 2020  ream=True,.     
-00015f30: 2020 2020 2020 2020 2020 2029 0a0a 2020             )..  
-00015f40: 2020 2020 2020 2020 2020 6172 7469 6661            artifa
-00015f50: 6374 5f63 6f6e 7465 6e74 5f75 726c 203d  ct_content_url =
-00015f60: 2073 7472 2861 7274 6966 6163 745f 7572   str(artifact_ur
-00015f70: 6c20 2b20 222f 636f 6e74 656e 7422 290a  l + "/content").
-00015f80: 2020 2020 2020 2020 2020 2020 6966 2072              if r
-00015f90: 6576 5f69 6420 6973 206e 6f74 204e 6f6e  ev_id is not Non
-00015fa0: 653a 0a20 2020 2020 2020 2020 2020 2020  e:.             
-00015fb0: 2020 2070 6172 616d 732e 7570 6461 7465     params.update
-00015fc0: 287b 2272 6576 6973 696f 6e5f 6964 223a  ({"revision_id":
-00015fd0: 2072 6576 5f69 647d 290a 2020 2020 2020   rev_id}).      
-00015fe0: 2020 2020 2020 6966 206e 6f74 2073 656c        if not sel
-00015ff0: 662e 5f63 6c69 656e 742e 4350 445f 7665  f._client.CPD_ve
-00016000: 7273 696f 6e3a 0a20 2020 2020 2020 2020  rsion:.         
-00016010: 2020 2020 2020 2072 203d 2072 6571 7565         r = reque
-00016020: 7374 732e 6765 7428 0a20 2020 2020 2020  sts.get(.       
-00016030: 2020 2020 2020 2020 2020 2020 2061 7274               art
-00016040: 6966 6163 745f 636f 6e74 656e 745f 7572  ifact_content_ur
-00016050: 6c2c 0a20 2020 2020 2020 2020 2020 2020  l,.             
-00016060: 2020 2020 2020 2070 6172 616d 733d 7061         params=pa
-00016070: 7261 6d73 2c0a 2020 2020 2020 2020 2020  rams,.          
-00016080: 2020 2020 2020 2020 2020 6865 6164 6572            header
-00016090: 733d 7365 6c66 2e5f 636c 6965 6e74 2e5f  s=self._client._
-000160a0: 6765 745f 6865 6164 6572 7328 292c 0a20  get_headers(),. 
-000160b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000160c0: 2020 2073 7472 6561 6d3d 5472 7565 2c0a     stream=True,.
-000160d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000160e0: 290a 2020 2020 2020 2020 2020 2020 6966  ).            if
-000160f0: 2072 2e73 7461 7475 735f 636f 6465 2021   r.status_code !
-00016100: 3d20 3230 303a 0a20 2020 2020 2020 2020  = 200:.         
-00016110: 2020 2020 2020 2072 6169 7365 2041 7069         raise Api
-00016120: 5265 7175 6573 7446 6169 6c75 7265 280a  RequestFailure(.
-00016130: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00016140: 2020 2020 2246 6169 6c75 7265 2064 7572      "Failure dur
-00016150: 696e 6720 7b7d 2e22 2e66 6f72 6d61 7428  ing {}.".format(
-00016160: 2264 6f77 6e6c 6f61 6469 6e67 206d 6f64  "downloading mod
-00016170: 656c 2229 2c20 720a 2020 2020 2020 2020  el"), r.        
-00016180: 2020 2020 2020 2020 290a 0a20 2020 2020          )..     
-00016190: 2020 2020 2020 2064 6f77 6e6c 6f61 6465         downloade
-000161a0: 645f 6d6f 6465 6c20 3d20 722e 636f 6e74  d_model = r.cont
-000161b0: 656e 740a 2020 2020 2020 2020 2020 2020  ent.            
-000161c0: 7365 6c66 2e5f 6c6f 6767 6572 2e69 6e66  self._logger.inf
-000161d0: 6f28 0a20 2020 2020 2020 2020 2020 2020  o(.             
-000161e0: 2020 2022 5375 6363 6573 7366 756c 6c79     "Successfully
-000161f0: 2064 6f77 6e6c 6f61 6465 6420 6172 7469   downloaded arti
-00016200: 6661 6374 2077 6974 6820 6172 7469 6661  fact with artifa
-00016210: 6374 5f75 726c 3a20 7b7d 222e 666f 726d  ct_url: {}".form
-00016220: 6174 280a 2020 2020 2020 2020 2020 2020  at(.            
-00016230: 2020 2020 2020 2020 6172 7469 6661 6374          artifact
-00016240: 5f75 726c 0a20 2020 2020 2020 2020 2020  _url.           
-00016250: 2020 2020 2029 0a20 2020 2020 2020 2020       ).         
-00016260: 2020 2029 0a20 2020 2020 2020 2065 7863     ).        exc
-00016270: 6570 7420 574d 4c43 6c69 656e 7445 7272  ept WMLClientErr
-00016280: 6f72 2061 7320 653a 0a20 2020 2020 2020  or as e:.       
-00016290: 2020 2020 2072 6169 7365 2065 0a20 2020       raise e.   
-000162a0: 2020 2020 2065 7863 6570 7420 4578 6365       except Exce
-000162b0: 7074 696f 6e20 6173 2065 3a0a 2020 2020  ption as e:.    
-000162c0: 2020 2020 2020 2020 6966 2061 7274 6966          if artif
-000162d0: 6163 745f 7572 6c20 6973 206e 6f74 204e  act_url is not N
-000162e0: 6f6e 653a 0a20 2020 2020 2020 2020 2020  one:.           
-000162f0: 2020 2020 2072 6169 7365 2057 4d4c 436c       raise WMLCl
-00016300: 6965 6e74 4572 726f 7228 0a20 2020 2020  ientError(.     
-00016310: 2020 2020 2020 2020 2020 2020 2020 2022                 "
-00016320: 446f 776e 6c6f 6164 696e 6720 6d6f 6465  Downloading mode
-00016330: 6c20 7769 7468 2061 7274 6966 6163 745f  l with artifact_
-00016340: 7572 6c3a 2027 7b7d 2720 6661 696c 6564  url: '{}' failed
-00016350: 2e22 2e66 6f72 6d61 7428 0a20 2020 2020  .".format(.     
-00016360: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00016370: 2020 2061 7274 6966 6163 745f 7572 6c0a     artifact_url.
-00016380: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00016390: 2020 2020 292c 0a20 2020 2020 2020 2020      ),.         
-000163a0: 2020 2020 2020 2020 2020 2065 2c0a 2020             e,.  
-000163b0: 2020 2020 2020 2020 2020 2020 2020 290a                ).
-000163c0: 2020 2020 2020 2020 2020 2020 656c 7365              else
-000163d0: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
-000163e0: 2020 7261 6973 6520 574d 4c43 6c69 656e    raise WMLClien
-000163f0: 7445 7272 6f72 2822 446f 776e 6c6f 6164  tError("Download
-00016400: 696e 6720 6d6f 6465 6c20 6661 696c 6564  ing model failed
-00016410: 2e22 2c20 6529 0a20 2020 2020 2020 2066  .", e).        f
-00016420: 696e 616c 6c79 3a0a 2020 2020 2020 2020  inally:.        
-00016430: 2020 2020 6966 2069 735f 6a73 6f6e 3a0a      if is_json:.
-00016440: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00016450: 7472 793a 0a20 2020 2020 2020 2020 2020  try:.           
-00016460: 2020 2020 2020 2020 206f 732e 7265 6d6f           os.remo
-00016470: 7665 2866 696c 656e 616d 6529 0a20 2020  ve(filename).   
-00016480: 2020 2020 2020 2020 2020 2020 2065 7863               exc
-00016490: 6570 7420 4578 6365 7074 696f 6e3a 0a20  ept Exception:. 
-000164a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000164b0: 2020 2070 6173 730a 2020 2020 2020 2020     pass.        
-000164c0: 7472 793a 0a20 2020 2020 2020 2020 2020  try:.           
-000164d0: 2077 6974 6820 6f70 656e 2866 696c 656e   with open(filen
-000164e0: 616d 652c 2022 7762 2229 2061 7320 663a  ame, "wb") as f:
-000164f0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00016500: 2066 2e77 7269 7465 2864 6f77 6e6c 6f61   f.write(downloa
-00016510: 6465 645f 6d6f 6465 6c29 0a0a 2020 2020  ded_model)..    
-00016520: 2020 2020 2020 2020 6966 2069 735f 6a73          if is_js
-00016530: 6f6e 3a0a 2020 2020 2020 2020 2020 2020  on:.            
-00016540: 2020 2020 696d 706f 7274 2074 6172 6669      import tarfi
-00016550: 6c65 0a0a 2020 2020 2020 2020 2020 2020  le..            
-00016560: 2020 2020 7461 7220 3d20 7461 7266 696c      tar = tarfil
-00016570: 652e 6f70 656e 2866 696c 656e 616d 652c  e.open(filename,
-00016580: 2022 723a 677a 2229 0a20 2020 2020 2020   "r:gz").       
-00016590: 2020 2020 2020 2020 2066 696c 655f 6e61           file_na
-000165a0: 6d65 203d 2074 6172 2e67 6574 6e61 6d65  me = tar.getname
-000165b0: 7328 295b 305d 0a20 2020 2020 2020 2020  s()[0].         
-000165c0: 2020 2020 2020 2069 6620 6e6f 7420 6669         if not fi
-000165d0: 6c65 5f6e 616d 652e 656e 6473 7769 7468  le_name.endswith
-000165e0: 2822 2e6a 736f 6e22 293a 0a20 2020 2020  (".json"):.     
-000165f0: 2020 2020 2020 2020 2020 2020 2020 2072                 r
-00016600: 6169 7365 2057 4d4c 436c 6965 6e74 4572  aise WMLClientEr
-00016610: 726f 7228 2244 6f77 6e6c 6f61 6465 6420  ror("Downloaded 
-00016620: 6d6f 6465 6c20 6973 206e 6f74 206a 736f  model is not jso
-00016630: 6e2e 2229 0a20 2020 2020 2020 2020 2020  n.").           
-00016640: 2020 2020 2074 6172 2e65 7874 7261 6374       tar.extract
-00016650: 616c 6c28 290a 2020 2020 2020 2020 2020  all().          
-00016660: 2020 2020 2020 7461 722e 636c 6f73 6528        tar.close(
-00016670: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
-00016680: 2020 6f73 2e72 656e 616d 6528 6669 6c65    os.rename(file
-00016690: 5f6e 616d 652c 206a 736f 6e5f 6669 6c65  _name, json_file
-000166a0: 6e61 6d65 290a 0a20 2020 2020 2020 2020  name)..         
-000166b0: 2020 2020 2020 206f 732e 7265 6d6f 7665         os.remove
-000166c0: 2866 696c 656e 616d 6529 0a20 2020 2020  (filename).     
-000166d0: 2020 2020 2020 2020 2020 2066 696c 656e             filen
-000166e0: 616d 6520 3d20 6a73 6f6e 5f66 696c 656e  ame = json_filen
-000166f0: 616d 650a 0a20 2020 2020 2020 2020 2020  ame..           
-00016700: 2070 7269 6e74 2822 5375 6363 6573 7366   print("Successf
-00016710: 756c 6c79 2073 6176 6564 206d 6f64 656c  ully saved model
-00016720: 2063 6f6e 7465 6e74 2074 6f20 6669 6c65   content to file
-00016730: 3a20 277b 7d27 222e 666f 726d 6174 2866  : '{}'".format(f
-00016740: 696c 656e 616d 6529 290a 2020 2020 2020  ilename)).      
-00016750: 2020 2020 2020 7265 7475 726e 206f 732e        return os.
-00016760: 6765 7463 7764 2829 202b 2022 2f22 202b  getcwd() + "/" +
-00016770: 2066 696c 656e 616d 650a 2020 2020 2020   filename.      
-00016780: 2020 6578 6365 7074 2049 4f45 7272 6f72    except IOError
-00016790: 2061 7320 653a 0a20 2020 2020 2020 2020   as e:.         
-000167a0: 2020 2072 6169 7365 2057 4d4c 436c 6965     raise WMLClie
-000167b0: 6e74 4572 726f 7228 0a20 2020 2020 2020  ntError(.       
-000167c0: 2020 2020 2020 2020 2022 5361 7669 6e67           "Saving
-000167d0: 206d 6f64 656c 2077 6974 6820 6172 7469   model with arti
-000167e0: 6661 6374 5f75 726c 3a20 277b 7d27 2066  fact_url: '{}' f
-000167f0: 6169 6c65 642e 222e 666f 726d 6174 2866  ailed.".format(f
-00016800: 696c 656e 616d 6529 2c20 650a 2020 2020  ilename), e.    
-00016810: 2020 2020 2020 2020 290a 0a20 2020 2064          )..    d
-00016820: 6566 2064 656c 6574 6528 7365 6c66 2c20  ef delete(self, 
-00016830: 6d6f 6465 6c5f 6964 3a20 7374 7220 7c20  model_id: str | 
-00016840: 4e6f 6e65 203d 204e 6f6e 652c 202a 2a6b  None = None, **k
-00016850: 7761 7267 733a 2041 6e79 2920 2d3e 2064  wargs: Any) -> d
-00016860: 6963 745b 7374 722c 2041 6e79 5d3a 0a20  ict[str, Any]:. 
-00016870: 2020 2020 2020 2022 2222 4465 6c65 7465         """Delete
-00016880: 206d 6f64 656c 2066 726f 6d20 7265 706f   model from repo
-00016890: 7369 746f 7279 2e0a 0a20 2020 2020 2020  sitory...       
-000168a0: 203a 7061 7261 6d20 6d6f 6465 6c5f 6964   :param model_id
-000168b0: 3a20 7374 6f72 6564 206d 6f64 656c 2049  : stored model I
-000168c0: 440a 2020 2020 2020 2020 3a74 7970 6520  D.        :type 
-000168d0: 6d6f 6465 6c5f 6964 3a20 7374 720a 0a20  model_id: str.. 
-000168e0: 2020 2020 2020 202a 2a45 7861 6d70 6c65         **Example
-000168f0: 2a2a 0a0a 2020 2020 2020 2020 2e2e 2063  **..        .. c
-00016900: 6f64 652d 626c 6f63 6b3a 3a20 7079 7468  ode-block:: pyth
-00016910: 6f6e 0a0a 2020 2020 2020 2020 2020 2020  on..            
-00016920: 636c 6965 6e74 2e6d 6f64 656c 732e 6465  client.models.de
-00016930: 6c65 7465 286d 6f64 656c 5f69 6429 0a20  lete(model_id). 
-00016940: 2020 2020 2020 2022 2222 0a20 2020 2020         """.     
-00016950: 2020 206d 6f64 656c 5f69 6420 3d20 5f67     model_id = _g
-00016960: 6574 5f69 645f 6672 6f6d 5f64 6570 7265  et_id_from_depre
-00016970: 6361 7465 645f 7569 6428 6b77 6172 6773  cated_uid(kwargs
-00016980: 2c20 6d6f 6465 6c5f 6964 2c20 226d 6f64  , model_id, "mod
-00016990: 656c 2229 0a20 2020 2020 2020 204d 6f64  el").        Mod
-000169a0: 656c 732e 5f76 616c 6964 6174 655f 7479  els._validate_ty
-000169b0: 7065 286d 6f64 656c 5f69 642c 2022 6d6f  pe(model_id, "mo
-000169c0: 6465 6c5f 6964 222c 2073 7472 2c20 4661  del_id", str, Fa
-000169d0: 6c73 6529 0a0a 2020 2020 2020 2020 6d6f  lse)..        mo
-000169e0: 6465 6c5f 656e 6470 6f69 6e74 203d 2028  del_endpoint = (
-000169f0: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-00016a00: 662e 5f63 6c69 656e 742e 7365 7276 6963  f._client.servic
-00016a10: 655f 696e 7374 616e 6365 2e5f 6872 6566  e_instance._href
-00016a20: 5f64 6566 696e 6974 696f 6e73 2e67 6574  _definitions.get
-00016a30: 5f70 7562 6c69 7368 6564 5f6d 6f64 656c  _published_model
-00016a40: 5f68 7265 6628 0a20 2020 2020 2020 2020  _href(.         
-00016a50: 2020 2020 2020 206d 6f64 656c 5f69 640a         model_id.
-00016a60: 2020 2020 2020 2020 2020 2020 290a 2020              ).  
-00016a70: 2020 2020 2020 290a 0a20 2020 2020 2020        )..       
-00016a80: 2073 656c 662e 5f6c 6f67 6765 722e 6465   self._logger.de
-00016a90: 6275 6728 0a20 2020 2020 2020 2020 2020  bug(.           
-00016aa0: 2022 4465 6c65 7469 6f6e 2061 7274 6966   "Deletion artif
-00016ab0: 6163 7420 6d6f 6465 6c20 656e 6470 6f69  act model endpoi
-00016ac0: 6e74 3a20 7b7d 222e 666f 726d 6174 286d  nt: {}".format(m
-00016ad0: 6f64 656c 5f65 6e64 706f 696e 7429 0a20  odel_endpoint). 
-00016ae0: 2020 2020 2020 2029 0a20 2020 2020 2020         ).       
-00016af0: 2069 6620 6e6f 7420 7365 6c66 2e5f 636c   if not self._cl
-00016b00: 6965 6e74 2e49 4350 5f50 4c41 5446 4f52  ient.ICP_PLATFOR
-00016b10: 4d5f 5350 4143 4553 3a0a 2020 2020 2020  M_SPACES:.      
-00016b20: 2020 2020 2020 6966 2073 656c 662e 5f69        if self._i
-00016b30: 665f 6465 706c 6f79 6d65 6e74 5f65 7869  f_deployment_exi
-00016b40: 7374 5f66 6f72 5f61 7373 6574 286d 6f64  st_for_asset(mod
-00016b50: 656c 5f69 6429 3a0a 2020 2020 2020 2020  el_id):.        
-00016b60: 2020 2020 2020 2020 7261 6973 6520 574d          raise WM
-00016b70: 4c43 6c69 656e 7445 7272 6f72 280a 2020  LClientError(.  
-00016b80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00016b90: 2020 2243 616e 6e6f 7420 6465 6c65 7465    "Cannot delete
-00016ba0: 206d 6f64 656c 2074 6861 7420 6861 7320   model that has 
-00016bb0: 6578 6973 7469 6e67 2064 6570 6c6f 796d  existing deploym
-00016bc0: 656e 7473 2e20 506c 6561 7365 2064 656c  ents. Please del
-00016bd0: 6574 6520 616c 6c20 6173 736f 6369 6174  ete all associat
-00016be0: 6564 2064 6570 6c6f 796d 656e 7473 2061  ed deployments a
-00016bf0: 6e64 2074 7279 2061 6761 696e 220a 2020  nd try again".  
-00016c00: 2020 2020 2020 2020 2020 2020 2020 290a                ).
-00016c10: 0a20 2020 2020 2020 2020 2020 2072 6573  .            res
-00016c20: 706f 6e73 655f 6465 6c65 7465 203d 2072  ponse_delete = r
-00016c30: 6571 7565 7374 732e 6465 6c65 7465 280a  equests.delete(.
-00016c40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00016c50: 6d6f 6465 6c5f 656e 6470 6f69 6e74 2c0a  model_endpoint,.
-00016c60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00016c70: 7061 7261 6d73 3d73 656c 662e 5f63 6c69  params=self._cli
-00016c80: 656e 742e 5f70 6172 616d 7328 292c 0a20  ent._params(),. 
-00016c90: 2020 2020 2020 2020 2020 2020 2020 2068                 h
-00016ca0: 6561 6465 7273 3d73 656c 662e 5f63 6c69  eaders=self._cli
-00016cb0: 656e 742e 5f67 6574 5f68 6561 6465 7273  ent._get_headers
-00016cc0: 2829 2c0a 2020 2020 2020 2020 2020 2020  (),.            
-00016cd0: 290a 2020 2020 2020 2020 656c 7365 3a0a  ).        else:.
-00016ce0: 2020 2020 2020 2020 2020 2020 2320 6368              # ch
-00016cf0: 6563 6b20 6966 2074 6865 206d 6f64 656c  eck if the model
-00016d00: 2061 7320 6120 636f 7272 6573 706f 6e64   as a correspond
-00016d10: 696e 6720 6465 706c 6f79 6d65 6e74 0a20  ing deployment. 
-00016d20: 2020 2020 2020 2020 2020 2069 6620 7365             if se
-00016d30: 6c66 2e5f 6966 5f64 6570 6c6f 796d 656e  lf._if_deploymen
-00016d40: 745f 6578 6973 745f 666f 725f 6173 7365  t_exist_for_asse
-00016d50: 7428 6d6f 6465 6c5f 6964 293a 0a20 2020  t(model_id):.   
-00016d60: 2020 2020 2020 2020 2020 2020 2072 6169               rai
-00016d70: 7365 2057 4d4c 436c 6965 6e74 4572 726f  se WMLClientErro
-00016d80: 7228 0a20 2020 2020 2020 2020 2020 2020  r(.             
-00016d90: 2020 2020 2020 2022 4361 6e6e 6f74 2064         "Cannot d
-00016da0: 656c 6574 6520 6d6f 6465 6c20 7468 6174  elete model that
-00016db0: 2068 6173 2065 7869 7374 696e 6720 6465   has existing de
-00016dc0: 706c 6f79 6d65 6e74 732e 2050 6c65 6173  ployments. Pleas
-00016dd0: 6520 6465 6c65 7465 2061 6c6c 2061 7373  e delete all ass
-00016de0: 6f63 6961 7465 6420 6465 706c 6f79 6d65  ociated deployme
-00016df0: 6e74 7320 616e 6420 7472 7920 6167 6169  nts and try agai
-00016e00: 6e22 0a20 2020 2020 2020 2020 2020 2020  n".             
-00016e10: 2020 2029 0a20 2020 2020 2020 2020 2020     ).           
-00016e20: 2072 6573 706f 6e73 655f 6465 6c65 7465   response_delete
-00016e30: 203d 2072 6571 7565 7374 732e 6465 6c65   = requests.dele
-00016e40: 7465 280a 2020 2020 2020 2020 2020 2020  te(.            
-00016e50: 2020 2020 6d6f 6465 6c5f 656e 6470 6f69      model_endpoi
-00016e60: 6e74 2c0a 2020 2020 2020 2020 2020 2020  nt,.            
-00016e70: 2020 2020 7061 7261 6d73 3d73 656c 662e      params=self.
-00016e80: 5f63 6c69 656e 742e 5f70 6172 616d 7328  _client._params(
-00016e90: 292c 0a20 2020 2020 2020 2020 2020 2020  ),.             
-00016ea0: 2020 2068 6561 6465 7273 3d73 656c 662e     headers=self.
-00016eb0: 5f63 6c69 656e 742e 5f67 6574 5f68 6561  _client._get_hea
-00016ec0: 6465 7273 2829 2c0a 2020 2020 2020 2020  ders(),.        
-00016ed0: 2020 2020 290a 2020 2020 2020 2020 7265      ).        re
-00016ee0: 7475 726e 2073 656c 662e 5f68 616e 646c  turn self._handl
-00016ef0: 655f 7265 7370 6f6e 7365 2832 3034 2c20  e_response(204, 
-00016f00: 226d 6f64 656c 2064 656c 6574 696f 6e22  "model deletion"
-00016f10: 2c20 7265 7370 6f6e 7365 5f64 656c 6574  , response_delet
-00016f20: 652c 2046 616c 7365 290a 0a20 2020 2040  e, False)..    @
-00016f30: 6f76 6572 6c6f 6164 0a20 2020 2064 6566  overload.    def
-00016f40: 2067 6574 5f64 6574 6169 6c73 280a 2020   get_details(.  
-00016f50: 2020 2020 2020 7365 6c66 2c0a 2020 2020        self,.    
-00016f60: 2020 2020 6d6f 6465 6c5f 6964 3a20 7374      model_id: st
-00016f70: 7220 3d20 2222 2c0a 2020 2020 2020 2020  r = "",.        
-00016f80: 6c69 6d69 743a 2069 6e74 207c 204e 6f6e  limit: int | Non
-00016f90: 6520 3d20 4e6f 6e65 2c0a 2020 2020 2020  e = None,.      
-00016fa0: 2020 6173 796e 6368 726f 6e6f 7573 3a20    asynchronous: 
-00016fb0: 626f 6f6c 203d 2046 616c 7365 2c0a 2020  bool = False,.  
-00016fc0: 2020 2020 2020 6765 745f 616c 6c3a 2062        get_all: b
-00016fd0: 6f6f 6c20 3d20 4661 6c73 652c 0a20 2020  ool = False,.   
-00016fe0: 2020 2020 2073 7065 635f 7374 6174 653a       spec_state:
-00016ff0: 2053 7065 6353 7461 7465 7320 7c20 4e6f   SpecStates | No
-00017000: 6e65 203d 204e 6f6e 652c 0a20 2020 2020  ne = None,.     
-00017010: 2020 202a 2a6b 7761 7267 733a 2041 6e79     **kwargs: Any
-00017020: 2c0a 2020 2020 2920 2d3e 2064 6963 745b  ,.    ) -> dict[
-00017030: 7374 722c 2041 6e79 5d3a 0a20 2020 2020  str, Any]:.     
-00017040: 2020 202e 2e2e 0a0a 2020 2020 406f 7665     .....    @ove
-00017050: 726c 6f61 640a 2020 2020 6465 6620 6765  rload.    def ge
-00017060: 745f 6465 7461 696c 7328 0a20 2020 2020  t_details(.     
-00017070: 2020 2073 656c 662c 0a20 2020 2020 2020     self,.       
-00017080: 206d 6f64 656c 5f69 643a 2073 7472 207c   model_id: str |
-00017090: 204e 6f6e 6520 3d20 4e6f 6e65 2c0a 2020   None = None,.  
-000170a0: 2020 2020 2020 6c69 6d69 743a 2069 6e74        limit: int
-000170b0: 207c 204e 6f6e 6520 3d20 4e6f 6e65 2c0a   | None = None,.
-000170c0: 2020 2020 2020 2020 6173 796e 6368 726f          asynchro
-000170d0: 6e6f 7573 3a20 626f 6f6c 203d 2046 616c  nous: bool = Fal
-000170e0: 7365 2c0a 2020 2020 2020 2020 6765 745f  se,.        get_
-000170f0: 616c 6c3a 2062 6f6f 6c20 3d20 4661 6c73  all: bool = Fals
-00017100: 652c 0a20 2020 2020 2020 2073 7065 635f  e,.        spec_
-00017110: 7374 6174 653a 2053 7065 6353 7461 7465  state: SpecState
-00017120: 7320 7c20 4e6f 6e65 203d 204e 6f6e 652c  s | None = None,
-00017130: 0a20 2020 2020 2020 202a 2a6b 7761 7267  .        **kwarg
-00017140: 733a 2041 6e79 2c0a 2020 2020 2920 2d3e  s: Any,.    ) ->
-00017150: 2064 6963 745b 7374 722c 2041 6e79 5d20   dict[str, Any] 
-00017160: 7c20 4765 6e65 7261 746f 723a 0a20 2020  | Generator:.   
-00017170: 2020 2020 202e 2e2e 0a0a 2020 2020 6465       .....    de
-00017180: 6620 6765 745f 6465 7461 696c 7328 0a20  f get_details(. 
-00017190: 2020 2020 2020 2073 656c 662c 0a20 2020         self,.   
-000171a0: 2020 2020 206d 6f64 656c 5f69 643a 2073       model_id: s
-000171b0: 7472 207c 204e 6f6e 6520 3d20 4e6f 6e65  tr | None = None
-000171c0: 2c0a 2020 2020 2020 2020 6c69 6d69 743a  ,.        limit:
-000171d0: 2069 6e74 207c 204e 6f6e 6520 3d20 4e6f   int | None = No
-000171e0: 6e65 2c0a 2020 2020 2020 2020 6173 796e  ne,.        asyn
-000171f0: 6368 726f 6e6f 7573 3a20 626f 6f6c 203d  chronous: bool =
-00017200: 2046 616c 7365 2c0a 2020 2020 2020 2020   False,.        
-00017210: 6765 745f 616c 6c3a 2062 6f6f 6c20 3d20  get_all: bool = 
-00017220: 4661 6c73 652c 0a20 2020 2020 2020 2073  False,.        s
-00017230: 7065 635f 7374 6174 653a 2053 7065 6353  pec_state: SpecS
-00017240: 7461 7465 7320 7c20 4e6f 6e65 203d 204e  tates | None = N
-00017250: 6f6e 652c 0a20 2020 2020 2020 202a 2a6b  one,.        **k
-00017260: 7761 7267 733a 2041 6e79 2c0a 2020 2020  wargs: Any,.    
-00017270: 2920 2d3e 2064 6963 745b 7374 722c 2041  ) -> dict[str, A
-00017280: 6e79 5d20 7c20 4765 6e65 7261 746f 723a  ny] | Generator:
-00017290: 0a20 2020 2020 2020 2022 2222 4765 7420  .        """Get 
-000172a0: 6d65 7461 6461 7461 206f 6620 7374 6f72  metadata of stor
-000172b0: 6564 206d 6f64 656c 732e 2049 6620 6d6f  ed models. If mo
-000172c0: 6465 6c20 6964 2069 7320 6e6f 7420 7370  del id is not sp
-000172d0: 6563 6966 6965 6420 7265 7475 726e 7320  ecified returns 
-000172e0: 616c 6c20 6d6f 6465 6c73 206d 6574 6164  all models metad
-000172f0: 6174 612e 0a0a 2020 2020 2020 2020 3a70  ata...        :p
-00017300: 6172 616d 206d 6f64 656c 5f69 643a 2073  aram model_id: s
-00017310: 746f 7265 6420 6d6f 6465 6c2c 2064 6566  tored model, def
-00017320: 696e 6974 696f 6e20 6f72 2070 6970 656c  inition or pipel
-00017330: 696e 6520 4944 0a20 2020 2020 2020 203a  ine ID.        :
-00017340: 7479 7065 206d 6f64 656c 5f69 643a 2073  type model_id: s
-00017350: 7472 2c20 6f70 7469 6f6e 616c 0a0a 2020  tr, optional..  
-00017360: 2020 2020 2020 3a70 6172 616d 206c 696d        :param lim
-00017370: 6974 3a20 6c69 6d69 7420 6e75 6d62 6572  it: limit number
-00017380: 206f 6620 6665 7463 6865 6420 7265 636f   of fetched reco
-00017390: 7264 730a 2020 2020 2020 2020 3a74 7970  rds.        :typ
-000173a0: 6520 6c69 6d69 743a 2069 6e74 2c20 6f70  e limit: int, op
-000173b0: 7469 6f6e 616c 0a0a 2020 2020 2020 2020  tional..        
-000173c0: 3a70 6172 616d 2061 7379 6e63 6872 6f6e  :param asynchron
-000173d0: 6f75 733a 2069 6620 6054 7275 6560 2c20  ous: if `True`, 
-000173e0: 6974 2077 696c 6c20 776f 726b 2061 7320  it will work as 
-000173f0: 6120 6765 6e65 7261 746f 720a 2020 2020  a generator.    
-00017400: 2020 2020 3a74 7970 6520 6173 796e 6368      :type asynch
-00017410: 726f 6e6f 7573 3a20 626f 6f6c 2c20 6f70  ronous: bool, op
-00017420: 7469 6f6e 616c 0a0a 2020 2020 2020 2020  tional..        
-00017430: 3a70 6172 616d 2067 6574 5f61 6c6c 3a20  :param get_all: 
-00017440: 6966 2060 5472 7565 602c 2069 7420 7769  if `True`, it wi
-00017450: 6c6c 2067 6574 2061 6c6c 2065 6e74 7269  ll get all entri
-00017460: 6573 2069 6e20 276c 696d 6974 6564 2720  es in 'limited' 
-00017470: 6368 756e 6b73 0a20 2020 2020 2020 203a  chunks.        :
-00017480: 7479 7065 2067 6574 5f61 6c6c 3a20 626f  type get_all: bo
-00017490: 6f6c 2c20 6f70 7469 6f6e 616c 0a0a 2020  ol, optional..  
-000174a0: 2020 2020 2020 3a70 6172 616d 2073 7065        :param spe
-000174b0: 635f 7374 6174 653a 2073 6f66 7477 6172  c_state: softwar
-000174c0: 6520 7370 6563 6966 6963 6174 696f 6e20  e specification 
-000174d0: 7374 6174 652c 2063 616e 2062 6520 7573  state, can be us
-000174e0: 6564 206f 6e6c 7920 7768 656e 2060 6d6f  ed only when `mo
-000174f0: 6465 6c5f 6964 6020 6973 204e 6f6e 650a  del_id` is None.
-00017500: 2020 2020 2020 2020 3a74 7970 6520 7370          :type sp
-00017510: 6563 5f73 7461 7465 3a20 5370 6563 5374  ec_state: SpecSt
-00017520: 6174 6573 2c20 6f70 7469 6f6e 616c 0a0a  ates, optional..
-00017530: 2020 2020 2020 2020 3a72 6574 7572 6e3a          :return:
-00017540: 2073 746f 7265 6420 6d6f 6465 6c28 7329   stored model(s)
-00017550: 206d 6574 6164 6174 610a 2020 2020 2020   metadata.      
-00017560: 2020 3a72 7479 7065 3a20 6469 6374 2028    :rtype: dict (
-00017570: 6966 2049 4420 6973 206e 6f74 204e 6f6e  if ID is not Non
-00017580: 6529 206f 7220 7b22 7265 736f 7572 6365  e) or {"resource
-00017590: 7322 3a20 5b64 6963 745d 7d20 2869 6620  s": [dict]} (if 
-000175a0: 4944 2069 7320 4e6f 6e65 290a 0a20 2020  ID is None)..   
-000175b0: 2020 2020 202e 2e20 6e6f 7465 3a3a 0a20       .. note::. 
-000175c0: 2020 2020 2020 2020 2020 2049 6e20 6375             In cu
-000175d0: 7272 656e 7420 696d 706c 656d 656e 7461  rrent implementa
-000175e0: 7469 6f6e 2073 6574 7469 6e67 2060 7370  tion setting `sp
-000175f0: 6563 5f73 7461 7465 3d54 7275 6560 206d  ec_state=True` m
-00017600: 6179 2062 7265 616b 2073 6574 2060 6c69  ay break set `li
-00017610: 6d69 7460 2c0a 2020 2020 2020 2020 2020  mit`,.          
-00017620: 2020 7265 7475 726e 696e 6720 6c65 7373    returning less
-00017630: 2072 6563 6f72 6473 2074 6861 6e20 7374   records than st
-00017640: 6174 6564 2062 7920 7365 7420 606c 696d  ated by set `lim
-00017650: 6974 602e 0a0a 2020 2020 2020 2020 2a2a  it`...        **
-00017660: 4578 616d 706c 652a 2a0a 0a20 2020 2020  Example**..     
-00017670: 2020 202e 2e20 636f 6465 2d62 6c6f 636b     .. code-block
-00017680: 3a3a 2070 7974 686f 6e0a 0a20 2020 2020  :: python..     
-00017690: 2020 2020 2020 206d 6f64 656c 5f64 6574         model_det
-000176a0: 6169 6c73 203d 2063 6c69 656e 742e 6d6f  ails = client.mo
-000176b0: 6465 6c73 2e67 6574 5f64 6574 6169 6c73  dels.get_details
-000176c0: 286d 6f64 656c 5f69 6429 0a20 2020 2020  (model_id).     
-000176d0: 2020 2020 2020 206d 6f64 656c 735f 6465         models_de
-000176e0: 7461 696c 7320 3d20 636c 6965 6e74 2e6d  tails = client.m
-000176f0: 6f64 656c 732e 6765 745f 6465 7461 696c  odels.get_detail
-00017700: 7328 290a 2020 2020 2020 2020 2020 2020  s().            
-00017710: 6d6f 6465 6c73 5f64 6574 6169 6c73 203d  models_details =
-00017720: 2063 6c69 656e 742e 6d6f 6465 6c73 2e67   client.models.g
-00017730: 6574 5f64 6574 6169 6c73 286c 696d 6974  et_details(limit
-00017740: 3d31 3030 290a 2020 2020 2020 2020 2020  =100).          
-00017750: 2020 6d6f 6465 6c73 5f64 6574 6169 6c73    models_details
-00017760: 203d 2063 6c69 656e 742e 6d6f 6465 6c73   = client.models
-00017770: 2e67 6574 5f64 6574 6169 6c73 286c 696d  .get_details(lim
-00017780: 6974 3d31 3030 2c20 6765 745f 616c 6c3d  it=100, get_all=
-00017790: 5472 7565 290a 2020 2020 2020 2020 2020  True).          
-000177a0: 2020 6d6f 6465 6c73 5f64 6574 6169 6c73    models_details
-000177b0: 203d 205b 5d0a 2020 2020 2020 2020 2020   = [].          
-000177c0: 2020 666f 7220 656e 7472 7920 696e 2063    for entry in c
-000177d0: 6c69 656e 742e 6d6f 6465 6c73 2e67 6574  lient.models.get
-000177e0: 5f64 6574 6169 6c73 286c 696d 6974 3d31  _details(limit=1
-000177f0: 3030 2c20 6173 796e 6368 726f 6e6f 7573  00, asynchronous
-00017800: 3d54 7275 652c 2067 6574 5f61 6c6c 3d54  =True, get_all=T
-00017810: 7275 6529 3a0a 2020 2020 2020 2020 2020  rue):.          
-00017820: 2020 2020 2020 6d6f 6465 6c73 5f64 6574        models_det
-00017830: 6169 6c73 2e65 7874 656e 6428 656e 7472  ails.extend(entr
-00017840: 7929 0a20 2020 2020 2020 2022 2222 0a20  y).        """. 
-00017850: 2020 2020 2020 206d 6f64 656c 5f69 6420         model_id 
-00017860: 3d20 5f67 6574 5f69 645f 6672 6f6d 5f64  = _get_id_from_d
-00017870: 6570 7265 6361 7465 645f 7569 6428 6b77  eprecated_uid(kw
-00017880: 6172 6773 2c20 6d6f 6465 6c5f 6964 2c20  args, model_id, 
-00017890: 226d 6f64 656c 222c 2054 7275 6529 0a20  "model", True). 
-000178a0: 2020 2020 2020 2069 6620 6c69 6d69 7420         if limit 
-000178b0: 616e 6420 7370 6563 5f73 7461 7465 3a0a  and spec_state:.
-000178c0: 2020 2020 2020 2020 2020 2020 7761 726e              warn
-000178d0: 696e 6773 2e77 6172 6e28 0a20 2020 2020  ings.warn(.     
-000178e0: 2020 2020 2020 2020 2020 2028 0a20 2020             (.   
-000178f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00017900: 2022 5761 726e 696e 673a 2049 6e20 6375   "Warning: In cu
-00017910: 7272 656e 7420 696d 706c 656d 656e 7461  rrent implementa
-00017920: 7469 6f6e 2073 6574 7469 6e67 2060 7370  tion setting `sp
-00017930: 6563 5f73 7461 7465 3d54 7275 6560 206d  ec_state=True` m
-00017940: 6179 2062 7265 616b 2073 6574 2060 6c69  ay break set `li
-00017950: 6d69 7460 2c20 220a 2020 2020 2020 2020  mit`, ".        
-00017960: 2020 2020 2020 2020 2020 2020 2272 6574              "ret
-00017970: 7572 6e69 6e67 206c 6573 7320 7265 636f  urning less reco
-00017980: 7264 7320 7468 616e 2073 7461 7465 6420  rds than stated 
-00017990: 6279 2073 6574 2060 6c69 6d69 7460 2e22  by set `limit`."
-000179a0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-000179b0: 2029 0a20 2020 2020 2020 2020 2020 2029   ).            )
-000179c0: 0a0a 2020 2020 2020 2020 2323 466f 7220  ..        ##For 
-000179d0: 4350 3444 2c20 6368 6563 6b20 6966 2065  CP4D, check if e
-000179e0: 6974 6865 7220 7370 6163 6520 6f72 2070  ither space or p
-000179f0: 726f 6a65 6374 2049 4420 6973 2073 6574  roject ID is set
-00017a00: 0a20 2020 2020 2020 2073 656c 662e 5f63  .        self._c
-00017a10: 6c69 656e 742e 5f63 6865 636b 5f69 665f  lient._check_if_
-00017a20: 6569 7468 6572 5f69 735f 7365 7428 290a  either_is_set().
-00017a30: 2020 2020 2020 2020 4d6f 6465 6c73 2e5f          Models._
-00017a40: 7661 6c69 6461 7465 5f74 7970 6528 6d6f  validate_type(mo
-00017a50: 6465 6c5f 6964 2c20 226d 6f64 656c 5f69  del_id, "model_i
-00017a60: 6422 2c20 7374 722c 2046 616c 7365 290a  d", str, False).
-00017a70: 2020 2020 2020 2020 4d6f 6465 6c73 2e5f          Models._
-00017a80: 7661 6c69 6461 7465 5f74 7970 6528 6c69  validate_type(li
-00017a90: 6d69 742c 2022 6c69 6d69 7422 2c20 696e  mit, "limit", in
-00017aa0: 742c 2046 616c 7365 290a 0a20 2020 2020  t, False)..     
-00017ab0: 2020 2075 726c 203d 2028 0a20 2020 2020     url = (.     
-00017ac0: 2020 2020 2020 2073 656c 662e 5f63 6c69         self._cli
-00017ad0: 656e 742e 7365 7276 6963 655f 696e 7374  ent.service_inst
-00017ae0: 616e 6365 2e5f 6872 6566 5f64 6566 696e  ance._href_defin
-00017af0: 6974 696f 6e73 2e67 6574 5f70 7562 6c69  itions.get_publi
-00017b00: 7368 6564 5f6d 6f64 656c 735f 6872 6566  shed_models_href
-00017b10: 2829 0a20 2020 2020 2020 2029 0a0a 2020  ().        )..  
-00017b20: 2020 2020 2020 6966 206d 6f64 656c 5f69        if model_i
-00017b30: 6420 6973 204e 6f6e 653a 0a20 2020 2020  d is None:.     
-00017b40: 2020 2020 2020 2066 696c 7465 725f 6675         filter_fu
-00017b50: 6e63 203d 2028 0a20 2020 2020 2020 2020  nc = (.         
-00017b60: 2020 2020 2020 2073 656c 662e 5f67 6574         self._get
-00017b70: 5f66 696c 7465 725f 6675 6e63 5f62 795f  _filter_func_by_
-00017b80: 7370 6563 5f69 6473 280a 2020 2020 2020  spec_ids(.      
-00017b90: 2020 2020 2020 2020 2020 2020 2020 7365                se
-00017ba0: 6c66 2e5f 6765 745f 616e 645f 6361 6368  lf._get_and_cach
-00017bb0: 655f 7370 6563 5f69 6473 5f66 6f72 5f73  e_spec_ids_for_s
-00017bc0: 7461 7465 2873 7065 635f 7374 6174 6529  tate(spec_state)
-00017bd0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00017be0: 2029 0a20 2020 2020 2020 2020 2020 2020   ).             
-00017bf0: 2020 2069 6620 7370 6563 5f73 7461 7465     if spec_state
-00017c00: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00017c10: 2065 6c73 6520 4e6f 6e65 0a20 2020 2020   else None.     
-00017c20: 2020 2020 2020 2029 0a0a 2020 2020 2020         )..      
-00017c30: 2020 2020 2020 7265 7475 726e 2073 656c        return sel
-00017c40: 662e 5f67 6574 5f61 7274 6966 6163 745f  f._get_artifact_
-00017c50: 6465 7461 696c 7328 0a20 2020 2020 2020  details(.       
-00017c60: 2020 2020 2020 2020 2075 726c 2c0a 2020           url,.  
-00017c70: 2020 2020 2020 2020 2020 2020 2020 6d6f                mo
-00017c80: 6465 6c5f 6964 2c0a 2020 2020 2020 2020  del_id,.        
-00017c90: 2020 2020 2020 2020 6c69 6d69 742c 0a20          limit,. 
-00017ca0: 2020 2020 2020 2020 2020 2020 2020 2022                 "
-00017cb0: 6d6f 6465 6c73 222c 0a20 2020 2020 2020  models",.       
-00017cc0: 2020 2020 2020 2020 205f 6173 796e 633d           _async=
-00017cd0: 6173 796e 6368 726f 6e6f 7573 2c0a 2020  asynchronous,.  
-00017ce0: 2020 2020 2020 2020 2020 2020 2020 5f61                _a
-00017cf0: 6c6c 3d67 6574 5f61 6c6c 2c0a 2020 2020  ll=get_all,.    
-00017d00: 2020 2020 2020 2020 2020 2020 5f66 696c              _fil
-00017d10: 7465 725f 6675 6e63 3d66 696c 7465 725f  ter_func=filter_
-00017d20: 6675 6e63 2c0a 2020 2020 2020 2020 2020  func,.          
-00017d30: 2020 290a 0a20 2020 2020 2020 2065 6c73    )..        els
-00017d40: 653a 0a20 2020 2020 2020 2020 2020 2072  e:.            r
-00017d50: 6574 7572 6e20 7365 6c66 2e5f 6765 745f  eturn self._get_
-00017d60: 6172 7469 6661 6374 5f64 6574 6169 6c73  artifact_details
-00017d70: 2875 726c 2c20 6d6f 6465 6c5f 6964 2c20  (url, model_id, 
-00017d80: 6c69 6d69 742c 2022 6d6f 6465 6c73 2229  limit, "models")
-00017d90: 0a0a 2020 2020 4073 7461 7469 636d 6574  ..    @staticmet
-00017da0: 686f 640a 2020 2020 6465 6620 6765 745f  hod.    def get_
-00017db0: 6872 6566 286d 6f64 656c 5f64 6574 6169  href(model_detai
-00017dc0: 6c73 3a20 6469 6374 5b73 7472 2c20 416e  ls: dict[str, An
-00017dd0: 795d 2920 2d3e 2073 7472 3a0a 2020 2020  y]) -> str:.    
-00017de0: 2020 2020 2222 2247 6574 2075 726c 206f      """Get url o
-00017df0: 6620 7374 6f72 6564 206d 6f64 656c 2e0a  f stored model..
-00017e00: 0a20 2020 2020 2020 203a 7061 7261 6d20  .        :param 
-00017e10: 6d6f 6465 6c5f 6465 7461 696c 733a 2073  model_details: s
-00017e20: 746f 7265 6420 6d6f 6465 6c20 6465 7461  tored model deta
-00017e30: 696c 730a 2020 2020 2020 2020 3a74 7970  ils.        :typ
-00017e40: 6520 6d6f 6465 6c5f 6465 7461 696c 733a  e model_details:
-00017e50: 2064 6963 740a 0a20 2020 2020 2020 203a   dict..        :
-00017e60: 7265 7475 726e 3a20 7572 6c20 746f 2073  return: url to s
-00017e70: 746f 7265 6420 6d6f 6465 6c0a 2020 2020  tored model.    
-00017e80: 2020 2020 3a72 7479 7065 3a20 7374 720a      :rtype: str.
-00017e90: 0a20 2020 2020 2020 202a 2a45 7861 6d70  .        **Examp
-00017ea0: 6c65 2a2a 0a0a 2020 2020 2020 2020 2e2e  le**..        ..
-00017eb0: 2063 6f64 652d 626c 6f63 6b3a 3a20 7079   code-block:: py
-00017ec0: 7468 6f6e 0a0a 2020 2020 2020 2020 2020  thon..          
-00017ed0: 2020 6d6f 6465 6c5f 7572 6c20 3d20 636c    model_url = cl
-00017ee0: 6965 6e74 2e6d 6f64 656c 732e 6765 745f  ient.models.get_
-00017ef0: 6872 6566 286d 6f64 656c 5f64 6574 6169  href(model_detai
-00017f00: 6c73 290a 2020 2020 2020 2020 2222 220a  ls).        """.
-00017f10: 0a20 2020 2020 2020 204d 6f64 656c 732e  .        Models.
-00017f20: 5f76 616c 6964 6174 655f 7479 7065 286d  _validate_type(m
-00017f30: 6f64 656c 5f64 6574 6169 6c73 2c20 226d  odel_details, "m
-00017f40: 6f64 656c 5f64 6574 6169 6c73 222c 206f  odel_details", o
-00017f50: 626a 6563 742c 2054 7275 6529 0a0a 2020  bject, True)..  
-00017f60: 2020 2020 2020 6966 2022 6173 7365 745f        if "asset_
-00017f70: 6964 2220 696e 206d 6f64 656c 5f64 6574  id" in model_det
-00017f80: 6169 6c73 5b22 6d65 7461 6461 7461 225d  ails["metadata"]
-00017f90: 3a0a 2020 2020 2020 2020 2020 2020 7265  :.            re
-00017fa0: 7475 726e 2057 4d4c 5265 736f 7572 6365  turn WMLResource
-00017fb0: 2e5f 6765 745f 7265 7175 6972 6564 5f65  ._get_required_e
-00017fc0: 6c65 6d65 6e74 5f66 726f 6d5f 6469 6374  lement_from_dict
-00017fd0: 280a 2020 2020 2020 2020 2020 2020 2020  (.              
-00017fe0: 2020 6d6f 6465 6c5f 6465 7461 696c 732c    model_details,
-00017ff0: 2022 6d6f 6465 6c5f 6465 7461 696c 7322   "model_details"
-00018000: 2c20 5b22 6d65 7461 6461 7461 222c 2022  , ["metadata", "
-00018010: 6872 6566 225d 0a20 2020 2020 2020 2020  href"].         
-00018020: 2020 2029 0a20 2020 2020 2020 2065 6c73     ).        els
-00018030: 653a 0a20 2020 2020 2020 2020 2020 2069  e:.            i
-00018040: 6620 2269 6422 206e 6f74 2069 6e20 6d6f  f "id" not in mo
-00018050: 6465 6c5f 6465 7461 696c 735b 226d 6574  del_details["met
-00018060: 6164 6174 6122 5d3a 0a20 2020 2020 2020  adata"]:.       
-00018070: 2020 2020 2020 2020 204d 6f64 656c 732e           Models.
-00018080: 5f76 616c 6964 6174 655f 7479 7065 5f6f  _validate_type_o
-00018090: 665f 6465 7461 696c 7328 6d6f 6465 6c5f  f_details(model_
-000180a0: 6465 7461 696c 732c 204d 4f44 454c 5f44  details, MODEL_D
-000180b0: 4554 4149 4c53 5f54 5950 4529 0a20 2020  ETAILS_TYPE).   
-000180c0: 2020 2020 2020 2020 2020 2020 2072 6574               ret
-000180d0: 7572 6e20 574d 4c52 6573 6f75 7263 652e  urn WMLResource.
-000180e0: 5f67 6574 5f72 6571 7569 7265 645f 656c  _get_required_el
-000180f0: 656d 656e 745f 6672 6f6d 5f64 6963 7428  ement_from_dict(
-00018100: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00018110: 2020 2020 206d 6f64 656c 5f64 6574 6169       model_detai
-00018120: 6c73 2c20 226d 6f64 656c 5f64 6574 6169  ls, "model_detai
-00018130: 6c73 222c 205b 226d 6574 6164 6174 6122  ls", ["metadata"
-00018140: 2c20 2268 7265 6622 5d0a 2020 2020 2020  , "href"].      
-00018150: 2020 2020 2020 2020 2020 290a 2020 2020            ).    
-00018160: 2020 2020 2020 2020 656c 7365 3a0a 2020          else:.  
-00018170: 2020 2020 2020 2020 2020 2020 2020 6d6f                mo
-00018180: 6465 6c5f 6964 203d 2057 4d4c 5265 736f  del_id = WMLReso
-00018190: 7572 6365 2e5f 6765 745f 7265 7175 6972  urce._get_requir
-000181a0: 6564 5f65 6c65 6d65 6e74 5f66 726f 6d5f  ed_element_from_
-000181b0: 6469 6374 280a 2020 2020 2020 2020 2020  dict(.          
-000181c0: 2020 2020 2020 2020 2020 6d6f 6465 6c5f            model_
-000181d0: 6465 7461 696c 732c 2022 6d6f 6465 6c5f  details, "model_
-000181e0: 6465 7461 696c 7322 2c20 5b22 6d65 7461  details", ["meta
-000181f0: 6461 7461 222c 2022 6964 225d 0a20 2020  data", "id"].   
-00018200: 2020 2020 2020 2020 2020 2020 2029 0a20               ). 
-00018210: 2020 2020 2020 2020 2020 2020 2020 2072                 r
-00018220: 6574 7572 6e20 222f 6d6c 2f76 342f 6d6f  eturn "/ml/v4/mo
-00018230: 6465 6c73 2f22 202b 206d 6f64 656c 5f69  dels/" + model_i
-00018240: 640a 0a20 2020 2040 7374 6174 6963 6d65  d..    @staticme
-00018250: 7468 6f64 0a20 2020 2064 6566 2067 6574  thod.    def get
-00018260: 5f75 6964 286d 6f64 656c 5f64 6574 6169  _uid(model_detai
-00018270: 6c73 3a20 6469 6374 5b73 7472 2c20 416e  ls: dict[str, An
-00018280: 795d 2920 2d3e 2073 7472 3a0a 2020 2020  y]) -> str:.    
-00018290: 2020 2020 2222 2247 6574 2075 6964 206f      """Get uid o
-000182a0: 6620 7374 6f72 6564 206d 6f64 656c 2e0a  f stored model..
-000182b0: 0a20 2020 2020 2020 202a 4465 7072 6563  .        *Deprec
-000182c0: 6174 6564 3a2a 2055 7365 2060 6067 6574  ated:* Use ``get
-000182d0: 5f69 6428 6d6f 6465 6c5f 6465 7461 696c  _id(model_detail
-000182e0: 7329 6060 2069 6e73 7465 6164 2e0a 0a20  s)`` instead... 
-000182f0: 2020 2020 2020 203a 7061 7261 6d20 6d6f         :param mo
-00018300: 6465 6c5f 6465 7461 696c 733a 2073 746f  del_details: sto
-00018310: 7265 6420 6d6f 6465 6c20 6465 7461 696c  red model detail
-00018320: 730a 2020 2020 2020 2020 3a74 7970 6520  s.        :type 
-00018330: 6d6f 6465 6c5f 6465 7461 696c 733a 2064  model_details: d
-00018340: 6963 740a 0a20 2020 2020 2020 203a 7265  ict..        :re
-00018350: 7475 726e 3a20 7569 6420 6f66 2073 746f  turn: uid of sto
-00018360: 7265 6420 6d6f 6465 6c0a 2020 2020 2020  red model.      
-00018370: 2020 3a72 7479 7065 3a20 7374 720a 0a20    :rtype: str.. 
-00018380: 2020 2020 2020 202a 2a45 7861 6d70 6c65         **Example
-00018390: 2a2a 0a0a 2020 2020 2020 2020 2e2e 2063  **..        .. c
-000183a0: 6f64 652d 626c 6f63 6b3a 3a20 7079 7468  ode-block:: pyth
-000183b0: 6f6e 0a0a 2020 2020 2020 2020 2020 2020  on..            
-000183c0: 6d6f 6465 6c5f 7569 6420 3d20 636c 6965  model_uid = clie
-000183d0: 6e74 2e6d 6f64 656c 732e 6765 745f 7569  nt.models.get_ui
-000183e0: 6428 6d6f 6465 6c5f 6465 7461 696c 7329  d(model_details)
-000183f0: 0a20 2020 2020 2020 2022 2222 0a20 2020  .        """.   
-00018400: 2020 2020 2077 6172 6e69 6e67 732e 7761       warnings.wa
-00018410: 726e 280a 2020 2020 2020 2020 2020 2020  rn(.            
-00018420: 2254 6869 7320 6d65 7468 6f64 2069 7320  "This method is 
-00018430: 6465 7072 6563 6174 6564 2c20 706c 6561  deprecated, plea
-00018440: 7365 2075 7365 204d 6f64 656c 732e 6765  se use Models.ge
-00018450: 745f 6964 286d 6f64 656c 5f64 6574 6169  t_id(model_detai
-00018460: 6c73 2920 696e 7374 6561 6422 0a20 2020  ls) instead".   
-00018470: 2020 2020 2029 0a20 2020 2020 2020 2072       ).        r
-00018480: 6574 7572 6e20 4d6f 6465 6c73 2e67 6574  eturn Models.get
-00018490: 5f69 6428 6d6f 6465 6c5f 6465 7461 696c  _id(model_detail
-000184a0: 7329 0a0a 2020 2020 4073 7461 7469 636d  s)..    @staticm
-000184b0: 6574 686f 640a 2020 2020 6465 6620 6765  ethod.    def ge
-000184c0: 745f 6964 286d 6f64 656c 5f64 6574 6169  t_id(model_detai
-000184d0: 6c73 3a20 6469 6374 5b73 7472 2c20 416e  ls: dict[str, An
-000184e0: 795d 2920 2d3e 2073 7472 3a0a 2020 2020  y]) -> str:.    
-000184f0: 2020 2020 2222 2247 6574 2069 6420 6f66      """Get id of
-00018500: 2073 746f 7265 6420 6d6f 6465 6c2e 0a0a   stored model...
-00018510: 2020 2020 2020 2020 3a70 6172 616d 206d          :param m
-00018520: 6f64 656c 5f64 6574 6169 6c73 3a20 7374  odel_details: st
-00018530: 6f72 6564 206d 6f64 656c 2064 6574 6169  ored model detai
-00018540: 6c73 0a20 2020 2020 2020 203a 7479 7065  ls.        :type
-00018550: 206d 6f64 656c 5f64 6574 6169 6c73 3a20   model_details: 
-00018560: 6469 6374 0a0a 2020 2020 2020 2020 3a72  dict..        :r
-00018570: 6574 7572 6e3a 2069 6420 6f66 2073 746f  eturn: id of sto
-00018580: 7265 6420 6d6f 6465 6c0a 2020 2020 2020  red model.      
-00018590: 2020 3a72 7479 7065 3a20 7374 720a 0a20    :rtype: str.. 
-000185a0: 2020 2020 2020 202a 2a45 7861 6d70 6c65         **Example
-000185b0: 2a2a 0a0a 2020 2020 2020 2020 2e2e 2063  **..        .. c
-000185c0: 6f64 652d 626c 6f63 6b3a 3a20 7079 7468  ode-block:: pyth
-000185d0: 6f6e 0a0a 2020 2020 2020 2020 2020 2020  on..            
-000185e0: 6d6f 6465 6c5f 6964 203d 2063 6c69 656e  model_id = clien
-000185f0: 742e 6d6f 6465 6c73 2e67 6574 5f69 6428  t.models.get_id(
-00018600: 6d6f 6465 6c5f 6465 7461 696c 7329 0a20  model_details). 
-00018610: 2020 2020 2020 2022 2222 0a20 2020 2020         """.     
-00018620: 2020 204d 6f64 656c 732e 5f76 616c 6964     Models._valid
-00018630: 6174 655f 7479 7065 286d 6f64 656c 5f64  ate_type(model_d
-00018640: 6574 6169 6c73 2c20 226d 6f64 656c 5f64  etails, "model_d
-00018650: 6574 6169 6c73 222c 206f 626a 6563 742c  etails", object,
-00018660: 2054 7275 6529 0a0a 2020 2020 2020 2020   True)..        
-00018670: 6966 2022 6173 7365 745f 6964 2220 696e  if "asset_id" in
-00018680: 206d 6f64 656c 5f64 6574 6169 6c73 5b22   model_details["
-00018690: 6d65 7461 6461 7461 225d 3a0a 2020 2020  metadata"]:.    
-000186a0: 2020 2020 2020 2020 7265 7475 726e 2057          return W
-000186b0: 4d4c 5265 736f 7572 6365 2e5f 6765 745f  MLResource._get_
-000186c0: 7265 7175 6972 6564 5f65 6c65 6d65 6e74  required_element
-000186d0: 5f66 726f 6d5f 6469 6374 280a 2020 2020  _from_dict(.    
-000186e0: 2020 2020 2020 2020 2020 2020 6d6f 6465              mode
-000186f0: 6c5f 6465 7461 696c 732c 2022 6d6f 6465  l_details, "mode
-00018700: 6c5f 6465 7461 696c 7322 2c20 5b22 6d65  l_details", ["me
-00018710: 7461 6461 7461 222c 2022 6173 7365 745f  tadata", "asset_
-00018720: 6964 225d 0a20 2020 2020 2020 2020 2020  id"].           
-00018730: 2029 0a20 2020 2020 2020 2065 6c73 653a   ).        else:
-00018740: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if 
-00018750: 2269 6422 206e 6f74 2069 6e20 6d6f 6465  "id" not in mode
-00018760: 6c5f 6465 7461 696c 735b 226d 6574 6164  l_details["metad
-00018770: 6174 6122 5d3a 0a20 2020 2020 2020 2020  ata"]:.         
-00018780: 2020 2020 2020 204d 6f64 656c 732e 5f76         Models._v
-00018790: 616c 6964 6174 655f 7479 7065 5f6f 665f  alidate_type_of_
-000187a0: 6465 7461 696c 7328 6d6f 6465 6c5f 6465  details(model_de
-000187b0: 7461 696c 732c 204d 4f44 454c 5f44 4554  tails, MODEL_DET
-000187c0: 4149 4c53 5f54 5950 4529 0a20 2020 2020  AILS_TYPE).     
-000187d0: 2020 2020 2020 2020 2020 2072 6574 7572             retur
-000187e0: 6e20 574d 4c52 6573 6f75 7263 652e 5f67  n WMLResource._g
-000187f0: 6574 5f72 6571 7569 7265 645f 656c 656d  et_required_elem
-00018800: 656e 745f 6672 6f6d 5f64 6963 7428 0a20  ent_from_dict(. 
-00018810: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00018820: 2020 206d 6f64 656c 5f64 6574 6169 6c73     model_details
-00018830: 2c20 226d 6f64 656c 5f64 6574 6169 6c73  , "model_details
-00018840: 222c 205b 226d 6574 6164 6174 6122 2c20  ", ["metadata", 
-00018850: 2267 7569 6422 5d0a 2020 2020 2020 2020  "guid"].        
-00018860: 2020 2020 2020 2020 290a 2020 2020 2020          ).      
-00018870: 2020 2020 2020 656c 7365 3a0a 2020 2020        else:.    
-00018880: 2020 2020 2020 2020 2020 2020 7265 7475              retu
-00018890: 726e 2057 4d4c 5265 736f 7572 6365 2e5f  rn WMLResource._
-000188a0: 6765 745f 7265 7175 6972 6564 5f65 6c65  get_required_ele
-000188b0: 6d65 6e74 5f66 726f 6d5f 6469 6374 280a  ment_from_dict(.
-000188c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000188d0: 2020 2020 6d6f 6465 6c5f 6465 7461 696c      model_detail
-000188e0: 732c 2022 6d6f 6465 6c5f 6465 7461 696c  s, "model_detail
-000188f0: 7322 2c20 5b22 6d65 7461 6461 7461 222c  s", ["metadata",
-00018900: 2022 6964 225d 0a20 2020 2020 2020 2020   "id"].         
-00018910: 2020 2020 2020 2029 0a0a 2020 2020 6465         )..    de
-00018920: 6620 6c69 7374 280a 2020 2020 2020 2020  f list(.        
-00018930: 7365 6c66 2c0a 2020 2020 2020 2020 6c69  self,.        li
-00018940: 6d69 743a 2069 6e74 207c 204e 6f6e 6520  mit: int | None 
-00018950: 3d20 4e6f 6e65 2c0a 2020 2020 2020 2020  = None,.        
-00018960: 6173 796e 6368 726f 6e6f 7573 3a20 626f  asynchronous: bo
-00018970: 6f6c 203d 2046 616c 7365 2c0a 2020 2020  ol = False,.    
-00018980: 2020 2020 6765 745f 616c 6c3a 2062 6f6f      get_all: boo
-00018990: 6c20 3d20 4661 6c73 652c 0a20 2020 2029  l = False,.    )
-000189a0: 202d 3e20 7061 6e64 6173 2e44 6174 6146   -> pandas.DataF
-000189b0: 7261 6d65 207c 2047 656e 6572 6174 6f72  rame | Generator
-000189c0: 3a0a 2020 2020 2020 2020 2222 224c 6973  :.        """Lis
-000189d0: 7473 2073 746f 7265 6420 6d6f 6465 6c73  ts stored models
-000189e0: 2069 6e20 6120 7461 626c 6520 666f 726d   in a table form
-000189f0: 6174 2e20 4966 206c 696d 6974 2069 7320  at. If limit is 
-00018a00: 7365 7420 746f 204e 6f6e 6520 7468 6572  set to None ther
-00018a10: 6520 7769 6c6c 2062 6520 6f6e 6c79 2066  e will be only f
-00018a20: 6972 7374 2035 3020 7265 636f 7264 7320  irst 50 records 
-00018a30: 7368 6f77 6e2e 0a0a 2020 2020 2020 2020  shown...        
-00018a40: 3a70 6172 616d 206c 696d 6974 3a20 6c69  :param limit: li
-00018a50: 6d69 7420 6e75 6d62 6572 206f 6620 6665  mit number of fe
-00018a60: 7463 6865 6420 7265 636f 7264 730a 2020  tched records.  
-00018a70: 2020 2020 2020 3a74 7970 6520 6c69 6d69        :type limi
-00018a80: 743a 2069 6e74 2c20 6f70 7469 6f6e 616c  t: int, optional
-00018a90: 0a0a 2020 2020 2020 2020 3a70 6172 616d  ..        :param
-00018aa0: 2061 7379 6e63 6872 6f6e 6f75 733a 2069   asynchronous: i
-00018ab0: 6620 6054 7275 6560 2c20 6974 2077 696c  f `True`, it wil
-00018ac0: 6c20 776f 726b 2061 7320 6120 6765 6e65  l work as a gene
-00018ad0: 7261 746f 720a 2020 2020 2020 2020 3a74  rator.        :t
-00018ae0: 7970 6520 6173 796e 6368 726f 6e6f 7573  ype asynchronous
-00018af0: 3a20 626f 6f6c 2c20 6f70 7469 6f6e 616c  : bool, optional
-00018b00: 0a0a 2020 2020 2020 2020 3a70 6172 616d  ..        :param
-00018b10: 2067 6574 5f61 6c6c 3a20 6966 2060 5472   get_all: if `Tr
-00018b20: 7565 602c 2069 7420 7769 6c6c 2067 6574  ue`, it will get
-00018b30: 2061 6c6c 2065 6e74 7269 6573 2069 6e20   all entries in 
-00018b40: 276c 696d 6974 6564 2720 6368 756e 6b73  'limited' chunks
-00018b50: 0a20 2020 2020 2020 203a 7479 7065 2067  .        :type g
-00018b60: 6574 5f61 6c6c 3a20 626f 6f6c 2c20 6f70  et_all: bool, op
-00018b70: 7469 6f6e 616c 0a0a 2020 2020 2020 2020  tional..        
-00018b80: 3a72 6574 7572 6e3a 2070 616e 6461 732e  :return: pandas.
-00018b90: 4461 7461 4672 616d 6520 7769 7468 206c  DataFrame with l
-00018ba0: 6973 7465 6420 6d6f 6465 6c73 206f 7220  isted models or 
-00018bb0: 6765 6e65 7261 746f 7220 6966 2060 6173  generator if `as
-00018bc0: 796e 6368 726f 6e6f 7573 6020 6973 2073  ynchronous` is s
-00018bd0: 6574 2074 6f20 5472 7565 0a20 2020 2020  et to True.     
-00018be0: 2020 203a 7274 7970 653a 2070 616e 6461     :rtype: panda
-00018bf0: 732e 4461 7461 4672 616d 6520 7c20 4765  s.DataFrame | Ge
-00018c00: 6e65 7261 746f 720a 0a20 2020 2020 2020  nerator..       
-00018c10: 202a 2a45 7861 6d70 6c65 2a2a 0a0a 2020   **Example**..  
-00018c20: 2020 2020 2020 2e2e 2063 6f64 652d 626c        .. code-bl
-00018c30: 6f63 6b3a 3a20 7079 7468 6f6e 0a0a 2020  ock:: python..  
-00018c40: 2020 2020 2020 2020 2020 636c 6965 6e74            client
-00018c50: 2e6d 6f64 656c 732e 6c69 7374 2829 0a20  .models.list(). 
-00018c60: 2020 2020 2020 2020 2020 2063 6c69 656e             clien
-00018c70: 742e 6d6f 6465 6c73 2e6c 6973 7428 6c69  t.models.list(li
-00018c80: 6d69 743d 3130 3029 0a20 2020 2020 2020  mit=100).       
-00018c90: 2020 2020 2063 6c69 656e 742e 6d6f 6465       client.mode
-00018ca0: 6c73 2e6c 6973 7428 6c69 6d69 743d 3130  ls.list(limit=10
-00018cb0: 302c 2067 6574 5f61 6c6c 3d54 7275 6529  0, get_all=True)
-00018cc0: 0a20 2020 2020 2020 2020 2020 205b 656e  .            [en
-00018cd0: 7472 7920 666f 7220 656e 7472 7920 696e  try for entry in
-00018ce0: 2063 6c69 656e 742e 6d6f 6465 6c73 2e6c   client.models.l
-00018cf0: 6973 7428 6c69 6d69 743d 3130 302c 2061  ist(limit=100, a
-00018d00: 7379 6e63 6872 6f6e 6f75 733d 5472 7565  synchronous=True
-00018d10: 2c20 6765 745f 616c 6c3d 5472 7565 295d  , get_all=True)]
-00018d20: 0a20 2020 2020 2020 2022 2222 0a0a 2020  .        """..  
-00018d30: 2020 2020 2020 2323 466f 7220 4350 3444        ##For CP4D
-00018d40: 2c20 6368 6563 6b20 6966 2065 6974 6865  , check if eithe
-00018d50: 7220 7370 6365 206f 7220 7072 6f6a 6563  r spce or projec
-00018d60: 7420 4944 2069 7320 7365 740a 2020 2020  t ID is set.    
-00018d70: 2020 2020 6465 6620 7072 6f63 6573 735f      def process_
-00018d80: 7265 736f 7572 6365 7328 7365 6c66 3a20  resources(self: 
-00018d90: 416e 792c 206d 6f64 656c 5f72 6573 6f75  Any, model_resou
-00018da0: 7263 6573 3a20 6469 6374 2920 2d3e 2070  rces: dict) -> p
-00018db0: 616e 6461 732e 4461 7461 4672 616d 653a  andas.DataFrame:
-00018dc0: 0a20 2020 2020 2020 2020 2020 206d 6f64  .            mod
-00018dd0: 656c 5f72 6573 6f75 7263 6573 203d 206d  el_resources = m
-00018de0: 6f64 656c 5f72 6573 6f75 7263 6573 5b22  odel_resources["
-00018df0: 7265 736f 7572 6365 7322 5d0a 2020 2020  resources"].    
-00018e00: 2020 2020 2020 2020 7377 5f73 7065 635f          sw_spec_
-00018e10: 696e 666f 203d 207b 0a20 2020 2020 2020  info = {.       
-00018e20: 2020 2020 2020 2020 2073 5b22 6964 225d           s["id"]
-00018e30: 3a20 730a 2020 2020 2020 2020 2020 2020  : s.            
-00018e40: 2020 2020 666f 7220 7320 696e 2073 656c      for s in sel
-00018e50: 662e 5f63 6c69 656e 742e 736f 6674 7761  f._client.softwa
-00018e60: 7265 5f73 7065 6369 6669 6361 7469 6f6e  re_specification
-00018e70: 732e 6765 745f 6465 7461 696c 7328 0a20  s.get_details(. 
-00018e80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00018e90: 2020 2073 7461 7465 5f69 6e66 6f3d 5472     state_info=Tr
-00018ea0: 7565 0a20 2020 2020 2020 2020 2020 2020  ue.             
-00018eb0: 2020 2029 5b22 7265 736f 7572 6365 7322     )["resources"
-00018ec0: 5d0a 2020 2020 2020 2020 2020 2020 7d0a  ].            }.
-00018ed0: 0a20 2020 2020 2020 2020 2020 2064 6566  .            def
-00018ee0: 2067 6574 5f73 7065 635f 696e 666f 2873   get_spec_info(s
-00018ef0: 7065 635f 6964 3a20 7374 722c 2070 726f  pec_id: str, pro
-00018f00: 703a 2073 7472 2920 2d3e 2073 7472 3a0a  p: str) -> str:.
+00014700: 2020 2020 2020 2020 290a 2020 2020 2020          ).      
+00014710: 2020 2020 2020 2020 2020 2020 2020 290a                ).
+00014720: 0a20 2020 2020 2020 2020 2020 2023 2077  .            # w
+00014730: 6974 6820 7661 6c69 6461 7469 6f6e 2073  ith validation s
+00014740: 686f 756c 6420 6265 2073 6f6d 6577 6865  hould be somewhe
+00014750: 7265 2065 6c73 652c 206f 6e20 7468 6520  re else, on the 
+00014760: 6265 6769 6e69 6e67 2c20 6275 7420 7768  begining, but wh
+00014770: 656e 2070 6174 6368 2077 696c 6c20 6265  en patch will be
+00014780: 2070 6f73 7369 626c 650a 2020 2020 2020   possible.      
+00014790: 2020 2020 2020 7061 7463 685f 7061 796c        patch_payl
+000147a0: 6f61 6420 3d20 7365 6c66 2e43 6f6e 6669  oad = self.Confi
+000147b0: 6775 7261 7469 6f6e 4d65 7461 4e61 6d65  gurationMetaName
+000147c0: 732e 5f67 656e 6572 6174 655f 7061 7463  s._generate_patc
+000147d0: 685f 7061 796c 6f61 6428 0a20 2020 2020  h_payload(.     
+000147e0: 2020 2020 2020 2020 2020 2064 6574 6169             detai
+000147f0: 6c73 5b22 656e 7469 7479 225d 2c20 6d65  ls["entity"], me
+00014800: 7461 5f70 726f 7073 2c20 7769 7468 5f76  ta_props, with_v
+00014810: 616c 6964 6174 696f 6e3d 5472 7565 0a20  alidation=True. 
+00014820: 2020 2020 2020 2020 2020 2029 0a20 2020             ).   
+00014830: 2020 2020 2020 2020 2072 6573 706f 6e73           respons
+00014840: 655f 7061 7463 6820 3d20 7265 7175 6573  e_patch = reques
+00014850: 7473 2e70 6174 6368 280a 2020 2020 2020  ts.patch(.      
+00014860: 2020 2020 2020 2020 2020 7572 6c2c 0a20            url,. 
+00014870: 2020 2020 2020 2020 2020 2020 2020 206a                 j
+00014880: 736f 6e3d 7061 7463 685f 7061 796c 6f61  son=patch_payloa
+00014890: 642c 0a20 2020 2020 2020 2020 2020 2020  d,.             
+000148a0: 2020 2070 6172 616d 733d 7365 6c66 2e5f     params=self._
+000148b0: 636c 6965 6e74 2e5f 7061 7261 6d73 2829  client._params()
+000148c0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+000148d0: 2020 6865 6164 6572 733d 7365 6c66 2e5f    headers=self._
+000148e0: 636c 6965 6e74 2e5f 6765 745f 6865 6164  client._get_head
+000148f0: 6572 7328 292c 0a20 2020 2020 2020 2020  ers(),.         
+00014900: 2020 2029 0a20 2020 2020 2020 2020 2020     ).           
+00014910: 2075 7064 6174 6564 5f64 6574 6169 6c73   updated_details
+00014920: 203d 2073 656c 662e 5f68 616e 646c 655f   = self._handle_
+00014930: 7265 7370 6f6e 7365 280a 2020 2020 2020  response(.      
+00014940: 2020 2020 2020 2020 2020 3230 302c 2022            200, "
+00014950: 6d6f 6465 6c20 7665 7273 696f 6e20 7061  model version pa
+00014960: 7463 6822 2c20 7265 7370 6f6e 7365 5f70  tch", response_p
+00014970: 6174 6368 0a20 2020 2020 2020 2020 2020  atch.           
+00014980: 2029 0a20 2020 2020 2020 2020 2020 2069   ).            i
+00014990: 6620 7570 6461 7465 5f6d 6f64 656c 2069  f update_model i
+000149a0: 7320 6e6f 7420 4e6f 6e65 3a0a 2020 2020  s not None:.    
+000149b0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+000149c0: 2e5f 7570 6461 7465 5f6d 6f64 656c 5f63  ._update_model_c
+000149d0: 6f6e 7465 6e74 286d 6f64 656c 5f69 642c  ontent(model_id,
+000149e0: 2064 6574 6169 6c73 2c20 7570 6461 7465   details, update
+000149f0: 5f6d 6f64 656c 290a 2020 2020 2020 2020  _model).        
+00014a00: 2020 2020 7265 7475 726e 2075 7064 6174      return updat
+00014a10: 6564 5f64 6574 6169 6c73 0a0a 2020 2020  ed_details..    
+00014a20: 2020 2020 7265 7475 726e 2073 656c 662e      return self.
+00014a30: 6765 745f 6465 7461 696c 7328 6d6f 6465  get_details(mode
+00014a40: 6c5f 6964 290a 0a20 2020 2064 6566 206c  l_id)..    def l
+00014a50: 6f61 6428 7365 6c66 2c20 6172 7469 6661  oad(self, artifa
+00014a60: 6374 5f69 643a 2073 7472 207c 204e 6f6e  ct_id: str | Non
+00014a70: 652c 202a 2a6b 7761 7267 733a 2041 6e79  e, **kwargs: Any
+00014a80: 2920 2d3e 2041 6e79 3a0a 2020 2020 2020  ) -> Any:.      
+00014a90: 2020 2222 224c 6f61 6420 6d6f 6465 6c20    """Load model 
+00014aa0: 6672 6f6d 2072 6570 6f73 6974 6f72 7920  from repository 
+00014ab0: 746f 206f 626a 6563 7420 696e 206c 6f63  to object in loc
+00014ac0: 616c 2065 6e76 6972 6f6e 6d65 6e74 2e0a  al environment..
+00014ad0: 0a20 2020 2020 2020 203a 7061 7261 6d20  .        :param 
+00014ae0: 6172 7469 6661 6374 5f69 643a 2073 746f  artifact_id: sto
+00014af0: 7265 6420 6d6f 6465 6c20 4944 0a20 2020  red model ID.   
+00014b00: 2020 2020 203a 7479 7065 2061 7274 6966       :type artif
+00014b10: 6163 745f 6964 3a20 7374 720a 0a20 2020  act_id: str..   
+00014b20: 2020 2020 203a 7265 7475 726e 3a20 7472       :return: tr
+00014b30: 6169 6e65 6420 6d6f 6465 6c0a 2020 2020  ained model.    
+00014b40: 2020 2020 3a72 7479 7065 3a20 6f62 6a65      :rtype: obje
+00014b50: 6374 0a0a 2020 2020 2020 2020 2a2a 4578  ct..        **Ex
+00014b60: 616d 706c 652a 2a0a 0a20 2020 2020 2020  ample**..       
+00014b70: 202e 2e20 636f 6465 2d62 6c6f 636b 3a3a   .. code-block::
+00014b80: 2070 7974 686f 6e0a 0a20 2020 2020 2020   python..       
+00014b90: 2020 2020 206d 6f64 656c 203d 2063 6c69       model = cli
+00014ba0: 656e 742e 6d6f 6465 6c73 2e6c 6f61 6428  ent.models.load(
+00014bb0: 6d6f 6465 6c5f 6964 290a 2020 2020 2020  model_id).      
+00014bc0: 2020 2222 220a 2020 2020 2020 2020 6172    """.        ar
+00014bd0: 7469 6661 6374 5f69 6420 3d20 5f67 6574  tifact_id = _get
+00014be0: 5f69 645f 6672 6f6d 5f64 6570 7265 6361  _id_from_depreca
+00014bf0: 7465 645f 7569 6428 6b77 6172 6773 2c20  ted_uid(kwargs, 
+00014c00: 6172 7469 6661 6374 5f69 642c 2022 6172  artifact_id, "ar
+00014c10: 7469 6661 6374 2229 0a20 2020 2020 2020  tifact").       
+00014c20: 2061 7274 6966 6163 745f 6964 203d 2063   artifact_id = c
+00014c30: 6173 7428 7374 722c 2061 7274 6966 6163  ast(str, artifac
+00014c40: 745f 6964 290a 2020 2020 2020 2020 4d6f  t_id).        Mo
+00014c50: 6465 6c73 2e5f 7661 6c69 6461 7465 5f74  dels._validate_t
+00014c60: 7970 6528 6172 7469 6661 6374 5f69 642c  ype(artifact_id,
+00014c70: 2022 6172 7469 6661 6374 5f69 6422 2c20   "artifact_id", 
+00014c80: 7374 722c 2046 616c 7365 290a 2020 2020  str, False).    
+00014c90: 2020 2020 2320 6368 6563 6b20 6966 2074      # check if t
+00014ca0: 6869 7320 6973 2074 656e 736f 7266 6c6f  his is tensorflo
+00014cb0: 7720 322e 7820 6d6f 6465 6c20 7479 7065  w 2.x model type
+00014cc0: 0a20 2020 2020 2020 206d 6f64 656c 5f64  .        model_d
+00014cd0: 6574 6169 6c73 203d 2073 656c 662e 6765  etails = self.ge
+00014ce0: 745f 6465 7461 696c 7328 6172 7469 6661  t_details(artifa
+00014cf0: 6374 5f69 6429 0a20 2020 2020 2020 2069  ct_id).        i
+00014d00: 6620 6d6f 6465 6c5f 6465 7461 696c 732e  f model_details.
+00014d10: 6765 7428 2265 6e74 6974 7922 2c20 7b7d  get("entity", {}
+00014d20: 292e 6765 7428 2274 7970 6522 2c20 2222  ).get("type", ""
+00014d30: 292e 7374 6172 7473 7769 7468 2822 7465  ).startswith("te
+00014d40: 6e73 6f72 666c 6f77 5f32 2e22 293a 0a20  nsorflow_2."):. 
+00014d50: 2020 2020 2020 2020 2020 2072 6574 7572             retur
+00014d60: 6e20 7365 6c66 2e5f 7466 3278 5f6c 6f61  n self._tf2x_loa
+00014d70: 645f 6d6f 6465 6c5f 696e 7374 616e 6365  d_model_instance
+00014d80: 2861 7274 6966 6163 745f 6964 290a 2020  (artifact_id).  
+00014d90: 2020 2020 2020 7472 793a 0a20 2020 2020        try:.     
+00014da0: 2020 2020 2020 2023 2043 6c6f 7564 2043         # Cloud C
+00014db0: 6f6e 7665 7267 656e 6365 3a20 4348 4b20  onvergence: CHK 
+00014dc0: 4946 2054 4849 5320 434f 4e44 4954 494f  IF THIS CONDITIO
+00014dd0: 4e20 4953 2043 4f52 5245 4354 2073 696e  N IS CORRECT sin
+00014de0: 6365 206c 6f61 6465 645f 6d6f 6465 6c0a  ce loaded_model.
+00014df0: 2020 2020 2020 2020 2020 2020 2320 6675              # fu
+00014e00: 6e63 7469 6f6e 616c 6974 7920 6265 6c6f  nctionality belo
+00014e10: 770a 2020 2020 2020 2020 2020 2020 6966  w.            if
+00014e20: 2028 0a20 2020 2020 2020 2020 2020 2020   (.             
+00014e30: 2020 2073 656c 662e 5f63 6c69 656e 742e     self._client.
+00014e40: 6465 6661 756c 745f 7370 6163 655f 6964  default_space_id
+00014e50: 2069 7320 4e6f 6e65 0a20 2020 2020 2020   is None.       
+00014e60: 2020 2020 2020 2020 2061 6e64 2073 656c           and sel
+00014e70: 662e 5f63 6c69 656e 742e 6465 6661 756c  f._client.defaul
+00014e80: 745f 7072 6f6a 6563 745f 6964 2069 7320  t_project_id is 
+00014e90: 4e6f 6e65 0a20 2020 2020 2020 2020 2020  None.           
+00014ea0: 2029 3a0a 2020 2020 2020 2020 2020 2020   ):.            
+00014eb0: 2020 2020 7261 6973 6520 574d 4c43 6c69      raise WMLCli
+00014ec0: 656e 7445 7272 6f72 280a 2020 2020 2020  entError(.      
+00014ed0: 2020 2020 2020 2020 2020 2020 2020 2249                "I
+00014ee0: 7420 6973 206d 616e 6461 746f 7279 2069  t is mandatory i
+00014ef0: 7320 7365 7420 7468 6520 7370 6163 6520  s set the space 
+00014f00: 6f72 2070 726f 6a65 6374 2e20 5c0a 2020  or project. \.  
+00014f10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00014f20: 2020 5573 6520 636c 6965 6e74 2e73 6574    Use client.set
+00014f30: 2e64 6566 6175 6c74 5f73 7061 6365 283c  .default_space(<
+00014f40: 5350 4143 455f 4944 3e29 2074 6f20 7365  SPACE_ID>) to se
+00014f50: 7420 7468 6520 7370 6163 6520 6f72 2063  t the space or c
+00014f60: 6c69 656e 742e 7365 742e 6465 6661 756c  lient.set.defaul
+00014f70: 745f 7072 6f6a 6563 7428 3c50 524f 4a45  t_project(<PROJE
+00014f80: 4354 5f49 443e 292e 220a 2020 2020 2020  CT_ID>).".      
+00014f90: 2020 2020 2020 2020 2020 290a 2020 2020            ).    
+00014fa0: 2020 2020 2020 2020 656c 7365 3a0a 2020          else:.  
+00014fb0: 2020 2020 2020 2020 2020 2020 2020 7175                qu
+00014fc0: 6572 795f 7061 7261 6d20 3d20 7365 6c66  ery_param = self
+00014fd0: 2e5f 636c 6965 6e74 2e5f 7061 7261 6d73  ._client._params
+00014fe0: 2829 0a20 2020 2020 2020 2020 2020 2020  ().             
+00014ff0: 2020 206c 6f61 6465 645f 6d6f 6465 6c20     loaded_model 
+00015000: 3d20 7365 6c66 2e5f 636c 6965 6e74 2e72  = self._client.r
+00015010: 6570 6f73 6974 6f72 792e 5f6d 6c5f 7265  epository._ml_re
+00015020: 706f 7369 746f 7279 5f63 6c69 656e 742e  pository_client.
+00015030: 6d6f 6465 6c73 2e5f 6765 745f 7634 5f63  models._get_v4_c
+00015040: 6c6f 7564 5f6d 6f64 656c 280a 2020 2020  loud_model(.    
+00015050: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00015060: 6172 7469 6661 6374 5f69 642c 2071 7565  artifact_id, que
+00015070: 7279 5f70 6172 616d 3d71 7565 7279 5f70  ry_param=query_p
+00015080: 6172 616d 0a20 2020 2020 2020 2020 2020  aram.           
+00015090: 2020 2020 2029 0a0a 2020 2020 2020 2020       )..        
+000150a0: 2020 2020 6c6f 6164 6564 5f6d 6f64 656c      loaded_model
+000150b0: 203d 206c 6f61 6465 645f 6d6f 6465 6c2e   = loaded_model.
+000150c0: 6d6f 6465 6c5f 696e 7374 616e 6365 2829  model_instance()
+000150d0: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
+000150e0: 662e 5f6c 6f67 6765 722e 696e 666f 280a  f._logger.info(.
+000150f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00015100: 2253 7563 6365 7373 6675 6c6c 7920 6c6f  "Successfully lo
+00015110: 6164 6564 2061 7274 6966 6163 7420 7769  aded artifact wi
+00015120: 7468 2061 7274 6966 6163 745f 6964 3a20  th artifact_id: 
+00015130: 7b7d 222e 666f 726d 6174 2861 7274 6966  {}".format(artif
+00015140: 6163 745f 6964 290a 2020 2020 2020 2020  act_id).        
+00015150: 2020 2020 290a 2020 2020 2020 2020 2020      ).          
+00015160: 2020 7265 7475 726e 206c 6f61 6465 645f    return loaded_
+00015170: 6d6f 6465 6c0a 2020 2020 2020 2020 6578  model.        ex
+00015180: 6365 7074 2045 7863 6570 7469 6f6e 2061  cept Exception a
+00015190: 7320 653a 0a20 2020 2020 2020 2020 2020  s e:.           
+000151a0: 2072 6169 7365 2057 4d4c 436c 6965 6e74   raise WMLClient
+000151b0: 4572 726f 7228 0a20 2020 2020 2020 2020  Error(.         
+000151c0: 2020 2020 2020 2022 4c6f 6164 696e 6720         "Loading 
+000151d0: 6d6f 6465 6c20 7769 7468 2061 7274 6966  model with artif
+000151e0: 6163 745f 6964 3a20 277b 7d27 2066 6169  act_id: '{}' fai
+000151f0: 6c65 642e 222e 666f 726d 6174 2861 7274  led.".format(art
+00015200: 6966 6163 745f 6964 292c 2065 0a20 2020  ifact_id), e.   
+00015210: 2020 2020 2020 2020 2029 0a0a 2020 2020           )..    
+00015220: 6465 6620 646f 776e 6c6f 6164 280a 2020  def download(.  
+00015230: 2020 2020 2020 7365 6c66 2c0a 2020 2020        self,.    
+00015240: 2020 2020 6d6f 6465 6c5f 6964 3a20 7374      model_id: st
+00015250: 7220 7c20 4e6f 6e65 2c0a 2020 2020 2020  r | None,.      
+00015260: 2020 6669 6c65 6e61 6d65 3a20 7374 7220    filename: str 
+00015270: 3d20 2264 6f77 6e6c 6f61 6465 645f 6d6f  = "downloaded_mo
+00015280: 6465 6c2e 7461 722e 677a 222c 0a20 2020  del.tar.gz",.   
+00015290: 2020 2020 2072 6576 5f69 643a 2073 7472       rev_id: str
+000152a0: 207c 204e 6f6e 6520 3d20 4e6f 6e65 2c0a   | None = None,.
+000152b0: 2020 2020 2020 2020 666f 726d 6174 3a20          format: 
+000152c0: 7374 7220 7c20 4e6f 6e65 203d 204e 6f6e  str | None = Non
+000152d0: 652c 0a20 2020 2020 2020 202a 2a6b 7761  e,.        **kwa
+000152e0: 7267 733a 2041 6e79 2c0a 2020 2020 2920  rgs: Any,.    ) 
+000152f0: 2d3e 2073 7472 207c 204e 6f6e 653a 0a20  -> str | None:. 
+00015300: 2020 2020 2020 2022 2222 446f 776e 6c6f         """Downlo
+00015310: 6164 206d 6f64 656c 2066 726f 6d20 7265  ad model from re
+00015320: 706f 7369 746f 7279 2074 6f20 6c6f 6361  pository to loca
+00015330: 6c20 6669 6c65 2e0a 0a20 2020 2020 2020  l file...       
+00015340: 203a 7061 7261 6d20 6d6f 6465 6c5f 6964   :param model_id
+00015350: 3a20 7374 6f72 6564 206d 6f64 656c 2049  : stored model I
+00015360: 440a 2020 2020 2020 2020 3a74 7970 6520  D.        :type 
+00015370: 6d6f 6465 6c5f 6964 3a20 7374 720a 0a20  model_id: str.. 
+00015380: 2020 2020 2020 203a 7061 7261 6d20 6669         :param fi
+00015390: 6c65 6e61 6d65 3a20 6e61 6d65 206f 6620  lename: name of 
+000153a0: 6c6f 6361 6c20 6669 6c65 2074 6f20 6372  local file to cr
+000153b0: 6561 7465 0a20 2020 2020 2020 203a 7479  eate.        :ty
+000153c0: 7065 2066 696c 656e 616d 653a 2073 7472  pe filename: str
+000153d0: 2c20 6f70 7469 6f6e 616c 0a0a 2020 2020  , optional..    
+000153e0: 2020 2020 3a70 6172 616d 2072 6576 5f69      :param rev_i
+000153f0: 643a 2072 6576 6973 696f 6e20 6964 0a20  d: revision id. 
+00015400: 2020 2020 2020 203a 7479 7065 2072 6576         :type rev
+00015410: 5f69 643a 2073 7472 2c20 6f70 7469 6f6e  _id: str, option
+00015420: 616c 0a0a 2020 2020 2020 2020 3a70 6172  al..        :par
+00015430: 616d 2066 6f72 6d61 743a 2066 6f72 6d61  am format: forma
+00015440: 7420 6f66 2074 6865 2063 6f6e 7465 6e74  t of the content
+00015450: 0a20 2020 2020 2020 203a 7479 7065 2066  .        :type f
+00015460: 6f72 6d61 743a 2073 7472 2c20 6f70 7469  ormat: str, opti
+00015470: 6f6e 616c 0a0a 2020 2020 2020 2020 2a2a  onal..        **
+00015480: 4578 616d 706c 652a 2a0a 0a20 2020 2020  Example**..     
+00015490: 2020 202e 2e20 636f 6465 2d62 6c6f 636b     .. code-block
+000154a0: 3a3a 2070 7974 686f 6e0a 0a20 2020 2020  :: python..     
+000154b0: 2020 2020 2020 2063 6c69 656e 742e 6d6f         client.mo
+000154c0: 6465 6c73 2e64 6f77 6e6c 6f61 6428 6d6f  dels.download(mo
+000154d0: 6465 6c5f 6964 2c20 276d 795f 6d6f 6465  del_id, 'my_mode
+000154e0: 6c2e 7461 722e 677a 2729 0a20 2020 2020  l.tar.gz').     
+000154f0: 2020 2022 2222 0a20 2020 2020 2020 206d     """.        m
+00015500: 6f64 656c 5f69 6420 3d20 5f67 6574 5f69  odel_id = _get_i
+00015510: 645f 6672 6f6d 5f64 6570 7265 6361 7465  d_from_deprecate
+00015520: 645f 7569 6428 6b77 6172 6773 2c20 6d6f  d_uid(kwargs, mo
+00015530: 6465 6c5f 6964 2c20 226d 6f64 656c 2229  del_id, "model")
+00015540: 0a20 2020 2020 2020 206d 6f64 656c 5f69  .        model_i
+00015550: 6420 3d20 6361 7374 2873 7472 2c20 6d6f  d = cast(str, mo
+00015560: 6465 6c5f 6964 290a 2020 2020 2020 2020  del_id).        
+00015570: 7265 765f 6964 203d 205f 6765 745f 6964  rev_id = _get_id
+00015580: 5f66 726f 6d5f 6465 7072 6563 6174 6564  _from_deprecated
+00015590: 5f75 6964 286b 7761 7267 732c 2072 6576  _uid(kwargs, rev
+000155a0: 5f69 642c 2022 7265 7622 2c20 5472 7565  _id, "rev", True
+000155b0: 290a 2020 2020 2020 2020 6966 206f 732e  ).        if os.
+000155c0: 7061 7468 2e69 7366 696c 6528 6669 6c65  path.isfile(file
+000155d0: 6e61 6d65 293a 0a20 2020 2020 2020 2020  name):.         
+000155e0: 2020 2072 6169 7365 2057 4d4c 436c 6965     raise WMLClie
+000155f0: 6e74 4572 726f 7228 0a20 2020 2020 2020  ntError(.       
+00015600: 2020 2020 2020 2020 2022 4669 6c65 2077           "File w
+00015610: 6974 6820 6e61 6d65 3a20 277b 7d27 2061  ith name: '{}' a
+00015620: 6c72 6561 6479 2065 7869 7374 732e 222e  lready exists.".
+00015630: 666f 726d 6174 2866 696c 656e 616d 6529  format(filename)
+00015640: 0a20 2020 2020 2020 2020 2020 2029 0a0a  .            )..
+00015650: 2020 2020 2020 2020 4d6f 6465 6c73 2e5f          Models._
+00015660: 7661 6c69 6461 7465 5f74 7970 6528 6d6f  validate_type(mo
+00015670: 6465 6c5f 6964 2c20 226d 6f64 656c 5f69  del_id, "model_i
+00015680: 6422 2c20 7374 722c 2054 7275 6529 0a20  d", str, True). 
+00015690: 2020 2020 2020 204d 6f64 656c 732e 5f76         Models._v
+000156a0: 616c 6964 6174 655f 7479 7065 2866 696c  alidate_type(fil
+000156b0: 656e 616d 652c 2022 6669 6c65 6e61 6d65  ename, "filename
+000156c0: 222c 2073 7472 2c20 5472 7565 290a 0a20  ", str, True).. 
+000156d0: 2020 2020 2020 2069 6620 6669 6c65 6e61         if filena
+000156e0: 6d65 2e65 6e64 7377 6974 6828 222e 6a73  me.endswith(".js
+000156f0: 6f6e 2229 3a0a 2020 2020 2020 2020 2020  on"):.          
+00015700: 2020 6973 5f6a 736f 6e20 3d20 5472 7565    is_json = True
+00015710: 0a20 2020 2020 2020 2020 2020 206a 736f  .            jso
+00015720: 6e5f 6669 6c65 6e61 6d65 203d 2066 696c  n_filename = fil
+00015730: 656e 616d 650a 2020 2020 2020 2020 2020  ename.          
+00015740: 2020 696d 706f 7274 2075 7569 640a 0a20    import uuid.. 
+00015750: 2020 2020 2020 2020 2020 2066 696c 656e             filen
+00015760: 616d 6520 3d20 6622 746d 705f 7b75 7569  ame = f"tmp_{uui
+00015770: 642e 7575 6964 3428 297d 2e74 6172 2e67  d.uuid4()}.tar.g
+00015780: 7a22 0a20 2020 2020 2020 2065 6c73 653a  z".        else:
+00015790: 0a20 2020 2020 2020 2020 2020 2069 735f  .            is_
+000157a0: 6a73 6f6e 203d 2046 616c 7365 0a0a 2020  json = False..  
+000157b0: 2020 2020 2020 6172 7469 6661 6374 5f75        artifact_u
+000157c0: 726c 203d 2028 0a20 2020 2020 2020 2020  rl = (.         
+000157d0: 2020 2073 656c 662e 5f63 6c69 656e 742e     self._client.
+000157e0: 7365 7276 6963 655f 696e 7374 616e 6365  service_instance
+000157f0: 2e5f 6872 6566 5f64 6566 696e 6974 696f  ._href_definitio
+00015800: 6e73 2e67 6574 5f6d 6f64 656c 5f6c 6173  ns.get_model_las
+00015810: 745f 7665 7273 696f 6e5f 6872 6566 280a  t_version_href(.
+00015820: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00015830: 6d6f 6465 6c5f 6964 0a20 2020 2020 2020  model_id.       
+00015840: 2020 2020 2029 0a20 2020 2020 2020 2029       ).        )
+00015850: 0a20 2020 2020 2020 2070 6172 616d 7320  .        params 
+00015860: 3d20 7365 6c66 2e5f 636c 6965 6e74 2e5f  = self._client._
+00015870: 7061 7261 6d73 2829 0a20 2020 2020 2020  params().       
+00015880: 2074 7279 3a0a 2020 2020 2020 2020 2020   try:.          
+00015890: 2020 7572 6c20 3d20 7365 6c66 2e5f 636c    url = self._cl
+000158a0: 6965 6e74 2e73 6572 7669 6365 5f69 6e73  ient.service_ins
+000158b0: 7461 6e63 652e 5f68 7265 665f 6465 6669  tance._href_defi
+000158c0: 6e69 7469 6f6e 732e 6765 745f 7075 626c  nitions.get_publ
+000158d0: 6973 6865 645f 6d6f 6465 6c5f 6872 6566  ished_model_href
+000158e0: 280a 2020 2020 2020 2020 2020 2020 2020  (.              
+000158f0: 2020 6d6f 6465 6c5f 6964 0a20 2020 2020    model_id.     
+00015900: 2020 2020 2020 2029 0a20 2020 2020 2020         ).       
+00015910: 2020 2020 206d 6f64 656c 5f67 6574 5f72       model_get_r
+00015920: 6573 706f 6e73 6520 3d20 7265 7175 6573  esponse = reques
+00015930: 7473 2e67 6574 280a 2020 2020 2020 2020  ts.get(.        
+00015940: 2020 2020 2020 2020 7572 6c2c 2070 6172          url, par
+00015950: 616d 733d 7365 6c66 2e5f 636c 6965 6e74  ams=self._client
+00015960: 2e5f 7061 7261 6d73 2829 2c20 6865 6164  ._params(), head
+00015970: 6572 733d 7365 6c66 2e5f 636c 6965 6e74  ers=self._client
+00015980: 2e5f 6765 745f 6865 6164 6572 7328 290a  ._get_headers().
+00015990: 2020 2020 2020 2020 2020 2020 290a 0a20              ).. 
+000159a0: 2020 2020 2020 2020 2020 206d 6f64 656c             model
+000159b0: 5f64 6574 6169 6c73 203d 2073 656c 662e  _details = self.
+000159c0: 5f68 616e 646c 655f 7265 7370 6f6e 7365  _handle_response
+000159d0: 2832 3030 2c20 2267 6574 206d 6f64 656c  (200, "get model
+000159e0: 222c 206d 6f64 656c 5f67 6574 5f72 6573  ", model_get_res
+000159f0: 706f 6e73 6529 0a20 2020 2020 2020 2020  ponse).         
+00015a00: 2020 2069 6620 7265 765f 6964 2069 7320     if rev_id is 
+00015a10: 6e6f 7420 4e6f 6e65 3a0a 2020 2020 2020  not None:.      
+00015a20: 2020 2020 2020 2020 2020 7061 7261 6d73            params
+00015a30: 2e75 7064 6174 6528 7b22 7265 7669 7369  .update({"revisi
+00015a40: 6f6e 5f69 6422 3a20 7265 765f 6964 7d29  on_id": rev_id})
+00015a50: 0a0a 2020 2020 2020 2020 2020 2020 6d6f  ..            mo
+00015a60: 6465 6c5f 7479 7065 203d 206d 6f64 656c  del_type = model
+00015a70: 5f64 6574 6169 6c73 5b22 656e 7469 7479  _details["entity
+00015a80: 225d 5b22 7479 7065 225d 0a20 2020 2020  "]["type"].     
+00015a90: 2020 2020 2020 2069 6620 280a 2020 2020         if (.    
+00015aa0: 2020 2020 2020 2020 2020 2020 6d6f 6465              mode
+00015ab0: 6c5f 7479 7065 2e73 7461 7274 7377 6974  l_type.startswit
+00015ac0: 6828 226b 6572 6173 5f22 290a 2020 2020  h("keras_").    
+00015ad0: 2020 2020 2020 2020 2020 2020 6f72 206d              or m
+00015ae0: 6f64 656c 5f74 7970 652e 7374 6172 7473  odel_type.starts
+00015af0: 7769 7468 2822 7363 696b 6974 2d6c 6561  with("scikit-lea
+00015b00: 726e 5f22 290a 2020 2020 2020 2020 2020  rn_").          
+00015b10: 2020 2020 2020 6f72 206d 6f64 656c 5f74        or model_t
+00015b20: 7970 652e 7374 6172 7473 7769 7468 2822  ype.startswith("
+00015b30: 7867 626f 6f73 745f 2229 0a20 2020 2020  xgboost_").     
+00015b40: 2020 2020 2020 2029 2061 6e64 2066 6f72         ) and for
+00015b50: 6d61 7420 6973 206e 6f74 204e 6f6e 653a  mat is not None:
+00015b60: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00015b70: 204d 6f64 656c 732e 5f76 616c 6964 6174   Models._validat
+00015b80: 655f 7479 7065 2866 6f72 6d61 742c 2022  e_type(format, "
+00015b90: 666f 726d 6174 222c 2073 7472 2c20 4661  format", str, Fa
+00015ba0: 6c73 6529 0a20 2020 2020 2020 2020 2020  lse).           
+00015bb0: 2020 2020 2069 6620 7374 7228 666f 726d       if str(form
+00015bc0: 6174 292e 7570 7065 7228 2920 3d3d 2022  at).upper() == "
+00015bd0: 434f 5245 4d4c 223a 0a20 2020 2020 2020  COREML":.       
+00015be0: 2020 2020 2020 2020 2020 2020 2070 6172               par
+00015bf0: 616d 732e 7570 6461 7465 287b 2263 6f6e  ams.update({"con
+00015c00: 7465 6e74 5f66 6f72 6d61 7422 3a20 2263  tent_format": "c
+00015c10: 6f72 656d 6c22 7d29 0a20 2020 2020 2020  oreml"}).       
+00015c20: 2020 2020 2020 2020 2065 6c73 653a 0a20           else:. 
+00015c30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00015c40: 2020 2070 6172 616d 732e 7570 6461 7465     params.update
+00015c50: 287b 2263 6f6e 7465 6e74 5f66 6f72 6d61  ({"content_forma
+00015c60: 7422 3a20 226e 6174 6976 6522 7d29 0a20  t": "native"}). 
+00015c70: 2020 2020 2020 2020 2020 2065 6c73 653a             else:
+00015c80: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00015c90: 2070 6172 616d 732e 7570 6461 7465 287b   params.update({
+00015ca0: 2263 6f6e 7465 6e74 5f66 6f72 6d61 7422  "content_format"
+00015cb0: 3a20 226e 6174 6976 6522 7d29 0a20 2020  : "native"}).   
+00015cc0: 2020 2020 2020 2020 2061 7274 6966 6163           artifac
+00015cd0: 745f 636f 6e74 656e 745f 7572 6c20 3d20  t_content_url = 
+00015ce0: 7374 7228 6172 7469 6661 6374 5f75 726c  str(artifact_url
+00015cf0: 202b 2022 2f64 6f77 6e6c 6f61 6422 290a   + "/download").
+00015d00: 2020 2020 2020 2020 2020 2020 6966 206d              if m
+00015d10: 6f64 656c 5f64 6574 6169 6c73 5b22 656e  odel_details["en
+00015d20: 7469 7479 225d 5b22 7479 7065 225d 203d  tity"]["type"] =
+00015d30: 3d20 2277 6d6c 2d68 7962 7269 645f 302e  = "wml-hybrid_0.
+00015d40: 3122 3a0a 2020 2020 2020 2020 2020 2020  1":.            
+00015d50: 2020 2020 7365 6c66 2e5f 646f 776e 6c6f      self._downlo
+00015d60: 6164 5f61 7574 6f5f 6169 5f6d 6f64 656c  ad_auto_ai_model
+00015d70: 5f63 6f6e 7465 6e74 280a 2020 2020 2020  _content(.      
+00015d80: 2020 2020 2020 2020 2020 2020 2020 6d6f                mo
+00015d90: 6465 6c5f 6964 2c20 6172 7469 6661 6374  del_id, artifact
+00015da0: 5f63 6f6e 7465 6e74 5f75 726c 2c20 6669  _content_url, fi
+00015db0: 6c65 6e61 6d65 0a20 2020 2020 2020 2020  lename.         
+00015dc0: 2020 2020 2020 2029 0a20 2020 2020 2020         ).       
+00015dd0: 2020 2020 2020 2020 2070 7269 6e74 2822           print("
+00015de0: 5375 6363 6573 7366 756c 6c79 2073 6176  Successfully sav
+00015df0: 6564 206d 6f64 656c 2063 6f6e 7465 6e74  ed model content
+00015e00: 2074 6f20 6669 6c65 3a20 277b 7d27 222e   to file: '{}'".
+00015e10: 666f 726d 6174 2866 696c 656e 616d 6529  format(filename)
+00015e20: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
+00015e30: 2020 7265 7475 726e 206f 732e 6765 7463    return os.getc
+00015e40: 7764 2829 202b 2022 2f22 202b 2066 696c  wd() + "/" + fil
+00015e50: 656e 616d 650a 2020 2020 2020 2020 2020  ename.          
+00015e60: 2020 656c 7365 3a0a 2020 2020 2020 2020    else:.        
+00015e70: 2020 2020 2020 2020 7220 3d20 7265 7175          r = requ
+00015e80: 6573 7473 2e67 6574 280a 2020 2020 2020  ests.get(.      
+00015e90: 2020 2020 2020 2020 2020 2020 2020 6172                ar
+00015ea0: 7469 6661 6374 5f63 6f6e 7465 6e74 5f75  tifact_content_u
+00015eb0: 726c 2c0a 2020 2020 2020 2020 2020 2020  rl,.            
+00015ec0: 2020 2020 2020 2020 7061 7261 6d73 3d70          params=p
+00015ed0: 6172 616d 732c 0a20 2020 2020 2020 2020  arams,.         
+00015ee0: 2020 2020 2020 2020 2020 2068 6561 6465             heade
+00015ef0: 7273 3d73 656c 662e 5f63 6c69 656e 742e  rs=self._client.
+00015f00: 5f67 6574 5f68 6561 6465 7273 2829 2c0a  _get_headers(),.
+00015f10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00015f20: 2020 2020 7374 7265 616d 3d54 7275 652c      stream=True,
+00015f30: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00015f40: 2029 0a0a 2020 2020 2020 2020 2020 2020   )..            
+00015f50: 6966 2072 2e73 7461 7475 735f 636f 6465  if r.status_code
+00015f60: 2021 3d20 3230 303a 0a20 2020 2020 2020   != 200:.       
+00015f70: 2020 2020 2020 2020 2072 6169 7365 2041           raise A
+00015f80: 7069 5265 7175 6573 7446 6169 6c75 7265  piRequestFailure
+00015f90: 280a 2020 2020 2020 2020 2020 2020 2020  (.              
+00015fa0: 2020 2020 2020 2246 6169 6c75 7265 2064        "Failure d
+00015fb0: 7572 696e 6720 7b7d 2e22 2e66 6f72 6d61  uring {}.".forma
+00015fc0: 7428 2264 6f77 6e6c 6f61 6469 6e67 206d  t("downloading m
+00015fd0: 6f64 656c 2229 2c20 720a 2020 2020 2020  odel"), r.      
+00015fe0: 2020 2020 2020 2020 2020 290a 0a20 2020            )..   
+00015ff0: 2020 2020 2020 2020 2064 6f77 6e6c 6f61           downloa
+00016000: 6465 645f 6d6f 6465 6c20 3d20 722e 636f  ded_model = r.co
+00016010: 6e74 656e 740a 2020 2020 2020 2020 2020  ntent.          
+00016020: 2020 7365 6c66 2e5f 6c6f 6767 6572 2e69    self._logger.i
+00016030: 6e66 6f28 0a20 2020 2020 2020 2020 2020  nfo(.           
+00016040: 2020 2020 2022 5375 6363 6573 7366 756c       "Successful
+00016050: 6c79 2064 6f77 6e6c 6f61 6465 6420 6172  ly downloaded ar
+00016060: 7469 6661 6374 2077 6974 6820 6172 7469  tifact with arti
+00016070: 6661 6374 5f75 726c 3a20 7b7d 222e 666f  fact_url: {}".fo
+00016080: 726d 6174 280a 2020 2020 2020 2020 2020  rmat(.          
+00016090: 2020 2020 2020 2020 2020 6172 7469 6661            artifa
+000160a0: 6374 5f75 726c 0a20 2020 2020 2020 2020  ct_url.         
+000160b0: 2020 2020 2020 2029 0a20 2020 2020 2020         ).       
+000160c0: 2020 2020 2029 0a20 2020 2020 2020 2065       ).        e
+000160d0: 7863 6570 7420 574d 4c43 6c69 656e 7445  xcept WMLClientE
+000160e0: 7272 6f72 2061 7320 653a 0a20 2020 2020  rror as e:.     
+000160f0: 2020 2020 2020 2072 6169 7365 2065 0a20         raise e. 
+00016100: 2020 2020 2020 2065 7863 6570 7420 4578         except Ex
+00016110: 6365 7074 696f 6e20 6173 2065 3a0a 2020  ception as e:.  
+00016120: 2020 2020 2020 2020 2020 6966 2061 7274            if art
+00016130: 6966 6163 745f 7572 6c20 6973 206e 6f74  ifact_url is not
+00016140: 204e 6f6e 653a 0a20 2020 2020 2020 2020   None:.         
+00016150: 2020 2020 2020 2072 6169 7365 2057 4d4c         raise WML
+00016160: 436c 6965 6e74 4572 726f 7228 0a20 2020  ClientError(.   
+00016170: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016180: 2022 446f 776e 6c6f 6164 696e 6720 6d6f   "Downloading mo
+00016190: 6465 6c20 7769 7468 2061 7274 6966 6163  del with artifac
+000161a0: 745f 7572 6c3a 2027 7b7d 2720 6661 696c  t_url: '{}' fail
+000161b0: 6564 2e22 2e66 6f72 6d61 7428 0a20 2020  ed.".format(.   
+000161c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000161d0: 2020 2020 2061 7274 6966 6163 745f 7572       artifact_ur
+000161e0: 6c0a 2020 2020 2020 2020 2020 2020 2020  l.              
+000161f0: 2020 2020 2020 292c 0a20 2020 2020 2020        ),.       
+00016200: 2020 2020 2020 2020 2020 2020 2065 2c0a               e,.
+00016210: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016220: 290a 2020 2020 2020 2020 2020 2020 656c  ).            el
+00016230: 7365 3a0a 2020 2020 2020 2020 2020 2020  se:.            
+00016240: 2020 2020 7261 6973 6520 574d 4c43 6c69      raise WMLCli
+00016250: 656e 7445 7272 6f72 2822 446f 776e 6c6f  entError("Downlo
+00016260: 6164 696e 6720 6d6f 6465 6c20 6661 696c  ading model fail
+00016270: 6564 2e22 2c20 6529 0a20 2020 2020 2020  ed.", e).       
+00016280: 2066 696e 616c 6c79 3a0a 2020 2020 2020   finally:.      
+00016290: 2020 2020 2020 6966 2069 735f 6a73 6f6e        if is_json
+000162a0: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
+000162b0: 2020 7472 793a 0a20 2020 2020 2020 2020    try:.         
+000162c0: 2020 2020 2020 2020 2020 206f 732e 7265             os.re
+000162d0: 6d6f 7665 2866 696c 656e 616d 6529 0a20  move(filename). 
+000162e0: 2020 2020 2020 2020 2020 2020 2020 2065                 e
+000162f0: 7863 6570 7420 4578 6365 7074 696f 6e3a  xcept Exception:
+00016300: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00016310: 2020 2020 2070 6173 730a 2020 2020 2020       pass.      
+00016320: 2020 7472 793a 0a20 2020 2020 2020 2020    try:.         
+00016330: 2020 2077 6974 6820 6f70 656e 2866 696c     with open(fil
+00016340: 656e 616d 652c 2022 7762 2229 2061 7320  ename, "wb") as 
+00016350: 663a 0a20 2020 2020 2020 2020 2020 2020  f:.             
+00016360: 2020 2066 2e77 7269 7465 2864 6f77 6e6c     f.write(downl
+00016370: 6f61 6465 645f 6d6f 6465 6c29 0a0a 2020  oaded_model)..  
+00016380: 2020 2020 2020 2020 2020 6966 2069 735f            if is_
+00016390: 6a73 6f6e 3a0a 2020 2020 2020 2020 2020  json:.          
+000163a0: 2020 2020 2020 696d 706f 7274 2074 6172        import tar
+000163b0: 6669 6c65 0a0a 2020 2020 2020 2020 2020  file..          
+000163c0: 2020 2020 2020 7461 7220 3d20 7461 7266        tar = tarf
+000163d0: 696c 652e 6f70 656e 2866 696c 656e 616d  ile.open(filenam
+000163e0: 652c 2022 723a 677a 2229 0a20 2020 2020  e, "r:gz").     
+000163f0: 2020 2020 2020 2020 2020 2066 696c 655f             file_
+00016400: 6e61 6d65 203d 2074 6172 2e67 6574 6e61  name = tar.getna
+00016410: 6d65 7328 295b 305d 0a20 2020 2020 2020  mes()[0].       
+00016420: 2020 2020 2020 2020 2069 6620 6e6f 7420           if not 
+00016430: 6669 6c65 5f6e 616d 652e 656e 6473 7769  file_name.endswi
+00016440: 7468 2822 2e6a 736f 6e22 293a 0a20 2020  th(".json"):.   
+00016450: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016460: 2072 6169 7365 2057 4d4c 436c 6965 6e74   raise WMLClient
+00016470: 4572 726f 7228 2244 6f77 6e6c 6f61 6465  Error("Downloade
+00016480: 6420 6d6f 6465 6c20 6973 206e 6f74 206a  d model is not j
+00016490: 736f 6e2e 2229 0a20 2020 2020 2020 2020  son.").         
+000164a0: 2020 2020 2020 2074 6172 2e65 7874 7261         tar.extra
+000164b0: 6374 616c 6c28 290a 2020 2020 2020 2020  ctall().        
+000164c0: 2020 2020 2020 2020 7461 722e 636c 6f73          tar.clos
+000164d0: 6528 290a 2020 2020 2020 2020 2020 2020  e().            
+000164e0: 2020 2020 6f73 2e72 656e 616d 6528 6669      os.rename(fi
+000164f0: 6c65 5f6e 616d 652c 206a 736f 6e5f 6669  le_name, json_fi
+00016500: 6c65 6e61 6d65 290a 0a20 2020 2020 2020  lename)..       
+00016510: 2020 2020 2020 2020 206f 732e 7265 6d6f           os.remo
+00016520: 7665 2866 696c 656e 616d 6529 0a20 2020  ve(filename).   
+00016530: 2020 2020 2020 2020 2020 2020 2066 696c               fil
+00016540: 656e 616d 6520 3d20 6a73 6f6e 5f66 696c  ename = json_fil
+00016550: 656e 616d 650a 0a20 2020 2020 2020 2020  ename..         
+00016560: 2020 2070 7269 6e74 2822 5375 6363 6573     print("Succes
+00016570: 7366 756c 6c79 2073 6176 6564 206d 6f64  sfully saved mod
+00016580: 656c 2063 6f6e 7465 6e74 2074 6f20 6669  el content to fi
+00016590: 6c65 3a20 277b 7d27 222e 666f 726d 6174  le: '{}'".format
+000165a0: 2866 696c 656e 616d 6529 290a 2020 2020  (filename)).    
+000165b0: 2020 2020 2020 2020 7265 7475 726e 206f          return o
+000165c0: 732e 6765 7463 7764 2829 202b 2022 2f22  s.getcwd() + "/"
+000165d0: 202b 2066 696c 656e 616d 650a 2020 2020   + filename.    
+000165e0: 2020 2020 6578 6365 7074 2049 4f45 7272      except IOErr
+000165f0: 6f72 2061 7320 653a 0a20 2020 2020 2020  or as e:.       
+00016600: 2020 2020 2072 6169 7365 2057 4d4c 436c       raise WMLCl
+00016610: 6965 6e74 4572 726f 7228 0a20 2020 2020  ientError(.     
+00016620: 2020 2020 2020 2020 2020 2022 5361 7669             "Savi
+00016630: 6e67 206d 6f64 656c 2077 6974 6820 6172  ng model with ar
+00016640: 7469 6661 6374 5f75 726c 3a20 277b 7d27  tifact_url: '{}'
+00016650: 2066 6169 6c65 642e 222e 666f 726d 6174   failed.".format
+00016660: 2866 696c 656e 616d 6529 2c20 650a 2020  (filename), e.  
+00016670: 2020 2020 2020 2020 2020 290a 0a20 2020            )..   
+00016680: 2064 6566 2064 656c 6574 6528 7365 6c66   def delete(self
+00016690: 2c20 6d6f 6465 6c5f 6964 3a20 7374 7220  , model_id: str 
+000166a0: 7c20 4e6f 6e65 203d 204e 6f6e 652c 202a  | None = None, *
+000166b0: 2a6b 7761 7267 733a 2041 6e79 2920 2d3e  *kwargs: Any) ->
+000166c0: 2064 6963 745b 7374 722c 2041 6e79 5d3a   dict[str, Any]:
+000166d0: 0a20 2020 2020 2020 2022 2222 4465 6c65  .        """Dele
+000166e0: 7465 206d 6f64 656c 2066 726f 6d20 7265  te model from re
+000166f0: 706f 7369 746f 7279 2e0a 0a20 2020 2020  pository...     
+00016700: 2020 203a 7061 7261 6d20 6d6f 6465 6c5f     :param model_
+00016710: 6964 3a20 7374 6f72 6564 206d 6f64 656c  id: stored model
+00016720: 2049 440a 2020 2020 2020 2020 3a74 7970   ID.        :typ
+00016730: 6520 6d6f 6465 6c5f 6964 3a20 7374 720a  e model_id: str.
+00016740: 0a20 2020 2020 2020 202a 2a45 7861 6d70  .        **Examp
+00016750: 6c65 2a2a 0a0a 2020 2020 2020 2020 2e2e  le**..        ..
+00016760: 2063 6f64 652d 626c 6f63 6b3a 3a20 7079   code-block:: py
+00016770: 7468 6f6e 0a0a 2020 2020 2020 2020 2020  thon..          
+00016780: 2020 636c 6965 6e74 2e6d 6f64 656c 732e    client.models.
+00016790: 6465 6c65 7465 286d 6f64 656c 5f69 6429  delete(model_id)
+000167a0: 0a20 2020 2020 2020 2022 2222 0a20 2020  .        """.   
+000167b0: 2020 2020 206d 6f64 656c 5f69 6420 3d20       model_id = 
+000167c0: 5f67 6574 5f69 645f 6672 6f6d 5f64 6570  _get_id_from_dep
+000167d0: 7265 6361 7465 645f 7569 6428 6b77 6172  recated_uid(kwar
+000167e0: 6773 2c20 6d6f 6465 6c5f 6964 2c20 226d  gs, model_id, "m
+000167f0: 6f64 656c 2229 0a20 2020 2020 2020 204d  odel").        M
+00016800: 6f64 656c 732e 5f76 616c 6964 6174 655f  odels._validate_
+00016810: 7479 7065 286d 6f64 656c 5f69 642c 2022  type(model_id, "
+00016820: 6d6f 6465 6c5f 6964 222c 2073 7472 2c20  model_id", str, 
+00016830: 4661 6c73 6529 0a0a 2020 2020 2020 2020  False)..        
+00016840: 6d6f 6465 6c5f 656e 6470 6f69 6e74 203d  model_endpoint =
+00016850: 2028 0a20 2020 2020 2020 2020 2020 2073   (.            s
+00016860: 656c 662e 5f63 6c69 656e 742e 7365 7276  elf._client.serv
+00016870: 6963 655f 696e 7374 616e 6365 2e5f 6872  ice_instance._hr
+00016880: 6566 5f64 6566 696e 6974 696f 6e73 2e67  ef_definitions.g
+00016890: 6574 5f70 7562 6c69 7368 6564 5f6d 6f64  et_published_mod
+000168a0: 656c 5f68 7265 6628 0a20 2020 2020 2020  el_href(.       
+000168b0: 2020 2020 2020 2020 206d 6f64 656c 5f69           model_i
+000168c0: 640a 2020 2020 2020 2020 2020 2020 290a  d.            ).
+000168d0: 2020 2020 2020 2020 290a 0a20 2020 2020          )..     
+000168e0: 2020 2073 656c 662e 5f6c 6f67 6765 722e     self._logger.
+000168f0: 6465 6275 6728 0a20 2020 2020 2020 2020  debug(.         
+00016900: 2020 2022 4465 6c65 7469 6f6e 2061 7274     "Deletion art
+00016910: 6966 6163 7420 6d6f 6465 6c20 656e 6470  ifact model endp
+00016920: 6f69 6e74 3a20 7b7d 222e 666f 726d 6174  oint: {}".format
+00016930: 286d 6f64 656c 5f65 6e64 706f 696e 7429  (model_endpoint)
+00016940: 0a20 2020 2020 2020 2029 0a20 2020 2020  .        ).     
+00016950: 2020 2069 6620 6e6f 7420 7365 6c66 2e5f     if not self._
+00016960: 636c 6965 6e74 2e49 4350 5f50 4c41 5446  client.ICP_PLATF
+00016970: 4f52 4d5f 5350 4143 4553 3a0a 2020 2020  ORM_SPACES:.    
+00016980: 2020 2020 2020 2020 6966 2073 656c 662e          if self.
+00016990: 5f69 665f 6465 706c 6f79 6d65 6e74 5f65  _if_deployment_e
+000169a0: 7869 7374 5f66 6f72 5f61 7373 6574 286d  xist_for_asset(m
+000169b0: 6f64 656c 5f69 6429 3a0a 2020 2020 2020  odel_id):.      
+000169c0: 2020 2020 2020 2020 2020 7261 6973 6520            raise 
+000169d0: 574d 4c43 6c69 656e 7445 7272 6f72 280a  WMLClientError(.
+000169e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000169f0: 2020 2020 2243 616e 6e6f 7420 6465 6c65      "Cannot dele
+00016a00: 7465 206d 6f64 656c 2074 6861 7420 6861  te model that ha
+00016a10: 7320 6578 6973 7469 6e67 2064 6570 6c6f  s existing deplo
+00016a20: 796d 656e 7473 2e20 506c 6561 7365 2064  yments. Please d
+00016a30: 656c 6574 6520 616c 6c20 6173 736f 6369  elete all associ
+00016a40: 6174 6564 2064 6570 6c6f 796d 656e 7473  ated deployments
+00016a50: 2061 6e64 2074 7279 2061 6761 696e 220a   and try again".
+00016a60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016a70: 290a 0a20 2020 2020 2020 2020 2020 2072  )..            r
+00016a80: 6573 706f 6e73 655f 6465 6c65 7465 203d  esponse_delete =
+00016a90: 2072 6571 7565 7374 732e 6465 6c65 7465   requests.delete
+00016aa0: 280a 2020 2020 2020 2020 2020 2020 2020  (.              
+00016ab0: 2020 6d6f 6465 6c5f 656e 6470 6f69 6e74    model_endpoint
+00016ac0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00016ad0: 2020 7061 7261 6d73 3d73 656c 662e 5f63    params=self._c
+00016ae0: 6c69 656e 742e 5f70 6172 616d 7328 292c  lient._params(),
+00016af0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00016b00: 2068 6561 6465 7273 3d73 656c 662e 5f63   headers=self._c
+00016b10: 6c69 656e 742e 5f67 6574 5f68 6561 6465  lient._get_heade
+00016b20: 7273 2829 2c0a 2020 2020 2020 2020 2020  rs(),.          
+00016b30: 2020 290a 2020 2020 2020 2020 656c 7365    ).        else
+00016b40: 3a0a 2020 2020 2020 2020 2020 2020 2320  :.            # 
+00016b50: 6368 6563 6b20 6966 2074 6865 206d 6f64  check if the mod
+00016b60: 656c 2061 7320 6120 636f 7272 6573 706f  el as a correspo
+00016b70: 6e64 696e 6720 6465 706c 6f79 6d65 6e74  nding deployment
+00016b80: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if 
+00016b90: 7365 6c66 2e5f 6966 5f64 6570 6c6f 796d  self._if_deploym
+00016ba0: 656e 745f 6578 6973 745f 666f 725f 6173  ent_exist_for_as
+00016bb0: 7365 7428 6d6f 6465 6c5f 6964 293a 0a20  set(model_id):. 
+00016bc0: 2020 2020 2020 2020 2020 2020 2020 2072                 r
+00016bd0: 6169 7365 2057 4d4c 436c 6965 6e74 4572  aise WMLClientEr
+00016be0: 726f 7228 0a20 2020 2020 2020 2020 2020  ror(.           
+00016bf0: 2020 2020 2020 2020 2022 4361 6e6e 6f74           "Cannot
+00016c00: 2064 656c 6574 6520 6d6f 6465 6c20 7468   delete model th
+00016c10: 6174 2068 6173 2065 7869 7374 696e 6720  at has existing 
+00016c20: 6465 706c 6f79 6d65 6e74 732e 2050 6c65  deployments. Ple
+00016c30: 6173 6520 6465 6c65 7465 2061 6c6c 2061  ase delete all a
+00016c40: 7373 6f63 6961 7465 6420 6465 706c 6f79  ssociated deploy
+00016c50: 6d65 6e74 7320 616e 6420 7472 7920 6167  ments and try ag
+00016c60: 6169 6e22 0a20 2020 2020 2020 2020 2020  ain".           
+00016c70: 2020 2020 2029 0a20 2020 2020 2020 2020       ).         
+00016c80: 2020 2072 6573 706f 6e73 655f 6465 6c65     response_dele
+00016c90: 7465 203d 2072 6571 7565 7374 732e 6465  te = requests.de
+00016ca0: 6c65 7465 280a 2020 2020 2020 2020 2020  lete(.          
+00016cb0: 2020 2020 2020 6d6f 6465 6c5f 656e 6470        model_endp
+00016cc0: 6f69 6e74 2c0a 2020 2020 2020 2020 2020  oint,.          
+00016cd0: 2020 2020 2020 7061 7261 6d73 3d73 656c        params=sel
+00016ce0: 662e 5f63 6c69 656e 742e 5f70 6172 616d  f._client._param
+00016cf0: 7328 292c 0a20 2020 2020 2020 2020 2020  s(),.           
+00016d00: 2020 2020 2068 6561 6465 7273 3d73 656c       headers=sel
+00016d10: 662e 5f63 6c69 656e 742e 5f67 6574 5f68  f._client._get_h
+00016d20: 6561 6465 7273 2829 2c0a 2020 2020 2020  eaders(),.      
+00016d30: 2020 2020 2020 290a 2020 2020 2020 2020        ).        
+00016d40: 7265 7475 726e 2073 656c 662e 5f68 616e  return self._han
+00016d50: 646c 655f 7265 7370 6f6e 7365 2832 3034  dle_response(204
+00016d60: 2c20 226d 6f64 656c 2064 656c 6574 696f  , "model deletio
+00016d70: 6e22 2c20 7265 7370 6f6e 7365 5f64 656c  n", response_del
+00016d80: 6574 652c 2046 616c 7365 290a 0a20 2020  ete, False)..   
+00016d90: 2040 6f76 6572 6c6f 6164 0a20 2020 2064   @overload.    d
+00016da0: 6566 2067 6574 5f64 6574 6169 6c73 280a  ef get_details(.
+00016db0: 2020 2020 2020 2020 7365 6c66 2c0a 2020          self,.  
+00016dc0: 2020 2020 2020 6d6f 6465 6c5f 6964 3a20        model_id: 
+00016dd0: 7374 7220 3d20 2222 2c0a 2020 2020 2020  str = "",.      
+00016de0: 2020 6c69 6d69 743a 2069 6e74 207c 204e    limit: int | N
+00016df0: 6f6e 6520 3d20 4e6f 6e65 2c0a 2020 2020  one = None,.    
+00016e00: 2020 2020 6173 796e 6368 726f 6e6f 7573      asynchronous
+00016e10: 3a20 626f 6f6c 203d 2046 616c 7365 2c0a  : bool = False,.
+00016e20: 2020 2020 2020 2020 6765 745f 616c 6c3a          get_all:
+00016e30: 2062 6f6f 6c20 3d20 4661 6c73 652c 0a20   bool = False,. 
+00016e40: 2020 2020 2020 2073 7065 635f 7374 6174         spec_stat
+00016e50: 653a 2053 7065 6353 7461 7465 7320 7c20  e: SpecStates | 
+00016e60: 4e6f 6e65 203d 204e 6f6e 652c 0a20 2020  None = None,.   
+00016e70: 2020 2020 202a 2a6b 7761 7267 733a 2041       **kwargs: A
+00016e80: 6e79 2c0a 2020 2020 2920 2d3e 2064 6963  ny,.    ) -> dic
+00016e90: 745b 7374 722c 2041 6e79 5d3a 202e 2e2e  t[str, Any]: ...
+00016ea0: 0a0a 2020 2020 406f 7665 726c 6f61 640a  ..    @overload.
+00016eb0: 2020 2020 6465 6620 6765 745f 6465 7461      def get_deta
+00016ec0: 696c 7328 0a20 2020 2020 2020 2073 656c  ils(.        sel
+00016ed0: 662c 0a20 2020 2020 2020 206d 6f64 656c  f,.        model
+00016ee0: 5f69 643a 2073 7472 207c 204e 6f6e 6520  _id: str | None 
+00016ef0: 3d20 4e6f 6e65 2c0a 2020 2020 2020 2020  = None,.        
+00016f00: 6c69 6d69 743a 2069 6e74 207c 204e 6f6e  limit: int | Non
+00016f10: 6520 3d20 4e6f 6e65 2c0a 2020 2020 2020  e = None,.      
+00016f20: 2020 6173 796e 6368 726f 6e6f 7573 3a20    asynchronous: 
+00016f30: 626f 6f6c 203d 2046 616c 7365 2c0a 2020  bool = False,.  
+00016f40: 2020 2020 2020 6765 745f 616c 6c3a 2062        get_all: b
+00016f50: 6f6f 6c20 3d20 4661 6c73 652c 0a20 2020  ool = False,.   
+00016f60: 2020 2020 2073 7065 635f 7374 6174 653a       spec_state:
+00016f70: 2053 7065 6353 7461 7465 7320 7c20 4e6f   SpecStates | No
+00016f80: 6e65 203d 204e 6f6e 652c 0a20 2020 2020  ne = None,.     
+00016f90: 2020 202a 2a6b 7761 7267 733a 2041 6e79     **kwargs: Any
+00016fa0: 2c0a 2020 2020 2920 2d3e 2064 6963 745b  ,.    ) -> dict[
+00016fb0: 7374 722c 2041 6e79 5d20 7c20 4765 6e65  str, Any] | Gene
+00016fc0: 7261 746f 723a 202e 2e2e 0a0a 2020 2020  rator: .....    
+00016fd0: 6465 6620 6765 745f 6465 7461 696c 7328  def get_details(
+00016fe0: 0a20 2020 2020 2020 2073 656c 662c 0a20  .        self,. 
+00016ff0: 2020 2020 2020 206d 6f64 656c 5f69 643a         model_id:
+00017000: 2073 7472 207c 204e 6f6e 6520 3d20 4e6f   str | None = No
+00017010: 6e65 2c0a 2020 2020 2020 2020 6c69 6d69  ne,.        limi
+00017020: 743a 2069 6e74 207c 204e 6f6e 6520 3d20  t: int | None = 
+00017030: 4e6f 6e65 2c0a 2020 2020 2020 2020 6173  None,.        as
+00017040: 796e 6368 726f 6e6f 7573 3a20 626f 6f6c  ynchronous: bool
+00017050: 203d 2046 616c 7365 2c0a 2020 2020 2020   = False,.      
+00017060: 2020 6765 745f 616c 6c3a 2062 6f6f 6c20    get_all: bool 
+00017070: 3d20 4661 6c73 652c 0a20 2020 2020 2020  = False,.       
+00017080: 2073 7065 635f 7374 6174 653a 2053 7065   spec_state: Spe
+00017090: 6353 7461 7465 7320 7c20 4e6f 6e65 203d  cStates | None =
+000170a0: 204e 6f6e 652c 0a20 2020 2020 2020 202a   None,.        *
+000170b0: 2a6b 7761 7267 733a 2041 6e79 2c0a 2020  *kwargs: Any,.  
+000170c0: 2020 2920 2d3e 2064 6963 745b 7374 722c    ) -> dict[str,
+000170d0: 2041 6e79 5d20 7c20 4765 6e65 7261 746f   Any] | Generato
+000170e0: 723a 0a20 2020 2020 2020 2022 2222 4765  r:.        """Ge
+000170f0: 7420 6d65 7461 6461 7461 206f 6620 7374  t metadata of st
+00017100: 6f72 6564 206d 6f64 656c 732e 2049 6620  ored models. If 
+00017110: 6d6f 6465 6c20 6964 2069 7320 6e6f 7420  model id is not 
+00017120: 7370 6563 6966 6965 6420 7265 7475 726e  specified return
+00017130: 7320 616c 6c20 6d6f 6465 6c73 206d 6574  s all models met
+00017140: 6164 6174 612e 0a0a 2020 2020 2020 2020  adata...        
+00017150: 3a70 6172 616d 206d 6f64 656c 5f69 643a  :param model_id:
+00017160: 2073 746f 7265 6420 6d6f 6465 6c2c 2064   stored model, d
+00017170: 6566 696e 6974 696f 6e20 6f72 2070 6970  efinition or pip
+00017180: 656c 696e 6520 4944 0a20 2020 2020 2020  eline ID.       
+00017190: 203a 7479 7065 206d 6f64 656c 5f69 643a   :type model_id:
+000171a0: 2073 7472 2c20 6f70 7469 6f6e 616c 0a0a   str, optional..
+000171b0: 2020 2020 2020 2020 3a70 6172 616d 206c          :param l
+000171c0: 696d 6974 3a20 6c69 6d69 7420 6e75 6d62  imit: limit numb
+000171d0: 6572 206f 6620 6665 7463 6865 6420 7265  er of fetched re
+000171e0: 636f 7264 730a 2020 2020 2020 2020 3a74  cords.        :t
+000171f0: 7970 6520 6c69 6d69 743a 2069 6e74 2c20  ype limit: int, 
+00017200: 6f70 7469 6f6e 616c 0a0a 2020 2020 2020  optional..      
+00017210: 2020 3a70 6172 616d 2061 7379 6e63 6872    :param asynchr
+00017220: 6f6e 6f75 733a 2069 6620 6054 7275 6560  onous: if `True`
+00017230: 2c20 6974 2077 696c 6c20 776f 726b 2061  , it will work a
+00017240: 7320 6120 6765 6e65 7261 746f 720a 2020  s a generator.  
+00017250: 2020 2020 2020 3a74 7970 6520 6173 796e        :type asyn
+00017260: 6368 726f 6e6f 7573 3a20 626f 6f6c 2c20  chronous: bool, 
+00017270: 6f70 7469 6f6e 616c 0a0a 2020 2020 2020  optional..      
+00017280: 2020 3a70 6172 616d 2067 6574 5f61 6c6c    :param get_all
+00017290: 3a20 6966 2060 5472 7565 602c 2069 7420  : if `True`, it 
+000172a0: 7769 6c6c 2067 6574 2061 6c6c 2065 6e74  will get all ent
+000172b0: 7269 6573 2069 6e20 276c 696d 6974 6564  ries in 'limited
+000172c0: 2720 6368 756e 6b73 0a20 2020 2020 2020  ' chunks.       
+000172d0: 203a 7479 7065 2067 6574 5f61 6c6c 3a20   :type get_all: 
+000172e0: 626f 6f6c 2c20 6f70 7469 6f6e 616c 0a0a  bool, optional..
+000172f0: 2020 2020 2020 2020 3a70 6172 616d 2073          :param s
+00017300: 7065 635f 7374 6174 653a 2073 6f66 7477  pec_state: softw
+00017310: 6172 6520 7370 6563 6966 6963 6174 696f  are specificatio
+00017320: 6e20 7374 6174 652c 2063 616e 2062 6520  n state, can be 
+00017330: 7573 6564 206f 6e6c 7920 7768 656e 2060  used only when `
+00017340: 6d6f 6465 6c5f 6964 6020 6973 204e 6f6e  model_id` is Non
+00017350: 650a 2020 2020 2020 2020 3a74 7970 6520  e.        :type 
+00017360: 7370 6563 5f73 7461 7465 3a20 5370 6563  spec_state: Spec
+00017370: 5374 6174 6573 2c20 6f70 7469 6f6e 616c  States, optional
+00017380: 0a0a 2020 2020 2020 2020 3a72 6574 7572  ..        :retur
+00017390: 6e3a 2073 746f 7265 6420 6d6f 6465 6c28  n: stored model(
+000173a0: 7329 206d 6574 6164 6174 610a 2020 2020  s) metadata.    
+000173b0: 2020 2020 3a72 7479 7065 3a20 6469 6374      :rtype: dict
+000173c0: 2028 6966 2049 4420 6973 206e 6f74 204e   (if ID is not N
+000173d0: 6f6e 6529 206f 7220 7b22 7265 736f 7572  one) or {"resour
+000173e0: 6365 7322 3a20 5b64 6963 745d 7d20 2869  ces": [dict]} (i
+000173f0: 6620 4944 2069 7320 4e6f 6e65 290a 0a20  f ID is None).. 
+00017400: 2020 2020 2020 202e 2e20 6e6f 7465 3a3a         .. note::
+00017410: 0a20 2020 2020 2020 2020 2020 2049 6e20  .            In 
+00017420: 6375 7272 656e 7420 696d 706c 656d 656e  current implemen
+00017430: 7461 7469 6f6e 2073 6574 7469 6e67 2060  tation setting `
+00017440: 7370 6563 5f73 7461 7465 3d54 7275 6560  spec_state=True`
+00017450: 206d 6179 2062 7265 616b 2073 6574 2060   may break set `
+00017460: 6c69 6d69 7460 2c0a 2020 2020 2020 2020  limit`,.        
+00017470: 2020 2020 7265 7475 726e 696e 6720 6c65      returning le
+00017480: 7373 2072 6563 6f72 6473 2074 6861 6e20  ss records than 
+00017490: 7374 6174 6564 2062 7920 7365 7420 606c  stated by set `l
+000174a0: 696d 6974 602e 0a0a 2020 2020 2020 2020  imit`...        
+000174b0: 2a2a 4578 616d 706c 652a 2a0a 0a20 2020  **Example**..   
+000174c0: 2020 2020 202e 2e20 636f 6465 2d62 6c6f       .. code-blo
+000174d0: 636b 3a3a 2070 7974 686f 6e0a 0a20 2020  ck:: python..   
+000174e0: 2020 2020 2020 2020 206d 6f64 656c 5f64           model_d
+000174f0: 6574 6169 6c73 203d 2063 6c69 656e 742e  etails = client.
+00017500: 6d6f 6465 6c73 2e67 6574 5f64 6574 6169  models.get_detai
+00017510: 6c73 286d 6f64 656c 5f69 6429 0a20 2020  ls(model_id).   
+00017520: 2020 2020 2020 2020 206d 6f64 656c 735f           models_
+00017530: 6465 7461 696c 7320 3d20 636c 6965 6e74  details = client
+00017540: 2e6d 6f64 656c 732e 6765 745f 6465 7461  .models.get_deta
+00017550: 696c 7328 290a 2020 2020 2020 2020 2020  ils().          
+00017560: 2020 6d6f 6465 6c73 5f64 6574 6169 6c73    models_details
+00017570: 203d 2063 6c69 656e 742e 6d6f 6465 6c73   = client.models
+00017580: 2e67 6574 5f64 6574 6169 6c73 286c 696d  .get_details(lim
+00017590: 6974 3d31 3030 290a 2020 2020 2020 2020  it=100).        
+000175a0: 2020 2020 6d6f 6465 6c73 5f64 6574 6169      models_detai
+000175b0: 6c73 203d 2063 6c69 656e 742e 6d6f 6465  ls = client.mode
+000175c0: 6c73 2e67 6574 5f64 6574 6169 6c73 286c  ls.get_details(l
+000175d0: 696d 6974 3d31 3030 2c20 6765 745f 616c  imit=100, get_al
+000175e0: 6c3d 5472 7565 290a 2020 2020 2020 2020  l=True).        
+000175f0: 2020 2020 6d6f 6465 6c73 5f64 6574 6169      models_detai
+00017600: 6c73 203d 205b 5d0a 2020 2020 2020 2020  ls = [].        
+00017610: 2020 2020 666f 7220 656e 7472 7920 696e      for entry in
+00017620: 2063 6c69 656e 742e 6d6f 6465 6c73 2e67   client.models.g
+00017630: 6574 5f64 6574 6169 6c73 286c 696d 6974  et_details(limit
+00017640: 3d31 3030 2c20 6173 796e 6368 726f 6e6f  =100, asynchrono
+00017650: 7573 3d54 7275 652c 2067 6574 5f61 6c6c  us=True, get_all
+00017660: 3d54 7275 6529 3a0a 2020 2020 2020 2020  =True):.        
+00017670: 2020 2020 2020 2020 6d6f 6465 6c73 5f64          models_d
+00017680: 6574 6169 6c73 2e65 7874 656e 6428 656e  etails.extend(en
+00017690: 7472 7929 0a20 2020 2020 2020 2022 2222  try).        """
+000176a0: 0a20 2020 2020 2020 206d 6f64 656c 5f69  .        model_i
+000176b0: 6420 3d20 5f67 6574 5f69 645f 6672 6f6d  d = _get_id_from
+000176c0: 5f64 6570 7265 6361 7465 645f 7569 6428  _deprecated_uid(
+000176d0: 6b77 6172 6773 2c20 6d6f 6465 6c5f 6964  kwargs, model_id
+000176e0: 2c20 226d 6f64 656c 222c 2054 7275 6529  , "model", True)
+000176f0: 0a20 2020 2020 2020 2069 6620 6c69 6d69  .        if limi
+00017700: 7420 616e 6420 7370 6563 5f73 7461 7465  t and spec_state
+00017710: 3a0a 2020 2020 2020 2020 2020 2020 7761  :.            wa
+00017720: 726e 696e 6773 2e77 6172 6e28 0a20 2020  rnings.warn(.   
+00017730: 2020 2020 2020 2020 2020 2020 2028 0a20               (. 
+00017740: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017750: 2020 2022 5761 726e 696e 673a 2049 6e20     "Warning: In 
+00017760: 6375 7272 656e 7420 696d 706c 656d 656e  current implemen
+00017770: 7461 7469 6f6e 2073 6574 7469 6e67 2060  tation setting `
+00017780: 7370 6563 5f73 7461 7465 3d54 7275 6560  spec_state=True`
+00017790: 206d 6179 2062 7265 616b 2073 6574 2060   may break set `
+000177a0: 6c69 6d69 7460 2c20 220a 2020 2020 2020  limit`, ".      
+000177b0: 2020 2020 2020 2020 2020 2020 2020 2272                "r
+000177c0: 6574 7572 6e69 6e67 206c 6573 7320 7265  eturning less re
+000177d0: 636f 7264 7320 7468 616e 2073 7461 7465  cords than state
+000177e0: 6420 6279 2073 6574 2060 6c69 6d69 7460  d by set `limit`
+000177f0: 2e22 0a20 2020 2020 2020 2020 2020 2020  .".             
+00017800: 2020 2029 0a20 2020 2020 2020 2020 2020     ).           
+00017810: 2029 0a0a 2020 2020 2020 2020 2323 466f   )..        ##Fo
+00017820: 7220 4350 3444 2c20 6368 6563 6b20 6966  r CP4D, check if
+00017830: 2065 6974 6865 7220 7370 6163 6520 6f72   either space or
+00017840: 2070 726f 6a65 6374 2049 4420 6973 2073   project ID is s
+00017850: 6574 0a20 2020 2020 2020 2073 656c 662e  et.        self.
+00017860: 5f63 6c69 656e 742e 5f63 6865 636b 5f69  _client._check_i
+00017870: 665f 6569 7468 6572 5f69 735f 7365 7428  f_either_is_set(
+00017880: 290a 2020 2020 2020 2020 4d6f 6465 6c73  ).        Models
+00017890: 2e5f 7661 6c69 6461 7465 5f74 7970 6528  ._validate_type(
+000178a0: 6d6f 6465 6c5f 6964 2c20 226d 6f64 656c  model_id, "model
+000178b0: 5f69 6422 2c20 7374 722c 2046 616c 7365  _id", str, False
+000178c0: 290a 2020 2020 2020 2020 4d6f 6465 6c73  ).        Models
+000178d0: 2e5f 7661 6c69 6461 7465 5f74 7970 6528  ._validate_type(
+000178e0: 6c69 6d69 742c 2022 6c69 6d69 7422 2c20  limit, "limit", 
+000178f0: 696e 742c 2046 616c 7365 290a 0a20 2020  int, False)..   
+00017900: 2020 2020 2075 726c 203d 2028 0a20 2020       url = (.   
+00017910: 2020 2020 2020 2020 2073 656c 662e 5f63           self._c
+00017920: 6c69 656e 742e 7365 7276 6963 655f 696e  lient.service_in
+00017930: 7374 616e 6365 2e5f 6872 6566 5f64 6566  stance._href_def
+00017940: 696e 6974 696f 6e73 2e67 6574 5f70 7562  initions.get_pub
+00017950: 6c69 7368 6564 5f6d 6f64 656c 735f 6872  lished_models_hr
+00017960: 6566 2829 0a20 2020 2020 2020 2029 0a0a  ef().        )..
+00017970: 2020 2020 2020 2020 6966 206d 6f64 656c          if model
+00017980: 5f69 6420 6973 204e 6f6e 653a 0a20 2020  _id is None:.   
+00017990: 2020 2020 2020 2020 2066 696c 7465 725f           filter_
+000179a0: 6675 6e63 203d 2028 0a20 2020 2020 2020  func = (.       
+000179b0: 2020 2020 2020 2020 2073 656c 662e 5f67           self._g
+000179c0: 6574 5f66 696c 7465 725f 6675 6e63 5f62  et_filter_func_b
+000179d0: 795f 7370 6563 5f69 6473 280a 2020 2020  y_spec_ids(.    
+000179e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000179f0: 7365 6c66 2e5f 6765 745f 616e 645f 6361  self._get_and_ca
+00017a00: 6368 655f 7370 6563 5f69 6473 5f66 6f72  che_spec_ids_for
+00017a10: 5f73 7461 7465 2873 7065 635f 7374 6174  _state(spec_stat
+00017a20: 6529 0a20 2020 2020 2020 2020 2020 2020  e).             
+00017a30: 2020 2029 0a20 2020 2020 2020 2020 2020     ).           
+00017a40: 2020 2020 2069 6620 7370 6563 5f73 7461       if spec_sta
+00017a50: 7465 0a20 2020 2020 2020 2020 2020 2020  te.             
+00017a60: 2020 2065 6c73 6520 4e6f 6e65 0a20 2020     else None.   
+00017a70: 2020 2020 2020 2020 2029 0a0a 2020 2020           )..    
+00017a80: 2020 2020 2020 2020 7265 7475 726e 2073          return s
+00017a90: 656c 662e 5f67 6574 5f61 7274 6966 6163  elf._get_artifac
+00017aa0: 745f 6465 7461 696c 7328 0a20 2020 2020  t_details(.     
+00017ab0: 2020 2020 2020 2020 2020 2075 726c 2c0a             url,.
+00017ac0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017ad0: 6d6f 6465 6c5f 6964 2c0a 2020 2020 2020  model_id,.      
+00017ae0: 2020 2020 2020 2020 2020 6c69 6d69 742c            limit,
+00017af0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00017b00: 2022 6d6f 6465 6c73 222c 0a20 2020 2020   "models",.     
+00017b10: 2020 2020 2020 2020 2020 205f 6173 796e             _asyn
+00017b20: 633d 6173 796e 6368 726f 6e6f 7573 2c0a  c=asynchronous,.
+00017b30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017b40: 5f61 6c6c 3d67 6574 5f61 6c6c 2c0a 2020  _all=get_all,.  
+00017b50: 2020 2020 2020 2020 2020 2020 2020 5f66                _f
+00017b60: 696c 7465 725f 6675 6e63 3d66 696c 7465  ilter_func=filte
+00017b70: 725f 6675 6e63 2c0a 2020 2020 2020 2020  r_func,.        
+00017b80: 2020 2020 290a 0a20 2020 2020 2020 2065      )..        e
+00017b90: 6c73 653a 0a20 2020 2020 2020 2020 2020  lse:.           
+00017ba0: 2072 6574 7572 6e20 7365 6c66 2e5f 6765   return self._ge
+00017bb0: 745f 6172 7469 6661 6374 5f64 6574 6169  t_artifact_detai
+00017bc0: 6c73 2875 726c 2c20 6d6f 6465 6c5f 6964  ls(url, model_id
+00017bd0: 2c20 6c69 6d69 742c 2022 6d6f 6465 6c73  , limit, "models
+00017be0: 2229 0a0a 2020 2020 4073 7461 7469 636d  ")..    @staticm
+00017bf0: 6574 686f 640a 2020 2020 6465 6620 6765  ethod.    def ge
+00017c00: 745f 6872 6566 286d 6f64 656c 5f64 6574  t_href(model_det
+00017c10: 6169 6c73 3a20 6469 6374 5b73 7472 2c20  ails: dict[str, 
+00017c20: 416e 795d 2920 2d3e 2073 7472 3a0a 2020  Any]) -> str:.  
+00017c30: 2020 2020 2020 2222 2247 6574 2075 726c        """Get url
+00017c40: 206f 6620 7374 6f72 6564 206d 6f64 656c   of stored model
+00017c50: 2e0a 0a20 2020 2020 2020 203a 7061 7261  ...        :para
+00017c60: 6d20 6d6f 6465 6c5f 6465 7461 696c 733a  m model_details:
+00017c70: 2073 746f 7265 6420 6d6f 6465 6c20 6465   stored model de
+00017c80: 7461 696c 730a 2020 2020 2020 2020 3a74  tails.        :t
+00017c90: 7970 6520 6d6f 6465 6c5f 6465 7461 696c  ype model_detail
+00017ca0: 733a 2064 6963 740a 0a20 2020 2020 2020  s: dict..       
+00017cb0: 203a 7265 7475 726e 3a20 7572 6c20 746f   :return: url to
+00017cc0: 2073 746f 7265 6420 6d6f 6465 6c0a 2020   stored model.  
+00017cd0: 2020 2020 2020 3a72 7479 7065 3a20 7374        :rtype: st
+00017ce0: 720a 0a20 2020 2020 2020 202a 2a45 7861  r..        **Exa
+00017cf0: 6d70 6c65 2a2a 0a0a 2020 2020 2020 2020  mple**..        
+00017d00: 2e2e 2063 6f64 652d 626c 6f63 6b3a 3a20  .. code-block:: 
+00017d10: 7079 7468 6f6e 0a0a 2020 2020 2020 2020  python..        
+00017d20: 2020 2020 6d6f 6465 6c5f 7572 6c20 3d20      model_url = 
+00017d30: 636c 6965 6e74 2e6d 6f64 656c 732e 6765  client.models.ge
+00017d40: 745f 6872 6566 286d 6f64 656c 5f64 6574  t_href(model_det
+00017d50: 6169 6c73 290a 2020 2020 2020 2020 2222  ails).        ""
+00017d60: 220a 0a20 2020 2020 2020 204d 6f64 656c  "..        Model
+00017d70: 732e 5f76 616c 6964 6174 655f 7479 7065  s._validate_type
+00017d80: 286d 6f64 656c 5f64 6574 6169 6c73 2c20  (model_details, 
+00017d90: 226d 6f64 656c 5f64 6574 6169 6c73 222c  "model_details",
+00017da0: 206f 626a 6563 742c 2054 7275 6529 0a0a   object, True)..
+00017db0: 2020 2020 2020 2020 6966 2022 6173 7365          if "asse
+00017dc0: 745f 6964 2220 696e 206d 6f64 656c 5f64  t_id" in model_d
+00017dd0: 6574 6169 6c73 5b22 6d65 7461 6461 7461  etails["metadata
+00017de0: 225d 3a0a 2020 2020 2020 2020 2020 2020  "]:.            
+00017df0: 7265 7475 726e 2057 4d4c 5265 736f 7572  return WMLResour
+00017e00: 6365 2e5f 6765 745f 7265 7175 6972 6564  ce._get_required
+00017e10: 5f65 6c65 6d65 6e74 5f66 726f 6d5f 6469  _element_from_di
+00017e20: 6374 280a 2020 2020 2020 2020 2020 2020  ct(.            
+00017e30: 2020 2020 6d6f 6465 6c5f 6465 7461 696c      model_detail
+00017e40: 732c 2022 6d6f 6465 6c5f 6465 7461 696c  s, "model_detail
+00017e50: 7322 2c20 5b22 6d65 7461 6461 7461 222c  s", ["metadata",
+00017e60: 2022 6872 6566 225d 0a20 2020 2020 2020   "href"].       
+00017e70: 2020 2020 2029 0a20 2020 2020 2020 2065       ).        e
+00017e80: 6c73 653a 0a20 2020 2020 2020 2020 2020  lse:.           
+00017e90: 2069 6620 2269 6422 206e 6f74 2069 6e20   if "id" not in 
+00017ea0: 6d6f 6465 6c5f 6465 7461 696c 735b 226d  model_details["m
+00017eb0: 6574 6164 6174 6122 5d3a 0a20 2020 2020  etadata"]:.     
+00017ec0: 2020 2020 2020 2020 2020 204d 6f64 656c             Model
+00017ed0: 732e 5f76 616c 6964 6174 655f 7479 7065  s._validate_type
+00017ee0: 5f6f 665f 6465 7461 696c 7328 6d6f 6465  _of_details(mode
+00017ef0: 6c5f 6465 7461 696c 732c 204d 4f44 454c  l_details, MODEL
+00017f00: 5f44 4554 4149 4c53 5f54 5950 4529 0a20  _DETAILS_TYPE). 
+00017f10: 2020 2020 2020 2020 2020 2020 2020 2072                 r
+00017f20: 6574 7572 6e20 574d 4c52 6573 6f75 7263  eturn WMLResourc
+00017f30: 652e 5f67 6574 5f72 6571 7569 7265 645f  e._get_required_
+00017f40: 656c 656d 656e 745f 6672 6f6d 5f64 6963  element_from_dic
+00017f50: 7428 0a20 2020 2020 2020 2020 2020 2020  t(.             
+00017f60: 2020 2020 2020 206d 6f64 656c 5f64 6574         model_det
+00017f70: 6169 6c73 2c20 226d 6f64 656c 5f64 6574  ails, "model_det
+00017f80: 6169 6c73 222c 205b 226d 6574 6164 6174  ails", ["metadat
+00017f90: 6122 2c20 2268 7265 6622 5d0a 2020 2020  a", "href"].    
+00017fa0: 2020 2020 2020 2020 2020 2020 290a 2020              ).  
+00017fb0: 2020 2020 2020 2020 2020 656c 7365 3a0a            else:.
+00017fc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017fd0: 6d6f 6465 6c5f 6964 203d 2057 4d4c 5265  model_id = WMLRe
+00017fe0: 736f 7572 6365 2e5f 6765 745f 7265 7175  source._get_requ
+00017ff0: 6972 6564 5f65 6c65 6d65 6e74 5f66 726f  ired_element_fro
+00018000: 6d5f 6469 6374 280a 2020 2020 2020 2020  m_dict(.        
+00018010: 2020 2020 2020 2020 2020 2020 6d6f 6465              mode
+00018020: 6c5f 6465 7461 696c 732c 2022 6d6f 6465  l_details, "mode
+00018030: 6c5f 6465 7461 696c 7322 2c20 5b22 6d65  l_details", ["me
+00018040: 7461 6461 7461 222c 2022 6964 225d 0a20  tadata", "id"]. 
+00018050: 2020 2020 2020 2020 2020 2020 2020 2029                 )
+00018060: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00018070: 2072 6574 7572 6e20 222f 6d6c 2f76 342f   return "/ml/v4/
+00018080: 6d6f 6465 6c73 2f22 202b 206d 6f64 656c  models/" + model
+00018090: 5f69 640a 0a20 2020 2040 7374 6174 6963  _id..    @static
+000180a0: 6d65 7468 6f64 0a20 2020 2064 6566 2067  method.    def g
+000180b0: 6574 5f75 6964 286d 6f64 656c 5f64 6574  et_uid(model_det
+000180c0: 6169 6c73 3a20 6469 6374 5b73 7472 2c20  ails: dict[str, 
+000180d0: 416e 795d 2920 2d3e 2073 7472 3a0a 2020  Any]) -> str:.  
+000180e0: 2020 2020 2020 2222 2247 6574 2075 6964        """Get uid
+000180f0: 206f 6620 7374 6f72 6564 206d 6f64 656c   of stored model
+00018100: 2e0a 0a20 2020 2020 2020 202a 4465 7072  ...        *Depr
+00018110: 6563 6174 6564 3a2a 2055 7365 2060 6067  ecated:* Use ``g
+00018120: 6574 5f69 6428 6d6f 6465 6c5f 6465 7461  et_id(model_deta
+00018130: 696c 7329 6060 2069 6e73 7465 6164 2e0a  ils)`` instead..
+00018140: 0a20 2020 2020 2020 203a 7061 7261 6d20  .        :param 
+00018150: 6d6f 6465 6c5f 6465 7461 696c 733a 2073  model_details: s
+00018160: 746f 7265 6420 6d6f 6465 6c20 6465 7461  tored model deta
+00018170: 696c 730a 2020 2020 2020 2020 3a74 7970  ils.        :typ
+00018180: 6520 6d6f 6465 6c5f 6465 7461 696c 733a  e model_details:
+00018190: 2064 6963 740a 0a20 2020 2020 2020 203a   dict..        :
+000181a0: 7265 7475 726e 3a20 7569 6420 6f66 2073  return: uid of s
+000181b0: 746f 7265 6420 6d6f 6465 6c0a 2020 2020  tored model.    
+000181c0: 2020 2020 3a72 7479 7065 3a20 7374 720a      :rtype: str.
+000181d0: 0a20 2020 2020 2020 202a 2a45 7861 6d70  .        **Examp
+000181e0: 6c65 2a2a 0a0a 2020 2020 2020 2020 2e2e  le**..        ..
+000181f0: 2063 6f64 652d 626c 6f63 6b3a 3a20 7079   code-block:: py
+00018200: 7468 6f6e 0a0a 2020 2020 2020 2020 2020  thon..          
+00018210: 2020 6d6f 6465 6c5f 7569 6420 3d20 636c    model_uid = cl
+00018220: 6965 6e74 2e6d 6f64 656c 732e 6765 745f  ient.models.get_
+00018230: 7569 6428 6d6f 6465 6c5f 6465 7461 696c  uid(model_detail
+00018240: 7329 0a20 2020 2020 2020 2022 2222 0a20  s).        """. 
+00018250: 2020 2020 2020 2077 6172 6e69 6e67 732e         warnings.
+00018260: 7761 726e 280a 2020 2020 2020 2020 2020  warn(.          
+00018270: 2020 2254 6869 7320 6d65 7468 6f64 2069    "This method i
+00018280: 7320 6465 7072 6563 6174 6564 2c20 706c  s deprecated, pl
+00018290: 6561 7365 2075 7365 204d 6f64 656c 732e  ease use Models.
+000182a0: 6765 745f 6964 286d 6f64 656c 5f64 6574  get_id(model_det
+000182b0: 6169 6c73 2920 696e 7374 6561 6422 0a20  ails) instead". 
+000182c0: 2020 2020 2020 2029 0a20 2020 2020 2020         ).       
+000182d0: 2072 6574 7572 6e20 4d6f 6465 6c73 2e67   return Models.g
+000182e0: 6574 5f69 6428 6d6f 6465 6c5f 6465 7461  et_id(model_deta
+000182f0: 696c 7329 0a0a 2020 2020 4073 7461 7469  ils)..    @stati
+00018300: 636d 6574 686f 640a 2020 2020 6465 6620  cmethod.    def 
+00018310: 6765 745f 6964 286d 6f64 656c 5f64 6574  get_id(model_det
+00018320: 6169 6c73 3a20 6469 6374 5b73 7472 2c20  ails: dict[str, 
+00018330: 416e 795d 2920 2d3e 2073 7472 3a0a 2020  Any]) -> str:.  
+00018340: 2020 2020 2020 2222 2247 6574 2069 6420        """Get id 
+00018350: 6f66 2073 746f 7265 6420 6d6f 6465 6c2e  of stored model.
+00018360: 0a0a 2020 2020 2020 2020 3a70 6172 616d  ..        :param
+00018370: 206d 6f64 656c 5f64 6574 6169 6c73 3a20   model_details: 
+00018380: 7374 6f72 6564 206d 6f64 656c 2064 6574  stored model det
+00018390: 6169 6c73 0a20 2020 2020 2020 203a 7479  ails.        :ty
+000183a0: 7065 206d 6f64 656c 5f64 6574 6169 6c73  pe model_details
+000183b0: 3a20 6469 6374 0a0a 2020 2020 2020 2020  : dict..        
+000183c0: 3a72 6574 7572 6e3a 2069 6420 6f66 2073  :return: id of s
+000183d0: 746f 7265 6420 6d6f 6465 6c0a 2020 2020  tored model.    
+000183e0: 2020 2020 3a72 7479 7065 3a20 7374 720a      :rtype: str.
+000183f0: 0a20 2020 2020 2020 202a 2a45 7861 6d70  .        **Examp
+00018400: 6c65 2a2a 0a0a 2020 2020 2020 2020 2e2e  le**..        ..
+00018410: 2063 6f64 652d 626c 6f63 6b3a 3a20 7079   code-block:: py
+00018420: 7468 6f6e 0a0a 2020 2020 2020 2020 2020  thon..          
+00018430: 2020 6d6f 6465 6c5f 6964 203d 2063 6c69    model_id = cli
+00018440: 656e 742e 6d6f 6465 6c73 2e67 6574 5f69  ent.models.get_i
+00018450: 6428 6d6f 6465 6c5f 6465 7461 696c 7329  d(model_details)
+00018460: 0a20 2020 2020 2020 2022 2222 0a20 2020  .        """.   
+00018470: 2020 2020 204d 6f64 656c 732e 5f76 616c       Models._val
+00018480: 6964 6174 655f 7479 7065 286d 6f64 656c  idate_type(model
+00018490: 5f64 6574 6169 6c73 2c20 226d 6f64 656c  _details, "model
+000184a0: 5f64 6574 6169 6c73 222c 206f 626a 6563  _details", objec
+000184b0: 742c 2054 7275 6529 0a0a 2020 2020 2020  t, True)..      
+000184c0: 2020 6966 2022 6173 7365 745f 6964 2220    if "asset_id" 
+000184d0: 696e 206d 6f64 656c 5f64 6574 6169 6c73  in model_details
+000184e0: 5b22 6d65 7461 6461 7461 225d 3a0a 2020  ["metadata"]:.  
+000184f0: 2020 2020 2020 2020 2020 7265 7475 726e            return
+00018500: 2057 4d4c 5265 736f 7572 6365 2e5f 6765   WMLResource._ge
+00018510: 745f 7265 7175 6972 6564 5f65 6c65 6d65  t_required_eleme
+00018520: 6e74 5f66 726f 6d5f 6469 6374 280a 2020  nt_from_dict(.  
+00018530: 2020 2020 2020 2020 2020 2020 2020 6d6f                mo
+00018540: 6465 6c5f 6465 7461 696c 732c 2022 6d6f  del_details, "mo
+00018550: 6465 6c5f 6465 7461 696c 7322 2c20 5b22  del_details", ["
+00018560: 6d65 7461 6461 7461 222c 2022 6173 7365  metadata", "asse
+00018570: 745f 6964 225d 0a20 2020 2020 2020 2020  t_id"].         
+00018580: 2020 2029 0a20 2020 2020 2020 2065 6c73     ).        els
+00018590: 653a 0a20 2020 2020 2020 2020 2020 2069  e:.            i
+000185a0: 6620 2269 6422 206e 6f74 2069 6e20 6d6f  f "id" not in mo
+000185b0: 6465 6c5f 6465 7461 696c 735b 226d 6574  del_details["met
+000185c0: 6164 6174 6122 5d3a 0a20 2020 2020 2020  adata"]:.       
+000185d0: 2020 2020 2020 2020 204d 6f64 656c 732e           Models.
+000185e0: 5f76 616c 6964 6174 655f 7479 7065 5f6f  _validate_type_o
+000185f0: 665f 6465 7461 696c 7328 6d6f 6465 6c5f  f_details(model_
+00018600: 6465 7461 696c 732c 204d 4f44 454c 5f44  details, MODEL_D
+00018610: 4554 4149 4c53 5f54 5950 4529 0a20 2020  ETAILS_TYPE).   
+00018620: 2020 2020 2020 2020 2020 2020 2072 6574               ret
+00018630: 7572 6e20 574d 4c52 6573 6f75 7263 652e  urn WMLResource.
+00018640: 5f67 6574 5f72 6571 7569 7265 645f 656c  _get_required_el
+00018650: 656d 656e 745f 6672 6f6d 5f64 6963 7428  ement_from_dict(
+00018660: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00018670: 2020 2020 206d 6f64 656c 5f64 6574 6169       model_detai
+00018680: 6c73 2c20 226d 6f64 656c 5f64 6574 6169  ls, "model_detai
+00018690: 6c73 222c 205b 226d 6574 6164 6174 6122  ls", ["metadata"
+000186a0: 2c20 2267 7569 6422 5d0a 2020 2020 2020  , "guid"].      
+000186b0: 2020 2020 2020 2020 2020 290a 2020 2020            ).    
+000186c0: 2020 2020 2020 2020 656c 7365 3a0a 2020          else:.  
+000186d0: 2020 2020 2020 2020 2020 2020 2020 7265                re
+000186e0: 7475 726e 2057 4d4c 5265 736f 7572 6365  turn WMLResource
+000186f0: 2e5f 6765 745f 7265 7175 6972 6564 5f65  ._get_required_e
+00018700: 6c65 6d65 6e74 5f66 726f 6d5f 6469 6374  lement_from_dict
+00018710: 280a 2020 2020 2020 2020 2020 2020 2020  (.              
+00018720: 2020 2020 2020 6d6f 6465 6c5f 6465 7461        model_deta
+00018730: 696c 732c 2022 6d6f 6465 6c5f 6465 7461  ils, "model_deta
+00018740: 696c 7322 2c20 5b22 6d65 7461 6461 7461  ils", ["metadata
+00018750: 222c 2022 6964 225d 0a20 2020 2020 2020  ", "id"].       
+00018760: 2020 2020 2020 2020 2029 0a0a 2020 2020           )..    
+00018770: 6465 6620 6c69 7374 280a 2020 2020 2020  def list(.      
+00018780: 2020 7365 6c66 2c0a 2020 2020 2020 2020    self,.        
+00018790: 6c69 6d69 743a 2069 6e74 207c 204e 6f6e  limit: int | Non
+000187a0: 6520 3d20 4e6f 6e65 2c0a 2020 2020 2020  e = None,.      
+000187b0: 2020 6173 796e 6368 726f 6e6f 7573 3a20    asynchronous: 
+000187c0: 626f 6f6c 203d 2046 616c 7365 2c0a 2020  bool = False,.  
+000187d0: 2020 2020 2020 6765 745f 616c 6c3a 2062        get_all: b
+000187e0: 6f6f 6c20 3d20 4661 6c73 652c 0a20 2020  ool = False,.   
+000187f0: 2029 202d 3e20 7061 6e64 6173 2e44 6174   ) -> pandas.Dat
+00018800: 6146 7261 6d65 207c 2047 656e 6572 6174  aFrame | Generat
+00018810: 6f72 3a0a 2020 2020 2020 2020 2222 224c  or:.        """L
+00018820: 6973 7473 2073 746f 7265 6420 6d6f 6465  ists stored mode
+00018830: 6c73 2069 6e20 6120 7461 626c 6520 666f  ls in a table fo
+00018840: 726d 6174 2e20 4966 206c 696d 6974 2069  rmat. If limit i
+00018850: 7320 7365 7420 746f 204e 6f6e 6520 7468  s set to None th
+00018860: 6572 6520 7769 6c6c 2062 6520 6f6e 6c79  ere will be only
+00018870: 2066 6972 7374 2035 3020 7265 636f 7264   first 50 record
+00018880: 7320 7368 6f77 6e2e 0a0a 2020 2020 2020  s shown...      
+00018890: 2020 3a70 6172 616d 206c 696d 6974 3a20    :param limit: 
+000188a0: 6c69 6d69 7420 6e75 6d62 6572 206f 6620  limit number of 
+000188b0: 6665 7463 6865 6420 7265 636f 7264 730a  fetched records.
+000188c0: 2020 2020 2020 2020 3a74 7970 6520 6c69          :type li
+000188d0: 6d69 743a 2069 6e74 2c20 6f70 7469 6f6e  mit: int, option
+000188e0: 616c 0a0a 2020 2020 2020 2020 3a70 6172  al..        :par
+000188f0: 616d 2061 7379 6e63 6872 6f6e 6f75 733a  am asynchronous:
+00018900: 2069 6620 6054 7275 6560 2c20 6974 2077   if `True`, it w
+00018910: 696c 6c20 776f 726b 2061 7320 6120 6765  ill work as a ge
+00018920: 6e65 7261 746f 720a 2020 2020 2020 2020  nerator.        
+00018930: 3a74 7970 6520 6173 796e 6368 726f 6e6f  :type asynchrono
+00018940: 7573 3a20 626f 6f6c 2c20 6f70 7469 6f6e  us: bool, option
+00018950: 616c 0a0a 2020 2020 2020 2020 3a70 6172  al..        :par
+00018960: 616d 2067 6574 5f61 6c6c 3a20 6966 2060  am get_all: if `
+00018970: 5472 7565 602c 2069 7420 7769 6c6c 2067  True`, it will g
+00018980: 6574 2061 6c6c 2065 6e74 7269 6573 2069  et all entries i
+00018990: 6e20 276c 696d 6974 6564 2720 6368 756e  n 'limited' chun
+000189a0: 6b73 0a20 2020 2020 2020 203a 7479 7065  ks.        :type
+000189b0: 2067 6574 5f61 6c6c 3a20 626f 6f6c 2c20   get_all: bool, 
+000189c0: 6f70 7469 6f6e 616c 0a0a 2020 2020 2020  optional..      
+000189d0: 2020 3a72 6574 7572 6e3a 2070 616e 6461    :return: panda
+000189e0: 732e 4461 7461 4672 616d 6520 7769 7468  s.DataFrame with
+000189f0: 206c 6973 7465 6420 6d6f 6465 6c73 206f   listed models o
+00018a00: 7220 6765 6e65 7261 746f 7220 6966 2060  r generator if `
+00018a10: 6173 796e 6368 726f 6e6f 7573 6020 6973  asynchronous` is
+00018a20: 2073 6574 2074 6f20 5472 7565 0a20 2020   set to True.   
+00018a30: 2020 2020 203a 7274 7970 653a 2070 616e       :rtype: pan
+00018a40: 6461 732e 4461 7461 4672 616d 6520 7c20  das.DataFrame | 
+00018a50: 4765 6e65 7261 746f 720a 0a20 2020 2020  Generator..     
+00018a60: 2020 202a 2a45 7861 6d70 6c65 2a2a 0a0a     **Example**..
+00018a70: 2020 2020 2020 2020 2e2e 2063 6f64 652d          .. code-
+00018a80: 626c 6f63 6b3a 3a20 7079 7468 6f6e 0a0a  block:: python..
+00018a90: 2020 2020 2020 2020 2020 2020 636c 6965              clie
+00018aa0: 6e74 2e6d 6f64 656c 732e 6c69 7374 2829  nt.models.list()
+00018ab0: 0a20 2020 2020 2020 2020 2020 2063 6c69  .            cli
+00018ac0: 656e 742e 6d6f 6465 6c73 2e6c 6973 7428  ent.models.list(
+00018ad0: 6c69 6d69 743d 3130 3029 0a20 2020 2020  limit=100).     
+00018ae0: 2020 2020 2020 2063 6c69 656e 742e 6d6f         client.mo
+00018af0: 6465 6c73 2e6c 6973 7428 6c69 6d69 743d  dels.list(limit=
+00018b00: 3130 302c 2067 6574 5f61 6c6c 3d54 7275  100, get_all=Tru
+00018b10: 6529 0a20 2020 2020 2020 2020 2020 205b  e).            [
+00018b20: 656e 7472 7920 666f 7220 656e 7472 7920  entry for entry 
+00018b30: 696e 2063 6c69 656e 742e 6d6f 6465 6c73  in client.models
+00018b40: 2e6c 6973 7428 6c69 6d69 743d 3130 302c  .list(limit=100,
+00018b50: 2061 7379 6e63 6872 6f6e 6f75 733d 5472   asynchronous=Tr
+00018b60: 7565 2c20 6765 745f 616c 6c3d 5472 7565  ue, get_all=True
+00018b70: 295d 0a20 2020 2020 2020 2022 2222 0a0a  )].        """..
+00018b80: 2020 2020 2020 2020 2323 466f 7220 4350          ##For CP
+00018b90: 3444 2c20 6368 6563 6b20 6966 2065 6974  4D, check if eit
+00018ba0: 6865 7220 7370 6365 206f 7220 7072 6f6a  her spce or proj
+00018bb0: 6563 7420 4944 2069 7320 7365 740a 2020  ect ID is set.  
+00018bc0: 2020 2020 2020 6465 6620 7072 6f63 6573        def proces
+00018bd0: 735f 7265 736f 7572 6365 7328 7365 6c66  s_resources(self
+00018be0: 3a20 416e 792c 206d 6f64 656c 5f72 6573  : Any, model_res
+00018bf0: 6f75 7263 6573 3a20 6469 6374 2920 2d3e  ources: dict) ->
+00018c00: 2070 616e 6461 732e 4461 7461 4672 616d   pandas.DataFram
+00018c10: 653a 0a20 2020 2020 2020 2020 2020 206d  e:.            m
+00018c20: 6f64 656c 5f72 6573 6f75 7263 6573 203d  odel_resources =
+00018c30: 206d 6f64 656c 5f72 6573 6f75 7263 6573   model_resources
+00018c40: 5b22 7265 736f 7572 6365 7322 5d0a 2020  ["resources"].  
+00018c50: 2020 2020 2020 2020 2020 7377 5f73 7065            sw_spe
+00018c60: 635f 696e 666f 203d 207b 0a20 2020 2020  c_info = {.     
+00018c70: 2020 2020 2020 2020 2020 2073 5b22 6964             s["id
+00018c80: 225d 3a20 730a 2020 2020 2020 2020 2020  "]: s.          
+00018c90: 2020 2020 2020 666f 7220 7320 696e 2073        for s in s
+00018ca0: 656c 662e 5f63 6c69 656e 742e 736f 6674  elf._client.soft
+00018cb0: 7761 7265 5f73 7065 6369 6669 6361 7469  ware_specificati
+00018cc0: 6f6e 732e 6765 745f 6465 7461 696c 7328  ons.get_details(
+00018cd0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00018ce0: 2020 2020 2073 7461 7465 5f69 6e66 6f3d       state_info=
+00018cf0: 5472 7565 0a20 2020 2020 2020 2020 2020  True.           
+00018d00: 2020 2020 2029 5b22 7265 736f 7572 6365       )["resource
+00018d10: 7322 5d0a 2020 2020 2020 2020 2020 2020  s"].            
+00018d20: 7d0a 0a20 2020 2020 2020 2020 2020 2064  }..            d
+00018d30: 6566 2067 6574 5f73 7065 635f 696e 666f  ef get_spec_info
+00018d40: 2873 7065 635f 6964 3a20 7374 722c 2070  (spec_id: str, p
+00018d50: 726f 703a 2073 7472 2920 2d3e 2073 7472  rop: str) -> str
+00018d60: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
+00018d70: 2020 6966 2073 7065 635f 6964 2061 6e64    if spec_id and
+00018d80: 2073 7065 635f 6964 2069 6e20 7377 5f73   spec_id in sw_s
+00018d90: 7065 635f 696e 666f 3a0a 2020 2020 2020  pec_info:.      
+00018da0: 2020 2020 2020 2020 2020 2020 2020 7265                re
+00018db0: 7475 726e 2073 775f 7370 6563 5f69 6e66  turn sw_spec_inf
+00018dc0: 6f5b 7370 6563 5f69 645d 2e67 6574 2870  o[spec_id].get(p
+00018dd0: 726f 702c 2022 2229 0a20 2020 2020 2020  rop, "").       
+00018de0: 2020 2020 2020 2020 2065 6c73 653a 0a20           else:. 
+00018df0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00018e00: 2020 2072 6574 7572 6e20 2222 0a0a 2020     return ""..  
+00018e10: 2020 2020 2020 2020 2020 6d6f 6465 6c5f            model_
+00018e20: 7661 6c75 6573 203d 205b 0a20 2020 2020  values = [.     
+00018e30: 2020 2020 2020 2020 2020 2028 0a20 2020             (.   
+00018e40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00018e50: 206d 5b22 6d65 7461 6461 7461 225d 5b22   m["metadata"]["
+00018e60: 6964 225d 2c0a 2020 2020 2020 2020 2020  id"],.          
+00018e70: 2020 2020 2020 2020 2020 6d5b 226d 6574            m["met
+00018e80: 6164 6174 6122 5d5b 226e 616d 6522 5d2c  adata"]["name"],
+00018e90: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00018ea0: 2020 2020 206d 5b22 6d65 7461 6461 7461       m["metadata
+00018eb0: 225d 5b22 6372 6561 7465 645f 6174 225d  "]["created_at"]
+00018ec0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00018ed0: 2020 2020 2020 6d5b 2265 6e74 6974 7922        m["entity"
+00018ee0: 5d5b 2274 7970 6522 5d2c 0a20 2020 2020  ]["type"],.     
+00018ef0: 2020 2020 2020 2020 2020 2020 2020 2067                 g
+00018f00: 6574 5f73 7065 635f 696e 666f 280a 2020  et_spec_info(.  
 00018f10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00018f20: 6966 2073 7065 635f 6964 2061 6e64 2073  if spec_id and s
-00018f30: 7065 635f 6964 2069 6e20 7377 5f73 7065  pec_id in sw_spe
-00018f40: 635f 696e 666f 3a0a 2020 2020 2020 2020  c_info:.        
-00018f50: 2020 2020 2020 2020 2020 2020 7265 7475              retu
-00018f60: 726e 2073 775f 7370 6563 5f69 6e66 6f5b  rn sw_spec_info[
-00018f70: 7370 6563 5f69 645d 2e67 6574 2870 726f  spec_id].get(pro
-00018f80: 702c 2022 2229 0a20 2020 2020 2020 2020  p, "").         
-00018f90: 2020 2020 2020 2065 6c73 653a 0a20 2020         else:.   
+00018f20: 2020 2020 2020 6d5b 2265 6e74 6974 7922        m["entity"
+00018f30: 5d2e 6765 7428 2273 6f66 7477 6172 655f  ].get("software_
+00018f40: 7370 6563 222c 207b 7d29 2e67 6574 2822  spec", {}).get("
+00018f50: 6964 2229 2c20 2273 7461 7465 220a 2020  id"), "state".  
+00018f60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00018f70: 2020 292c 0a20 2020 2020 2020 2020 2020    ),.           
+00018f80: 2020 2020 2020 2020 2067 6574 5f73 7065           get_spe
+00018f90: 635f 696e 666f 280a 2020 2020 2020 2020  c_info(.        
 00018fa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00018fb0: 2072 6574 7572 6e20 2222 0a0a 2020 2020   return ""..    
-00018fc0: 2020 2020 2020 2020 6d6f 6465 6c5f 7661          model_va
-00018fd0: 6c75 6573 203d 205b 0a20 2020 2020 2020  lues = [.       
-00018fe0: 2020 2020 2020 2020 2028 0a20 2020 2020           (.     
-00018ff0: 2020 2020 2020 2020 2020 2020 2020 206d                 m
-00019000: 5b22 6d65 7461 6461 7461 225d 5b22 6964  ["metadata"]["id
-00019010: 225d 2c0a 2020 2020 2020 2020 2020 2020  "],.            
-00019020: 2020 2020 2020 2020 6d5b 226d 6574 6164          m["metad
-00019030: 6174 6122 5d5b 226e 616d 6522 5d2c 0a20  ata"]["name"],. 
-00019040: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019050: 2020 206d 5b22 6d65 7461 6461 7461 225d     m["metadata"]
-00019060: 5b22 6372 6561 7465 645f 6174 225d 2c0a  ["created_at"],.
-00019070: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019080: 2020 2020 6d5b 2265 6e74 6974 7922 5d5b      m["entity"][
-00019090: 2274 7970 6522 5d2c 0a20 2020 2020 2020  "type"],.       
-000190a0: 2020 2020 2020 2020 2020 2020 2067 6574               get
-000190b0: 5f73 7065 635f 696e 666f 280a 2020 2020  _spec_info(.    
-000190c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000190d0: 2020 2020 6d5b 2265 6e74 6974 7922 5d2e      m["entity"].
-000190e0: 6765 7428 2273 6f66 7477 6172 655f 7370  get("software_sp
-000190f0: 6563 222c 207b 7d29 2e67 6574 2822 6964  ec", {}).get("id
-00019100: 2229 2c20 2273 7461 7465 220a 2020 2020  "), "state".    
-00019110: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019120: 292c 0a20 2020 2020 2020 2020 2020 2020  ),.             
-00019130: 2020 2020 2020 2067 6574 5f73 7065 635f         get_spec_
-00019140: 696e 666f 280a 2020 2020 2020 2020 2020  info(.          
-00019150: 2020 2020 2020 2020 2020 2020 2020 6d5b                m[
-00019160: 2265 6e74 6974 7922 5d2e 6765 7428 2273  "entity"].get("s
-00019170: 6f66 7477 6172 655f 7370 6563 222c 207b  oftware_spec", {
-00019180: 7d29 2e67 6574 2822 6964 2229 2c20 2272  }).get("id"), "r
-00019190: 6570 6c61 6365 6d65 6e74 220a 2020 2020  eplacement".    
-000191a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000191b0: 292c 0a20 2020 2020 2020 2020 2020 2020  ),.             
-000191c0: 2020 2029 0a20 2020 2020 2020 2020 2020     ).           
-000191d0: 2020 2020 2066 6f72 206d 2069 6e20 6d6f       for m in mo
-000191e0: 6465 6c5f 7265 736f 7572 6365 730a 2020  del_resources.  
-000191f0: 2020 2020 2020 2020 2020 5d0a 0a20 2020            ]..   
-00019200: 2020 2020 2020 2020 2074 6162 6c65 203d           table =
-00019210: 2073 656c 662e 5f6c 6973 7428 0a20 2020   self._list(.   
-00019220: 2020 2020 2020 2020 2020 2020 206d 6f64               mod
-00019230: 656c 5f76 616c 7565 732c 0a20 2020 2020  el_values,.     
-00019240: 2020 2020 2020 2020 2020 205b 2249 4422             ["ID"
-00019250: 2c20 224e 414d 4522 2c20 2243 5245 4154  , "NAME", "CREAT
-00019260: 4544 222c 2022 5459 5045 222c 2022 5350  ED", "TYPE", "SP
-00019270: 4543 5f53 5441 5445 222c 2022 5350 4543  EC_STATE", "SPEC
-00019280: 5f52 4550 4c41 4345 4d45 4e54 225d 2c0a  _REPLACEMENT"],.
-00019290: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000192a0: 6c69 6d69 742c 0a20 2020 2020 2020 2020  limit,.         
-000192b0: 2020 2020 2020 205f 4445 4641 554c 545f         _DEFAULT_
-000192c0: 4c49 5354 5f4c 454e 4754 482c 0a20 2020  LIST_LENGTH,.   
-000192d0: 2020 2020 2020 2020 2029 0a20 2020 2020           ).     
-000192e0: 2020 2020 2020 2072 6574 7572 6e20 7461         return ta
-000192f0: 626c 650a 0a20 2020 2020 2020 2073 656c  ble..        sel
-00019300: 662e 5f63 6c69 656e 742e 5f63 6865 636b  f._client._check
-00019310: 5f69 665f 6569 7468 6572 5f69 735f 7365  _if_either_is_se
-00019320: 7428 290a 2020 2020 2020 2020 6966 2061  t().        if a
-00019330: 7379 6e63 6872 6f6e 6f75 733a 0a20 2020  synchronous:.   
-00019340: 2020 2020 2020 2020 2072 6574 7572 6e20           return 
-00019350: 280a 2020 2020 2020 2020 2020 2020 2020  (.              
-00019360: 2020 7072 6f63 6573 735f 7265 736f 7572    process_resour
-00019370: 6365 7328 7365 6c66 2c20 6d6f 6465 6c5f  ces(self, model_
-00019380: 7265 736f 7572 6365 7329 2020 2320 7479  resources)  # ty
-00019390: 7065 3a20 6967 6e6f 7265 5b61 7267 2d74  pe: ignore[arg-t
-000193a0: 7970 655d 0a20 2020 2020 2020 2020 2020  ype].           
-000193b0: 2020 2020 2066 6f72 206d 6f64 656c 5f72       for model_r
-000193c0: 6573 6f75 7263 6573 2069 6e20 7365 6c66  esources in self
-000193d0: 2e67 6574 5f64 6574 6169 6c73 280a 2020  .get_details(.  
-000193e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000193f0: 2020 6c69 6d69 743d 6c69 6d69 742c 2061    limit=limit, a
-00019400: 7379 6e63 6872 6f6e 6f75 733d 6173 796e  synchronous=asyn
-00019410: 6368 726f 6e6f 7573 2c20 6765 745f 616c  chronous, get_al
-00019420: 6c3d 6765 745f 616c 6c0a 2020 2020 2020  l=get_all.      
-00019430: 2020 2020 2020 2020 2020 290a 2020 2020            ).    
-00019440: 2020 2020 2020 2020 290a 0a20 2020 2020          )..     
-00019450: 2020 2065 6c73 653a 0a20 2020 2020 2020     else:.       
-00019460: 2020 2020 206d 6f64 656c 5f72 6573 6f75       model_resou
-00019470: 7263 6573 203d 2073 656c 662e 6765 745f  rces = self.get_
-00019480: 6465 7461 696c 7328 6c69 6d69 743d 6c69  details(limit=li
-00019490: 6d69 742c 2067 6574 5f61 6c6c 3d67 6574  mit, get_all=get
-000194a0: 5f61 6c6c 290a 2020 2020 2020 2020 2020  _all).          
-000194b0: 2020 7265 7475 726e 2070 726f 6365 7373    return process
-000194c0: 5f72 6573 6f75 7263 6573 2873 656c 662c  _resources(self,
-000194d0: 206d 6f64 656c 5f72 6573 6f75 7263 6573   model_resources
-000194e0: 290a 0a20 2020 2064 6566 2063 7265 6174  )..    def creat
-000194f0: 655f 7265 7669 7369 6f6e 280a 2020 2020  e_revision(.    
-00019500: 2020 2020 7365 6c66 2c20 6d6f 6465 6c5f      self, model_
-00019510: 6964 3a20 7374 7220 7c20 4e6f 6e65 203d  id: str | None =
-00019520: 204e 6f6e 652c 202a 2a6b 7761 7267 733a   None, **kwargs:
-00019530: 2041 6e79 0a20 2020 2029 202d 3e20 6469   Any.    ) -> di
-00019540: 6374 5b73 7472 2c20 416e 795d 3a0a 2020  ct[str, Any]:.  
-00019550: 2020 2020 2020 2222 2243 7265 6174 6520        """Create 
-00019560: 7265 7669 7369 6f6e 2066 6f72 2074 6865  revision for the
-00019570: 2067 6976 656e 206d 6f64 656c 2069 642e   given model id.
-00019580: 0a0a 2020 2020 2020 2020 3a70 6172 616d  ..        :param
-00019590: 206d 6f64 656c 5f69 643a 2073 746f 7265   model_id: store
-000195a0: 6420 6d6f 6465 6c20 4944 0a20 2020 2020  d model ID.     
-000195b0: 2020 203a 7479 7065 206d 6f64 656c 5f69     :type model_i
-000195c0: 643a 2073 7472 0a0a 2020 2020 2020 2020  d: str..        
-000195d0: 3a72 6574 7572 6e3a 2073 746f 7265 6420  :return: stored 
-000195e0: 6d6f 6465 6c20 7265 7669 7369 6f6e 7320  model revisions 
-000195f0: 6d65 7461 6461 7461 0a20 2020 2020 2020  metadata.       
-00019600: 203a 7274 7970 653a 2064 6963 740a 0a20   :rtype: dict.. 
-00019610: 2020 2020 2020 202a 2a45 7861 6d70 6c65         **Example
-00019620: 2a2a 0a0a 2020 2020 2020 2020 2e2e 2063  **..        .. c
-00019630: 6f64 652d 626c 6f63 6b3a 3a20 7079 7468  ode-block:: pyth
-00019640: 6f6e 0a0a 2020 2020 2020 2020 2020 2020  on..            
-00019650: 6d6f 6465 6c5f 6465 7461 696c 7320 3d20  model_details = 
-00019660: 636c 6965 6e74 2e6d 6f64 656c 732e 6372  client.models.cr
-00019670: 6561 7465 5f72 6576 6973 696f 6e28 6d6f  eate_revision(mo
-00019680: 6465 6c5f 6964 290a 2020 2020 2020 2020  del_id).        
-00019690: 2222 220a 2020 2020 2020 2020 2323 466f  """.        ##Fo
-000196a0: 7220 4350 3444 2c20 6368 6563 6b20 6966  r CP4D, check if
-000196b0: 2065 6974 6865 7220 7370 6365 206f 7220   either spce or 
-000196c0: 7072 6f6a 6563 7420 4944 2069 7320 7365  project ID is se
-000196d0: 740a 2020 2020 2020 2020 6d6f 6465 6c5f  t.        model_
-000196e0: 6964 203d 205f 6765 745f 6964 5f66 726f  id = _get_id_fro
-000196f0: 6d5f 6465 7072 6563 6174 6564 5f75 6964  m_deprecated_uid
-00019700: 286b 7761 7267 732c 206d 6f64 656c 5f69  (kwargs, model_i
-00019710: 642c 2022 6d6f 6465 6c22 290a 2020 2020  d, "model").    
-00019720: 2020 2020 7365 6c66 2e5f 636c 6965 6e74      self._client
-00019730: 2e5f 6368 6563 6b5f 6966 5f65 6974 6865  ._check_if_eithe
-00019740: 725f 6973 5f73 6574 2829 0a20 2020 2020  r_is_set().     
-00019750: 2020 204d 6f64 656c 732e 5f76 616c 6964     Models._valid
-00019760: 6174 655f 7479 7065 286d 6f64 656c 5f69  ate_type(model_i
-00019770: 642c 2022 6d6f 6465 6c5f 6964 222c 2073  d, "model_id", s
-00019780: 7472 2c20 4661 6c73 6529 0a0a 2020 2020  tr, False)..    
-00019790: 2020 2020 7572 6c20 3d20 280a 2020 2020      url = (.    
-000197a0: 2020 2020 2020 2020 7365 6c66 2e5f 636c          self._cl
-000197b0: 6965 6e74 2e73 6572 7669 6365 5f69 6e73  ient.service_ins
-000197c0: 7461 6e63 652e 5f68 7265 665f 6465 6669  tance._href_defi
-000197d0: 6e69 7469 6f6e 732e 6765 745f 7075 626c  nitions.get_publ
-000197e0: 6973 6865 645f 6d6f 6465 6c73 5f68 7265  ished_models_hre
-000197f0: 6628 290a 2020 2020 2020 2020 290a 2020  f().        ).  
-00019800: 2020 2020 2020 7265 7475 726e 2073 656c        return sel
-00019810: 662e 5f63 7265 6174 655f 7265 7669 7369  f._create_revisi
-00019820: 6f6e 5f61 7274 6966 6163 7428 7572 6c2c  on_artifact(url,
-00019830: 206d 6f64 656c 5f69 642c 2022 6d6f 6465   model_id, "mode
-00019840: 6c73 2229 0a0a 2020 2020 6465 6620 6c69  ls")..    def li
-00019850: 7374 5f72 6576 6973 696f 6e73 280a 2020  st_revisions(.  
-00019860: 2020 2020 2020 7365 6c66 2c20 6d6f 6465        self, mode
-00019870: 6c5f 6964 3a20 7374 7220 7c20 4e6f 6e65  l_id: str | None
-00019880: 203d 204e 6f6e 652c 206c 696d 6974 3a20   = None, limit: 
-00019890: 696e 7420 7c20 4e6f 6e65 203d 204e 6f6e  int | None = Non
-000198a0: 652c 202a 2a6b 7761 7267 733a 2041 6e79  e, **kwargs: Any
-000198b0: 0a20 2020 2029 202d 3e20 7061 6e64 6173  .    ) -> pandas
-000198c0: 2e44 6174 6146 7261 6d65 3a0a 2020 2020  .DataFrame:.    
-000198d0: 2020 2020 2222 2250 7269 6e74 2061 6c6c      """Print all
-000198e0: 2072 6576 6973 696f 6e20 666f 7220 7468   revision for th
-000198f0: 6520 6769 7665 6e20 6d6f 6465 6c20 6964  e given model id
-00019900: 2069 6e20 6120 7461 626c 6520 666f 726d   in a table form
-00019910: 6174 2e0a 0a20 2020 2020 2020 203a 7061  at...        :pa
-00019920: 7261 6d20 6d6f 6465 6c5f 6964 3a20 556e  ram model_id: Un
-00019930: 6971 7565 2069 6420 6f66 2073 746f 7265  ique id of store
-00019940: 6420 6d6f 6465 6c0a 2020 2020 2020 2020  d model.        
-00019950: 3a74 7970 6520 6d6f 6465 6c5f 6964 3a20  :type model_id: 
-00019960: 7374 720a 0a20 2020 2020 2020 203a 7061  str..        :pa
-00019970: 7261 6d20 6c69 6d69 743a 206c 696d 6974  ram limit: limit
-00019980: 206e 756d 6265 7220 6f66 2066 6574 6368   number of fetch
-00019990: 6564 2072 6563 6f72 6473 0a20 2020 2020  ed records.     
-000199a0: 2020 203a 7479 7065 206c 696d 6974 3a20     :type limit: 
-000199b0: 696e 742c 206f 7074 696f 6e61 6c0a 0a20  int, optional.. 
-000199c0: 2020 2020 2020 203a 7265 7475 726e 3a20         :return: 
-000199d0: 7061 6e64 6173 2e44 6174 6146 7261 6d65  pandas.DataFrame
-000199e0: 2077 6974 6820 6c69 7374 6564 2072 6576   with listed rev
-000199f0: 6973 696f 6e73 0a20 2020 2020 2020 203a  isions.        :
-00019a00: 7274 7970 653a 2070 616e 6461 732e 4461  rtype: pandas.Da
-00019a10: 7461 4672 616d 650a 0a20 2020 2020 2020  taFrame..       
-00019a20: 202a 2a45 7861 6d70 6c65 2a2a 0a0a 2020   **Example**..  
-00019a30: 2020 2020 2020 2e2e 2063 6f64 652d 626c        .. code-bl
-00019a40: 6f63 6b3a 3a20 7079 7468 6f6e 0a0a 2020  ock:: python..  
-00019a50: 2020 2020 2020 2020 2020 636c 6965 6e74            client
-00019a60: 2e6d 6f64 656c 732e 6c69 7374 5f72 6576  .models.list_rev
-00019a70: 6973 696f 6e73 286d 6f64 656c 5f69 6429  isions(model_id)
-00019a80: 0a20 2020 2020 2020 2022 2222 0a20 2020  .        """.   
-00019a90: 2020 2020 206d 6f64 656c 5f69 6420 3d20       model_id = 
-00019aa0: 5f67 6574 5f69 645f 6672 6f6d 5f64 6570  _get_id_from_dep
-00019ab0: 7265 6361 7465 645f 7569 6428 6b77 6172  recated_uid(kwar
-00019ac0: 6773 2c20 6d6f 6465 6c5f 6964 2c20 226d  gs, model_id, "m
-00019ad0: 6f64 656c 2229 0a20 2020 2020 2020 2023  odel").        #
-00019ae0: 2346 6f72 2043 5034 442c 2063 6865 636b  #For CP4D, check
-00019af0: 2069 6620 6569 7468 6572 2073 7063 6520   if either spce 
-00019b00: 6f72 2070 726f 6a65 6374 2049 4420 6973  or project ID is
-00019b10: 2073 6574 0a20 2020 2020 2020 2073 656c   set.        sel
-00019b20: 662e 5f63 6c69 656e 742e 5f63 6865 636b  f._client._check
-00019b30: 5f69 665f 6569 7468 6572 5f69 735f 7365  _if_either_is_se
-00019b40: 7428 290a 0a20 2020 2020 2020 204d 6f64  t()..        Mod
-00019b50: 656c 732e 5f76 616c 6964 6174 655f 7479  els._validate_ty
-00019b60: 7065 286d 6f64 656c 5f69 642c 2022 6d6f  pe(model_id, "mo
-00019b70: 6465 6c5f 6964 222c 2073 7472 2c20 5472  del_id", str, Tr
-00019b80: 7565 290a 0a20 2020 2020 2020 2075 726c  ue)..        url
-00019b90: 203d 2028 0a20 2020 2020 2020 2020 2020   = (.           
-00019ba0: 2073 656c 662e 5f63 6c69 656e 742e 7365   self._client.se
-00019bb0: 7276 6963 655f 696e 7374 616e 6365 2e5f  rvice_instance._
-00019bc0: 6872 6566 5f64 6566 696e 6974 696f 6e73  href_definitions
-00019bd0: 2e67 6574 5f70 7562 6c69 7368 6564 5f6d  .get_published_m
-00019be0: 6f64 656c 735f 6872 6566 2829 0a20 2020  odels_href().   
-00019bf0: 2020 2020 2020 2020 202b 2022 2f22 0a20           + "/". 
-00019c00: 2020 2020 2020 2020 2020 202b 206d 6f64             + mod
-00019c10: 656c 5f69 640a 2020 2020 2020 2020 290a  el_id.        ).
-00019c20: 2020 2020 2020 2020 6d6f 6465 6c5f 7265          model_re
-00019c30: 736f 7572 6365 7320 3d20 7365 6c66 2e5f  sources = self._
-00019c40: 6765 745f 6172 7469 6661 6374 5f64 6574  get_artifact_det
-00019c50: 6169 6c73 280a 2020 2020 2020 2020 2020  ails(.          
-00019c60: 2020 7572 6c2c 2022 7265 7669 7369 6f6e    url, "revision
-00019c70: 7322 2c20 6c69 6d69 742c 2022 6d6f 6465  s", limit, "mode
-00019c80: 6c20 7265 7669 7369 6f6e 7322 0a20 2020  l revisions".   
-00019c90: 2020 2020 2029 5b22 7265 736f 7572 6365       )["resource
-00019ca0: 7322 5d0a 2020 2020 2020 2020 6d6f 6465  s"].        mode
-00019cb0: 6c5f 7661 6c75 6573 203d 205b 0a20 2020  l_values = [.   
-00019cc0: 2020 2020 2020 2020 2028 6d5b 226d 6574           (m["met
-00019cd0: 6164 6174 6122 5d5b 2272 6576 225d 2c20  adata"]["rev"], 
-00019ce0: 6d5b 226d 6574 6164 6174 6122 5d5b 226e  m["metadata"]["n
-00019cf0: 616d 6522 5d2c 206d 5b22 6d65 7461 6461  ame"], m["metada
-00019d00: 7461 225d 5b22 6372 6561 7465 645f 6174  ta"]["created_at
-00019d10: 225d 290a 2020 2020 2020 2020 2020 2020  "]).            
-00019d20: 666f 7220 6d20 696e 206d 6f64 656c 5f72  for m in model_r
-00019d30: 6573 6f75 7263 6573 0a20 2020 2020 2020  esources.       
-00019d40: 205d 0a0a 2020 2020 2020 2020 7461 626c   ]..        tabl
-00019d50: 6520 3d20 7365 6c66 2e5f 6c69 7374 280a  e = self._list(.
-00019d60: 2020 2020 2020 2020 2020 2020 6d6f 6465              mode
-00019d70: 6c5f 7661 6c75 6573 2c20 5b22 5245 5622  l_values, ["REV"
-00019d80: 2c20 224e 414d 4522 2c20 2243 5245 4154  , "NAME", "CREAT
-00019d90: 4544 225d 2c20 6c69 6d69 742c 205f 4445  ED"], limit, _DE
-00019da0: 4641 554c 545f 4c49 5354 5f4c 454e 4754  FAULT_LIST_LENGT
-00019db0: 480a 2020 2020 2020 2020 290a 2020 2020  H.        ).    
-00019dc0: 2020 2020 7265 7475 726e 2074 6162 6c65      return table
-00019dd0: 0a0a 2020 2020 6465 6620 6765 745f 7265  ..    def get_re
-00019de0: 7669 7369 6f6e 5f64 6574 6169 6c73 280a  vision_details(.
-00019df0: 2020 2020 2020 2020 7365 6c66 2c20 6d6f          self, mo
-00019e00: 6465 6c5f 6964 3a20 7374 7220 7c20 4e6f  del_id: str | No
-00019e10: 6e65 203d 204e 6f6e 652c 2072 6576 5f69  ne = None, rev_i
-00019e20: 643a 2073 7472 207c 204e 6f6e 6520 3d20  d: str | None = 
-00019e30: 4e6f 6e65 2c20 2a2a 6b77 6172 6773 3a20  None, **kwargs: 
-00019e40: 416e 790a 2020 2020 2920 2d3e 2064 6963  Any.    ) -> dic
-00019e50: 745b 7374 722c 2041 6e79 5d3a 0a20 2020  t[str, Any]:.   
-00019e60: 2020 2020 2022 2222 4765 7420 6d65 7461       """Get meta
-00019e70: 6461 7461 206f 6620 7374 6f72 6564 206d  data of stored m
-00019e80: 6f64 656c 7320 7370 6563 6966 6963 2072  odels specific r
-00019e90: 6576 6973 696f 6e2e 0a0a 2020 2020 2020  evision...      
-00019ea0: 2020 3a70 6172 616d 206d 6f64 656c 5f69    :param model_i
-00019eb0: 643a 2073 746f 7265 6420 6d6f 6465 6c2c  d: stored model,
-00019ec0: 2064 6566 696e 6974 696f 6e20 6f72 2070   definition or p
-00019ed0: 6970 656c 696e 6520 4944 0a20 2020 2020  ipeline ID.     
-00019ee0: 2020 203a 7479 7065 206d 6f64 656c 5f69     :type model_i
-00019ef0: 643a 2073 7472 0a0a 2020 2020 2020 2020  d: str..        
-00019f00: 3a70 6172 616d 2072 6576 5f69 643a 2055  :param rev_id: U
-00019f10: 6e69 7175 6520 4964 206f 6620 7468 6520  nique Id of the 
-00019f20: 7374 6f72 6564 206d 6f64 656c 2072 6576  stored model rev
-00019f30: 6973 696f 6e0a 2020 2020 2020 2020 3a74  ision.        :t
-00019f40: 7970 6520 7265 765f 6964 3a20 7374 720a  ype rev_id: str.
-00019f50: 0a20 2020 2020 2020 203a 7265 7475 726e  .        :return
-00019f60: 3a20 7374 6f72 6564 206d 6f64 656c 2873  : stored model(s
-00019f70: 2920 6d65 7461 6461 7461 0a20 2020 2020  ) metadata.     
-00019f80: 2020 203a 7274 7970 653a 2064 6963 740a     :rtype: dict.
-00019f90: 0a20 2020 2020 2020 202a 2a45 7861 6d70  .        **Examp
-00019fa0: 6c65 2a2a 0a0a 2020 2020 2020 2020 2e2e  le**..        ..
-00019fb0: 2063 6f64 652d 626c 6f63 6b3a 3a20 7079   code-block:: py
-00019fc0: 7468 6f6e 0a0a 2020 2020 2020 2020 2020  thon..          
-00019fd0: 2020 6d6f 6465 6c5f 6465 7461 696c 7320    model_details 
-00019fe0: 3d20 636c 6965 6e74 2e6d 6f64 656c 732e  = client.models.
-00019ff0: 6765 745f 7265 7669 7369 6f6e 5f64 6574  get_revision_det
-0001a000: 6169 6c73 286d 6f64 656c 5f69 642c 2072  ails(model_id, r
-0001a010: 6576 5f69 6429 0a20 2020 2020 2020 2022  ev_id).        "
-0001a020: 2222 0a20 2020 2020 2020 206d 6f64 656c  "".        model
-0001a030: 5f69 6420 3d20 5f67 6574 5f69 645f 6672  _id = _get_id_fr
-0001a040: 6f6d 5f64 6570 7265 6361 7465 645f 7569  om_deprecated_ui
-0001a050: 6428 6b77 6172 6773 2c20 6d6f 6465 6c5f  d(kwargs, model_
-0001a060: 6964 2c20 226d 6f64 656c 2229 0a20 2020  id, "model").   
-0001a070: 2020 2020 2072 6576 5f69 6420 3d20 5f67       rev_id = _g
-0001a080: 6574 5f69 645f 6672 6f6d 5f64 6570 7265  et_id_from_depre
-0001a090: 6361 7465 645f 7569 6428 6b77 6172 6773  cated_uid(kwargs
-0001a0a0: 2c20 7265 765f 6964 2c20 2272 6576 2229  , rev_id, "rev")
-0001a0b0: 0a0a 2020 2020 2020 2020 6966 2069 7369  ..        if isi
-0001a0c0: 6e73 7461 6e63 6528 7265 765f 6964 2c20  nstance(rev_id, 
-0001a0d0: 696e 7429 3a0a 2020 2020 2020 2020 2020  int):.          
-0001a0e0: 2020 7761 726e 696e 6773 2e77 6172 6e28    warnings.warn(
-0001a0f0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0001a100: 2022 6072 6576 5f69 6460 2070 6172 616d   "`rev_id` param
-0001a110: 6574 6572 2074 7970 6520 6173 2069 6e74  eter type as int
-0001a120: 2069 7320 6465 7072 6563 6174 6564 2c20   is deprecated, 
-0001a130: 706c 6561 7365 2063 6f6e 7665 7274 2074  please convert t
-0001a140: 6f20 7374 7220 696e 7374 6561 6422 2c0a  o str instead",.
-0001a150: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001a160: 6361 7465 676f 7279 3d44 6570 7265 6361  category=Depreca
-0001a170: 7469 6f6e 5761 726e 696e 672c 0a20 2020  tionWarning,.   
-0001a180: 2020 2020 2020 2020 2029 0a20 2020 2020           ).     
-0001a190: 2020 2020 2020 2072 6576 5f69 6420 3d20         rev_id = 
-0001a1a0: 7374 7228 7265 765f 6964 290a 0a20 2020  str(rev_id)..   
-0001a1b0: 2020 2020 2023 2346 6f72 2043 5034 442c       ##For CP4D,
-0001a1c0: 2063 6865 636b 2069 6620 6569 7468 6572   check if either
-0001a1d0: 2073 7063 6520 6f72 2070 726f 6a65 6374   spce or project
-0001a1e0: 2049 4420 6973 2073 6574 0a20 2020 2020   ID is set.     
-0001a1f0: 2020 2073 656c 662e 5f63 6c69 656e 742e     self._client.
-0001a200: 5f63 6865 636b 5f69 665f 6569 7468 6572  _check_if_either
-0001a210: 5f69 735f 7365 7428 290a 2020 2020 2020  _is_set().      
-0001a220: 2020 4d6f 6465 6c73 2e5f 7661 6c69 6461    Models._valida
-0001a230: 7465 5f74 7970 6528 6d6f 6465 6c5f 6964  te_type(model_id
-0001a240: 2c20 226d 6f64 656c 5f69 6422 2c20 7374  , "model_id", st
-0001a250: 722c 2054 7275 6529 0a20 2020 2020 2020  r, True).       
-0001a260: 204d 6f64 656c 732e 5f76 616c 6964 6174   Models._validat
-0001a270: 655f 7479 7065 2872 6576 5f69 642c 2022  e_type(rev_id, "
-0001a280: 7265 765f 6964 222c 2073 7472 2c20 5472  rev_id", str, Tr
-0001a290: 7565 290a 0a20 2020 2020 2020 2075 726c  ue)..        url
-0001a2a0: 203d 2028 0a20 2020 2020 2020 2020 2020   = (.           
-0001a2b0: 2073 656c 662e 5f63 6c69 656e 742e 7365   self._client.se
-0001a2c0: 7276 6963 655f 696e 7374 616e 6365 2e5f  rvice_instance._
-0001a2d0: 6872 6566 5f64 6566 696e 6974 696f 6e73  href_definitions
-0001a2e0: 2e67 6574 5f70 7562 6c69 7368 6564 5f6d  .get_published_m
-0001a2f0: 6f64 656c 735f 6872 6566 2829 0a20 2020  odels_href().   
-0001a300: 2020 2020 2020 2020 202b 2022 2f22 0a20           + "/". 
-0001a310: 2020 2020 2020 2020 2020 202b 206d 6f64             + mod
-0001a320: 656c 5f69 640a 2020 2020 2020 2020 290a  el_id.        ).
-0001a330: 2020 2020 2020 2020 7265 7475 726e 2073          return s
-0001a340: 656c 662e 5f67 6574 5f77 6974 685f 6f72  elf._get_with_or
-0001a350: 5f77 6974 686f 7574 5f6c 696d 6974 280a  _without_limit(.
-0001a360: 2020 2020 2020 2020 2020 2020 7572 6c2c              url,
-0001a370: 0a20 2020 2020 2020 2020 2020 206c 696d  .            lim
-0001a380: 6974 3d4e 6f6e 652c 0a20 2020 2020 2020  it=None,.       
-0001a390: 2020 2020 206f 705f 6e61 6d65 3d22 6d6f       op_name="mo
-0001a3a0: 6465 6c22 2c0a 2020 2020 2020 2020 2020  del",.          
-0001a3b0: 2020 7375 6d6d 6172 793d 4e6f 6e65 2c0a    summary=None,.
-0001a3c0: 2020 2020 2020 2020 2020 2020 7072 655f              pre_
-0001a3d0: 6465 6669 6e65 643d 4e6f 6e65 2c0a 2020  defined=None,.  
-0001a3e0: 2020 2020 2020 2020 2020 7265 7669 7369            revisi
-0001a3f0: 6f6e 3d72 6576 5f69 642c 0a20 2020 2020  on=rev_id,.     
-0001a400: 2020 2029 0a0a 2020 2020 6465 6620 7072     )..    def pr
-0001a410: 6f6d 6f74 6528 0a20 2020 2020 2020 2073  omote(.        s
-0001a420: 656c 662c 206d 6f64 656c 5f69 643a 2073  elf, model_id: s
-0001a430: 7472 2c20 736f 7572 6365 5f70 726f 6a65  tr, source_proje
-0001a440: 6374 5f69 643a 2073 7472 2c20 7461 7267  ct_id: str, targ
-0001a450: 6574 5f73 7061 6365 5f69 643a 2073 7472  et_space_id: str
-0001a460: 0a20 2020 2029 202d 3e20 6469 6374 5b73  .    ) -> dict[s
-0001a470: 7472 2c20 416e 795d 207c 204e 6f6e 653a  tr, Any] | None:
-0001a480: 0a20 2020 2020 2020 2022 2222 5072 6f6d  .        """Prom
-0001a490: 6f74 6520 6d6f 6465 6c20 6672 6f6d 2070  ote model from p
-0001a4a0: 726f 6a65 6374 2074 6f20 7370 6163 652e  roject to space.
-0001a4b0: 2053 7570 706f 7274 6564 206f 6e6c 7920   Supported only 
-0001a4c0: 666f 7220 4942 4d20 436c 6f75 6420 5061  for IBM Cloud Pa
-0001a4d0: 6bc2 ae20 666f 7220 4461 7461 2e0a 0a20  k.. for Data... 
-0001a4e0: 2020 2020 2020 202a 4465 7072 6563 6174         *Deprecat
-0001a4f0: 6564 3a2a 2055 7365 2060 636c 6965 6e74  ed:* Use `client
-0001a500: 2e73 7061 6365 732e 7072 6f6d 6f74 6528  .spaces.promote(
-0001a510: 6173 7365 745f 6964 2c20 736f 7572 6365  asset_id, source
-0001a520: 5f70 726f 6a65 6374 5f69 642c 2074 6172  _project_id, tar
-0001a530: 6765 745f 7370 6163 655f 6964 2960 2069  get_space_id)` i
-0001a540: 6e73 7465 6164 2e0a 2020 2020 2020 2020  nstead..        
-0001a550: 2222 220a 2020 2020 2020 2020 7761 726e  """.        warn
-0001a560: 696e 6773 2e77 6172 6e28 0a20 2020 2020  ings.warn(.     
-0001a570: 2020 2020 2020 2022 4e6f 7465 3a20 4675         "Note: Fu
-0001a580: 6e63 7469 6f6e 2060 636c 6965 6e74 2e72  nction `client.r
-0001a590: 6570 6f73 6974 6f72 792e 7072 6f6d 6f74  epository.promot
-0001a5a0: 655f 6d6f 6465 6c28 6d6f 6465 6c5f 6964  e_model(model_id
-0001a5b0: 2c20 736f 7572 6365 5f70 726f 6a65 6374  , source_project
-0001a5c0: 5f69 642c 2074 6172 6765 745f 7370 6163  _id, target_spac
-0001a5d0: 655f 6964 2960 2022 0a20 2020 2020 2020  e_id)` ".       
-0001a5e0: 2020 2020 2022 6861 7320 6265 656e 2064       "has been d
-0001a5f0: 6570 7265 6361 7465 642e 2055 7365 2060  eprecated. Use `
-0001a600: 636c 6965 6e74 2e73 7061 6365 732e 7072  client.spaces.pr
-0001a610: 6f6d 6f74 6528 6173 7365 745f 6964 2c20  omote(asset_id, 
-0001a620: 736f 7572 6365 5f70 726f 6a65 6374 5f69  source_project_i
-0001a630: 642c 2074 6172 6765 745f 7370 6163 655f  d, target_space_
-0001a640: 6964 2960 2069 6e73 7465 6164 2e22 0a20  id)` instead.". 
-0001a650: 2020 2020 2020 2029 0a20 2020 2020 2020         ).       
-0001a660: 2074 7279 3a0a 2020 2020 2020 2020 2020   try:.          
-0001a670: 2020 7265 7475 726e 2073 656c 662e 5f63    return self._c
-0001a680: 6c69 656e 742e 7370 6163 6573 2e70 726f  lient.spaces.pro
-0001a690: 6d6f 7465 280a 2020 2020 2020 2020 2020  mote(.          
-0001a6a0: 2020 2020 2020 6d6f 6465 6c5f 6964 2c20        model_id, 
-0001a6b0: 736f 7572 6365 5f70 726f 6a65 6374 5f69  source_project_i
-0001a6c0: 642c 2074 6172 6765 745f 7370 6163 655f  d, target_space_
-0001a6d0: 6964 0a20 2020 2020 2020 2020 2020 2029  id.            )
-0001a6e0: 0a20 2020 2020 2020 2065 7863 6570 7420  .        except 
-0001a6f0: 5072 6f6d 6f74 696f 6e46 6169 6c65 6420  PromotionFailed 
-0001a700: 6173 2065 3a0a 2020 2020 2020 2020 2020  as e:.          
-0001a710: 2020 7261 6973 6520 4d6f 6465 6c50 726f    raise ModelPro
-0001a720: 6d6f 7469 6f6e 4661 696c 6564 280a 2020  motionFailed(.  
-0001a730: 2020 2020 2020 2020 2020 2020 2020 652e                e.
-0001a740: 7072 6f6a 6563 745f 6964 2c20 652e 7370  project_id, e.sp
-0001a750: 6163 655f 6964 2c20 652e 7072 6f6d 6f74  ace_id, e.promot
-0001a760: 696f 6e5f 7265 7370 6f6e 7365 2c20 7374  ion_response, st
-0001a770: 7228 652e 7265 6173 6f6e 290a 2020 2020  r(e.reason).    
-0001a780: 2020 2020 2020 2020 290a 0a20 2020 2064          )..    d
-0001a790: 6566 205f 7570 6461 7465 5f6d 6f64 656c  ef _update_model
-0001a7a0: 5f63 6f6e 7465 6e74 280a 2020 2020 2020  _content(.      
-0001a7b0: 2020 7365 6c66 2c20 6d6f 6465 6c5f 6964    self, model_id
-0001a7c0: 3a20 7374 722c 2075 7064 6174 6564 5f64  : str, updated_d
-0001a7d0: 6574 6169 6c73 3a20 6469 6374 5b73 7472  etails: dict[str
-0001a7e0: 2c20 416e 795d 2c20 7570 6461 7465 5f6d  , Any], update_m
-0001a7f0: 6f64 656c 3a20 416e 790a 2020 2020 2920  odel: Any.    ) 
-0001a800: 2d3e 204e 6f6e 653a 0a0a 2020 2020 2020  -> None:..      
-0001a810: 2020 6d6f 6465 6c20 3d20 636f 7079 2e63    model = copy.c
-0001a820: 6f70 7928 7570 6461 7465 5f6d 6f64 656c  opy(update_model
-0001a830: 290a 2020 2020 2020 2020 6d6f 6465 6c5f  ).        model_
-0001a840: 7479 7065 203d 2075 7064 6174 6564 5f64  type = updated_d
-0001a850: 6574 6169 6c73 5b22 656e 7469 7479 225d  etails["entity"]
-0001a860: 5b22 7479 7065 225d 0a0a 2020 2020 2020  ["type"]..      
-0001a870: 2020 6465 6620 6973 5f78 6d6c 286d 6f64    def is_xml(mod
-0001a880: 656c 5f66 696c 6570 6174 683a 2073 7472  el_filepath: str
-0001a890: 2920 2d3e 2062 6f6f 6c3a 0a20 2020 2020  ) -> bool:.     
-0001a8a0: 2020 2020 2020 2069 6620 6f73 2e70 6174         if os.pat
-0001a8b0: 682e 7370 6c69 7465 7874 286f 732e 7061  h.splitext(os.pa
-0001a8c0: 7468 2e62 6173 656e 616d 6528 6d6f 6465  th.basename(mode
-0001a8d0: 6c5f 6669 6c65 7061 7468 2929 5b2d 315d  l_filepath))[-1]
-0001a8e0: 203d 3d20 222e 706d 6d6c 223a 0a20 2020   == ".pmml":.   
-0001a8f0: 2020 2020 2020 2020 2020 2020 2072 6169               rai
-0001a900: 7365 2057 4d4c 436c 6965 6e74 4572 726f  se WMLClientErro
-0001a910: 7228 0a20 2020 2020 2020 2020 2020 2020  r(.             
-0001a920: 2020 2020 2020 2022 5468 6520 6669 6c65         "The file
-0001a930: 206e 616d 6520 6861 7320 616e 2075 6e73   name has an uns
-0001a940: 7570 706f 7274 6564 2065 7874 656e 7369  upported extensi
-0001a950: 6f6e 2e20 5265 6e61 6d65 2074 6865 2066  on. Rename the f
-0001a960: 696c 6520 7769 7468 2061 202e 786d 6c20  ile with a .xml 
-0001a970: 6578 7465 6e73 696f 6e2e 220a 2020 2020  extension.".    
-0001a980: 2020 2020 2020 2020 2020 2020 290a 2020              ).  
-0001a990: 2020 2020 2020 2020 2020 7265 7475 726e            return
-0001a9a0: 206f 732e 7061 7468 2e73 706c 6974 6578   os.path.splitex
-0001a9b0: 7428 6f73 2e70 6174 682e 6261 7365 6e61  t(os.path.basena
-0001a9c0: 6d65 286d 6f64 656c 5f66 696c 6570 6174  me(model_filepat
-0001a9d0: 6829 295b 2d31 5d20 3d3d 2022 2e78 6d6c  h))[-1] == ".xml
-0001a9e0: 220a 0a20 2020 2020 2020 2069 6d70 6f72  "..        impor
-0001a9f0: 7420 7461 7266 696c 650a 2020 2020 2020  t tarfile.      
-0001aa00: 2020 696d 706f 7274 207a 6970 6669 6c65    import zipfile
-0001aa10: 0a0a 2020 2020 2020 2020 6d6f 6465 6c5f  ..        model_
-0001aa20: 6669 6c65 7061 7468 203d 206d 6f64 656c  filepath = model
-0001aa30: 0a0a 2020 2020 2020 2020 6966 2022 7363  ..        if "sc
-0001aa40: 696b 6974 2d6c 6561 726e 5f22 2069 6e20  ikit-learn_" in 
-0001aa50: 6d6f 6465 6c5f 7479 7065 206f 7220 226d  model_type or "m
-0001aa60: 6c6c 6962 5f22 2069 6e20 6d6f 6465 6c5f  llib_" in model_
-0001aa70: 7479 7065 3a0a 2020 2020 2020 2020 2020  type:.          
-0001aa80: 2020 6d65 7461 5f70 726f 7073 203d 2075    meta_props = u
-0001aa90: 7064 6174 6564 5f64 6574 6169 6c73 5b22  pdated_details["
-0001aaa0: 656e 7469 7479 225d 0a20 2020 2020 2020  entity"].       
-0001aab0: 2020 2020 206d 6574 615f 6461 7461 203d       meta_data =
-0001aac0: 204d 6574 6150 726f 7073 286d 6574 615f   MetaProps(meta_
-0001aad0: 7072 6f70 7329 0a20 2020 2020 2020 2020  props).         
-0001aae0: 2020 206e 616d 6520 3d20 7570 6461 7465     name = update
-0001aaf0: 645f 6465 7461 696c 735b 226d 6574 6164  d_details["metad
-0001ab00: 6174 6122 5d5b 226e 616d 6522 5d0a 2020  ata"]["name"].  
-0001ab10: 2020 2020 2020 2020 2020 6d6f 6465 6c5f            model_
-0001ab20: 6172 7469 6661 6374 203d 204d 4c52 6570  artifact = MLRep
-0001ab30: 6f73 6974 6f72 7941 7274 6966 6163 7428  ositoryArtifact(
-0001ab40: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0001ab50: 2075 7064 6174 655f 6d6f 6465 6c2c 206e   update_model, n
-0001ab60: 616d 653d 6e61 6d65 2c20 6d65 7461 5f70  ame=name, meta_p
-0001ab70: 726f 7073 3d6d 6574 615f 6461 7461 2c20  rops=meta_data, 
-0001ab80: 7472 6169 6e69 6e67 5f64 6174 613d 4e6f  training_data=No
-0001ab90: 6e65 0a20 2020 2020 2020 2020 2020 2029  ne.            )
-0001aba0: 0a20 2020 2020 2020 2020 2020 206d 6f64  .            mod
-0001abb0: 656c 5f61 7274 6966 6163 742e 7569 6420  el_artifact.uid 
-0001abc0: 3d20 6d6f 6465 6c5f 6964 0a20 2020 2020  = model_id.     
-0001abd0: 2020 2020 2020 2071 7565 7279 5f70 6172         query_par
-0001abe0: 616d 7320 3d20 7365 6c66 2e5f 636c 6965  ams = self._clie
-0001abf0: 6e74 2e5f 7061 7261 6d73 2829 0a20 2020  nt._params().   
-0001ac00: 2020 2020 2020 2020 2071 7565 7279 5f70           query_p
-0001ac10: 6172 616d 732e 7570 6461 7465 287b 2263  arams.update({"c
-0001ac20: 6f6e 7465 6e74 5f66 6f72 6d61 7422 3a20  ontent_format": 
-0001ac30: 226e 6174 6976 6522 7d29 0a20 2020 2020  "native"}).     
-0001ac40: 2020 2020 2020 2073 656c 662e 5f63 6c69         self._cli
-0001ac50: 656e 742e 7265 706f 7369 746f 7279 2e5f  ent.repository._
-0001ac60: 6d6c 5f72 6570 6f73 6974 6f72 795f 636c  ml_repository_cl
-0001ac70: 6965 6e74 2e6d 6f64 656c 732e 7570 6c6f  ient.models.uplo
-0001ac80: 6164 5f63 6f6e 7465 6e74 280a 2020 2020  ad_content(.    
-0001ac90: 2020 2020 2020 2020 2020 2020 6d6f 6465              mode
-0001aca0: 6c5f 6172 7469 6661 6374 2c20 7175 6572  l_artifact, quer
-0001acb0: 795f 7061 7261 6d3d 7175 6572 795f 7061  y_param=query_pa
-0001acc0: 7261 6d73 2c20 6e6f 5f64 656c 6574 653d  rams, no_delete=
-0001acd0: 5472 7565 0a20 2020 2020 2020 2020 2020  True.           
-0001ace0: 2029 0a20 2020 2020 2020 2065 6c73 653a   ).        else:
-0001acf0: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if 
-0001ad00: 280a 2020 2020 2020 2020 2020 2020 2020  (.              
-0001ad10: 2020 286f 732e 7061 7468 2e73 6570 2069    (os.path.sep i
-0001ad20: 6e20 7570 6461 7465 5f6d 6f64 656c 290a  n update_model).
-0001ad30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ad40: 6f72 206f 732e 7061 7468 2e69 7366 696c  or os.path.isfil
-0001ad50: 6528 7570 6461 7465 5f6d 6f64 656c 290a  e(update_model).
-0001ad60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ad70: 6f72 206f 732e 7061 7468 2e69 7364 6972  or os.path.isdir
-0001ad80: 2875 7064 6174 655f 6d6f 6465 6c29 0a20  (update_model). 
-0001ad90: 2020 2020 2020 2020 2020 2029 3a0a 2020             ):.  
-0001ada0: 2020 2020 2020 2020 2020 2020 2020 6966                if
-0001adb0: 206e 6f74 206f 732e 7061 7468 2e69 7366   not os.path.isf
-0001adc0: 696c 6528 7570 6461 7465 5f6d 6f64 656c  ile(update_model
-0001add0: 2920 616e 6420 6e6f 7420 6f73 2e70 6174  ) and not os.pat
-0001ade0: 682e 6973 6469 7228 7570 6461 7465 5f6d  h.isdir(update_m
-0001adf0: 6f64 656c 293a 0a20 2020 2020 2020 2020  odel):.         
-0001ae00: 2020 2020 2020 2020 2020 2072 6169 7365             raise
-0001ae10: 2057 4d4c 436c 6965 6e74 4572 726f 7228   WMLClientError(
-0001ae20: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0001ae30: 2020 2020 2020 2020 2022 496e 7661 6c69           "Invali
-0001ae40: 6420 7061 7468 3a20 6e65 6974 6865 7220  d path: neither 
-0001ae50: 6669 6c65 206e 6f72 2064 6972 6563 746f  file nor directo
-0001ae60: 7279 2065 7869 7374 7320 756e 6465 7220  ry exists under 
-0001ae70: 7468 6973 2070 6174 683a 2027 7b7d 272e  this path: '{}'.
-0001ae80: 222e 666f 726d 6174 280a 2020 2020 2020  ".format(.      
-0001ae90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001aea0: 2020 2020 2020 6d6f 6465 6c0a 2020 2020        model.    
-0001aeb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001aec0: 2020 2020 290a 2020 2020 2020 2020 2020      ).          
-0001aed0: 2020 2020 2020 2020 2020 290a 0a20 2020            )..   
-0001aee0: 2020 2020 2020 2020 2069 6620 6f73 2e70           if os.p
-0001aef0: 6174 682e 6973 6469 7228 6d6f 6465 6c29  ath.isdir(model)
-0001af00: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
-0001af10: 2020 6966 2022 7465 6e73 6f72 666c 6f77    if "tensorflow
-0001af20: 2220 696e 206d 6f64 656c 5f74 7970 653a  " in model_type:
-0001af30: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0001af40: 2020 2020 2023 2054 4f44 4f20 6375 7272       # TODO curr
-0001af50: 656e 746c 7920 7461 722e 677a 2069 7320  ently tar.gz is 
-0001af60: 7265 7175 6972 6564 2066 6f72 2074 656e  required for ten
-0001af70: 736f 7266 6c6f 7720 2d20 7468 6520 7361  sorflow - the sa
-0001af80: 6d65 2065 7874 2073 686f 756c 6420 6265  me ext should be
-0001af90: 2073 7570 706f 7274 6564 2066 6f72 2061   supported for a
-0001afa0: 6c6c 2066 7261 6d65 776f 726b 730a 2020  ll frameworks.  
-0001afb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001afc0: 2020 6966 206f 732e 7061 7468 2e62 6173    if os.path.bas
-0001afd0: 656e 616d 6528 6d6f 6465 6c29 203d 3d20  ename(model) == 
-0001afe0: 2222 3a0a 2020 2020 2020 2020 2020 2020  "":.            
-0001aff0: 2020 2020 2020 2020 2020 2020 6d6f 6465              mode
-0001b000: 6c20 3d20 6f73 2e70 6174 682e 6469 726e  l = os.path.dirn
-0001b010: 616d 6528 7570 6461 7465 5f6d 6f64 656c  ame(update_model
-0001b020: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
-0001b030: 2020 2020 2020 6669 6c65 6e61 6d65 203d        filename =
-0001b040: 206f 732e 7061 7468 2e62 6173 656e 616d   os.path.basenam
-0001b050: 6528 7570 6461 7465 5f6d 6f64 656c 2920  e(update_model) 
-0001b060: 2b20 222e 7461 722e 677a 220a 2020 2020  + ".tar.gz".    
-0001b070: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001b080: 6375 7272 656e 745f 6469 7220 3d20 6f73  current_dir = os
-0001b090: 2e67 6574 6377 6428 290a 2020 2020 2020  .getcwd().      
-0001b0a0: 2020 2020 2020 2020 2020 2020 2020 6f73                os
-0001b0b0: 2e63 6864 6972 286d 6f64 656c 290a 2020  .chdir(model).  
-0001b0c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001b0d0: 2020 7461 7267 6574 5f70 6174 6820 3d20    target_path = 
-0001b0e0: 6f73 2e70 6174 682e 6469 726e 616d 6528  os.path.dirname(
-0001b0f0: 6d6f 6465 6c29 0a0a 2020 2020 2020 2020  model)..        
-0001b100: 2020 2020 2020 2020 2020 2020 7769 7468              with
-0001b110: 2074 6172 6669 6c65 2e6f 7065 6e28 6f73   tarfile.open(os
-0001b120: 2e70 6174 682e 6a6f 696e 2822 2e2e 222c  .path.join("..",
-0001b130: 2066 696c 656e 616d 6529 2c20 6d6f 6465   filename), mode
-0001b140: 3d22 773a 677a 2229 2061 7320 7461 723a  ="w:gz") as tar:
-0001b150: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0001b160: 2020 2020 2020 2020 2074 6172 2e61 6464           tar.add
-0001b170: 2822 2e22 290a 0a20 2020 2020 2020 2020  (".")..         
-0001b180: 2020 2020 2020 2020 2020 206f 732e 6368             os.ch
-0001b190: 6469 7228 6375 7272 656e 745f 6469 7229  dir(current_dir)
-0001b1a0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0001b1b0: 2020 2020 206d 6f64 656c 5f66 696c 6570       model_filep
-0001b1c0: 6174 6820 3d20 6f73 2e70 6174 682e 6a6f  ath = os.path.jo
-0001b1d0: 696e 2874 6172 6765 745f 7061 7468 2c20  in(target_path, 
-0001b1e0: 6669 6c65 6e61 6d65 290a 2020 2020 2020  filename).      
-0001b1f0: 2020 2020 2020 2020 2020 2020 2020 6966                if
-0001b200: 2028 0a20 2020 2020 2020 2020 2020 2020   (.             
-0001b210: 2020 2020 2020 2020 2020 2074 6172 6669             tarfi
-0001b220: 6c65 2e69 735f 7461 7266 696c 6528 6d6f  le.is_tarfile(mo
-0001b230: 6465 6c5f 6669 6c65 7061 7468 290a 2020  del_filepath).  
-0001b240: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001b250: 2020 2020 2020 6f72 207a 6970 6669 6c65        or zipfile
-0001b260: 2e69 735f 7a69 7066 696c 6528 6d6f 6465  .is_zipfile(mode
-0001b270: 6c5f 6669 6c65 7061 7468 290a 2020 2020  l_filepath).    
-0001b280: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001b290: 2020 2020 6f72 2069 735f 786d 6c28 6d6f      or is_xml(mo
-0001b2a0: 6465 6c5f 6669 6c65 7061 7468 290a 2020  del_filepath).  
-0001b2b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001b2c0: 2020 293a 0a20 2020 2020 2020 2020 2020    ):.           
+00018fb0: 6d5b 2265 6e74 6974 7922 5d2e 6765 7428  m["entity"].get(
+00018fc0: 2273 6f66 7477 6172 655f 7370 6563 222c  "software_spec",
+00018fd0: 207b 7d29 2e67 6574 2822 6964 2229 2c20   {}).get("id"), 
+00018fe0: 2272 6570 6c61 6365 6d65 6e74 220a 2020  "replacement".  
+00018ff0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019000: 2020 292c 0a20 2020 2020 2020 2020 2020    ),.           
+00019010: 2020 2020 2029 0a20 2020 2020 2020 2020       ).         
+00019020: 2020 2020 2020 2066 6f72 206d 2069 6e20         for m in 
+00019030: 6d6f 6465 6c5f 7265 736f 7572 6365 730a  model_resources.
+00019040: 2020 2020 2020 2020 2020 2020 5d0a 0a20              ].. 
+00019050: 2020 2020 2020 2020 2020 2074 6162 6c65             table
+00019060: 203d 2073 656c 662e 5f6c 6973 7428 0a20   = self._list(. 
+00019070: 2020 2020 2020 2020 2020 2020 2020 206d                 m
+00019080: 6f64 656c 5f76 616c 7565 732c 0a20 2020  odel_values,.   
+00019090: 2020 2020 2020 2020 2020 2020 205b 2249               ["I
+000190a0: 4422 2c20 224e 414d 4522 2c20 2243 5245  D", "NAME", "CRE
+000190b0: 4154 4544 222c 2022 5459 5045 222c 2022  ATED", "TYPE", "
+000190c0: 5350 4543 5f53 5441 5445 222c 2022 5350  SPEC_STATE", "SP
+000190d0: 4543 5f52 4550 4c41 4345 4d45 4e54 225d  EC_REPLACEMENT"]
+000190e0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+000190f0: 2020 6c69 6d69 742c 0a20 2020 2020 2020    limit,.       
+00019100: 2020 2020 2020 2020 205f 4445 4641 554c           _DEFAUL
+00019110: 545f 4c49 5354 5f4c 454e 4754 482c 0a20  T_LIST_LENGTH,. 
+00019120: 2020 2020 2020 2020 2020 2029 0a20 2020             ).   
+00019130: 2020 2020 2020 2020 2072 6574 7572 6e20           return 
+00019140: 7461 626c 650a 0a20 2020 2020 2020 2073  table..        s
+00019150: 656c 662e 5f63 6c69 656e 742e 5f63 6865  elf._client._che
+00019160: 636b 5f69 665f 6569 7468 6572 5f69 735f  ck_if_either_is_
+00019170: 7365 7428 290a 2020 2020 2020 2020 6966  set().        if
+00019180: 2061 7379 6e63 6872 6f6e 6f75 733a 0a20   asynchronous:. 
+00019190: 2020 2020 2020 2020 2020 2072 6574 7572             retur
+000191a0: 6e20 280a 2020 2020 2020 2020 2020 2020  n (.            
+000191b0: 2020 2020 7072 6f63 6573 735f 7265 736f      process_reso
+000191c0: 7572 6365 7328 7365 6c66 2c20 6d6f 6465  urces(self, mode
+000191d0: 6c5f 7265 736f 7572 6365 7329 2020 2320  l_resources)  # 
+000191e0: 7479 7065 3a20 6967 6e6f 7265 5b61 7267  type: ignore[arg
+000191f0: 2d74 7970 655d 0a20 2020 2020 2020 2020  -type].         
+00019200: 2020 2020 2020 2066 6f72 206d 6f64 656c         for model
+00019210: 5f72 6573 6f75 7263 6573 2069 6e20 7365  _resources in se
+00019220: 6c66 2e67 6574 5f64 6574 6169 6c73 280a  lf.get_details(.
+00019230: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019240: 2020 2020 6c69 6d69 743d 6c69 6d69 742c      limit=limit,
+00019250: 2061 7379 6e63 6872 6f6e 6f75 733d 6173   asynchronous=as
+00019260: 796e 6368 726f 6e6f 7573 2c20 6765 745f  ynchronous, get_
+00019270: 616c 6c3d 6765 745f 616c 6c0a 2020 2020  all=get_all.    
+00019280: 2020 2020 2020 2020 2020 2020 290a 2020              ).  
+00019290: 2020 2020 2020 2020 2020 290a 0a20 2020            )..   
+000192a0: 2020 2020 2065 6c73 653a 0a20 2020 2020       else:.     
+000192b0: 2020 2020 2020 206d 6f64 656c 5f72 6573         model_res
+000192c0: 6f75 7263 6573 203d 2073 656c 662e 6765  ources = self.ge
+000192d0: 745f 6465 7461 696c 7328 6c69 6d69 743d  t_details(limit=
+000192e0: 6c69 6d69 742c 2067 6574 5f61 6c6c 3d67  limit, get_all=g
+000192f0: 6574 5f61 6c6c 290a 2020 2020 2020 2020  et_all).        
+00019300: 2020 2020 7265 7475 726e 2070 726f 6365      return proce
+00019310: 7373 5f72 6573 6f75 7263 6573 2873 656c  ss_resources(sel
+00019320: 662c 206d 6f64 656c 5f72 6573 6f75 7263  f, model_resourc
+00019330: 6573 290a 0a20 2020 2064 6566 2063 7265  es)..    def cre
+00019340: 6174 655f 7265 7669 7369 6f6e 280a 2020  ate_revision(.  
+00019350: 2020 2020 2020 7365 6c66 2c20 6d6f 6465        self, mode
+00019360: 6c5f 6964 3a20 7374 7220 7c20 4e6f 6e65  l_id: str | None
+00019370: 203d 204e 6f6e 652c 202a 2a6b 7761 7267   = None, **kwarg
+00019380: 733a 2041 6e79 0a20 2020 2029 202d 3e20  s: Any.    ) -> 
+00019390: 6469 6374 5b73 7472 2c20 416e 795d 3a0a  dict[str, Any]:.
+000193a0: 2020 2020 2020 2020 2222 2243 7265 6174          """Creat
+000193b0: 6520 7265 7669 7369 6f6e 2066 6f72 2074  e revision for t
+000193c0: 6865 2067 6976 656e 206d 6f64 656c 2069  he given model i
+000193d0: 642e 0a0a 2020 2020 2020 2020 3a70 6172  d...        :par
+000193e0: 616d 206d 6f64 656c 5f69 643a 2073 746f  am model_id: sto
+000193f0: 7265 6420 6d6f 6465 6c20 4944 0a20 2020  red model ID.   
+00019400: 2020 2020 203a 7479 7065 206d 6f64 656c       :type model
+00019410: 5f69 643a 2073 7472 0a0a 2020 2020 2020  _id: str..      
+00019420: 2020 3a72 6574 7572 6e3a 2073 746f 7265    :return: store
+00019430: 6420 6d6f 6465 6c20 7265 7669 7369 6f6e  d model revision
+00019440: 7320 6d65 7461 6461 7461 0a20 2020 2020  s metadata.     
+00019450: 2020 203a 7274 7970 653a 2064 6963 740a     :rtype: dict.
+00019460: 0a20 2020 2020 2020 202a 2a45 7861 6d70  .        **Examp
+00019470: 6c65 2a2a 0a0a 2020 2020 2020 2020 2e2e  le**..        ..
+00019480: 2063 6f64 652d 626c 6f63 6b3a 3a20 7079   code-block:: py
+00019490: 7468 6f6e 0a0a 2020 2020 2020 2020 2020  thon..          
+000194a0: 2020 6d6f 6465 6c5f 6465 7461 696c 7320    model_details 
+000194b0: 3d20 636c 6965 6e74 2e6d 6f64 656c 732e  = client.models.
+000194c0: 6372 6561 7465 5f72 6576 6973 696f 6e28  create_revision(
+000194d0: 6d6f 6465 6c5f 6964 290a 2020 2020 2020  model_id).      
+000194e0: 2020 2222 220a 2020 2020 2020 2020 2323    """.        ##
+000194f0: 466f 7220 4350 3444 2c20 6368 6563 6b20  For CP4D, check 
+00019500: 6966 2065 6974 6865 7220 7370 6365 206f  if either spce o
+00019510: 7220 7072 6f6a 6563 7420 4944 2069 7320  r project ID is 
+00019520: 7365 740a 2020 2020 2020 2020 6d6f 6465  set.        mode
+00019530: 6c5f 6964 203d 205f 6765 745f 6964 5f66  l_id = _get_id_f
+00019540: 726f 6d5f 6465 7072 6563 6174 6564 5f75  rom_deprecated_u
+00019550: 6964 286b 7761 7267 732c 206d 6f64 656c  id(kwargs, model
+00019560: 5f69 642c 2022 6d6f 6465 6c22 290a 2020  _id, "model").  
+00019570: 2020 2020 2020 7365 6c66 2e5f 636c 6965        self._clie
+00019580: 6e74 2e5f 6368 6563 6b5f 6966 5f65 6974  nt._check_if_eit
+00019590: 6865 725f 6973 5f73 6574 2829 0a20 2020  her_is_set().   
+000195a0: 2020 2020 204d 6f64 656c 732e 5f76 616c       Models._val
+000195b0: 6964 6174 655f 7479 7065 286d 6f64 656c  idate_type(model
+000195c0: 5f69 642c 2022 6d6f 6465 6c5f 6964 222c  _id, "model_id",
+000195d0: 2073 7472 2c20 4661 6c73 6529 0a0a 2020   str, False)..  
+000195e0: 2020 2020 2020 7572 6c20 3d20 280a 2020        url = (.  
+000195f0: 2020 2020 2020 2020 2020 7365 6c66 2e5f            self._
+00019600: 636c 6965 6e74 2e73 6572 7669 6365 5f69  client.service_i
+00019610: 6e73 7461 6e63 652e 5f68 7265 665f 6465  nstance._href_de
+00019620: 6669 6e69 7469 6f6e 732e 6765 745f 7075  finitions.get_pu
+00019630: 626c 6973 6865 645f 6d6f 6465 6c73 5f68  blished_models_h
+00019640: 7265 6628 290a 2020 2020 2020 2020 290a  ref().        ).
+00019650: 2020 2020 2020 2020 7265 7475 726e 2073          return s
+00019660: 656c 662e 5f63 7265 6174 655f 7265 7669  elf._create_revi
+00019670: 7369 6f6e 5f61 7274 6966 6163 7428 7572  sion_artifact(ur
+00019680: 6c2c 206d 6f64 656c 5f69 642c 2022 6d6f  l, model_id, "mo
+00019690: 6465 6c73 2229 0a0a 2020 2020 6465 6620  dels")..    def 
+000196a0: 6c69 7374 5f72 6576 6973 696f 6e73 280a  list_revisions(.
+000196b0: 2020 2020 2020 2020 7365 6c66 2c20 6d6f          self, mo
+000196c0: 6465 6c5f 6964 3a20 7374 7220 7c20 4e6f  del_id: str | No
+000196d0: 6e65 203d 204e 6f6e 652c 206c 696d 6974  ne = None, limit
+000196e0: 3a20 696e 7420 7c20 4e6f 6e65 203d 204e  : int | None = N
+000196f0: 6f6e 652c 202a 2a6b 7761 7267 733a 2041  one, **kwargs: A
+00019700: 6e79 0a20 2020 2029 202d 3e20 7061 6e64  ny.    ) -> pand
+00019710: 6173 2e44 6174 6146 7261 6d65 3a0a 2020  as.DataFrame:.  
+00019720: 2020 2020 2020 2222 2250 7269 6e74 2061        """Print a
+00019730: 6c6c 2072 6576 6973 696f 6e20 666f 7220  ll revision for 
+00019740: 7468 6520 6769 7665 6e20 6d6f 6465 6c20  the given model 
+00019750: 6964 2069 6e20 6120 7461 626c 6520 666f  id in a table fo
+00019760: 726d 6174 2e0a 0a20 2020 2020 2020 203a  rmat...        :
+00019770: 7061 7261 6d20 6d6f 6465 6c5f 6964 3a20  param model_id: 
+00019780: 556e 6971 7565 2069 6420 6f66 2073 746f  Unique id of sto
+00019790: 7265 6420 6d6f 6465 6c0a 2020 2020 2020  red model.      
+000197a0: 2020 3a74 7970 6520 6d6f 6465 6c5f 6964    :type model_id
+000197b0: 3a20 7374 720a 0a20 2020 2020 2020 203a  : str..        :
+000197c0: 7061 7261 6d20 6c69 6d69 743a 206c 696d  param limit: lim
+000197d0: 6974 206e 756d 6265 7220 6f66 2066 6574  it number of fet
+000197e0: 6368 6564 2072 6563 6f72 6473 0a20 2020  ched records.   
+000197f0: 2020 2020 203a 7479 7065 206c 696d 6974       :type limit
+00019800: 3a20 696e 742c 206f 7074 696f 6e61 6c0a  : int, optional.
+00019810: 0a20 2020 2020 2020 203a 7265 7475 726e  .        :return
+00019820: 3a20 7061 6e64 6173 2e44 6174 6146 7261  : pandas.DataFra
+00019830: 6d65 2077 6974 6820 6c69 7374 6564 2072  me with listed r
+00019840: 6576 6973 696f 6e73 0a20 2020 2020 2020  evisions.       
+00019850: 203a 7274 7970 653a 2070 616e 6461 732e   :rtype: pandas.
+00019860: 4461 7461 4672 616d 650a 0a20 2020 2020  DataFrame..     
+00019870: 2020 202a 2a45 7861 6d70 6c65 2a2a 0a0a     **Example**..
+00019880: 2020 2020 2020 2020 2e2e 2063 6f64 652d          .. code-
+00019890: 626c 6f63 6b3a 3a20 7079 7468 6f6e 0a0a  block:: python..
+000198a0: 2020 2020 2020 2020 2020 2020 636c 6965              clie
+000198b0: 6e74 2e6d 6f64 656c 732e 6c69 7374 5f72  nt.models.list_r
+000198c0: 6576 6973 696f 6e73 286d 6f64 656c 5f69  evisions(model_i
+000198d0: 6429 0a20 2020 2020 2020 2022 2222 0a20  d).        """. 
+000198e0: 2020 2020 2020 206d 6f64 656c 5f69 6420         model_id 
+000198f0: 3d20 5f67 6574 5f69 645f 6672 6f6d 5f64  = _get_id_from_d
+00019900: 6570 7265 6361 7465 645f 7569 6428 6b77  eprecated_uid(kw
+00019910: 6172 6773 2c20 6d6f 6465 6c5f 6964 2c20  args, model_id, 
+00019920: 226d 6f64 656c 2229 0a20 2020 2020 2020  "model").       
+00019930: 2023 2346 6f72 2043 5034 442c 2063 6865   ##For CP4D, che
+00019940: 636b 2069 6620 6569 7468 6572 2073 7063  ck if either spc
+00019950: 6520 6f72 2070 726f 6a65 6374 2049 4420  e or project ID 
+00019960: 6973 2073 6574 0a20 2020 2020 2020 2073  is set.        s
+00019970: 656c 662e 5f63 6c69 656e 742e 5f63 6865  elf._client._che
+00019980: 636b 5f69 665f 6569 7468 6572 5f69 735f  ck_if_either_is_
+00019990: 7365 7428 290a 0a20 2020 2020 2020 204d  set()..        M
+000199a0: 6f64 656c 732e 5f76 616c 6964 6174 655f  odels._validate_
+000199b0: 7479 7065 286d 6f64 656c 5f69 642c 2022  type(model_id, "
+000199c0: 6d6f 6465 6c5f 6964 222c 2073 7472 2c20  model_id", str, 
+000199d0: 5472 7565 290a 0a20 2020 2020 2020 2075  True)..        u
+000199e0: 726c 203d 2028 0a20 2020 2020 2020 2020  rl = (.         
+000199f0: 2020 2073 656c 662e 5f63 6c69 656e 742e     self._client.
+00019a00: 7365 7276 6963 655f 696e 7374 616e 6365  service_instance
+00019a10: 2e5f 6872 6566 5f64 6566 696e 6974 696f  ._href_definitio
+00019a20: 6e73 2e67 6574 5f70 7562 6c69 7368 6564  ns.get_published
+00019a30: 5f6d 6f64 656c 735f 6872 6566 2829 0a20  _models_href(). 
+00019a40: 2020 2020 2020 2020 2020 202b 2022 2f22             + "/"
+00019a50: 0a20 2020 2020 2020 2020 2020 202b 206d  .            + m
+00019a60: 6f64 656c 5f69 640a 2020 2020 2020 2020  odel_id.        
+00019a70: 290a 2020 2020 2020 2020 6d6f 6465 6c5f  ).        model_
+00019a80: 7265 736f 7572 6365 7320 3d20 7365 6c66  resources = self
+00019a90: 2e5f 6765 745f 6172 7469 6661 6374 5f64  ._get_artifact_d
+00019aa0: 6574 6169 6c73 280a 2020 2020 2020 2020  etails(.        
+00019ab0: 2020 2020 7572 6c2c 2022 7265 7669 7369      url, "revisi
+00019ac0: 6f6e 7322 2c20 6c69 6d69 742c 2022 6d6f  ons", limit, "mo
+00019ad0: 6465 6c20 7265 7669 7369 6f6e 7322 0a20  del revisions". 
+00019ae0: 2020 2020 2020 2029 5b22 7265 736f 7572         )["resour
+00019af0: 6365 7322 5d0a 2020 2020 2020 2020 6d6f  ces"].        mo
+00019b00: 6465 6c5f 7661 6c75 6573 203d 205b 0a20  del_values = [. 
+00019b10: 2020 2020 2020 2020 2020 2028 6d5b 226d             (m["m
+00019b20: 6574 6164 6174 6122 5d5b 2272 6576 225d  etadata"]["rev"]
+00019b30: 2c20 6d5b 226d 6574 6164 6174 6122 5d5b  , m["metadata"][
+00019b40: 226e 616d 6522 5d2c 206d 5b22 6d65 7461  "name"], m["meta
+00019b50: 6461 7461 225d 5b22 6372 6561 7465 645f  data"]["created_
+00019b60: 6174 225d 290a 2020 2020 2020 2020 2020  at"]).          
+00019b70: 2020 666f 7220 6d20 696e 206d 6f64 656c    for m in model
+00019b80: 5f72 6573 6f75 7263 6573 0a20 2020 2020  _resources.     
+00019b90: 2020 205d 0a0a 2020 2020 2020 2020 7461     ]..        ta
+00019ba0: 626c 6520 3d20 7365 6c66 2e5f 6c69 7374  ble = self._list
+00019bb0: 280a 2020 2020 2020 2020 2020 2020 6d6f  (.            mo
+00019bc0: 6465 6c5f 7661 6c75 6573 2c20 5b22 5245  del_values, ["RE
+00019bd0: 5622 2c20 224e 414d 4522 2c20 2243 5245  V", "NAME", "CRE
+00019be0: 4154 4544 225d 2c20 6c69 6d69 742c 205f  ATED"], limit, _
+00019bf0: 4445 4641 554c 545f 4c49 5354 5f4c 454e  DEFAULT_LIST_LEN
+00019c00: 4754 480a 2020 2020 2020 2020 290a 2020  GTH.        ).  
+00019c10: 2020 2020 2020 7265 7475 726e 2074 6162        return tab
+00019c20: 6c65 0a0a 2020 2020 6465 6620 6765 745f  le..    def get_
+00019c30: 7265 7669 7369 6f6e 5f64 6574 6169 6c73  revision_details
+00019c40: 280a 2020 2020 2020 2020 7365 6c66 2c20  (.        self, 
+00019c50: 6d6f 6465 6c5f 6964 3a20 7374 7220 7c20  model_id: str | 
+00019c60: 4e6f 6e65 203d 204e 6f6e 652c 2072 6576  None = None, rev
+00019c70: 5f69 643a 2073 7472 207c 204e 6f6e 6520  _id: str | None 
+00019c80: 3d20 4e6f 6e65 2c20 2a2a 6b77 6172 6773  = None, **kwargs
+00019c90: 3a20 416e 790a 2020 2020 2920 2d3e 2064  : Any.    ) -> d
+00019ca0: 6963 745b 7374 722c 2041 6e79 5d3a 0a20  ict[str, Any]:. 
+00019cb0: 2020 2020 2020 2022 2222 4765 7420 6d65         """Get me
+00019cc0: 7461 6461 7461 206f 6620 7374 6f72 6564  tadata of stored
+00019cd0: 206d 6f64 656c 7320 7370 6563 6966 6963   models specific
+00019ce0: 2072 6576 6973 696f 6e2e 0a0a 2020 2020   revision...    
+00019cf0: 2020 2020 3a70 6172 616d 206d 6f64 656c      :param model
+00019d00: 5f69 643a 2073 746f 7265 6420 6d6f 6465  _id: stored mode
+00019d10: 6c2c 2064 6566 696e 6974 696f 6e20 6f72  l, definition or
+00019d20: 2070 6970 656c 696e 6520 4944 0a20 2020   pipeline ID.   
+00019d30: 2020 2020 203a 7479 7065 206d 6f64 656c       :type model
+00019d40: 5f69 643a 2073 7472 0a0a 2020 2020 2020  _id: str..      
+00019d50: 2020 3a70 6172 616d 2072 6576 5f69 643a    :param rev_id:
+00019d60: 2055 6e69 7175 6520 4964 206f 6620 7468   Unique Id of th
+00019d70: 6520 7374 6f72 6564 206d 6f64 656c 2072  e stored model r
+00019d80: 6576 6973 696f 6e0a 2020 2020 2020 2020  evision.        
+00019d90: 3a74 7970 6520 7265 765f 6964 3a20 7374  :type rev_id: st
+00019da0: 720a 0a20 2020 2020 2020 203a 7265 7475  r..        :retu
+00019db0: 726e 3a20 7374 6f72 6564 206d 6f64 656c  rn: stored model
+00019dc0: 2873 2920 6d65 7461 6461 7461 0a20 2020  (s) metadata.   
+00019dd0: 2020 2020 203a 7274 7970 653a 2064 6963       :rtype: dic
+00019de0: 740a 0a20 2020 2020 2020 202a 2a45 7861  t..        **Exa
+00019df0: 6d70 6c65 2a2a 0a0a 2020 2020 2020 2020  mple**..        
+00019e00: 2e2e 2063 6f64 652d 626c 6f63 6b3a 3a20  .. code-block:: 
+00019e10: 7079 7468 6f6e 0a0a 2020 2020 2020 2020  python..        
+00019e20: 2020 2020 6d6f 6465 6c5f 6465 7461 696c      model_detail
+00019e30: 7320 3d20 636c 6965 6e74 2e6d 6f64 656c  s = client.model
+00019e40: 732e 6765 745f 7265 7669 7369 6f6e 5f64  s.get_revision_d
+00019e50: 6574 6169 6c73 286d 6f64 656c 5f69 642c  etails(model_id,
+00019e60: 2072 6576 5f69 6429 0a20 2020 2020 2020   rev_id).       
+00019e70: 2022 2222 0a20 2020 2020 2020 206d 6f64   """.        mod
+00019e80: 656c 5f69 6420 3d20 5f67 6574 5f69 645f  el_id = _get_id_
+00019e90: 6672 6f6d 5f64 6570 7265 6361 7465 645f  from_deprecated_
+00019ea0: 7569 6428 6b77 6172 6773 2c20 6d6f 6465  uid(kwargs, mode
+00019eb0: 6c5f 6964 2c20 226d 6f64 656c 2229 0a20  l_id, "model"). 
+00019ec0: 2020 2020 2020 2072 6576 5f69 6420 3d20         rev_id = 
+00019ed0: 5f67 6574 5f69 645f 6672 6f6d 5f64 6570  _get_id_from_dep
+00019ee0: 7265 6361 7465 645f 7569 6428 6b77 6172  recated_uid(kwar
+00019ef0: 6773 2c20 7265 765f 6964 2c20 2272 6576  gs, rev_id, "rev
+00019f00: 2229 0a0a 2020 2020 2020 2020 6966 2069  ")..        if i
+00019f10: 7369 6e73 7461 6e63 6528 7265 765f 6964  sinstance(rev_id
+00019f20: 2c20 696e 7429 3a0a 2020 2020 2020 2020  , int):.        
+00019f30: 2020 2020 7761 726e 696e 6773 2e77 6172      warnings.war
+00019f40: 6e28 0a20 2020 2020 2020 2020 2020 2020  n(.             
+00019f50: 2020 2022 6072 6576 5f69 6460 2070 6172     "`rev_id` par
+00019f60: 616d 6574 6572 2074 7970 6520 6173 2069  ameter type as i
+00019f70: 6e74 2069 7320 6465 7072 6563 6174 6564  nt is deprecated
+00019f80: 2c20 706c 6561 7365 2063 6f6e 7665 7274  , please convert
+00019f90: 2074 6f20 7374 7220 696e 7374 6561 6422   to str instead"
+00019fa0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00019fb0: 2020 6361 7465 676f 7279 3d44 6570 7265    category=Depre
+00019fc0: 6361 7469 6f6e 5761 726e 696e 672c 0a20  cationWarning,. 
+00019fd0: 2020 2020 2020 2020 2020 2029 0a20 2020             ).   
+00019fe0: 2020 2020 2020 2020 2072 6576 5f69 6420           rev_id 
+00019ff0: 3d20 7374 7228 7265 765f 6964 290a 0a20  = str(rev_id).. 
+0001a000: 2020 2020 2020 2023 2346 6f72 2043 5034         ##For CP4
+0001a010: 442c 2063 6865 636b 2069 6620 6569 7468  D, check if eith
+0001a020: 6572 2073 7063 6520 6f72 2070 726f 6a65  er spce or proje
+0001a030: 6374 2049 4420 6973 2073 6574 0a20 2020  ct ID is set.   
+0001a040: 2020 2020 2073 656c 662e 5f63 6c69 656e       self._clien
+0001a050: 742e 5f63 6865 636b 5f69 665f 6569 7468  t._check_if_eith
+0001a060: 6572 5f69 735f 7365 7428 290a 2020 2020  er_is_set().    
+0001a070: 2020 2020 4d6f 6465 6c73 2e5f 7661 6c69      Models._vali
+0001a080: 6461 7465 5f74 7970 6528 6d6f 6465 6c5f  date_type(model_
+0001a090: 6964 2c20 226d 6f64 656c 5f69 6422 2c20  id, "model_id", 
+0001a0a0: 7374 722c 2054 7275 6529 0a20 2020 2020  str, True).     
+0001a0b0: 2020 204d 6f64 656c 732e 5f76 616c 6964     Models._valid
+0001a0c0: 6174 655f 7479 7065 2872 6576 5f69 642c  ate_type(rev_id,
+0001a0d0: 2022 7265 765f 6964 222c 2073 7472 2c20   "rev_id", str, 
+0001a0e0: 5472 7565 290a 0a20 2020 2020 2020 2075  True)..        u
+0001a0f0: 726c 203d 2028 0a20 2020 2020 2020 2020  rl = (.         
+0001a100: 2020 2073 656c 662e 5f63 6c69 656e 742e     self._client.
+0001a110: 7365 7276 6963 655f 696e 7374 616e 6365  service_instance
+0001a120: 2e5f 6872 6566 5f64 6566 696e 6974 696f  ._href_definitio
+0001a130: 6e73 2e67 6574 5f70 7562 6c69 7368 6564  ns.get_published
+0001a140: 5f6d 6f64 656c 735f 6872 6566 2829 0a20  _models_href(). 
+0001a150: 2020 2020 2020 2020 2020 202b 2022 2f22             + "/"
+0001a160: 0a20 2020 2020 2020 2020 2020 202b 206d  .            + m
+0001a170: 6f64 656c 5f69 640a 2020 2020 2020 2020  odel_id.        
+0001a180: 290a 2020 2020 2020 2020 7265 7475 726e  ).        return
+0001a190: 2073 656c 662e 5f67 6574 5f77 6974 685f   self._get_with_
+0001a1a0: 6f72 5f77 6974 686f 7574 5f6c 696d 6974  or_without_limit
+0001a1b0: 280a 2020 2020 2020 2020 2020 2020 7572  (.            ur
+0001a1c0: 6c2c 0a20 2020 2020 2020 2020 2020 206c  l,.            l
+0001a1d0: 696d 6974 3d4e 6f6e 652c 0a20 2020 2020  imit=None,.     
+0001a1e0: 2020 2020 2020 206f 705f 6e61 6d65 3d22         op_name="
+0001a1f0: 6d6f 6465 6c22 2c0a 2020 2020 2020 2020  model",.        
+0001a200: 2020 2020 7375 6d6d 6172 793d 4e6f 6e65      summary=None
+0001a210: 2c0a 2020 2020 2020 2020 2020 2020 7072  ,.            pr
+0001a220: 655f 6465 6669 6e65 643d 4e6f 6e65 2c0a  e_defined=None,.
+0001a230: 2020 2020 2020 2020 2020 2020 7265 7669              revi
+0001a240: 7369 6f6e 3d72 6576 5f69 642c 0a20 2020  sion=rev_id,.   
+0001a250: 2020 2020 2029 0a0a 2020 2020 6465 6620       )..    def 
+0001a260: 7072 6f6d 6f74 6528 0a20 2020 2020 2020  promote(.       
+0001a270: 2073 656c 662c 206d 6f64 656c 5f69 643a   self, model_id:
+0001a280: 2073 7472 2c20 736f 7572 6365 5f70 726f   str, source_pro
+0001a290: 6a65 6374 5f69 643a 2073 7472 2c20 7461  ject_id: str, ta
+0001a2a0: 7267 6574 5f73 7061 6365 5f69 643a 2073  rget_space_id: s
+0001a2b0: 7472 0a20 2020 2029 202d 3e20 6469 6374  tr.    ) -> dict
+0001a2c0: 5b73 7472 2c20 416e 795d 207c 204e 6f6e  [str, Any] | Non
+0001a2d0: 653a 0a20 2020 2020 2020 2022 2222 5072  e:.        """Pr
+0001a2e0: 6f6d 6f74 6520 6d6f 6465 6c20 6672 6f6d  omote model from
+0001a2f0: 2070 726f 6a65 6374 2074 6f20 7370 6163   project to spac
+0001a300: 652e 2053 7570 706f 7274 6564 206f 6e6c  e. Supported onl
+0001a310: 7920 666f 7220 4942 4d20 436c 6f75 6420  y for IBM Cloud 
+0001a320: 5061 6bc2 ae20 666f 7220 4461 7461 2e0a  Pak.. for Data..
+0001a330: 0a20 2020 2020 2020 202a 4465 7072 6563  .        *Deprec
+0001a340: 6174 6564 3a2a 2055 7365 2060 636c 6965  ated:* Use `clie
+0001a350: 6e74 2e73 7061 6365 732e 7072 6f6d 6f74  nt.spaces.promot
+0001a360: 6528 6173 7365 745f 6964 2c20 736f 7572  e(asset_id, sour
+0001a370: 6365 5f70 726f 6a65 6374 5f69 642c 2074  ce_project_id, t
+0001a380: 6172 6765 745f 7370 6163 655f 6964 2960  arget_space_id)`
+0001a390: 2069 6e73 7465 6164 2e0a 2020 2020 2020   instead..      
+0001a3a0: 2020 2222 220a 2020 2020 2020 2020 7761    """.        wa
+0001a3b0: 726e 696e 6773 2e77 6172 6e28 0a20 2020  rnings.warn(.   
+0001a3c0: 2020 2020 2020 2020 2022 4e6f 7465 3a20           "Note: 
+0001a3d0: 4675 6e63 7469 6f6e 2060 636c 6965 6e74  Function `client
+0001a3e0: 2e72 6570 6f73 6974 6f72 792e 7072 6f6d  .repository.prom
+0001a3f0: 6f74 655f 6d6f 6465 6c28 6d6f 6465 6c5f  ote_model(model_
+0001a400: 6964 2c20 736f 7572 6365 5f70 726f 6a65  id, source_proje
+0001a410: 6374 5f69 642c 2074 6172 6765 745f 7370  ct_id, target_sp
+0001a420: 6163 655f 6964 2960 2022 0a20 2020 2020  ace_id)` ".     
+0001a430: 2020 2020 2020 2022 6861 7320 6265 656e         "has been
+0001a440: 2064 6570 7265 6361 7465 642e 2055 7365   deprecated. Use
+0001a450: 2060 636c 6965 6e74 2e73 7061 6365 732e   `client.spaces.
+0001a460: 7072 6f6d 6f74 6528 6173 7365 745f 6964  promote(asset_id
+0001a470: 2c20 736f 7572 6365 5f70 726f 6a65 6374  , source_project
+0001a480: 5f69 642c 2074 6172 6765 745f 7370 6163  _id, target_spac
+0001a490: 655f 6964 2960 2069 6e73 7465 6164 2e22  e_id)` instead."
+0001a4a0: 0a20 2020 2020 2020 2029 0a20 2020 2020  .        ).     
+0001a4b0: 2020 2074 7279 3a0a 2020 2020 2020 2020     try:.        
+0001a4c0: 2020 2020 7265 7475 726e 2073 656c 662e      return self.
+0001a4d0: 5f63 6c69 656e 742e 7370 6163 6573 2e70  _client.spaces.p
+0001a4e0: 726f 6d6f 7465 280a 2020 2020 2020 2020  romote(.        
+0001a4f0: 2020 2020 2020 2020 6d6f 6465 6c5f 6964          model_id
+0001a500: 2c20 736f 7572 6365 5f70 726f 6a65 6374  , source_project
+0001a510: 5f69 642c 2074 6172 6765 745f 7370 6163  _id, target_spac
+0001a520: 655f 6964 0a20 2020 2020 2020 2020 2020  e_id.           
+0001a530: 2029 0a20 2020 2020 2020 2065 7863 6570   ).        excep
+0001a540: 7420 5072 6f6d 6f74 696f 6e46 6169 6c65  t PromotionFaile
+0001a550: 6420 6173 2065 3a0a 2020 2020 2020 2020  d as e:.        
+0001a560: 2020 2020 7261 6973 6520 4d6f 6465 6c50      raise ModelP
+0001a570: 726f 6d6f 7469 6f6e 4661 696c 6564 280a  romotionFailed(.
+0001a580: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001a590: 652e 7072 6f6a 6563 745f 6964 2c20 652e  e.project_id, e.
+0001a5a0: 7370 6163 655f 6964 2c20 652e 7072 6f6d  space_id, e.prom
+0001a5b0: 6f74 696f 6e5f 7265 7370 6f6e 7365 2c20  otion_response, 
+0001a5c0: 7374 7228 652e 7265 6173 6f6e 290a 2020  str(e.reason).  
+0001a5d0: 2020 2020 2020 2020 2020 290a 0a20 2020            )..   
+0001a5e0: 2064 6566 205f 7570 6461 7465 5f6d 6f64   def _update_mod
+0001a5f0: 656c 5f63 6f6e 7465 6e74 280a 2020 2020  el_content(.    
+0001a600: 2020 2020 7365 6c66 2c20 6d6f 6465 6c5f      self, model_
+0001a610: 6964 3a20 7374 722c 2075 7064 6174 6564  id: str, updated
+0001a620: 5f64 6574 6169 6c73 3a20 6469 6374 5b73  _details: dict[s
+0001a630: 7472 2c20 416e 795d 2c20 7570 6461 7465  tr, Any], update
+0001a640: 5f6d 6f64 656c 3a20 416e 790a 2020 2020  _model: Any.    
+0001a650: 2920 2d3e 204e 6f6e 653a 0a0a 2020 2020  ) -> None:..    
+0001a660: 2020 2020 6d6f 6465 6c20 3d20 636f 7079      model = copy
+0001a670: 2e63 6f70 7928 7570 6461 7465 5f6d 6f64  .copy(update_mod
+0001a680: 656c 290a 2020 2020 2020 2020 6d6f 6465  el).        mode
+0001a690: 6c5f 7479 7065 203d 2075 7064 6174 6564  l_type = updated
+0001a6a0: 5f64 6574 6169 6c73 5b22 656e 7469 7479  _details["entity
+0001a6b0: 225d 5b22 7479 7065 225d 0a0a 2020 2020  "]["type"]..    
+0001a6c0: 2020 2020 6465 6620 6973 5f78 6d6c 286d      def is_xml(m
+0001a6d0: 6f64 656c 5f66 696c 6570 6174 683a 2073  odel_filepath: s
+0001a6e0: 7472 2920 2d3e 2062 6f6f 6c3a 0a20 2020  tr) -> bool:.   
+0001a6f0: 2020 2020 2020 2020 2069 6620 6f73 2e70           if os.p
+0001a700: 6174 682e 7370 6c69 7465 7874 286f 732e  ath.splitext(os.
+0001a710: 7061 7468 2e62 6173 656e 616d 6528 6d6f  path.basename(mo
+0001a720: 6465 6c5f 6669 6c65 7061 7468 2929 5b2d  del_filepath))[-
+0001a730: 315d 203d 3d20 222e 706d 6d6c 223a 0a20  1] == ".pmml":. 
+0001a740: 2020 2020 2020 2020 2020 2020 2020 2072                 r
+0001a750: 6169 7365 2057 4d4c 436c 6965 6e74 4572  aise WMLClientEr
+0001a760: 726f 7228 0a20 2020 2020 2020 2020 2020  ror(.           
+0001a770: 2020 2020 2020 2020 2022 5468 6520 6669           "The fi
+0001a780: 6c65 206e 616d 6520 6861 7320 616e 2075  le name has an u
+0001a790: 6e73 7570 706f 7274 6564 2065 7874 656e  nsupported exten
+0001a7a0: 7369 6f6e 2e20 5265 6e61 6d65 2074 6865  sion. Rename the
+0001a7b0: 2066 696c 6520 7769 7468 2061 202e 786d   file with a .xm
+0001a7c0: 6c20 6578 7465 6e73 696f 6e2e 220a 2020  l extension.".  
+0001a7d0: 2020 2020 2020 2020 2020 2020 2020 290a                ).
+0001a7e0: 2020 2020 2020 2020 2020 2020 7265 7475              retu
+0001a7f0: 726e 206f 732e 7061 7468 2e73 706c 6974  rn os.path.split
+0001a800: 6578 7428 6f73 2e70 6174 682e 6261 7365  ext(os.path.base
+0001a810: 6e61 6d65 286d 6f64 656c 5f66 696c 6570  name(model_filep
+0001a820: 6174 6829 295b 2d31 5d20 3d3d 2022 2e78  ath))[-1] == ".x
+0001a830: 6d6c 220a 0a20 2020 2020 2020 2069 6d70  ml"..        imp
+0001a840: 6f72 7420 7461 7266 696c 650a 2020 2020  ort tarfile.    
+0001a850: 2020 2020 696d 706f 7274 207a 6970 6669      import zipfi
+0001a860: 6c65 0a0a 2020 2020 2020 2020 6d6f 6465  le..        mode
+0001a870: 6c5f 6669 6c65 7061 7468 203d 206d 6f64  l_filepath = mod
+0001a880: 656c 0a0a 2020 2020 2020 2020 6966 2022  el..        if "
+0001a890: 7363 696b 6974 2d6c 6561 726e 5f22 2069  scikit-learn_" i
+0001a8a0: 6e20 6d6f 6465 6c5f 7479 7065 206f 7220  n model_type or 
+0001a8b0: 226d 6c6c 6962 5f22 2069 6e20 6d6f 6465  "mllib_" in mode
+0001a8c0: 6c5f 7479 7065 3a0a 2020 2020 2020 2020  l_type:.        
+0001a8d0: 2020 2020 6d65 7461 5f70 726f 7073 203d      meta_props =
+0001a8e0: 2075 7064 6174 6564 5f64 6574 6169 6c73   updated_details
+0001a8f0: 5b22 656e 7469 7479 225d 0a20 2020 2020  ["entity"].     
+0001a900: 2020 2020 2020 206d 6574 615f 6461 7461         meta_data
+0001a910: 203d 204d 6574 6150 726f 7073 286d 6574   = MetaProps(met
+0001a920: 615f 7072 6f70 7329 0a20 2020 2020 2020  a_props).       
+0001a930: 2020 2020 206e 616d 6520 3d20 7570 6461       name = upda
+0001a940: 7465 645f 6465 7461 696c 735b 226d 6574  ted_details["met
+0001a950: 6164 6174 6122 5d5b 226e 616d 6522 5d0a  adata"]["name"].
+0001a960: 2020 2020 2020 2020 2020 2020 6d6f 6465              mode
+0001a970: 6c5f 6172 7469 6661 6374 203d 204d 4c52  l_artifact = MLR
+0001a980: 6570 6f73 6974 6f72 7941 7274 6966 6163  epositoryArtifac
+0001a990: 7428 0a20 2020 2020 2020 2020 2020 2020  t(.             
+0001a9a0: 2020 2075 7064 6174 655f 6d6f 6465 6c2c     update_model,
+0001a9b0: 206e 616d 653d 6e61 6d65 2c20 6d65 7461   name=name, meta
+0001a9c0: 5f70 726f 7073 3d6d 6574 615f 6461 7461  _props=meta_data
+0001a9d0: 2c20 7472 6169 6e69 6e67 5f64 6174 613d  , training_data=
+0001a9e0: 4e6f 6e65 0a20 2020 2020 2020 2020 2020  None.           
+0001a9f0: 2029 0a20 2020 2020 2020 2020 2020 206d   ).            m
+0001aa00: 6f64 656c 5f61 7274 6966 6163 742e 7569  odel_artifact.ui
+0001aa10: 6420 3d20 6d6f 6465 6c5f 6964 0a20 2020  d = model_id.   
+0001aa20: 2020 2020 2020 2020 2071 7565 7279 5f70           query_p
+0001aa30: 6172 616d 7320 3d20 7365 6c66 2e5f 636c  arams = self._cl
+0001aa40: 6965 6e74 2e5f 7061 7261 6d73 2829 0a20  ient._params(). 
+0001aa50: 2020 2020 2020 2020 2020 2071 7565 7279             query
+0001aa60: 5f70 6172 616d 732e 7570 6461 7465 287b  _params.update({
+0001aa70: 2263 6f6e 7465 6e74 5f66 6f72 6d61 7422  "content_format"
+0001aa80: 3a20 226e 6174 6976 6522 7d29 0a20 2020  : "native"}).   
+0001aa90: 2020 2020 2020 2020 2073 656c 662e 5f63           self._c
+0001aaa0: 6c69 656e 742e 7265 706f 7369 746f 7279  lient.repository
+0001aab0: 2e5f 6d6c 5f72 6570 6f73 6974 6f72 795f  ._ml_repository_
+0001aac0: 636c 6965 6e74 2e6d 6f64 656c 732e 7570  client.models.up
+0001aad0: 6c6f 6164 5f63 6f6e 7465 6e74 280a 2020  load_content(.  
+0001aae0: 2020 2020 2020 2020 2020 2020 2020 6d6f                mo
+0001aaf0: 6465 6c5f 6172 7469 6661 6374 2c20 7175  del_artifact, qu
+0001ab00: 6572 795f 7061 7261 6d3d 7175 6572 795f  ery_param=query_
+0001ab10: 7061 7261 6d73 2c20 6e6f 5f64 656c 6574  params, no_delet
+0001ab20: 653d 5472 7565 0a20 2020 2020 2020 2020  e=True.         
+0001ab30: 2020 2029 0a20 2020 2020 2020 2065 6c73     ).        els
+0001ab40: 653a 0a20 2020 2020 2020 2020 2020 2069  e:.            i
+0001ab50: 6620 280a 2020 2020 2020 2020 2020 2020  f (.            
+0001ab60: 2020 2020 286f 732e 7061 7468 2e73 6570      (os.path.sep
+0001ab70: 2069 6e20 7570 6461 7465 5f6d 6f64 656c   in update_model
+0001ab80: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
+0001ab90: 2020 6f72 206f 732e 7061 7468 2e69 7366    or os.path.isf
+0001aba0: 696c 6528 7570 6461 7465 5f6d 6f64 656c  ile(update_model
+0001abb0: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
+0001abc0: 2020 6f72 206f 732e 7061 7468 2e69 7364    or os.path.isd
+0001abd0: 6972 2875 7064 6174 655f 6d6f 6465 6c29  ir(update_model)
+0001abe0: 0a20 2020 2020 2020 2020 2020 2029 3a0a  .            ):.
+0001abf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001ac00: 6966 206e 6f74 206f 732e 7061 7468 2e69  if not os.path.i
+0001ac10: 7366 696c 6528 7570 6461 7465 5f6d 6f64  sfile(update_mod
+0001ac20: 656c 2920 616e 6420 6e6f 7420 6f73 2e70  el) and not os.p
+0001ac30: 6174 682e 6973 6469 7228 7570 6461 7465  ath.isdir(update
+0001ac40: 5f6d 6f64 656c 293a 0a20 2020 2020 2020  _model):.       
+0001ac50: 2020 2020 2020 2020 2020 2020 2072 6169               rai
+0001ac60: 7365 2057 4d4c 436c 6965 6e74 4572 726f  se WMLClientErro
+0001ac70: 7228 0a20 2020 2020 2020 2020 2020 2020  r(.             
+0001ac80: 2020 2020 2020 2020 2020 2022 496e 7661             "Inva
+0001ac90: 6c69 6420 7061 7468 3a20 6e65 6974 6865  lid path: neithe
+0001aca0: 7220 6669 6c65 206e 6f72 2064 6972 6563  r file nor direc
+0001acb0: 746f 7279 2065 7869 7374 7320 756e 6465  tory exists unde
+0001acc0: 7220 7468 6973 2070 6174 683a 2027 7b7d  r this path: '{}
+0001acd0: 272e 222e 666f 726d 6174 280a 2020 2020  '.".format(.    
+0001ace0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001acf0: 2020 2020 2020 2020 6d6f 6465 6c0a 2020          model.  
+0001ad00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001ad10: 2020 2020 2020 290a 2020 2020 2020 2020        ).        
+0001ad20: 2020 2020 2020 2020 2020 2020 290a 0a20              ).. 
+0001ad30: 2020 2020 2020 2020 2020 2069 6620 6f73             if os
+0001ad40: 2e70 6174 682e 6973 6469 7228 6d6f 6465  .path.isdir(mode
+0001ad50: 6c29 3a0a 2020 2020 2020 2020 2020 2020  l):.            
+0001ad60: 2020 2020 6966 2022 7465 6e73 6f72 666c      if "tensorfl
+0001ad70: 6f77 2220 696e 206d 6f64 656c 5f74 7970  ow" in model_typ
+0001ad80: 653a 0a20 2020 2020 2020 2020 2020 2020  e:.             
+0001ad90: 2020 2020 2020 2023 2054 4f44 4f20 6375         # TODO cu
+0001ada0: 7272 656e 746c 7920 7461 722e 677a 2069  rrently tar.gz i
+0001adb0: 7320 7265 7175 6972 6564 2066 6f72 2074  s required for t
+0001adc0: 656e 736f 7266 6c6f 7720 2d20 7468 6520  ensorflow - the 
+0001add0: 7361 6d65 2065 7874 2073 686f 756c 6420  same ext should 
+0001ade0: 6265 2073 7570 706f 7274 6564 2066 6f72  be supported for
+0001adf0: 2061 6c6c 2066 7261 6d65 776f 726b 730a   all frameworks.
+0001ae00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001ae10: 2020 2020 6966 206f 732e 7061 7468 2e62      if os.path.b
+0001ae20: 6173 656e 616d 6528 6d6f 6465 6c29 203d  asename(model) =
+0001ae30: 3d20 2222 3a0a 2020 2020 2020 2020 2020  = "":.          
+0001ae40: 2020 2020 2020 2020 2020 2020 2020 6d6f                mo
+0001ae50: 6465 6c20 3d20 6f73 2e70 6174 682e 6469  del = os.path.di
+0001ae60: 726e 616d 6528 7570 6461 7465 5f6d 6f64  rname(update_mod
+0001ae70: 656c 290a 2020 2020 2020 2020 2020 2020  el).            
+0001ae80: 2020 2020 2020 2020 6669 6c65 6e61 6d65          filename
+0001ae90: 203d 206f 732e 7061 7468 2e62 6173 656e   = os.path.basen
+0001aea0: 616d 6528 7570 6461 7465 5f6d 6f64 656c  ame(update_model
+0001aeb0: 2920 2b20 222e 7461 722e 677a 220a 2020  ) + ".tar.gz".  
+0001aec0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001aed0: 2020 6375 7272 656e 745f 6469 7220 3d20    current_dir = 
+0001aee0: 6f73 2e67 6574 6377 6428 290a 2020 2020  os.getcwd().    
+0001aef0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001af00: 6f73 2e63 6864 6972 286d 6f64 656c 290a  os.chdir(model).
+0001af10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001af20: 2020 2020 7461 7267 6574 5f70 6174 6820      target_path 
+0001af30: 3d20 6f73 2e70 6174 682e 6469 726e 616d  = os.path.dirnam
+0001af40: 6528 6d6f 6465 6c29 0a0a 2020 2020 2020  e(model)..      
+0001af50: 2020 2020 2020 2020 2020 2020 2020 7769                wi
+0001af60: 7468 2074 6172 6669 6c65 2e6f 7065 6e28  th tarfile.open(
+0001af70: 6f73 2e70 6174 682e 6a6f 696e 2822 2e2e  os.path.join("..
+0001af80: 222c 2066 696c 656e 616d 6529 2c20 6d6f  ", filename), mo
+0001af90: 6465 3d22 773a 677a 2229 2061 7320 7461  de="w:gz") as ta
+0001afa0: 723a 0a20 2020 2020 2020 2020 2020 2020  r:.             
+0001afb0: 2020 2020 2020 2020 2020 2074 6172 2e61             tar.a
+0001afc0: 6464 2822 2e22 290a 0a20 2020 2020 2020  dd(".")..       
+0001afd0: 2020 2020 2020 2020 2020 2020 206f 732e               os.
+0001afe0: 6368 6469 7228 6375 7272 656e 745f 6469  chdir(current_di
+0001aff0: 7229 0a20 2020 2020 2020 2020 2020 2020  r).             
+0001b000: 2020 2020 2020 206d 6f64 656c 5f66 696c         model_fil
+0001b010: 6570 6174 6820 3d20 6f73 2e70 6174 682e  epath = os.path.
+0001b020: 6a6f 696e 2874 6172 6765 745f 7061 7468  join(target_path
+0001b030: 2c20 6669 6c65 6e61 6d65 290a 2020 2020  , filename).    
+0001b040: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001b050: 6966 2028 0a20 2020 2020 2020 2020 2020  if (.           
+0001b060: 2020 2020 2020 2020 2020 2020 2074 6172               tar
+0001b070: 6669 6c65 2e69 735f 7461 7266 696c 6528  file.is_tarfile(
+0001b080: 6d6f 6465 6c5f 6669 6c65 7061 7468 290a  model_filepath).
+0001b090: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001b0a0: 2020 2020 2020 2020 6f72 207a 6970 6669          or zipfi
+0001b0b0: 6c65 2e69 735f 7a69 7066 696c 6528 6d6f  le.is_zipfile(mo
+0001b0c0: 6465 6c5f 6669 6c65 7061 7468 290a 2020  del_filepath).  
+0001b0d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001b0e0: 2020 2020 2020 6f72 2069 735f 786d 6c28        or is_xml(
+0001b0f0: 6d6f 6465 6c5f 6669 6c65 7061 7468 290a  model_filepath).
+0001b100: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001b110: 2020 2020 293a 0a20 2020 2020 2020 2020      ):.         
+0001b120: 2020 2020 2020 2020 2020 2020 2020 2070                 p
+0001b130: 6174 685f 746f 5f61 7263 6869 7665 203d  ath_to_archive =
+0001b140: 206d 6f64 656c 5f66 696c 6570 6174 680a   model_filepath.
+0001b150: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001b160: 656c 7365 3a0a 2020 2020 2020 2020 2020  else:.          
+0001b170: 2020 2020 2020 2020 2020 6966 2022 6361            if "ca
+0001b180: 6666 6522 2069 6e20 6d6f 6465 6c5f 7479  ffe" in model_ty
+0001b190: 7065 3a0a 2020 2020 2020 2020 2020 2020  pe:.            
+0001b1a0: 2020 2020 2020 2020 2020 2020 7261 6973              rais
+0001b1b0: 6520 574d 4c43 6c69 656e 7445 7272 6f72  e WMLClientError
+0001b1c0: 280a 2020 2020 2020 2020 2020 2020 2020  (.              
+0001b1d0: 2020 2020 2020 2020 2020 2020 2020 2249                "I
+0001b1e0: 6e76 616c 6964 206d 6f64 656c 2066 696c  nvalid model fil
+0001b1f0: 6520 7061 7468 2020 7370 6563 6966 6965  e path  specifie
+0001b200: 6420 666f 723a 2027 7b7d 272e 222e 666f  d for: '{}'.".fo
+0001b210: 726d 6174 280a 2020 2020 2020 2020 2020  rmat(.          
+0001b220: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001b230: 2020 2020 2020 6d6f 6465 6c5f 7479 7065        model_type
+0001b240: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0001b250: 2020 2020 2020 2020 2020 2020 2029 0a20               ). 
+0001b260: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001b270: 2020 2020 2020 2029 0a20 2020 2020 2020         ).       
+0001b280: 2020 2020 2020 2020 2020 2020 206c 6f61               loa
+0001b290: 6465 645f 6d6f 6465 6c20 3d20 6c6f 6164  ded_model = load
+0001b2a0: 5f6d 6f64 656c 5f66 726f 6d5f 6469 7265  _model_from_dire
+0001b2b0: 6374 6f72 7928 6d6f 6465 6c5f 7479 7065  ctory(model_type
+0001b2c0: 2c20 6d6f 6465 6c29 0a20 2020 2020 2020  , model).       
 0001b2d0: 2020 2020 2020 2020 2020 2020 2070 6174               pat
-0001b2e0: 685f 746f 5f61 7263 6869 7665 203d 206d  h_to_archive = m
-0001b2f0: 6f64 656c 5f66 696c 6570 6174 680a 2020  odel_filepath.  
-0001b300: 2020 2020 2020 2020 2020 2020 2020 656c                el
-0001b310: 7365 3a0a 2020 2020 2020 2020 2020 2020  se:.            
-0001b320: 2020 2020 2020 2020 6966 2022 6361 6666          if "caff
-0001b330: 6522 2069 6e20 6d6f 6465 6c5f 7479 7065  e" in model_type
-0001b340: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
-0001b350: 2020 2020 2020 2020 2020 7261 6973 6520            raise 
-0001b360: 574d 4c43 6c69 656e 7445 7272 6f72 280a  WMLClientError(.
-0001b370: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001b380: 2020 2020 2020 2020 2020 2020 2249 6e76              "Inv
-0001b390: 616c 6964 206d 6f64 656c 2066 696c 6520  alid model file 
-0001b3a0: 7061 7468 2020 7370 6563 6966 6965 6420  path  specified 
-0001b3b0: 666f 723a 2027 7b7d 272e 222e 666f 726d  for: '{}'.".form
-0001b3c0: 6174 280a 2020 2020 2020 2020 2020 2020  at(.            
-0001b3d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001b3e0: 2020 2020 6d6f 6465 6c5f 7479 7065 0a20      model_type. 
-0001b3f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001b400: 2020 2020 2020 2020 2020 2029 0a20 2020             ).   
-0001b410: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001b420: 2020 2020 2029 0a20 2020 2020 2020 2020       ).         
-0001b430: 2020 2020 2020 2020 2020 206c 6f61 6465             loade
-0001b440: 645f 6d6f 6465 6c20 3d20 6c6f 6164 5f6d  d_model = load_m
-0001b450: 6f64 656c 5f66 726f 6d5f 6469 7265 6374  odel_from_direct
-0001b460: 6f72 7928 6d6f 6465 6c5f 7479 7065 2c20  ory(model_type, 
-0001b470: 6d6f 6465 6c29 0a20 2020 2020 2020 2020  model).         
-0001b480: 2020 2020 2020 2020 2020 2070 6174 685f             path_
-0001b490: 746f 5f61 7263 6869 7665 203d 206c 6f61  to_archive = loa
-0001b4a0: 6465 645f 6d6f 6465 6c0a 2020 2020 2020  ded_model.      
-0001b4b0: 2020 2020 2020 656c 6966 2069 735f 786d        elif is_xm
-0001b4c0: 6c28 6d6f 6465 6c5f 6669 6c65 7061 7468  l(model_filepath
-0001b4d0: 293a 0a20 2020 2020 2020 2020 2020 2020  ):.             
-0001b4e0: 2020 2070 6174 685f 746f 5f61 7263 6869     path_to_archi
-0001b4f0: 7665 203d 206d 6f64 656c 5f66 696c 6570  ve = model_filep
-0001b500: 6174 680a 2020 2020 2020 2020 2020 2020  ath.            
-0001b510: 656c 6966 2074 6172 6669 6c65 2e69 735f  elif tarfile.is_
-0001b520: 7461 7266 696c 6528 6d6f 6465 6c5f 6669  tarfile(model_fi
-0001b530: 6c65 7061 7468 2920 6f72 207a 6970 6669  lepath) or zipfi
-0001b540: 6c65 2e69 735f 7a69 7066 696c 6528 0a20  le.is_zipfile(. 
-0001b550: 2020 2020 2020 2020 2020 2020 2020 206d                 m
-0001b560: 6f64 656c 5f66 696c 6570 6174 680a 2020  odel_filepath.  
-0001b570: 2020 2020 2020 2020 2020 293a 0a20 2020            ):.   
-0001b580: 2020 2020 2020 2020 2020 2020 2070 6174               pat
-0001b590: 685f 746f 5f61 7263 6869 7665 203d 206d  h_to_archive = m
-0001b5a0: 6f64 656c 5f66 696c 6570 6174 680a 0a20  odel_filepath.. 
-0001b5b0: 2020 2020 2020 2020 2020 2065 6c73 653a             else:
-0001b5c0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0001b5d0: 2072 6169 7365 2057 4d4c 436c 6965 6e74   raise WMLClient
-0001b5e0: 4572 726f 7228 0a20 2020 2020 2020 2020  Error(.         
-0001b5f0: 2020 2020 2020 2020 2020 2022 5361 7669             "Savi
-0001b600: 6e67 2074 7261 696e 6564 206d 6f64 656c  ng trained model
-0001b610: 2069 6e20 7265 706f 7369 746f 7279 2066   in repository f
-0001b620: 6169 6c65 642e 2027 7b7d 2720 6669 6c65  ailed. '{}' file
-0001b630: 2064 6f65 7320 6e6f 7420 6861 7665 2076   does not have v
-0001b640: 616c 6964 2066 6f72 6d61 7422 2e66 6f72  alid format".for
-0001b650: 6d61 7428 0a20 2020 2020 2020 2020 2020  mat(.           
-0001b660: 2020 2020 2020 2020 2020 2020 206d 6f64               mod
-0001b670: 656c 5f66 696c 6570 6174 680a 2020 2020  el_filepath.    
-0001b680: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001b690: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
-0001b6a0: 2020 290a 0a20 2020 2020 2020 2020 2020    )..           
-0001b6b0: 2075 726c 203d 2028 0a20 2020 2020 2020   url = (.       
-0001b6c0: 2020 2020 2020 2020 2073 656c 662e 5f63           self._c
-0001b6d0: 6c69 656e 742e 7365 7276 6963 655f 696e  lient.service_in
-0001b6e0: 7374 616e 6365 2e5f 6872 6566 5f64 6566  stance._href_def
-0001b6f0: 696e 6974 696f 6e73 2e67 6574 5f70 7562  initions.get_pub
-0001b700: 6c69 7368 6564 5f6d 6f64 656c 5f68 7265  lished_model_hre
-0001b710: 6628 0a20 2020 2020 2020 2020 2020 2020  f(.             
-0001b720: 2020 2020 2020 206d 6f64 656c 5f69 640a         model_id.
-0001b730: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001b740: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
-0001b750: 2020 2b20 222f 636f 6e74 656e 7422 0a20    + "/content". 
-0001b760: 2020 2020 2020 2020 2020 2029 0a20 2020             ).   
-0001b770: 2020 2020 2020 2020 2077 6974 6820 6f70           with op
-0001b780: 656e 2870 6174 685f 746f 5f61 7263 6869  en(path_to_archi
-0001b790: 7665 2c20 2272 6222 2920 6173 2066 3a0a  ve, "rb") as f:.
-0001b7a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001b7b0: 6966 2069 735f 786d 6c28 7061 7468 5f74  if is_xml(path_t
-0001b7c0: 6f5f 6172 6368 6976 6529 3a0a 2020 2020  o_archive):.    
-0001b7d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001b7e0: 7265 7370 6f6e 7365 203d 2072 6571 7565  response = reque
-0001b7f0: 7374 732e 7075 7428 0a20 2020 2020 2020  sts.put(.       
-0001b800: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001b810: 2075 726c 2c0a 2020 2020 2020 2020 2020   url,.          
-0001b820: 2020 2020 2020 2020 2020 2020 2020 6461                da
-0001b830: 7461 3d66 2c0a 2020 2020 2020 2020 2020  ta=f,.          
-0001b840: 2020 2020 2020 2020 2020 2020 2020 7061                pa
-0001b850: 7261 6d73 3d73 656c 662e 5f63 6c69 656e  rams=self._clien
-0001b860: 742e 5f70 6172 616d 7328 292c 0a20 2020  t._params(),.   
-0001b870: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001b880: 2020 2020 2068 6561 6465 7273 3d73 656c       headers=sel
-0001b890: 662e 5f63 6c69 656e 742e 5f67 6574 5f68  f._client._get_h
-0001b8a0: 6561 6465 7273 280a 2020 2020 2020 2020  eaders(.        
-0001b8b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001b8c0: 2020 2020 636f 6e74 656e 745f 7479 7065      content_type
-0001b8d0: 3d22 6170 706c 6963 6174 696f 6e2f 786d  ="application/xm
-0001b8e0: 6c22 0a20 2020 2020 2020 2020 2020 2020  l".             
-0001b8f0: 2020 2020 2020 2020 2020 2029 2c0a 2020             ),.  
+0001b2e0: 685f 746f 5f61 7263 6869 7665 203d 206c  h_to_archive = l
+0001b2f0: 6f61 6465 645f 6d6f 6465 6c0a 2020 2020  oaded_model.    
+0001b300: 2020 2020 2020 2020 656c 6966 2069 735f          elif is_
+0001b310: 786d 6c28 6d6f 6465 6c5f 6669 6c65 7061  xml(model_filepa
+0001b320: 7468 293a 0a20 2020 2020 2020 2020 2020  th):.           
+0001b330: 2020 2020 2070 6174 685f 746f 5f61 7263       path_to_arc
+0001b340: 6869 7665 203d 206d 6f64 656c 5f66 696c  hive = model_fil
+0001b350: 6570 6174 680a 2020 2020 2020 2020 2020  epath.          
+0001b360: 2020 656c 6966 2074 6172 6669 6c65 2e69    elif tarfile.i
+0001b370: 735f 7461 7266 696c 6528 6d6f 6465 6c5f  s_tarfile(model_
+0001b380: 6669 6c65 7061 7468 2920 6f72 207a 6970  filepath) or zip
+0001b390: 6669 6c65 2e69 735f 7a69 7066 696c 6528  file.is_zipfile(
+0001b3a0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0001b3b0: 206d 6f64 656c 5f66 696c 6570 6174 680a   model_filepath.
+0001b3c0: 2020 2020 2020 2020 2020 2020 293a 0a20              ):. 
+0001b3d0: 2020 2020 2020 2020 2020 2020 2020 2070                 p
+0001b3e0: 6174 685f 746f 5f61 7263 6869 7665 203d  ath_to_archive =
+0001b3f0: 206d 6f64 656c 5f66 696c 6570 6174 680a   model_filepath.
+0001b400: 0a20 2020 2020 2020 2020 2020 2065 6c73  .            els
+0001b410: 653a 0a20 2020 2020 2020 2020 2020 2020  e:.             
+0001b420: 2020 2072 6169 7365 2057 4d4c 436c 6965     raise WMLClie
+0001b430: 6e74 4572 726f 7228 0a20 2020 2020 2020  ntError(.       
+0001b440: 2020 2020 2020 2020 2020 2020 2022 5361               "Sa
+0001b450: 7669 6e67 2074 7261 696e 6564 206d 6f64  ving trained mod
+0001b460: 656c 2069 6e20 7265 706f 7369 746f 7279  el in repository
+0001b470: 2066 6169 6c65 642e 2027 7b7d 2720 6669   failed. '{}' fi
+0001b480: 6c65 2064 6f65 7320 6e6f 7420 6861 7665  le does not have
+0001b490: 2076 616c 6964 2066 6f72 6d61 7422 2e66   valid format".f
+0001b4a0: 6f72 6d61 7428 0a20 2020 2020 2020 2020  ormat(.         
+0001b4b0: 2020 2020 2020 2020 2020 2020 2020 206d                 m
+0001b4c0: 6f64 656c 5f66 696c 6570 6174 680a 2020  odel_filepath.  
+0001b4d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001b4e0: 2020 290a 2020 2020 2020 2020 2020 2020    ).            
+0001b4f0: 2020 2020 290a 0a20 2020 2020 2020 2020      )..         
+0001b500: 2020 2075 726c 203d 2028 0a20 2020 2020     url = (.     
+0001b510: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+0001b520: 5f63 6c69 656e 742e 7365 7276 6963 655f  _client.service_
+0001b530: 696e 7374 616e 6365 2e5f 6872 6566 5f64  instance._href_d
+0001b540: 6566 696e 6974 696f 6e73 2e67 6574 5f70  efinitions.get_p
+0001b550: 7562 6c69 7368 6564 5f6d 6f64 656c 5f68  ublished_model_h
+0001b560: 7265 6628 0a20 2020 2020 2020 2020 2020  ref(.           
+0001b570: 2020 2020 2020 2020 206d 6f64 656c 5f69           model_i
+0001b580: 640a 2020 2020 2020 2020 2020 2020 2020  d.              
+0001b590: 2020 290a 2020 2020 2020 2020 2020 2020    ).            
+0001b5a0: 2020 2020 2b20 222f 636f 6e74 656e 7422      + "/content"
+0001b5b0: 0a20 2020 2020 2020 2020 2020 2029 0a20  .            ). 
+0001b5c0: 2020 2020 2020 2020 2020 2077 6974 6820             with 
+0001b5d0: 6f70 656e 2870 6174 685f 746f 5f61 7263  open(path_to_arc
+0001b5e0: 6869 7665 2c20 2272 6222 2920 6173 2066  hive, "rb") as f
+0001b5f0: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
+0001b600: 2020 6966 2069 735f 786d 6c28 7061 7468    if is_xml(path
+0001b610: 5f74 6f5f 6172 6368 6976 6529 3a0a 2020  _to_archive):.  
+0001b620: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001b630: 2020 7265 7370 6f6e 7365 203d 2072 6571    response = req
+0001b640: 7565 7374 732e 7075 7428 0a20 2020 2020  uests.put(.     
+0001b650: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001b660: 2020 2075 726c 2c0a 2020 2020 2020 2020     url,.        
+0001b670: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001b680: 6461 7461 3d66 2c0a 2020 2020 2020 2020  data=f,.        
+0001b690: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001b6a0: 7061 7261 6d73 3d73 656c 662e 5f63 6c69  params=self._cli
+0001b6b0: 656e 742e 5f70 6172 616d 7328 292c 0a20  ent._params(),. 
+0001b6c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001b6d0: 2020 2020 2020 2068 6561 6465 7273 3d73         headers=s
+0001b6e0: 656c 662e 5f63 6c69 656e 742e 5f67 6574  elf._client._get
+0001b6f0: 5f68 6561 6465 7273 280a 2020 2020 2020  _headers(.      
+0001b700: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001b710: 2020 2020 2020 636f 6e74 656e 745f 7479        content_ty
+0001b720: 7065 3d22 6170 706c 6963 6174 696f 6e2f  pe="application/
+0001b730: 786d 6c22 0a20 2020 2020 2020 2020 2020  xml".           
+0001b740: 2020 2020 2020 2020 2020 2020 2029 2c0a               ),.
+0001b750: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001b760: 2020 2020 290a 2020 2020 2020 2020 2020      ).          
+0001b770: 2020 2020 2020 656c 7365 3a0a 2020 2020        else:.    
+0001b780: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001b790: 7170 6172 616d 7320 3d20 7365 6c66 2e5f  qparams = self._
+0001b7a0: 636c 6965 6e74 2e5f 7061 7261 6d73 2829  client._params()
+0001b7b0: 0a0a 2020 2020 2020 2020 2020 2020 2020  ..              
+0001b7c0: 2020 2020 2020 6966 206d 6f64 656c 5f74        if model_t
+0001b7d0: 7970 652e 7374 6172 7473 7769 7468 2822  ype.startswith("
+0001b7e0: 776d 6c2d 6879 6272 6964 5f30 2229 3a0a  wml-hybrid_0"):.
+0001b7f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001b800: 2020 2020 2020 2020 7265 7370 6f6e 7365          response
+0001b810: 203d 2073 656c 662e 5f75 706c 6f61 645f   = self._upload_
+0001b820: 6175 746f 6169 5f6d 6f64 656c 5f63 6f6e  autoai_model_con
+0001b830: 7465 6e74 2866 2c20 7572 6c2c 2071 7061  tent(f, url, qpa
+0001b840: 7261 6d73 290a 2020 2020 2020 2020 2020  rams).          
+0001b850: 2020 2020 2020 2020 2020 656c 7365 3a0a            else:.
+0001b860: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001b870: 2020 2020 2020 2020 7170 6172 616d 732e          qparams.
+0001b880: 7570 6461 7465 287b 2263 6f6e 7465 6e74  update({"content
+0001b890: 5f66 6f72 6d61 7422 3a20 226e 6174 6976  _format": "nativ
+0001b8a0: 6522 7d29 0a20 2020 2020 2020 2020 2020  e"}).           
+0001b8b0: 2020 2020 2020 2020 2020 2020 2072 6573               res
+0001b8c0: 706f 6e73 6520 3d20 7265 7175 6573 7473  ponse = requests
+0001b8d0: 2e70 7574 280a 2020 2020 2020 2020 2020  .put(.          
+0001b8e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001b8f0: 2020 7572 6c2c 0a20 2020 2020 2020 2020    url,.         
 0001b900: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001b910: 2020 290a 2020 2020 2020 2020 2020 2020    ).            
-0001b920: 2020 2020 656c 7365 3a0a 2020 2020 2020      else:.      
-0001b930: 2020 2020 2020 2020 2020 2020 2020 7170                qp
-0001b940: 6172 616d 7320 3d20 7365 6c66 2e5f 636c  arams = self._cl
-0001b950: 6965 6e74 2e5f 7061 7261 6d73 2829 0a0a  ient._params()..
-0001b960: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001b970: 2020 2020 6966 206d 6f64 656c 5f74 7970      if model_typ
-0001b980: 652e 7374 6172 7473 7769 7468 2822 776d  e.startswith("wm
-0001b990: 6c2d 6879 6272 6964 5f30 2229 3a0a 2020  l-hybrid_0"):.  
-0001b9a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001b9b0: 2020 2020 2020 7265 7370 6f6e 7365 203d        response =
-0001b9c0: 2073 656c 662e 5f75 706c 6f61 645f 6175   self._upload_au
-0001b9d0: 746f 6169 5f6d 6f64 656c 5f63 6f6e 7465  toai_model_conte
-0001b9e0: 6e74 2866 2c20 7572 6c2c 2071 7061 7261  nt(f, url, qpara
-0001b9f0: 6d73 290a 2020 2020 2020 2020 2020 2020  ms).            
-0001ba00: 2020 2020 2020 2020 656c 7365 3a0a 2020          else:.  
-0001ba10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ba20: 2020 2020 2020 7170 6172 616d 732e 7570        qparams.up
-0001ba30: 6461 7465 287b 2263 6f6e 7465 6e74 5f66  date({"content_f
-0001ba40: 6f72 6d61 7422 3a20 226e 6174 6976 6522  ormat": "native"
-0001ba50: 7d29 0a20 2020 2020 2020 2020 2020 2020  }).             
-0001ba60: 2020 2020 2020 2020 2020 2072 6573 706f             respo
-0001ba70: 6e73 6520 3d20 7265 7175 6573 7473 2e70  nse = requests.p
-0001ba80: 7574 280a 2020 2020 2020 2020 2020 2020  ut(.            
-0001ba90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001baa0: 7572 6c2c 0a20 2020 2020 2020 2020 2020  url,.           
-0001bab0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001bac0: 2064 6174 613d 662c 0a20 2020 2020 2020   data=f,.       
-0001bad0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001bae0: 2020 2020 2070 6172 616d 733d 7170 6172       params=qpar
-0001baf0: 616d 732c 0a20 2020 2020 2020 2020 2020  ams,.           
-0001bb00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001bb10: 2068 6561 6465 7273 3d73 656c 662e 5f63   headers=self._c
-0001bb20: 6c69 656e 742e 5f67 6574 5f68 6561 6465  lient._get_heade
-0001bb30: 7273 280a 2020 2020 2020 2020 2020 2020  rs(.            
-0001bb40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001bb50: 2020 2020 636f 6e74 656e 745f 7479 7065      content_type
-0001bb60: 3d22 6170 706c 6963 6174 696f 6e2f 6f63  ="application/oc
-0001bb70: 7465 742d 7374 7265 616d 220a 2020 2020  tet-stream".    
-0001bb80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001bb90: 2020 2020 2020 2020 292c 0a20 2020 2020          ),.     
-0001bba0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001bbb0: 2020 2029 0a20 2020 2020 2020 2020 2020     ).           
-0001bbc0: 2020 2020 2073 656c 662e 5f68 616e 646c       self._handl
-0001bbd0: 655f 7265 7370 6f6e 7365 2832 3031 2c20  e_response(201, 
-0001bbe0: 2275 706c 6f61 6469 6e67 206d 6f64 656c  "uploading model
-0001bbf0: 2063 6f6e 7465 6e74 222c 2072 6573 706f   content", respo
-0001bc00: 6e73 652c 2046 616c 7365 290a 0a20 2020  nse, False)..   
-0001bc10: 2064 6566 205f 6372 6561 7465 5f63 6c6f   def _create_clo
-0001bc20: 7564 5f6d 6f64 656c 5f70 6179 6c6f 6164  ud_model_payload
-0001bc30: 280a 2020 2020 2020 2020 7365 6c66 2c0a  (.        self,.
-0001bc40: 2020 2020 2020 2020 6d65 7461 5f70 726f          meta_pro
-0001bc50: 7073 3a20 6469 6374 5b73 7472 2c20 416e  ps: dict[str, An
-0001bc60: 795d 2c0a 2020 2020 2020 2020 6665 6174  y],.        feat
-0001bc70: 7572 655f 6e61 6d65 733a 2046 6561 7475  ure_names: Featu
-0001bc80: 7265 4e61 6d65 7341 7272 6179 5479 7065  reNamesArrayType
-0001bc90: 207c 204e 6f6e 6520 3d20 4e6f 6e65 2c0a   | None = None,.
-0001bca0: 2020 2020 2020 2020 6c61 6265 6c5f 636f          label_co
-0001bcb0: 6c75 6d6e 5f6e 616d 6573 3a20 4c61 6265  lumn_names: Labe
-0001bcc0: 6c43 6f6c 756d 6e4e 616d 6573 5479 7065  lColumnNamesType
-0001bcd0: 207c 204e 6f6e 6520 3d20 4e6f 6e65 2c0a   | None = None,.
-0001bce0: 2020 2020 2920 2d3e 2064 6963 745b 7374      ) -> dict[st
-0001bcf0: 722c 2041 6e79 5d3a 0a20 2020 2020 2020  r, Any]:.       
-0001bd00: 206d 6574 6164 6174 6120 3d20 636f 7079   metadata = copy
-0001bd10: 2e64 6565 7063 6f70 7928 6d65 7461 5f70  .deepcopy(meta_p
-0001bd20: 726f 7073 290a 2020 2020 2020 2020 6966  rops).        if
-0001bd30: 2073 656c 662e 5f63 6c69 656e 742e 6465   self._client.de
-0001bd40: 6661 756c 745f 7370 6163 655f 6964 2069  fault_space_id i
-0001bd50: 7320 6e6f 7420 4e6f 6e65 3a0a 2020 2020  s not None:.    
-0001bd60: 2020 2020 2020 2020 6d65 7461 6461 7461          metadata
-0001bd70: 5b22 7370 6163 655f 6964 225d 203d 2073  ["space_id"] = s
-0001bd80: 656c 662e 5f63 6c69 656e 742e 6465 6661  elf._client.defa
-0001bd90: 756c 745f 7370 6163 655f 6964 0a20 2020  ult_space_id.   
-0001bda0: 2020 2020 2065 6c73 653a 0a20 2020 2020       else:.     
-0001bdb0: 2020 2020 2020 2069 6620 7365 6c66 2e5f         if self._
-0001bdc0: 636c 6965 6e74 2e64 6566 6175 6c74 5f70  client.default_p
-0001bdd0: 726f 6a65 6374 5f69 6420 6973 206e 6f74  roject_id is not
-0001bde0: 204e 6f6e 653a 0a20 2020 2020 2020 2020   None:.         
-0001bdf0: 2020 2020 2020 206d 6574 6164 6174 612e         metadata.
-0001be00: 7570 6461 7465 287b 2270 726f 6a65 6374  update({"project
-0001be10: 5f69 6422 3a20 7365 6c66 2e5f 636c 6965  _id": self._clie
-0001be20: 6e74 2e64 6566 6175 6c74 5f70 726f 6a65  nt.default_proje
-0001be30: 6374 5f69 647d 290a 2020 2020 2020 2020  ct_id}).        
-0001be40: 2020 2020 656c 7365 3a0a 2020 2020 2020      else:.      
-0001be50: 2020 2020 2020 2020 2020 7261 6973 6520            raise 
-0001be60: 574d 4c43 6c69 656e 7445 7272 6f72 280a  WMLClientError(.
-0001be70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001be80: 2020 2020 2249 7420 6973 206d 616e 6461      "It is manda
-0001be90: 746f 7279 2069 7320 7365 7420 7468 6520  tory is set the 
-0001bea0: 7370 6163 6520 6f72 2050 726f 6a65 6374  space or Project
-0001beb0: 2e20 5c0a 2020 2020 2020 2020 2020 2020  . \.            
-0001bec0: 2020 2020 2055 7365 2063 6c69 656e 742e       Use client.
-0001bed0: 7365 742e 6465 6661 756c 745f 7370 6163  set.default_spac
-0001bee0: 6528 3c53 5041 4345 5f49 443e 2920 746f  e(<SPACE_ID>) to
-0001bef0: 2073 6574 2074 6865 2073 7061 6365 206f   set the space o
-0001bf00: 7222 0a20 2020 2020 2020 2020 2020 2020  r".             
-0001bf10: 2020 2020 2020 2022 2055 7365 2063 6c69         " Use cli
-0001bf20: 656e 742e 7365 742e 6465 6661 756c 745f  ent.set.default_
-0001bf30: 7072 6f6a 6563 7428 3c50 524f 4a45 4354  project(<PROJECT
-0001bf40: 5f49 4429 220a 2020 2020 2020 2020 2020  _ID)".          
-0001bf50: 2020 2020 2020 290a 0a20 2020 2020 2020        )..       
-0001bf60: 2069 6620 280a 2020 2020 2020 2020 2020   if (.          
-0001bf70: 2020 7365 6c66 2e43 6f6e 6669 6775 7261    self.Configura
-0001bf80: 7469 6f6e 4d65 7461 4e61 6d65 732e 5255  tionMetaNames.RU
-0001bf90: 4e54 494d 455f 4944 2069 6e20 6d65 7461  NTIME_ID in meta
-0001bfa0: 5f70 726f 7073 0a20 2020 2020 2020 2020  _props.         
-0001bfb0: 2020 2061 6e64 2073 656c 662e 436f 6e66     and self.Conf
-0001bfc0: 6967 7572 6174 696f 6e4d 6574 614e 616d  igurationMetaNam
-0001bfd0: 6573 2e53 4f46 5457 4152 455f 5350 4543  es.SOFTWARE_SPEC
-0001bfe0: 5f49 4420 6e6f 7420 696e 206d 6574 615f  _ID not in meta_
-0001bff0: 7072 6f70 730a 2020 2020 2020 2020 293a  props.        ):
-0001c000: 0a20 2020 2020 2020 2020 2020 2072 6169  .            rai
-0001c010: 7365 2057 4d4c 436c 6965 6e74 4572 726f  se WMLClientErro
-0001c020: 7228 0a20 2020 2020 2020 2020 2020 2020  r(.             
-0001c030: 2020 2022 496e 7661 6c69 6420 696e 7075     "Invalid inpu
-0001c040: 742e 2020 5255 4e54 494d 455f 4944 2069  t.  RUNTIME_ID i
-0001c050: 7320 6e6f 7420 7375 7070 6f72 7465 6420  s not supported 
-0001c060: 696e 2063 6c6f 7564 2065 6e76 6972 6f6e  in cloud environ
-0001c070: 6d65 6e74 2e20 5370 6563 6966 7920 534f  ment. Specify SO
-0001c080: 4654 5741 5245 5f53 5045 435f 4944 220a  FTWARE_SPEC_ID".
-0001c090: 2020 2020 2020 2020 2020 2020 290a 0a20              ).. 
-0001c0a0: 2020 2020 2020 2069 6620 7365 6c66 2e43         if self.C
-0001c0b0: 6f6e 6669 6775 7261 7469 6f6e 4d65 7461  onfigurationMeta
-0001c0c0: 4e61 6d65 732e 534f 4654 5741 5245 5f53  Names.SOFTWARE_S
-0001c0d0: 5045 435f 4944 2069 6e20 6d65 7461 5f70  PEC_ID in meta_p
-0001c0e0: 726f 7073 3a0a 2020 2020 2020 2020 2020  rops:.          
-0001c0f0: 2020 7365 6c66 2e5f 7661 6c69 6461 7465    self._validate
-0001c100: 5f6d 6574 615f 7072 6f70 280a 2020 2020  _meta_prop(.    
-0001c110: 2020 2020 2020 2020 2020 2020 6d65 7461              meta
-0001c120: 5f70 726f 7073 2c20 7365 6c66 2e43 6f6e  _props, self.Con
-0001c130: 6669 6775 7261 7469 6f6e 4d65 7461 4e61  figurationMetaNa
-0001c140: 6d65 732e 534f 4654 5741 5245 5f53 5045  mes.SOFTWARE_SPE
-0001c150: 435f 4944 2c20 7374 722c 2054 7275 650a  C_ID, str, True.
-0001c160: 2020 2020 2020 2020 2020 2020 290a 2020              ).  
-0001c170: 2020 2020 2020 2020 2020 6d65 7461 6461            metada
-0001c180: 7461 2e75 7064 6174 6528 0a20 2020 2020  ta.update(.     
-0001c190: 2020 2020 2020 2020 2020 207b 0a20 2020             {.   
-0001c1a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001c1b0: 2073 656c 662e 436f 6e66 6967 7572 6174   self.Configurat
-0001c1c0: 696f 6e4d 6574 614e 616d 6573 2e53 4f46  ionMetaNames.SOF
-0001c1d0: 5457 4152 455f 5350 4543 5f49 443a 207b  TWARE_SPEC_ID: {
-0001c1e0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0001c1f0: 2020 2020 2020 2020 2022 6964 223a 206d           "id": m
-0001c200: 6574 615f 7072 6f70 735b 7365 6c66 2e43  eta_props[self.C
-0001c210: 6f6e 6669 6775 7261 7469 6f6e 4d65 7461  onfigurationMeta
-0001c220: 4e61 6d65 732e 534f 4654 5741 5245 5f53  Names.SOFTWARE_S
-0001c230: 5045 435f 4944 5d0a 2020 2020 2020 2020  PEC_ID].        
-0001c240: 2020 2020 2020 2020 2020 2020 7d0a 2020              }.  
-0001c250: 2020 2020 2020 2020 2020 2020 2020 7d0a                }.
-0001c260: 2020 2020 2020 2020 2020 2020 290a 0a20              ).. 
-0001c270: 2020 2020 2020 2069 6620 7365 6c66 2e43         if self.C
-0001c280: 6f6e 6669 6775 7261 7469 6f6e 4d65 7461  onfigurationMeta
-0001c290: 4e61 6d65 732e 5049 5045 4c49 4e45 5f49  Names.PIPELINE_I
-0001c2a0: 4420 696e 206d 6574 615f 7072 6f70 733a  D in meta_props:
-0001c2b0: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-0001c2c0: 662e 5f76 616c 6964 6174 655f 6d65 7461  f._validate_meta
-0001c2d0: 5f70 726f 7028 0a20 2020 2020 2020 2020  _prop(.         
-0001c2e0: 2020 2020 2020 206d 6574 615f 7072 6f70         meta_prop
-0001c2f0: 732c 2073 656c 662e 436f 6e66 6967 7572  s, self.Configur
-0001c300: 6174 696f 6e4d 6574 614e 616d 6573 2e50  ationMetaNames.P
-0001c310: 4950 454c 494e 455f 4944 2c20 7374 722c  IPELINE_ID, str,
-0001c320: 2046 616c 7365 0a20 2020 2020 2020 2020   False.         
-0001c330: 2020 2029 0a20 2020 2020 2020 2020 2020     ).           
-0001c340: 206d 6574 6164 6174 612e 7570 6461 7465   metadata.update
-0001c350: 280a 2020 2020 2020 2020 2020 2020 2020  (.              
-0001c360: 2020 7b0a 2020 2020 2020 2020 2020 2020    {.            
-0001c370: 2020 2020 2020 2020 7365 6c66 2e43 6f6e          self.Con
-0001c380: 6669 6775 7261 7469 6f6e 4d65 7461 4e61  figurationMetaNa
-0001c390: 6d65 732e 5049 5045 4c49 4e45 5f49 443a  mes.PIPELINE_ID:
-0001c3a0: 207b 0a20 2020 2020 2020 2020 2020 2020   {.             
-0001c3b0: 2020 2020 2020 2020 2020 2022 6964 223a             "id":
-0001c3c0: 206d 6574 615f 7072 6f70 735b 0a20 2020   meta_props[.   
-0001c3d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001c3e0: 2020 2020 2020 2020 2073 656c 662e 5f63           self._c
-0001c3f0: 6c69 656e 742e 7265 706f 7369 746f 7279  lient.repository
-0001c400: 2e4d 6f64 656c 4d65 7461 4e61 6d65 732e  .ModelMetaNames.
-0001c410: 5049 5045 4c49 4e45 5f49 440a 2020 2020  PIPELINE_ID.    
-0001c420: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001c430: 2020 2020 5d0a 2020 2020 2020 2020 2020      ].          
-0001c440: 2020 2020 2020 2020 2020 7d0a 2020 2020            }.    
-0001c450: 2020 2020 2020 2020 2020 2020 7d0a 2020              }.  
-0001c460: 2020 2020 2020 2020 2020 290a 0a20 2020            )..   
-0001c470: 2020 2020 2069 6620 7365 6c66 2e43 6f6e       if self.Con
-0001c480: 6669 6775 7261 7469 6f6e 4d65 7461 4e61  figurationMetaNa
-0001c490: 6d65 732e 4d4f 4445 4c5f 4445 4649 4e49  mes.MODEL_DEFINI
-0001c4a0: 5449 4f4e 5f49 4420 696e 206d 6574 615f  TION_ID in meta_
-0001c4b0: 7072 6f70 733a 0a20 2020 2020 2020 2020  props:.         
-0001c4c0: 2020 2073 656c 662e 5f76 616c 6964 6174     self._validat
-0001c4d0: 655f 6d65 7461 5f70 726f 7028 0a20 2020  e_meta_prop(.   
-0001c4e0: 2020 2020 2020 2020 2020 2020 206d 6574               met
-0001c4f0: 615f 7072 6f70 732c 2073 656c 662e 436f  a_props, self.Co
-0001c500: 6e66 6967 7572 6174 696f 6e4d 6574 614e  nfigurationMetaN
-0001c510: 616d 6573 2e4d 4f44 454c 5f44 4546 494e  ames.MODEL_DEFIN
-0001c520: 4954 494f 4e5f 4944 2c20 7374 722c 2046  ITION_ID, str, F
-0001c530: 616c 7365 0a20 2020 2020 2020 2020 2020  alse.           
-0001c540: 2029 0a20 2020 2020 2020 2020 2020 206d   ).            m
-0001c550: 6574 6164 6174 612e 7570 6461 7465 280a  etadata.update(.
-0001c560: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001c570: 7b0a 2020 2020 2020 2020 2020 2020 2020  {.              
-0001c580: 2020 2020 2020 7365 6c66 2e43 6f6e 6669        self.Confi
-0001c590: 6775 7261 7469 6f6e 4d65 7461 4e61 6d65  gurationMetaName
-0001c5a0: 732e 4d4f 4445 4c5f 4445 4649 4e49 5449  s.MODEL_DEFINITI
-0001c5b0: 4f4e 5f49 443a 207b 0a20 2020 2020 2020  ON_ID: {.       
-0001c5c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001c5d0: 2022 6964 223a 206d 6574 615f 7072 6f70   "id": meta_prop
-0001c5e0: 735b 0a20 2020 2020 2020 2020 2020 2020  s[.             
-0001c5f0: 2020 2020 2020 2020 2020 2020 2020 2073                 s
-0001c600: 656c 662e 5f63 6c69 656e 742e 7265 706f  elf._client.repo
-0001c610: 7369 746f 7279 2e4d 6f64 656c 4d65 7461  sitory.ModelMeta
-0001c620: 4e61 6d65 732e 4d4f 4445 4c5f 4445 4649  Names.MODEL_DEFI
-0001c630: 4e49 5449 4f4e 5f49 440a 2020 2020 2020  NITION_ID.      
-0001c640: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001c650: 2020 5d0a 2020 2020 2020 2020 2020 2020    ].            
-0001c660: 2020 2020 2020 2020 7d0a 2020 2020 2020          }.      
-0001c670: 2020 2020 2020 2020 2020 7d0a 2020 2020            }.    
-0001c680: 2020 2020 2020 2020 290a 0a20 2020 2020          )..     
-0001c690: 2020 2069 6620 280a 2020 2020 2020 2020     if (.        
-0001c6a0: 2020 2020 7365 6c66 2e43 6f6e 6669 6775      self.Configu
-0001c6b0: 7261 7469 6f6e 4d65 7461 4e61 6d65 732e  rationMetaNames.
-0001c6c0: 494d 504f 5254 2069 6e20 6d65 7461 5f70  IMPORT in meta_p
-0001c6d0: 726f 7073 0a20 2020 2020 2020 2020 2020  rops.           
-0001c6e0: 2061 6e64 206d 6574 615f 7072 6f70 735b   and meta_props[
-0001c6f0: 7365 6c66 2e43 6f6e 6669 6775 7261 7469  self.Configurati
-0001c700: 6f6e 4d65 7461 4e61 6d65 732e 494d 504f  onMetaNames.IMPO
-0001c710: 5254 5d20 6973 206e 6f74 204e 6f6e 650a  RT] is not None.
-0001c720: 2020 2020 2020 2020 293a 0a20 2020 2020          ):.     
-0001c730: 2020 2020 2020 2070 7269 6e74 280a 2020         print(.  
-0001c740: 2020 2020 2020 2020 2020 2020 2020 2257                "W
-0001c750: 4152 4e49 4e47 3a20 496e 7661 6c69 6420  ARNING: Invalid 
-0001c760: 696e 7075 742e 2049 4d50 4f52 5420 6973  input. IMPORT is
-0001c770: 206e 6f74 2073 7570 706f 7274 6564 2069   not supported i
-0001c780: 6e20 636c 6f75 6420 656e 7669 726f 6e6d  n cloud environm
-0001c790: 656e 742e 220a 2020 2020 2020 2020 2020  ent.".          
-0001c7a0: 2020 290a 0a20 2020 2020 2020 2069 6620    )..        if 
-0001c7b0: 280a 2020 2020 2020 2020 2020 2020 7365  (.            se
-0001c7c0: 6c66 2e43 6f6e 6669 6775 7261 7469 6f6e  lf.Configuration
-0001c7d0: 4d65 7461 4e61 6d65 732e 5452 4149 4e49  MetaNames.TRAINI
-0001c7e0: 4e47 5f4c 4942 5f49 4420 696e 206d 6574  NG_LIB_ID in met
-0001c7f0: 615f 7072 6f70 730a 2020 2020 2020 2020  a_props.        
-0001c800: 2020 2020 616e 6420 6d65 7461 5f70 726f      and meta_pro
-0001c810: 7073 5b73 656c 662e 436f 6e66 6967 7572  ps[self.Configur
-0001c820: 6174 696f 6e4d 6574 614e 616d 6573 2e49  ationMetaNames.I
-0001c830: 4d50 4f52 545d 2069 7320 6e6f 7420 4e6f  MPORT] is not No
-0001c840: 6e65 0a20 2020 2020 2020 2029 3a0a 2020  ne.        ):.  
-0001c850: 2020 2020 2020 2020 2020 7072 696e 7428            print(
-0001c860: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0001c870: 2022 5741 524e 494e 473a 2049 6e76 616c   "WARNING: Inval
-0001c880: 6964 2069 6e70 7574 2e20 5452 4149 4e49  id input. TRAINI
-0001c890: 4e47 5f4c 4942 5f49 4420 6973 206e 6f74  NG_LIB_ID is not
-0001c8a0: 2073 7570 706f 7274 6564 2069 6e20 636c   supported in cl
-0001c8b0: 6f75 6420 656e 7669 726f 6e6d 656e 742e  oud environment.
-0001c8c0: 220a 2020 2020 2020 2020 2020 2020 290a  ".            ).
-0001c8d0: 0a20 2020 2020 2020 2069 6e70 7574 5f73  .        input_s
-0001c8e0: 6368 656d 6120 3d20 5b5d 0a20 2020 2020  chema = [].     
-0001c8f0: 2020 206f 7574 7075 745f 7363 6865 6d61     output_schema
-0001c900: 203d 205b 5d0a 2020 2020 2020 2020 6966   = [].        if
-0001c910: 2028 0a20 2020 2020 2020 2020 2020 2073   (.            s
-0001c920: 656c 662e 436f 6e66 6967 7572 6174 696f  elf.Configuratio
-0001c930: 6e4d 6574 614e 616d 6573 2e49 4e50 5554  nMetaNames.INPUT
-0001c940: 5f44 4154 415f 5343 4845 4d41 2069 6e20  _DATA_SCHEMA in 
-0001c950: 6d65 7461 5f70 726f 7073 0a20 2020 2020  meta_props.     
-0001c960: 2020 2020 2020 2061 6e64 206d 6574 615f         and meta_
-0001c970: 7072 6f70 735b 7365 6c66 2e43 6f6e 6669  props[self.Confi
-0001c980: 6775 7261 7469 6f6e 4d65 7461 4e61 6d65  gurationMetaName
-0001c990: 732e 494e 5055 545f 4441 5441 5f53 4348  s.INPUT_DATA_SCH
-0001c9a0: 454d 415d 2069 7320 6e6f 7420 4e6f 6e65  EMA] is not None
-0001c9b0: 0a20 2020 2020 2020 2029 3a0a 2020 2020  .        ):.    
-0001c9c0: 2020 2020 2020 2020 6966 2069 7369 6e73          if isins
-0001c9d0: 7461 6e63 6528 0a20 2020 2020 2020 2020  tance(.         
-0001c9e0: 2020 2020 2020 206d 6574 615f 7072 6f70         meta_prop
-0001c9f0: 735b 7365 6c66 2e43 6f6e 6669 6775 7261  s[self.Configura
-0001ca00: 7469 6f6e 4d65 7461 4e61 6d65 732e 494e  tionMetaNames.IN
-0001ca10: 5055 545f 4441 5441 5f53 4348 454d 415d  PUT_DATA_SCHEMA]
-0001ca20: 2c20 6c69 7374 0a20 2020 2020 2020 2020  , list.         
-0001ca30: 2020 2029 3a0a 2020 2020 2020 2020 2020     ):.          
-0001ca40: 2020 2020 2020 7365 6c66 2e5f 7661 6c69        self._vali
-0001ca50: 6461 7465 5f6d 6574 615f 7072 6f70 280a  date_meta_prop(.
-0001ca60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ca70: 2020 2020 6d65 7461 5f70 726f 7073 2c0a      meta_props,.
-0001ca80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ca90: 2020 2020 7365 6c66 2e43 6f6e 6669 6775      self.Configu
-0001caa0: 7261 7469 6f6e 4d65 7461 4e61 6d65 732e  rationMetaNames.
-0001cab0: 494e 5055 545f 4441 5441 5f53 4348 454d  INPUT_DATA_SCHEM
-0001cac0: 412c 0a20 2020 2020 2020 2020 2020 2020  A,.             
-0001cad0: 2020 2020 2020 206c 6973 742c 0a20 2020         list,.   
-0001cae0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001caf0: 2046 616c 7365 2c0a 2020 2020 2020 2020   False,.        
-0001cb00: 2020 2020 2020 2020 290a 2020 2020 2020          ).      
-0001cb10: 2020 2020 2020 2020 2020 696e 7075 745f            input_
-0001cb20: 7363 6865 6d61 203d 206d 6574 615f 7072  schema = meta_pr
-0001cb30: 6f70 735b 7365 6c66 2e43 6f6e 6669 6775  ops[self.Configu
-0001cb40: 7261 7469 6f6e 4d65 7461 4e61 6d65 732e  rationMetaNames.
-0001cb50: 494e 5055 545f 4441 5441 5f53 4348 454d  INPUT_DATA_SCHEM
-0001cb60: 415d 0a20 2020 2020 2020 2020 2020 2065  A].            e
-0001cb70: 6c73 653a 0a20 2020 2020 2020 2020 2020  lse:.           
-0001cb80: 2020 2020 2073 656c 662e 5f76 616c 6964       self._valid
-0001cb90: 6174 655f 6d65 7461 5f70 726f 7028 0a20  ate_meta_prop(. 
-0001cba0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001cbb0: 2020 206d 6574 615f 7072 6f70 732c 0a20     meta_props,. 
-0001cbc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001cbd0: 2020 2073 656c 662e 436f 6e66 6967 7572     self.Configur
-0001cbe0: 6174 696f 6e4d 6574 614e 616d 6573 2e49  ationMetaNames.I
-0001cbf0: 4e50 5554 5f44 4154 415f 5343 4845 4d41  NPUT_DATA_SCHEMA
-0001cc00: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-0001cc10: 2020 2020 2020 6469 6374 2c0a 2020 2020        dict,.    
-0001cc20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001cc30: 4661 6c73 652c 0a20 2020 2020 2020 2020  False,.         
-0001cc40: 2020 2020 2020 2029 0a20 2020 2020 2020         ).       
-0001cc50: 2020 2020 2020 2020 2069 6e70 7574 5f73           input_s
-0001cc60: 6368 656d 6120 3d20 5b0a 2020 2020 2020  chema = [.      
-0001cc70: 2020 2020 2020 2020 2020 2020 2020 6d65                me
-0001cc80: 7461 5f70 726f 7073 5b73 656c 662e 436f  ta_props[self.Co
-0001cc90: 6e66 6967 7572 6174 696f 6e4d 6574 614e  nfigurationMetaN
-0001cca0: 616d 6573 2e49 4e50 5554 5f44 4154 415f  ames.INPUT_DATA_
-0001ccb0: 5343 4845 4d41 5d0a 2020 2020 2020 2020  SCHEMA].        
-0001ccc0: 2020 2020 2020 2020 5d0a 2020 2020 2020          ].      
-0001ccd0: 2020 2020 2020 6d65 7461 6461 7461 2e70        metadata.p
-0001cce0: 6f70 2873 656c 662e 436f 6e66 6967 7572  op(self.Configur
-0001ccf0: 6174 696f 6e4d 6574 614e 616d 6573 2e49  ationMetaNames.I
-0001cd00: 4e50 5554 5f44 4154 415f 5343 4845 4d41  NPUT_DATA_SCHEMA
-0001cd10: 290a 0a20 2020 2020 2020 2069 6620 280a  )..        if (.
-0001cd20: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-0001cd30: 2e43 6f6e 6669 6775 7261 7469 6f6e 4d65  .ConfigurationMe
-0001cd40: 7461 4e61 6d65 732e 4f55 5450 5554 5f44  taNames.OUTPUT_D
-0001cd50: 4154 415f 5343 4845 4d41 2069 6e20 6d65  ATA_SCHEMA in me
-0001cd60: 7461 5f70 726f 7073 0a20 2020 2020 2020  ta_props.       
-0001cd70: 2020 2020 2061 6e64 206d 6574 615f 7072       and meta_pr
-0001cd80: 6f70 735b 7365 6c66 2e43 6f6e 6669 6775  ops[self.Configu
-0001cd90: 7261 7469 6f6e 4d65 7461 4e61 6d65 732e  rationMetaNames.
-0001cda0: 4f55 5450 5554 5f44 4154 415f 5343 4845  OUTPUT_DATA_SCHE
-0001cdb0: 4d41 5d20 6973 206e 6f74 204e 6f6e 650a  MA] is not None.
-0001cdc0: 2020 2020 2020 2020 293a 0a20 2020 2020          ):.     
-0001cdd0: 2020 2020 2020 2069 6620 7374 7228 6d65         if str(me
-0001cde0: 7461 5f70 726f 7073 5b73 656c 662e 436f  ta_props[self.Co
-0001cdf0: 6e66 6967 7572 6174 696f 6e4d 6574 614e  nfigurationMetaN
-0001ce00: 616d 6573 2e54 5950 455d 292e 7374 6172  ames.TYPE]).star
-0001ce10: 7473 7769 7468 2822 646f 2d22 293a 0a20  tswith("do-"):. 
-0001ce20: 2020 2020 2020 2020 2020 2020 2020 2069                 i
-0001ce30: 6620 6973 696e 7374 616e 6365 280a 2020  f isinstance(.  
-0001ce40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ce50: 2020 6d65 7461 5f70 726f 7073 5b73 656c    meta_props[sel
-0001ce60: 662e 436f 6e66 6967 7572 6174 696f 6e4d  f.ConfigurationM
-0001ce70: 6574 614e 616d 6573 2e4f 5554 5055 545f  etaNames.OUTPUT_
-0001ce80: 4441 5441 5f53 4348 454d 415d 2c20 6469  DATA_SCHEMA], di
-0001ce90: 6374 0a20 2020 2020 2020 2020 2020 2020  ct.             
-0001cea0: 2020 2029 3a0a 2020 2020 2020 2020 2020     ):.          
-0001ceb0: 2020 2020 2020 2020 2020 7365 6c66 2e5f            self._
-0001cec0: 7661 6c69 6461 7465 5f6d 6574 615f 7072  validate_meta_pr
-0001ced0: 6f70 280a 2020 2020 2020 2020 2020 2020  op(.            
-0001cee0: 2020 2020 2020 2020 2020 2020 6d65 7461              meta
-0001cef0: 5f70 726f 7073 2c0a 2020 2020 2020 2020  _props,.        
-0001cf00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001cf10: 7365 6c66 2e43 6f6e 6669 6775 7261 7469  self.Configurati
-0001cf20: 6f6e 4d65 7461 4e61 6d65 732e 4f55 5450  onMetaNames.OUTP
-0001cf30: 5554 5f44 4154 415f 5343 4845 4d41 2c0a  UT_DATA_SCHEMA,.
-0001cf40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001cf50: 2020 2020 2020 2020 6469 6374 2c0a 2020          dict,.  
-0001cf60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001cf70: 2020 2020 2020 4661 6c73 652c 0a20 2020        False,.   
-0001cf80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001cf90: 2029 0a20 2020 2020 2020 2020 2020 2020   ).             
-0001cfa0: 2020 2020 2020 206f 7574 7075 745f 7363         output_sc
-0001cfb0: 6865 6d61 203d 205b 0a20 2020 2020 2020  hema = [.       
-0001cfc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001cfd0: 206d 6574 615f 7072 6f70 735b 7365 6c66   meta_props[self
-0001cfe0: 2e43 6f6e 6669 6775 7261 7469 6f6e 4d65  .ConfigurationMe
-0001cff0: 7461 4e61 6d65 732e 4f55 5450 5554 5f44  taNames.OUTPUT_D
-0001d000: 4154 415f 5343 4845 4d41 5d0a 2020 2020  ATA_SCHEMA].    
-0001d010: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001d020: 5d0a 2020 2020 2020 2020 2020 2020 2020  ].              
-0001d030: 2020 656c 7365 3a0a 2020 2020 2020 2020    else:.        
-0001d040: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-0001d050: 2e5f 7661 6c69 6461 7465 5f6d 6574 615f  ._validate_meta_
-0001d060: 7072 6f70 280a 2020 2020 2020 2020 2020  prop(.          
-0001d070: 2020 2020 2020 2020 2020 2020 2020 6d65                me
-0001d080: 7461 5f70 726f 7073 2c0a 2020 2020 2020  ta_props,.      
-0001d090: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001d0a0: 2020 7365 6c66 2e43 6f6e 6669 6775 7261    self.Configura
-0001d0b0: 7469 6f6e 4d65 7461 4e61 6d65 732e 4f55  tionMetaNames.OU
-0001d0c0: 5450 5554 5f44 4154 415f 5343 4845 4d41  TPUT_DATA_SCHEMA
-0001d0d0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-0001d0e0: 2020 2020 2020 2020 2020 6c69 7374 2c0a            list,.
-0001d0f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001d100: 2020 2020 2020 2020 4661 6c73 652c 0a20          False,. 
+0001b910: 2020 2064 6174 613d 662c 0a20 2020 2020     data=f,.     
+0001b920: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001b930: 2020 2020 2020 2070 6172 616d 733d 7170         params=qp
+0001b940: 6172 616d 732c 0a20 2020 2020 2020 2020  arams,.         
+0001b950: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001b960: 2020 2068 6561 6465 7273 3d73 656c 662e     headers=self.
+0001b970: 5f63 6c69 656e 742e 5f67 6574 5f68 6561  _client._get_hea
+0001b980: 6465 7273 280a 2020 2020 2020 2020 2020  ders(.          
+0001b990: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001b9a0: 2020 2020 2020 636f 6e74 656e 745f 7479        content_ty
+0001b9b0: 7065 3d22 6170 706c 6963 6174 696f 6e2f  pe="application/
+0001b9c0: 6f63 7465 742d 7374 7265 616d 220a 2020  octet-stream".  
+0001b9d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001b9e0: 2020 2020 2020 2020 2020 292c 0a20 2020            ),.   
+0001b9f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001ba00: 2020 2020 2029 0a20 2020 2020 2020 2020       ).         
+0001ba10: 2020 2020 2020 2073 656c 662e 5f68 616e         self._han
+0001ba20: 646c 655f 7265 7370 6f6e 7365 2832 3031  dle_response(201
+0001ba30: 2c20 2275 706c 6f61 6469 6e67 206d 6f64  , "uploading mod
+0001ba40: 656c 2063 6f6e 7465 6e74 222c 2072 6573  el content", res
+0001ba50: 706f 6e73 652c 2046 616c 7365 290a 0a20  ponse, False).. 
+0001ba60: 2020 2064 6566 205f 6372 6561 7465 5f63     def _create_c
+0001ba70: 6c6f 7564 5f6d 6f64 656c 5f70 6179 6c6f  loud_model_paylo
+0001ba80: 6164 280a 2020 2020 2020 2020 7365 6c66  ad(.        self
+0001ba90: 2c0a 2020 2020 2020 2020 6d65 7461 5f70  ,.        meta_p
+0001baa0: 726f 7073 3a20 6469 6374 5b73 7472 2c20  rops: dict[str, 
+0001bab0: 416e 795d 2c0a 2020 2020 2020 2020 6665  Any],.        fe
+0001bac0: 6174 7572 655f 6e61 6d65 733a 2046 6561  ature_names: Fea
+0001bad0: 7475 7265 4e61 6d65 7341 7272 6179 5479  tureNamesArrayTy
+0001bae0: 7065 207c 204e 6f6e 6520 3d20 4e6f 6e65  pe | None = None
+0001baf0: 2c0a 2020 2020 2020 2020 6c61 6265 6c5f  ,.        label_
+0001bb00: 636f 6c75 6d6e 5f6e 616d 6573 3a20 4c61  column_names: La
+0001bb10: 6265 6c43 6f6c 756d 6e4e 616d 6573 5479  belColumnNamesTy
+0001bb20: 7065 207c 204e 6f6e 6520 3d20 4e6f 6e65  pe | None = None
+0001bb30: 2c0a 2020 2020 2920 2d3e 2064 6963 745b  ,.    ) -> dict[
+0001bb40: 7374 722c 2041 6e79 5d3a 0a20 2020 2020  str, Any]:.     
+0001bb50: 2020 206d 6574 6164 6174 6120 3d20 636f     metadata = co
+0001bb60: 7079 2e64 6565 7063 6f70 7928 6d65 7461  py.deepcopy(meta
+0001bb70: 5f70 726f 7073 290a 2020 2020 2020 2020  _props).        
+0001bb80: 6966 2073 656c 662e 5f63 6c69 656e 742e  if self._client.
+0001bb90: 6465 6661 756c 745f 7370 6163 655f 6964  default_space_id
+0001bba0: 2069 7320 6e6f 7420 4e6f 6e65 3a0a 2020   is not None:.  
+0001bbb0: 2020 2020 2020 2020 2020 6d65 7461 6461            metada
+0001bbc0: 7461 5b22 7370 6163 655f 6964 225d 203d  ta["space_id"] =
+0001bbd0: 2073 656c 662e 5f63 6c69 656e 742e 6465   self._client.de
+0001bbe0: 6661 756c 745f 7370 6163 655f 6964 0a20  fault_space_id. 
+0001bbf0: 2020 2020 2020 2065 6c73 653a 0a20 2020         else:.   
+0001bc00: 2020 2020 2020 2020 2069 6620 7365 6c66           if self
+0001bc10: 2e5f 636c 6965 6e74 2e64 6566 6175 6c74  ._client.default
+0001bc20: 5f70 726f 6a65 6374 5f69 6420 6973 206e  _project_id is n
+0001bc30: 6f74 204e 6f6e 653a 0a20 2020 2020 2020  ot None:.       
+0001bc40: 2020 2020 2020 2020 206d 6574 6164 6174           metadat
+0001bc50: 612e 7570 6461 7465 287b 2270 726f 6a65  a.update({"proje
+0001bc60: 6374 5f69 6422 3a20 7365 6c66 2e5f 636c  ct_id": self._cl
+0001bc70: 6965 6e74 2e64 6566 6175 6c74 5f70 726f  ient.default_pro
+0001bc80: 6a65 6374 5f69 647d 290a 2020 2020 2020  ject_id}).      
+0001bc90: 2020 2020 2020 656c 7365 3a0a 2020 2020        else:.    
+0001bca0: 2020 2020 2020 2020 2020 2020 7261 6973              rais
+0001bcb0: 6520 574d 4c43 6c69 656e 7445 7272 6f72  e WMLClientError
+0001bcc0: 280a 2020 2020 2020 2020 2020 2020 2020  (.              
+0001bcd0: 2020 2020 2020 2249 7420 6973 206d 616e        "It is man
+0001bce0: 6461 746f 7279 2069 7320 7365 7420 7468  datory is set th
+0001bcf0: 6520 7370 6163 6520 6f72 2050 726f 6a65  e space or Proje
+0001bd00: 6374 2e20 5c0a 2020 2020 2020 2020 2020  ct. \.          
+0001bd10: 2020 2020 2020 2055 7365 2063 6c69 656e         Use clien
+0001bd20: 742e 7365 742e 6465 6661 756c 745f 7370  t.set.default_sp
+0001bd30: 6163 6528 3c53 5041 4345 5f49 443e 2920  ace(<SPACE_ID>) 
+0001bd40: 746f 2073 6574 2074 6865 2073 7061 6365  to set the space
+0001bd50: 206f 7222 0a20 2020 2020 2020 2020 2020   or".           
+0001bd60: 2020 2020 2020 2020 2022 2055 7365 2063           " Use c
+0001bd70: 6c69 656e 742e 7365 742e 6465 6661 756c  lient.set.defaul
+0001bd80: 745f 7072 6f6a 6563 7428 3c50 524f 4a45  t_project(<PROJE
+0001bd90: 4354 5f49 4429 220a 2020 2020 2020 2020  CT_ID)".        
+0001bda0: 2020 2020 2020 2020 290a 0a20 2020 2020          )..     
+0001bdb0: 2020 2069 6620 280a 2020 2020 2020 2020     if (.        
+0001bdc0: 2020 2020 7365 6c66 2e43 6f6e 6669 6775      self.Configu
+0001bdd0: 7261 7469 6f6e 4d65 7461 4e61 6d65 732e  rationMetaNames.
+0001bde0: 5255 4e54 494d 455f 4944 2069 6e20 6d65  RUNTIME_ID in me
+0001bdf0: 7461 5f70 726f 7073 0a20 2020 2020 2020  ta_props.       
+0001be00: 2020 2020 2061 6e64 2073 656c 662e 436f       and self.Co
+0001be10: 6e66 6967 7572 6174 696f 6e4d 6574 614e  nfigurationMetaN
+0001be20: 616d 6573 2e53 4f46 5457 4152 455f 5350  ames.SOFTWARE_SP
+0001be30: 4543 5f49 4420 6e6f 7420 696e 206d 6574  EC_ID not in met
+0001be40: 615f 7072 6f70 730a 2020 2020 2020 2020  a_props.        
+0001be50: 293a 0a20 2020 2020 2020 2020 2020 2072  ):.            r
+0001be60: 6169 7365 2057 4d4c 436c 6965 6e74 4572  aise WMLClientEr
+0001be70: 726f 7228 0a20 2020 2020 2020 2020 2020  ror(.           
+0001be80: 2020 2020 2022 496e 7661 6c69 6420 696e       "Invalid in
+0001be90: 7075 742e 2020 5255 4e54 494d 455f 4944  put.  RUNTIME_ID
+0001bea0: 2069 7320 6e6f 7420 7375 7070 6f72 7465   is not supporte
+0001beb0: 6420 696e 2063 6c6f 7564 2065 6e76 6972  d in cloud envir
+0001bec0: 6f6e 6d65 6e74 2e20 5370 6563 6966 7920  onment. Specify 
+0001bed0: 534f 4654 5741 5245 5f53 5045 435f 4944  SOFTWARE_SPEC_ID
+0001bee0: 220a 2020 2020 2020 2020 2020 2020 290a  ".            ).
+0001bef0: 0a20 2020 2020 2020 2069 6620 7365 6c66  .        if self
+0001bf00: 2e43 6f6e 6669 6775 7261 7469 6f6e 4d65  .ConfigurationMe
+0001bf10: 7461 4e61 6d65 732e 534f 4654 5741 5245  taNames.SOFTWARE
+0001bf20: 5f53 5045 435f 4944 2069 6e20 6d65 7461  _SPEC_ID in meta
+0001bf30: 5f70 726f 7073 3a0a 2020 2020 2020 2020  _props:.        
+0001bf40: 2020 2020 7365 6c66 2e5f 7661 6c69 6461      self._valida
+0001bf50: 7465 5f6d 6574 615f 7072 6f70 280a 2020  te_meta_prop(.  
+0001bf60: 2020 2020 2020 2020 2020 2020 2020 6d65                me
+0001bf70: 7461 5f70 726f 7073 2c20 7365 6c66 2e43  ta_props, self.C
+0001bf80: 6f6e 6669 6775 7261 7469 6f6e 4d65 7461  onfigurationMeta
+0001bf90: 4e61 6d65 732e 534f 4654 5741 5245 5f53  Names.SOFTWARE_S
+0001bfa0: 5045 435f 4944 2c20 7374 722c 2054 7275  PEC_ID, str, Tru
+0001bfb0: 650a 2020 2020 2020 2020 2020 2020 290a  e.            ).
+0001bfc0: 2020 2020 2020 2020 2020 2020 6d65 7461              meta
+0001bfd0: 6461 7461 2e75 7064 6174 6528 0a20 2020  data.update(.   
+0001bfe0: 2020 2020 2020 2020 2020 2020 207b 0a20               {. 
+0001bff0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001c000: 2020 2073 656c 662e 436f 6e66 6967 7572     self.Configur
+0001c010: 6174 696f 6e4d 6574 614e 616d 6573 2e53  ationMetaNames.S
+0001c020: 4f46 5457 4152 455f 5350 4543 5f49 443a  OFTWARE_SPEC_ID:
+0001c030: 207b 0a20 2020 2020 2020 2020 2020 2020   {.             
+0001c040: 2020 2020 2020 2020 2020 2022 6964 223a             "id":
+0001c050: 206d 6574 615f 7072 6f70 735b 7365 6c66   meta_props[self
+0001c060: 2e43 6f6e 6669 6775 7261 7469 6f6e 4d65  .ConfigurationMe
+0001c070: 7461 4e61 6d65 732e 534f 4654 5741 5245  taNames.SOFTWARE
+0001c080: 5f53 5045 435f 4944 5d0a 2020 2020 2020  _SPEC_ID].      
+0001c090: 2020 2020 2020 2020 2020 2020 2020 7d0a                }.
+0001c0a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001c0b0: 7d0a 2020 2020 2020 2020 2020 2020 290a  }.            ).
+0001c0c0: 0a20 2020 2020 2020 2069 6620 7365 6c66  .        if self
+0001c0d0: 2e43 6f6e 6669 6775 7261 7469 6f6e 4d65  .ConfigurationMe
+0001c0e0: 7461 4e61 6d65 732e 5049 5045 4c49 4e45  taNames.PIPELINE
+0001c0f0: 5f49 4420 696e 206d 6574 615f 7072 6f70  _ID in meta_prop
+0001c100: 733a 0a20 2020 2020 2020 2020 2020 2073  s:.            s
+0001c110: 656c 662e 5f76 616c 6964 6174 655f 6d65  elf._validate_me
+0001c120: 7461 5f70 726f 7028 0a20 2020 2020 2020  ta_prop(.       
+0001c130: 2020 2020 2020 2020 206d 6574 615f 7072           meta_pr
+0001c140: 6f70 732c 2073 656c 662e 436f 6e66 6967  ops, self.Config
+0001c150: 7572 6174 696f 6e4d 6574 614e 616d 6573  urationMetaNames
+0001c160: 2e50 4950 454c 494e 455f 4944 2c20 7374  .PIPELINE_ID, st
+0001c170: 722c 2046 616c 7365 0a20 2020 2020 2020  r, False.       
+0001c180: 2020 2020 2029 0a20 2020 2020 2020 2020       ).         
+0001c190: 2020 206d 6574 6164 6174 612e 7570 6461     metadata.upda
+0001c1a0: 7465 280a 2020 2020 2020 2020 2020 2020  te(.            
+0001c1b0: 2020 2020 7b0a 2020 2020 2020 2020 2020      {.          
+0001c1c0: 2020 2020 2020 2020 2020 7365 6c66 2e43            self.C
+0001c1d0: 6f6e 6669 6775 7261 7469 6f6e 4d65 7461  onfigurationMeta
+0001c1e0: 4e61 6d65 732e 5049 5045 4c49 4e45 5f49  Names.PIPELINE_I
+0001c1f0: 443a 207b 0a20 2020 2020 2020 2020 2020  D: {.           
+0001c200: 2020 2020 2020 2020 2020 2020 2022 6964               "id
+0001c210: 223a 206d 6574 615f 7072 6f70 735b 0a20  ": meta_props[. 
+0001c220: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001c230: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+0001c240: 5f63 6c69 656e 742e 7265 706f 7369 746f  _client.reposito
+0001c250: 7279 2e4d 6f64 656c 4d65 7461 4e61 6d65  ry.ModelMetaName
+0001c260: 732e 5049 5045 4c49 4e45 5f49 440a 2020  s.PIPELINE_ID.  
+0001c270: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001c280: 2020 2020 2020 5d0a 2020 2020 2020 2020        ].        
+0001c290: 2020 2020 2020 2020 2020 2020 7d0a 2020              }.  
+0001c2a0: 2020 2020 2020 2020 2020 2020 2020 7d0a                }.
+0001c2b0: 2020 2020 2020 2020 2020 2020 290a 0a20              ).. 
+0001c2c0: 2020 2020 2020 2069 6620 7365 6c66 2e43         if self.C
+0001c2d0: 6f6e 6669 6775 7261 7469 6f6e 4d65 7461  onfigurationMeta
+0001c2e0: 4e61 6d65 732e 4d4f 4445 4c5f 4445 4649  Names.MODEL_DEFI
+0001c2f0: 4e49 5449 4f4e 5f49 4420 696e 206d 6574  NITION_ID in met
+0001c300: 615f 7072 6f70 733a 0a20 2020 2020 2020  a_props:.       
+0001c310: 2020 2020 2073 656c 662e 5f76 616c 6964       self._valid
+0001c320: 6174 655f 6d65 7461 5f70 726f 7028 0a20  ate_meta_prop(. 
+0001c330: 2020 2020 2020 2020 2020 2020 2020 206d                 m
+0001c340: 6574 615f 7072 6f70 732c 2073 656c 662e  eta_props, self.
+0001c350: 436f 6e66 6967 7572 6174 696f 6e4d 6574  ConfigurationMet
+0001c360: 614e 616d 6573 2e4d 4f44 454c 5f44 4546  aNames.MODEL_DEF
+0001c370: 494e 4954 494f 4e5f 4944 2c20 7374 722c  INITION_ID, str,
+0001c380: 2046 616c 7365 0a20 2020 2020 2020 2020   False.         
+0001c390: 2020 2029 0a20 2020 2020 2020 2020 2020     ).           
+0001c3a0: 206d 6574 6164 6174 612e 7570 6461 7465   metadata.update
+0001c3b0: 280a 2020 2020 2020 2020 2020 2020 2020  (.              
+0001c3c0: 2020 7b0a 2020 2020 2020 2020 2020 2020    {.            
+0001c3d0: 2020 2020 2020 2020 7365 6c66 2e43 6f6e          self.Con
+0001c3e0: 6669 6775 7261 7469 6f6e 4d65 7461 4e61  figurationMetaNa
+0001c3f0: 6d65 732e 4d4f 4445 4c5f 4445 4649 4e49  mes.MODEL_DEFINI
+0001c400: 5449 4f4e 5f49 443a 207b 0a20 2020 2020  TION_ID: {.     
+0001c410: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001c420: 2020 2022 6964 223a 206d 6574 615f 7072     "id": meta_pr
+0001c430: 6f70 735b 0a20 2020 2020 2020 2020 2020  ops[.           
+0001c440: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001c450: 2073 656c 662e 5f63 6c69 656e 742e 7265   self._client.re
+0001c460: 706f 7369 746f 7279 2e4d 6f64 656c 4d65  pository.ModelMe
+0001c470: 7461 4e61 6d65 732e 4d4f 4445 4c5f 4445  taNames.MODEL_DE
+0001c480: 4649 4e49 5449 4f4e 5f49 440a 2020 2020  FINITION_ID.    
+0001c490: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001c4a0: 2020 2020 5d0a 2020 2020 2020 2020 2020      ].          
+0001c4b0: 2020 2020 2020 2020 2020 7d0a 2020 2020            }.    
+0001c4c0: 2020 2020 2020 2020 2020 2020 7d0a 2020              }.  
+0001c4d0: 2020 2020 2020 2020 2020 290a 0a20 2020            )..   
+0001c4e0: 2020 2020 2069 6620 280a 2020 2020 2020       if (.      
+0001c4f0: 2020 2020 2020 7365 6c66 2e43 6f6e 6669        self.Confi
+0001c500: 6775 7261 7469 6f6e 4d65 7461 4e61 6d65  gurationMetaName
+0001c510: 732e 494d 504f 5254 2069 6e20 6d65 7461  s.IMPORT in meta
+0001c520: 5f70 726f 7073 0a20 2020 2020 2020 2020  _props.         
+0001c530: 2020 2061 6e64 206d 6574 615f 7072 6f70     and meta_prop
+0001c540: 735b 7365 6c66 2e43 6f6e 6669 6775 7261  s[self.Configura
+0001c550: 7469 6f6e 4d65 7461 4e61 6d65 732e 494d  tionMetaNames.IM
+0001c560: 504f 5254 5d20 6973 206e 6f74 204e 6f6e  PORT] is not Non
+0001c570: 650a 2020 2020 2020 2020 293a 0a20 2020  e.        ):.   
+0001c580: 2020 2020 2020 2020 2070 7269 6e74 280a           print(.
+0001c590: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001c5a0: 2257 4152 4e49 4e47 3a20 496e 7661 6c69  "WARNING: Invali
+0001c5b0: 6420 696e 7075 742e 2049 4d50 4f52 5420  d input. IMPORT 
+0001c5c0: 6973 206e 6f74 2073 7570 706f 7274 6564  is not supported
+0001c5d0: 2069 6e20 636c 6f75 6420 656e 7669 726f   in cloud enviro
+0001c5e0: 6e6d 656e 742e 220a 2020 2020 2020 2020  nment.".        
+0001c5f0: 2020 2020 290a 0a20 2020 2020 2020 2069      )..        i
+0001c600: 6620 280a 2020 2020 2020 2020 2020 2020  f (.            
+0001c610: 7365 6c66 2e43 6f6e 6669 6775 7261 7469  self.Configurati
+0001c620: 6f6e 4d65 7461 4e61 6d65 732e 5452 4149  onMetaNames.TRAI
+0001c630: 4e49 4e47 5f4c 4942 5f49 4420 696e 206d  NING_LIB_ID in m
+0001c640: 6574 615f 7072 6f70 730a 2020 2020 2020  eta_props.      
+0001c650: 2020 2020 2020 616e 6420 6d65 7461 5f70        and meta_p
+0001c660: 726f 7073 5b73 656c 662e 436f 6e66 6967  rops[self.Config
+0001c670: 7572 6174 696f 6e4d 6574 614e 616d 6573  urationMetaNames
+0001c680: 2e49 4d50 4f52 545d 2069 7320 6e6f 7420  .IMPORT] is not 
+0001c690: 4e6f 6e65 0a20 2020 2020 2020 2029 3a0a  None.        ):.
+0001c6a0: 2020 2020 2020 2020 2020 2020 7072 696e              prin
+0001c6b0: 7428 0a20 2020 2020 2020 2020 2020 2020  t(.             
+0001c6c0: 2020 2022 5741 524e 494e 473a 2049 6e76     "WARNING: Inv
+0001c6d0: 616c 6964 2069 6e70 7574 2e20 5452 4149  alid input. TRAI
+0001c6e0: 4e49 4e47 5f4c 4942 5f49 4420 6973 206e  NING_LIB_ID is n
+0001c6f0: 6f74 2073 7570 706f 7274 6564 2069 6e20  ot supported in 
+0001c700: 636c 6f75 6420 656e 7669 726f 6e6d 656e  cloud environmen
+0001c710: 742e 220a 2020 2020 2020 2020 2020 2020  t.".            
+0001c720: 290a 0a20 2020 2020 2020 2069 6e70 7574  )..        input
+0001c730: 5f73 6368 656d 6120 3d20 5b5d 0a20 2020  _schema = [].   
+0001c740: 2020 2020 206f 7574 7075 745f 7363 6865       output_sche
+0001c750: 6d61 203d 205b 5d0a 2020 2020 2020 2020  ma = [].        
+0001c760: 6966 2028 0a20 2020 2020 2020 2020 2020  if (.           
+0001c770: 2073 656c 662e 436f 6e66 6967 7572 6174   self.Configurat
+0001c780: 696f 6e4d 6574 614e 616d 6573 2e49 4e50  ionMetaNames.INP
+0001c790: 5554 5f44 4154 415f 5343 4845 4d41 2069  UT_DATA_SCHEMA i
+0001c7a0: 6e20 6d65 7461 5f70 726f 7073 0a20 2020  n meta_props.   
+0001c7b0: 2020 2020 2020 2020 2061 6e64 206d 6574           and met
+0001c7c0: 615f 7072 6f70 735b 7365 6c66 2e43 6f6e  a_props[self.Con
+0001c7d0: 6669 6775 7261 7469 6f6e 4d65 7461 4e61  figurationMetaNa
+0001c7e0: 6d65 732e 494e 5055 545f 4441 5441 5f53  mes.INPUT_DATA_S
+0001c7f0: 4348 454d 415d 2069 7320 6e6f 7420 4e6f  CHEMA] is not No
+0001c800: 6e65 0a20 2020 2020 2020 2029 3a0a 2020  ne.        ):.  
+0001c810: 2020 2020 2020 2020 2020 6966 2069 7369            if isi
+0001c820: 6e73 7461 6e63 6528 0a20 2020 2020 2020  nstance(.       
+0001c830: 2020 2020 2020 2020 206d 6574 615f 7072           meta_pr
+0001c840: 6f70 735b 7365 6c66 2e43 6f6e 6669 6775  ops[self.Configu
+0001c850: 7261 7469 6f6e 4d65 7461 4e61 6d65 732e  rationMetaNames.
+0001c860: 494e 5055 545f 4441 5441 5f53 4348 454d  INPUT_DATA_SCHEM
+0001c870: 415d 2c20 6c69 7374 0a20 2020 2020 2020  A], list.       
+0001c880: 2020 2020 2029 3a0a 2020 2020 2020 2020       ):.        
+0001c890: 2020 2020 2020 2020 7365 6c66 2e5f 7661          self._va
+0001c8a0: 6c69 6461 7465 5f6d 6574 615f 7072 6f70  lidate_meta_prop
+0001c8b0: 280a 2020 2020 2020 2020 2020 2020 2020  (.              
+0001c8c0: 2020 2020 2020 6d65 7461 5f70 726f 7073        meta_props
+0001c8d0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+0001c8e0: 2020 2020 2020 7365 6c66 2e43 6f6e 6669        self.Confi
+0001c8f0: 6775 7261 7469 6f6e 4d65 7461 4e61 6d65  gurationMetaName
+0001c900: 732e 494e 5055 545f 4441 5441 5f53 4348  s.INPUT_DATA_SCH
+0001c910: 454d 412c 0a20 2020 2020 2020 2020 2020  EMA,.           
+0001c920: 2020 2020 2020 2020 206c 6973 742c 0a20           list,. 
+0001c930: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001c940: 2020 2046 616c 7365 2c0a 2020 2020 2020     False,.      
+0001c950: 2020 2020 2020 2020 2020 290a 2020 2020            ).    
+0001c960: 2020 2020 2020 2020 2020 2020 696e 7075              inpu
+0001c970: 745f 7363 6865 6d61 203d 206d 6574 615f  t_schema = meta_
+0001c980: 7072 6f70 735b 7365 6c66 2e43 6f6e 6669  props[self.Confi
+0001c990: 6775 7261 7469 6f6e 4d65 7461 4e61 6d65  gurationMetaName
+0001c9a0: 732e 494e 5055 545f 4441 5441 5f53 4348  s.INPUT_DATA_SCH
+0001c9b0: 454d 415d 0a20 2020 2020 2020 2020 2020  EMA].           
+0001c9c0: 2065 6c73 653a 0a20 2020 2020 2020 2020   else:.         
+0001c9d0: 2020 2020 2020 2073 656c 662e 5f76 616c         self._val
+0001c9e0: 6964 6174 655f 6d65 7461 5f70 726f 7028  idate_meta_prop(
+0001c9f0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0001ca00: 2020 2020 206d 6574 615f 7072 6f70 732c       meta_props,
+0001ca10: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0001ca20: 2020 2020 2073 656c 662e 436f 6e66 6967       self.Config
+0001ca30: 7572 6174 696f 6e4d 6574 614e 616d 6573  urationMetaNames
+0001ca40: 2e49 4e50 5554 5f44 4154 415f 5343 4845  .INPUT_DATA_SCHE
+0001ca50: 4d41 2c0a 2020 2020 2020 2020 2020 2020  MA,.            
+0001ca60: 2020 2020 2020 2020 6469 6374 2c0a 2020          dict,.  
+0001ca70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001ca80: 2020 4661 6c73 652c 0a20 2020 2020 2020    False,.       
+0001ca90: 2020 2020 2020 2020 2029 0a20 2020 2020           ).     
+0001caa0: 2020 2020 2020 2020 2020 2069 6e70 7574             input
+0001cab0: 5f73 6368 656d 6120 3d20 5b0a 2020 2020  _schema = [.    
+0001cac0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001cad0: 6d65 7461 5f70 726f 7073 5b73 656c 662e  meta_props[self.
+0001cae0: 436f 6e66 6967 7572 6174 696f 6e4d 6574  ConfigurationMet
+0001caf0: 614e 616d 6573 2e49 4e50 5554 5f44 4154  aNames.INPUT_DAT
+0001cb00: 415f 5343 4845 4d41 5d0a 2020 2020 2020  A_SCHEMA].      
+0001cb10: 2020 2020 2020 2020 2020 5d0a 2020 2020            ].    
+0001cb20: 2020 2020 2020 2020 6d65 7461 6461 7461          metadata
+0001cb30: 2e70 6f70 2873 656c 662e 436f 6e66 6967  .pop(self.Config
+0001cb40: 7572 6174 696f 6e4d 6574 614e 616d 6573  urationMetaNames
+0001cb50: 2e49 4e50 5554 5f44 4154 415f 5343 4845  .INPUT_DATA_SCHE
+0001cb60: 4d41 290a 0a20 2020 2020 2020 2069 6620  MA)..        if 
+0001cb70: 280a 2020 2020 2020 2020 2020 2020 7365  (.            se
+0001cb80: 6c66 2e43 6f6e 6669 6775 7261 7469 6f6e  lf.Configuration
+0001cb90: 4d65 7461 4e61 6d65 732e 4f55 5450 5554  MetaNames.OUTPUT
+0001cba0: 5f44 4154 415f 5343 4845 4d41 2069 6e20  _DATA_SCHEMA in 
+0001cbb0: 6d65 7461 5f70 726f 7073 0a20 2020 2020  meta_props.     
+0001cbc0: 2020 2020 2020 2061 6e64 206d 6574 615f         and meta_
+0001cbd0: 7072 6f70 735b 7365 6c66 2e43 6f6e 6669  props[self.Confi
+0001cbe0: 6775 7261 7469 6f6e 4d65 7461 4e61 6d65  gurationMetaName
+0001cbf0: 732e 4f55 5450 5554 5f44 4154 415f 5343  s.OUTPUT_DATA_SC
+0001cc00: 4845 4d41 5d20 6973 206e 6f74 204e 6f6e  HEMA] is not Non
+0001cc10: 650a 2020 2020 2020 2020 293a 0a20 2020  e.        ):.   
+0001cc20: 2020 2020 2020 2020 2069 6620 7374 7228           if str(
+0001cc30: 6d65 7461 5f70 726f 7073 5b73 656c 662e  meta_props[self.
+0001cc40: 436f 6e66 6967 7572 6174 696f 6e4d 6574  ConfigurationMet
+0001cc50: 614e 616d 6573 2e54 5950 455d 292e 7374  aNames.TYPE]).st
+0001cc60: 6172 7473 7769 7468 2822 646f 2d22 293a  artswith("do-"):
+0001cc70: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0001cc80: 2069 6620 6973 696e 7374 616e 6365 280a   if isinstance(.
+0001cc90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001cca0: 2020 2020 6d65 7461 5f70 726f 7073 5b73      meta_props[s
+0001ccb0: 656c 662e 436f 6e66 6967 7572 6174 696f  elf.Configuratio
+0001ccc0: 6e4d 6574 614e 616d 6573 2e4f 5554 5055  nMetaNames.OUTPU
+0001ccd0: 545f 4441 5441 5f53 4348 454d 415d 2c20  T_DATA_SCHEMA], 
+0001cce0: 6469 6374 0a20 2020 2020 2020 2020 2020  dict.           
+0001ccf0: 2020 2020 2029 3a0a 2020 2020 2020 2020       ):.        
+0001cd00: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+0001cd10: 2e5f 7661 6c69 6461 7465 5f6d 6574 615f  ._validate_meta_
+0001cd20: 7072 6f70 280a 2020 2020 2020 2020 2020  prop(.          
+0001cd30: 2020 2020 2020 2020 2020 2020 2020 6d65                me
+0001cd40: 7461 5f70 726f 7073 2c0a 2020 2020 2020  ta_props,.      
+0001cd50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001cd60: 2020 7365 6c66 2e43 6f6e 6669 6775 7261    self.Configura
+0001cd70: 7469 6f6e 4d65 7461 4e61 6d65 732e 4f55  tionMetaNames.OU
+0001cd80: 5450 5554 5f44 4154 415f 5343 4845 4d41  TPUT_DATA_SCHEMA
+0001cd90: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+0001cda0: 2020 2020 2020 2020 2020 6469 6374 2c0a            dict,.
+0001cdb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001cdc0: 2020 2020 2020 2020 4661 6c73 652c 0a20          False,. 
+0001cdd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001cde0: 2020 2029 0a20 2020 2020 2020 2020 2020     ).           
+0001cdf0: 2020 2020 2020 2020 206f 7574 7075 745f           output_
+0001ce00: 7363 6865 6d61 203d 205b 0a20 2020 2020  schema = [.     
+0001ce10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001ce20: 2020 206d 6574 615f 7072 6f70 735b 7365     meta_props[se
+0001ce30: 6c66 2e43 6f6e 6669 6775 7261 7469 6f6e  lf.Configuration
+0001ce40: 4d65 7461 4e61 6d65 732e 4f55 5450 5554  MetaNames.OUTPUT
+0001ce50: 5f44 4154 415f 5343 4845 4d41 5d0a 2020  _DATA_SCHEMA].  
+0001ce60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001ce70: 2020 5d0a 2020 2020 2020 2020 2020 2020    ].            
+0001ce80: 2020 2020 656c 7365 3a0a 2020 2020 2020      else:.      
+0001ce90: 2020 2020 2020 2020 2020 2020 2020 7365                se
+0001cea0: 6c66 2e5f 7661 6c69 6461 7465 5f6d 6574  lf._validate_met
+0001ceb0: 615f 7072 6f70 280a 2020 2020 2020 2020  a_prop(.        
+0001cec0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001ced0: 6d65 7461 5f70 726f 7073 2c0a 2020 2020  meta_props,.    
+0001cee0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001cef0: 2020 2020 7365 6c66 2e43 6f6e 6669 6775      self.Configu
+0001cf00: 7261 7469 6f6e 4d65 7461 4e61 6d65 732e  rationMetaNames.
+0001cf10: 4f55 5450 5554 5f44 4154 415f 5343 4845  OUTPUT_DATA_SCHE
+0001cf20: 4d41 2c0a 2020 2020 2020 2020 2020 2020  MA,.            
+0001cf30: 2020 2020 2020 2020 2020 2020 6c69 7374              list
+0001cf40: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+0001cf50: 2020 2020 2020 2020 2020 4661 6c73 652c            False,
+0001cf60: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0001cf70: 2020 2020 2029 0a20 2020 2020 2020 2020       ).         
+0001cf80: 2020 2020 2020 2020 2020 206f 7574 7075             outpu
+0001cf90: 745f 7363 6865 6d61 203d 206d 6574 615f  t_schema = meta_
+0001cfa0: 7072 6f70 735b 0a20 2020 2020 2020 2020  props[.         
+0001cfb0: 2020 2020 2020 2020 2020 2020 2020 2073                 s
+0001cfc0: 656c 662e 436f 6e66 6967 7572 6174 696f  elf.Configuratio
+0001cfd0: 6e4d 6574 614e 616d 6573 2e4f 5554 5055  nMetaNames.OUTPU
+0001cfe0: 545f 4441 5441 5f53 4348 454d 410a 2020  T_DATA_SCHEMA.  
+0001cff0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d000: 2020 5d0a 2020 2020 2020 2020 2020 2020    ].            
+0001d010: 656c 7365 3a0a 2020 2020 2020 2020 2020  else:.          
+0001d020: 2020 2020 2020 7365 6c66 2e5f 7661 6c69        self._vali
+0001d030: 6461 7465 5f6d 6574 615f 7072 6f70 280a  date_meta_prop(.
+0001d040: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d050: 2020 2020 6d65 7461 5f70 726f 7073 2c0a      meta_props,.
+0001d060: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d070: 2020 2020 7365 6c66 2e43 6f6e 6669 6775      self.Configu
+0001d080: 7261 7469 6f6e 4d65 7461 4e61 6d65 732e  rationMetaNames.
+0001d090: 4f55 5450 5554 5f44 4154 415f 5343 4845  OUTPUT_DATA_SCHE
+0001d0a0: 4d41 2c0a 2020 2020 2020 2020 2020 2020  MA,.            
+0001d0b0: 2020 2020 2020 2020 6469 6374 2c0a 2020          dict,.  
+0001d0c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d0d0: 2020 4661 6c73 652c 0a20 2020 2020 2020    False,.       
+0001d0e0: 2020 2020 2020 2020 2029 0a20 2020 2020           ).     
+0001d0f0: 2020 2020 2020 2020 2020 206f 7574 7075             outpu
+0001d100: 745f 7363 6865 6d61 203d 205b 0a20 2020  t_schema = [.   
 0001d110: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001d120: 2020 2029 0a20 2020 2020 2020 2020 2020     ).           
-0001d130: 2020 2020 2020 2020 206f 7574 7075 745f           output_
-0001d140: 7363 6865 6d61 203d 206d 6574 615f 7072  schema = meta_pr
-0001d150: 6f70 735b 0a20 2020 2020 2020 2020 2020  ops[.           
-0001d160: 2020 2020 2020 2020 2020 2020 2073 656c               sel
-0001d170: 662e 436f 6e66 6967 7572 6174 696f 6e4d  f.ConfigurationM
-0001d180: 6574 614e 616d 6573 2e4f 5554 5055 545f  etaNames.OUTPUT_
-0001d190: 4441 5441 5f53 4348 454d 410a 2020 2020  DATA_SCHEMA.    
-0001d1a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001d1b0: 5d0a 2020 2020 2020 2020 2020 2020 656c  ].            el
-0001d1c0: 7365 3a0a 2020 2020 2020 2020 2020 2020  se:.            
-0001d1d0: 2020 2020 7365 6c66 2e5f 7661 6c69 6461      self._valida
-0001d1e0: 7465 5f6d 6574 615f 7072 6f70 280a 2020  te_meta_prop(.  
-0001d1f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001d200: 2020 6d65 7461 5f70 726f 7073 2c0a 2020    meta_props,.  
-0001d210: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001d220: 2020 7365 6c66 2e43 6f6e 6669 6775 7261    self.Configura
-0001d230: 7469 6f6e 4d65 7461 4e61 6d65 732e 4f55  tionMetaNames.OU
-0001d240: 5450 5554 5f44 4154 415f 5343 4845 4d41  TPUT_DATA_SCHEMA
-0001d250: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-0001d260: 2020 2020 2020 6469 6374 2c0a 2020 2020        dict,.    
-0001d270: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001d280: 4661 6c73 652c 0a20 2020 2020 2020 2020  False,.         
-0001d290: 2020 2020 2020 2029 0a20 2020 2020 2020         ).       
-0001d2a0: 2020 2020 2020 2020 206f 7574 7075 745f           output_
-0001d2b0: 7363 6865 6d61 203d 205b 0a20 2020 2020  schema = [.     
-0001d2c0: 2020 2020 2020 2020 2020 2020 2020 206d                 m
-0001d2d0: 6574 615f 7072 6f70 735b 7365 6c66 2e43  eta_props[self.C
-0001d2e0: 6f6e 6669 6775 7261 7469 6f6e 4d65 7461  onfigurationMeta
-0001d2f0: 4e61 6d65 732e 4f55 5450 5554 5f44 4154  Names.OUTPUT_DAT
-0001d300: 415f 5343 4845 4d41 5d0a 2020 2020 2020  A_SCHEMA].      
-0001d310: 2020 2020 2020 2020 2020 5d0a 2020 2020            ].    
-0001d320: 2020 2020 2020 2020 6d65 7461 6461 7461          metadata
-0001d330: 2e70 6f70 2873 656c 662e 436f 6e66 6967  .pop(self.Config
-0001d340: 7572 6174 696f 6e4d 6574 614e 616d 6573  urationMetaNames
-0001d350: 2e4f 5554 5055 545f 4441 5441 5f53 4348  .OUTPUT_DATA_SCH
-0001d360: 454d 4129 0a0a 2020 2020 2020 2020 6966  EMA)..        if
-0001d370: 206c 656e 2869 6e70 7574 5f73 6368 656d   len(input_schem
-0001d380: 6129 2021 3d20 3020 6f72 206c 656e 286f  a) != 0 or len(o
-0001d390: 7574 7075 745f 7363 6865 6d61 2920 213d  utput_schema) !=
-0001d3a0: 2030 3a0a 2020 2020 2020 2020 2020 2020   0:.            
-0001d3b0: 6d65 7461 6461 7461 2e75 7064 6174 6528  metadata.update(
-0001d3c0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0001d3d0: 207b 2273 6368 656d 6173 223a 207b 2269   {"schemas": {"i
-0001d3e0: 6e70 7574 223a 2069 6e70 7574 5f73 6368  nput": input_sch
-0001d3f0: 656d 612c 2022 6f75 7470 7574 223a 206f  ema, "output": o
-0001d400: 7574 7075 745f 7363 6865 6d61 7d7d 0a20  utput_schema}}. 
-0001d410: 2020 2020 2020 2020 2020 2029 0a0a 2020             )..  
-0001d420: 2020 2020 2020 6966 206c 6162 656c 5f63        if label_c
-0001d430: 6f6c 756d 6e5f 6e61 6d65 733a 0a20 2020  olumn_names:.   
-0001d440: 2020 2020 2020 2020 206d 6574 6164 6174           metadat
-0001d450: 615b 226c 6162 656c 5f63 6f6c 756d 6e22  a["label_column"
-0001d460: 5d20 3d20 6c61 6265 6c5f 636f 6c75 6d6e  ] = label_column
-0001d470: 5f6e 616d 6573 5b30 5d0a 0a20 2020 2020  _names[0]..     
-0001d480: 2020 2072 6574 7572 6e20 6d65 7461 6461     return metada
-0001d490: 7461 0a0a 2020 2020 6465 6620 5f70 7562  ta..    def _pub
-0001d4a0: 6c69 7368 5f66 726f 6d5f 6f62 6a65 6374  lish_from_object
-0001d4b0: 5f63 6c6f 7564 280a 2020 2020 2020 2020  _cloud(.        
-0001d4c0: 7365 6c66 2c0a 2020 2020 2020 2020 6d6f  self,.        mo
-0001d4d0: 6465 6c3a 204d 4c4d 6f64 656c 5479 7065  del: MLModelType
-0001d4e0: 2c0a 2020 2020 2020 2020 6d65 7461 5f70  ,.        meta_p
-0001d4f0: 726f 7073 3a20 6469 6374 5b73 7472 2c20  rops: dict[str, 
-0001d500: 416e 795d 2c0a 2020 2020 2020 2020 7472  Any],.        tr
-0001d510: 6169 6e69 6e67 5f64 6174 613a 2054 7261  aining_data: Tra
-0001d520: 696e 696e 6744 6174 6154 7970 6520 7c20  iningDataType | 
-0001d530: 4e6f 6e65 203d 204e 6f6e 652c 0a20 2020  None = None,.   
-0001d540: 2020 2020 2074 7261 696e 696e 675f 7461       training_ta
-0001d550: 7267 6574 3a20 5472 6169 6e69 6e67 5461  rget: TrainingTa
-0001d560: 7267 6574 5479 7065 207c 204e 6f6e 6520  rgetType | None 
-0001d570: 3d20 4e6f 6e65 2c0a 2020 2020 2020 2020  = None,.        
-0001d580: 7069 7065 6c69 6e65 3a20 5069 7065 6c69  pipeline: Pipeli
-0001d590: 6e65 5479 7065 207c 204e 6f6e 6520 3d20  neType | None = 
-0001d5a0: 4e6f 6e65 2c0a 2020 2020 2020 2020 6665  None,.        fe
-0001d5b0: 6174 7572 655f 6e61 6d65 733a 2046 6561  ature_names: Fea
-0001d5c0: 7475 7265 4e61 6d65 7341 7272 6179 5479  tureNamesArrayTy
-0001d5d0: 7065 207c 204e 6f6e 6520 3d20 4e6f 6e65  pe | None = None
-0001d5e0: 2c0a 2020 2020 2020 2020 6c61 6265 6c5f  ,.        label_
-0001d5f0: 636f 6c75 6d6e 5f6e 616d 6573 3a20 4c61  column_names: La
-0001d600: 6265 6c43 6f6c 756d 6e4e 616d 6573 5479  belColumnNamesTy
-0001d610: 7065 207c 204e 6f6e 6520 3d20 4e6f 6e65  pe | None = None
-0001d620: 2c0a 2020 2020 2020 2020 7072 6f6a 6563  ,.        projec
-0001d630: 745f 6964 3a20 7374 7220 7c20 4e6f 6e65  t_id: str | None
-0001d640: 203d 204e 6f6e 652c 0a20 2020 2029 202d   = None,.    ) -
-0001d650: 3e20 6469 6374 5b73 7472 2c20 416e 795d  > dict[str, Any]
-0001d660: 3a0a 2020 2020 2020 2020 2222 2253 746f  :.        """Sto
-0001d670: 7265 206d 6f64 656c 2066 726f 6d20 6f62  re model from ob
-0001d680: 6a65 6374 2069 6e20 6d65 6d6f 7279 2069  ject in memory i
-0001d690: 6e74 6f20 5761 7473 6f6e 204d 6163 6869  nto Watson Machi
-0001d6a0: 6e65 204c 6561 726e 696e 6720 7265 706f  ne Learning repo
-0001d6b0: 7369 746f 7279 206f 6e20 436c 6f75 642e  sitory on Cloud.
-0001d6c0: 2222 220a 2020 2020 2020 2020 7365 6c66  """.        self
-0001d6d0: 2e5f 7661 6c69 6461 7465 5f6d 6574 615f  ._validate_meta_
-0001d6e0: 7072 6f70 280a 2020 2020 2020 2020 2020  prop(.          
-0001d6f0: 2020 6d65 7461 5f70 726f 7073 2c20 7365    meta_props, se
-0001d700: 6c66 2e43 6f6e 6669 6775 7261 7469 6f6e  lf.Configuration
-0001d710: 4d65 7461 4e61 6d65 732e 4e41 4d45 2c20  MetaNames.NAME, 
-0001d720: 7374 722c 2054 7275 650a 2020 2020 2020  str, True.      
-0001d730: 2020 290a 2020 2020 2020 2020 6966 2028    ).        if (
-0001d740: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-0001d750: 662e 436f 6e66 6967 7572 6174 696f 6e4d  f.ConfigurationM
-0001d760: 6574 614e 616d 6573 2e52 554e 5449 4d45  etaNames.RUNTIME
-0001d770: 5f49 4420 696e 206d 6574 615f 7072 6f70  _ID in meta_prop
-0001d780: 730a 2020 2020 2020 2020 2020 2020 616e  s.            an
-0001d790: 6420 7365 6c66 2e43 6f6e 6669 6775 7261  d self.Configura
-0001d7a0: 7469 6f6e 4d65 7461 4e61 6d65 732e 534f  tionMetaNames.SO
-0001d7b0: 4654 5741 5245 5f53 5045 435f 4944 206e  FTWARE_SPEC_ID n
-0001d7c0: 6f74 2069 6e20 6d65 7461 5f70 726f 7073  ot in meta_props
-0001d7d0: 0a20 2020 2020 2020 2029 3a0a 2020 2020  .        ):.    
-0001d7e0: 2020 2020 2020 2020 7261 6973 6520 574d          raise WM
-0001d7f0: 4c43 6c69 656e 7445 7272 6f72 280a 2020  LClientError(.  
-0001d800: 2020 2020 2020 2020 2020 2020 2020 2249                "I
-0001d810: 6e76 616c 6964 2069 6e70 7574 2e20 5255  nvalid input. RU
-0001d820: 4e54 494d 455f 4944 2069 7320 6e6f 206c  NTIME_ID is no l
-0001d830: 6f6e 6765 7220 7375 7070 6f72 7465 642c  onger supported,
-0001d840: 2069 6e73 7465 6164 206f 6620 7468 6174   instead of that
-0001d850: 2070 726f 7669 6465 2053 4f46 5457 4152   provide SOFTWAR
-0001d860: 455f 5350 4543 5f49 4420 696e 206d 6574  E_SPEC_ID in met
-0001d870: 615f 7072 6f70 732e 220a 2020 2020 2020  a_props.".      
-0001d880: 2020 2020 2020 290a 2020 2020 2020 2020        ).        
-0001d890: 656c 7365 3a0a 2020 2020 2020 2020 2020  else:.          
-0001d8a0: 2020 6966 2028 0a20 2020 2020 2020 2020    if (.         
-0001d8b0: 2020 2020 2020 2073 656c 662e 436f 6e66         self.Conf
-0001d8c0: 6967 7572 6174 696f 6e4d 6574 614e 616d  igurationMetaNam
-0001d8d0: 6573 2e53 4f46 5457 4152 455f 5350 4543  es.SOFTWARE_SPEC
-0001d8e0: 5f49 4420 6e6f 7420 696e 206d 6574 615f  _ID not in meta_
-0001d8f0: 7072 6f70 730a 2020 2020 2020 2020 2020  props.          
-0001d900: 2020 2020 2020 616e 6420 7365 6c66 2e43        and self.C
-0001d910: 6f6e 6669 6775 7261 7469 6f6e 4d65 7461  onfigurationMeta
-0001d920: 4e61 6d65 732e 5255 4e54 494d 455f 4944  Names.RUNTIME_ID
-0001d930: 206e 6f74 2069 6e20 6d65 7461 5f70 726f   not in meta_pro
-0001d940: 7073 0a20 2020 2020 2020 2020 2020 2029  ps.            )
-0001d950: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
-0001d960: 2020 7261 6973 6520 574d 4c43 6c69 656e    raise WMLClien
-0001d970: 7445 7272 6f72 280a 2020 2020 2020 2020  tError(.        
-0001d980: 2020 2020 2020 2020 2020 2020 2249 6e76              "Inv
-0001d990: 616c 6964 2069 6e70 7574 2e20 4974 2069  alid input. It i
-0001d9a0: 7320 6d61 6e64 6174 6f72 7920 746f 2070  s mandatory to p
-0001d9b0: 726f 7669 6465 2053 4f46 5457 4152 455f  rovide SOFTWARE_
-0001d9c0: 5350 4543 5f49 4420 696e 206d 6574 615f  SPEC_ID in meta_
-0001d9d0: 7072 6f70 732e 220a 2020 2020 2020 2020  props.".        
-0001d9e0: 2020 2020 2020 2020 290a 2020 2020 2020          ).      
-0001d9f0: 2020 7472 793a 0a20 2020 2020 2020 2020    try:.         
-0001da00: 2020 2069 6620 2270 7973 7061 726b 2e6d     if "pyspark.m
-0001da10: 6c2e 7069 7065 6c69 6e65 2e50 6970 656c  l.pipeline.Pipel
-0001da20: 696e 654d 6f64 656c 2220 696e 2073 7472  ineModel" in str
-0001da30: 2874 7970 6528 6d6f 6465 6c29 293a 0a20  (type(model)):. 
-0001da40: 2020 2020 2020 2020 2020 2020 2020 2069                 i
-0001da50: 6620 7069 7065 6c69 6e65 2069 7320 4e6f  f pipeline is No
-0001da60: 6e65 206f 7220 7472 6169 6e69 6e67 5f64  ne or training_d
-0001da70: 6174 6120 6973 204e 6f6e 653a 0a20 2020  ata is None:.   
-0001da80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001da90: 2072 6169 7365 2057 4d4c 436c 6965 6e74   raise WMLClient
-0001daa0: 4572 726f 7228 0a20 2020 2020 2020 2020  Error(.         
-0001dab0: 2020 2020 2020 2020 2020 2020 2020 2022                 "
-0001dac0: 7069 7065 6c69 6e65 2061 6e64 2074 7261  pipeline and tra
-0001dad0: 696e 696e 675f 6461 7461 2061 7265 2065  ining_data are e
-0001dae0: 7870 6563 7465 6420 666f 7220 7370 6172  xpected for spar
-0001daf0: 6b20 6d6f 6465 6c73 2e22 0a20 2020 2020  k models.".     
-0001db00: 2020 2020 2020 2020 2020 2020 2020 2029                 )
-0001db10: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0001db20: 206e 616d 6520 3d20 6d65 7461 5f70 726f   name = meta_pro
-0001db30: 7073 5b73 656c 662e 436f 6e66 6967 7572  ps[self.Configur
-0001db40: 6174 696f 6e4d 6574 614e 616d 6573 2e4e  ationMetaNames.N
-0001db50: 414d 455d 0a20 2020 2020 2020 2020 2020  AME].           
-0001db60: 2020 2020 2076 6572 7369 6f6e 203d 2022       version = "
-0001db70: 312e 3022 0a20 2020 2020 2020 2020 2020  1.0".           
-0001db80: 2020 2020 2070 6c61 7466 6f72 6d20 3d20       platform = 
-0001db90: 7b22 6e61 6d65 223a 2022 7079 7468 6f6e  {"name": "python
-0001dba0: 222c 2022 7665 7273 696f 6e73 223a 205b  ", "versions": [
-0001dbb0: 2233 2e36 225d 7d0a 2020 2020 2020 2020  "3.6"]}.        
-0001dbc0: 2020 2020 2020 2020 6c69 6272 6172 795f          library_
-0001dbd0: 7461 7220 3d20 7365 6c66 2e5f 7361 7665  tar = self._save
-0001dbe0: 5f6c 6962 7261 7279 5f61 7263 6869 7665  _library_archive
-0001dbf0: 2870 6970 656c 696e 6529 0a20 2020 2020  (pipeline).     
-0001dc00: 2020 2020 2020 2020 2020 206d 6f64 656c             model
-0001dc10: 5f64 6566 696e 6974 696f 6e5f 7072 6f70  _definition_prop
-0001dc20: 7320 3d20 7b0a 2020 2020 2020 2020 2020  s = {.          
-0001dc30: 2020 2020 2020 2020 2020 7365 6c66 2e5f            self._
-0001dc40: 636c 6965 6e74 2e6d 6f64 656c 5f64 6566  client.model_def
-0001dc50: 696e 6974 696f 6e73 2e43 6f6e 6669 6775  initions.Configu
-0001dc60: 7261 7469 6f6e 4d65 7461 4e61 6d65 732e  rationMetaNames.
-0001dc70: 4e41 4d45 3a20 6e61 6d65 0a20 2020 2020  NAME: name.     
-0001dc80: 2020 2020 2020 2020 2020 2020 2020 202b                 +
-0001dc90: 2022 5f22 0a20 2020 2020 2020 2020 2020   "_".           
-0001dca0: 2020 2020 2020 2020 202b 2075 6964 5f67           + uid_g
-0001dcb0: 656e 6572 6174 6528 3829 2c0a 2020 2020  enerate(8),.    
-0001dcc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001dcd0: 7365 6c66 2e5f 636c 6965 6e74 2e6d 6f64  self._client.mod
-0001dce0: 656c 5f64 6566 696e 6974 696f 6e73 2e43  el_definitions.C
-0001dcf0: 6f6e 6669 6775 7261 7469 6f6e 4d65 7461  onfigurationMeta
-0001dd00: 4e61 6d65 732e 5645 5253 494f 4e3a 2076  Names.VERSION: v
-0001dd10: 6572 7369 6f6e 2c0a 2020 2020 2020 2020  ersion,.        
-0001dd20: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-0001dd30: 2e5f 636c 6965 6e74 2e6d 6f64 656c 5f64  ._client.model_d
-0001dd40: 6566 696e 6974 696f 6e73 2e43 6f6e 6669  efinitions.Confi
-0001dd50: 6775 7261 7469 6f6e 4d65 7461 4e61 6d65  gurationMetaName
-0001dd60: 732e 504c 4154 464f 524d 3a20 706c 6174  s.PLATFORM: plat
-0001dd70: 666f 726d 2c0a 2020 2020 2020 2020 2020  form,.          
-0001dd80: 2020 2020 2020 7d0a 2020 2020 2020 2020        }.        
-0001dd90: 2020 2020 2020 2020 6d6f 6465 6c5f 6465          model_de
-0001dda0: 6669 6e69 7469 6f6e 5f64 6574 6169 6c73  finition_details
-0001ddb0: 203d 2073 656c 662e 5f63 6c69 656e 742e   = self._client.
-0001ddc0: 6d6f 6465 6c5f 6465 6669 6e69 7469 6f6e  model_definition
-0001ddd0: 732e 7374 6f72 6528 0a20 2020 2020 2020  s.store(.       
-0001dde0: 2020 2020 2020 2020 2020 2020 206c 6962               lib
-0001ddf0: 7261 7279 5f74 6172 2c20 6d6f 6465 6c5f  rary_tar, model_
-0001de00: 6465 6669 6e69 7469 6f6e 5f70 726f 7073  definition_props
-0001de10: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0001de20: 2029 0a20 2020 2020 2020 2020 2020 2020   ).             
-0001de30: 2020 206d 6f64 656c 5f64 6566 696e 6974     model_definit
-0001de40: 696f 6e5f 6964 203d 2073 656c 662e 5f63  ion_id = self._c
-0001de50: 6c69 656e 742e 6d6f 6465 6c5f 6465 6669  lient.model_defi
-0001de60: 6e69 7469 6f6e 732e 6765 745f 6964 280a  nitions.get_id(.
+0001d120: 206d 6574 615f 7072 6f70 735b 7365 6c66   meta_props[self
+0001d130: 2e43 6f6e 6669 6775 7261 7469 6f6e 4d65  .ConfigurationMe
+0001d140: 7461 4e61 6d65 732e 4f55 5450 5554 5f44  taNames.OUTPUT_D
+0001d150: 4154 415f 5343 4845 4d41 5d0a 2020 2020  ATA_SCHEMA].    
+0001d160: 2020 2020 2020 2020 2020 2020 5d0a 2020              ].  
+0001d170: 2020 2020 2020 2020 2020 6d65 7461 6461            metada
+0001d180: 7461 2e70 6f70 2873 656c 662e 436f 6e66  ta.pop(self.Conf
+0001d190: 6967 7572 6174 696f 6e4d 6574 614e 616d  igurationMetaNam
+0001d1a0: 6573 2e4f 5554 5055 545f 4441 5441 5f53  es.OUTPUT_DATA_S
+0001d1b0: 4348 454d 4129 0a0a 2020 2020 2020 2020  CHEMA)..        
+0001d1c0: 6966 206c 656e 2869 6e70 7574 5f73 6368  if len(input_sch
+0001d1d0: 656d 6129 2021 3d20 3020 6f72 206c 656e  ema) != 0 or len
+0001d1e0: 286f 7574 7075 745f 7363 6865 6d61 2920  (output_schema) 
+0001d1f0: 213d 2030 3a0a 2020 2020 2020 2020 2020  != 0:.          
+0001d200: 2020 6d65 7461 6461 7461 2e75 7064 6174    metadata.updat
+0001d210: 6528 0a20 2020 2020 2020 2020 2020 2020  e(.             
+0001d220: 2020 207b 2273 6368 656d 6173 223a 207b     {"schemas": {
+0001d230: 2269 6e70 7574 223a 2069 6e70 7574 5f73  "input": input_s
+0001d240: 6368 656d 612c 2022 6f75 7470 7574 223a  chema, "output":
+0001d250: 206f 7574 7075 745f 7363 6865 6d61 7d7d   output_schema}}
+0001d260: 0a20 2020 2020 2020 2020 2020 2029 0a0a  .            )..
+0001d270: 2020 2020 2020 2020 6966 206c 6162 656c          if label
+0001d280: 5f63 6f6c 756d 6e5f 6e61 6d65 733a 0a20  _column_names:. 
+0001d290: 2020 2020 2020 2020 2020 206d 6574 6164             metad
+0001d2a0: 6174 615b 226c 6162 656c 5f63 6f6c 756d  ata["label_colum
+0001d2b0: 6e22 5d20 3d20 6c61 6265 6c5f 636f 6c75  n"] = label_colu
+0001d2c0: 6d6e 5f6e 616d 6573 5b30 5d0a 0a20 2020  mn_names[0]..   
+0001d2d0: 2020 2020 2072 6574 7572 6e20 6d65 7461       return meta
+0001d2e0: 6461 7461 0a0a 2020 2020 6465 6620 5f70  data..    def _p
+0001d2f0: 7562 6c69 7368 5f66 726f 6d5f 6f62 6a65  ublish_from_obje
+0001d300: 6374 5f63 6c6f 7564 280a 2020 2020 2020  ct_cloud(.      
+0001d310: 2020 7365 6c66 2c0a 2020 2020 2020 2020    self,.        
+0001d320: 6d6f 6465 6c3a 204d 4c4d 6f64 656c 5479  model: MLModelTy
+0001d330: 7065 2c0a 2020 2020 2020 2020 6d65 7461  pe,.        meta
+0001d340: 5f70 726f 7073 3a20 6469 6374 5b73 7472  _props: dict[str
+0001d350: 2c20 416e 795d 2c0a 2020 2020 2020 2020  , Any],.        
+0001d360: 7472 6169 6e69 6e67 5f64 6174 613a 2054  training_data: T
+0001d370: 7261 696e 696e 6744 6174 6154 7970 6520  rainingDataType 
+0001d380: 7c20 4e6f 6e65 203d 204e 6f6e 652c 0a20  | None = None,. 
+0001d390: 2020 2020 2020 2074 7261 696e 696e 675f         training_
+0001d3a0: 7461 7267 6574 3a20 5472 6169 6e69 6e67  target: Training
+0001d3b0: 5461 7267 6574 5479 7065 207c 204e 6f6e  TargetType | Non
+0001d3c0: 6520 3d20 4e6f 6e65 2c0a 2020 2020 2020  e = None,.      
+0001d3d0: 2020 7069 7065 6c69 6e65 3a20 5069 7065    pipeline: Pipe
+0001d3e0: 6c69 6e65 5479 7065 207c 204e 6f6e 6520  lineType | None 
+0001d3f0: 3d20 4e6f 6e65 2c0a 2020 2020 2020 2020  = None,.        
+0001d400: 6665 6174 7572 655f 6e61 6d65 733a 2046  feature_names: F
+0001d410: 6561 7475 7265 4e61 6d65 7341 7272 6179  eatureNamesArray
+0001d420: 5479 7065 207c 204e 6f6e 6520 3d20 4e6f  Type | None = No
+0001d430: 6e65 2c0a 2020 2020 2020 2020 6c61 6265  ne,.        labe
+0001d440: 6c5f 636f 6c75 6d6e 5f6e 616d 6573 3a20  l_column_names: 
+0001d450: 4c61 6265 6c43 6f6c 756d 6e4e 616d 6573  LabelColumnNames
+0001d460: 5479 7065 207c 204e 6f6e 6520 3d20 4e6f  Type | None = No
+0001d470: 6e65 2c0a 2020 2020 2020 2020 7072 6f6a  ne,.        proj
+0001d480: 6563 745f 6964 3a20 7374 7220 7c20 4e6f  ect_id: str | No
+0001d490: 6e65 203d 204e 6f6e 652c 0a20 2020 2029  ne = None,.    )
+0001d4a0: 202d 3e20 6469 6374 5b73 7472 2c20 416e   -> dict[str, An
+0001d4b0: 795d 3a0a 2020 2020 2020 2020 2222 2253  y]:.        """S
+0001d4c0: 746f 7265 206d 6f64 656c 2066 726f 6d20  tore model from 
+0001d4d0: 6f62 6a65 6374 2069 6e20 6d65 6d6f 7279  object in memory
+0001d4e0: 2069 6e74 6f20 5761 7473 6f6e 204d 6163   into Watson Mac
+0001d4f0: 6869 6e65 204c 6561 726e 696e 6720 7265  hine Learning re
+0001d500: 706f 7369 746f 7279 206f 6e20 436c 6f75  pository on Clou
+0001d510: 642e 2222 220a 2020 2020 2020 2020 7365  d.""".        se
+0001d520: 6c66 2e5f 7661 6c69 6461 7465 5f6d 6574  lf._validate_met
+0001d530: 615f 7072 6f70 280a 2020 2020 2020 2020  a_prop(.        
+0001d540: 2020 2020 6d65 7461 5f70 726f 7073 2c20      meta_props, 
+0001d550: 7365 6c66 2e43 6f6e 6669 6775 7261 7469  self.Configurati
+0001d560: 6f6e 4d65 7461 4e61 6d65 732e 4e41 4d45  onMetaNames.NAME
+0001d570: 2c20 7374 722c 2054 7275 650a 2020 2020  , str, True.    
+0001d580: 2020 2020 290a 2020 2020 2020 2020 6966      ).        if
+0001d590: 2028 0a20 2020 2020 2020 2020 2020 2073   (.            s
+0001d5a0: 656c 662e 436f 6e66 6967 7572 6174 696f  elf.Configuratio
+0001d5b0: 6e4d 6574 614e 616d 6573 2e52 554e 5449  nMetaNames.RUNTI
+0001d5c0: 4d45 5f49 4420 696e 206d 6574 615f 7072  ME_ID in meta_pr
+0001d5d0: 6f70 730a 2020 2020 2020 2020 2020 2020  ops.            
+0001d5e0: 616e 6420 7365 6c66 2e43 6f6e 6669 6775  and self.Configu
+0001d5f0: 7261 7469 6f6e 4d65 7461 4e61 6d65 732e  rationMetaNames.
+0001d600: 534f 4654 5741 5245 5f53 5045 435f 4944  SOFTWARE_SPEC_ID
+0001d610: 206e 6f74 2069 6e20 6d65 7461 5f70 726f   not in meta_pro
+0001d620: 7073 0a20 2020 2020 2020 2029 3a0a 2020  ps.        ):.  
+0001d630: 2020 2020 2020 2020 2020 7261 6973 6520            raise 
+0001d640: 574d 4c43 6c69 656e 7445 7272 6f72 280a  WMLClientError(.
+0001d650: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d660: 2249 6e76 616c 6964 2069 6e70 7574 2e20  "Invalid input. 
+0001d670: 5255 4e54 494d 455f 4944 2069 7320 6e6f  RUNTIME_ID is no
+0001d680: 206c 6f6e 6765 7220 7375 7070 6f72 7465   longer supporte
+0001d690: 642c 2069 6e73 7465 6164 206f 6620 7468  d, instead of th
+0001d6a0: 6174 2070 726f 7669 6465 2053 4f46 5457  at provide SOFTW
+0001d6b0: 4152 455f 5350 4543 5f49 4420 696e 206d  ARE_SPEC_ID in m
+0001d6c0: 6574 615f 7072 6f70 732e 220a 2020 2020  eta_props.".    
+0001d6d0: 2020 2020 2020 2020 290a 2020 2020 2020          ).      
+0001d6e0: 2020 656c 7365 3a0a 2020 2020 2020 2020    else:.        
+0001d6f0: 2020 2020 6966 2028 0a20 2020 2020 2020      if (.       
+0001d700: 2020 2020 2020 2020 2073 656c 662e 436f           self.Co
+0001d710: 6e66 6967 7572 6174 696f 6e4d 6574 614e  nfigurationMetaN
+0001d720: 616d 6573 2e53 4f46 5457 4152 455f 5350  ames.SOFTWARE_SP
+0001d730: 4543 5f49 4420 6e6f 7420 696e 206d 6574  EC_ID not in met
+0001d740: 615f 7072 6f70 730a 2020 2020 2020 2020  a_props.        
+0001d750: 2020 2020 2020 2020 616e 6420 7365 6c66          and self
+0001d760: 2e43 6f6e 6669 6775 7261 7469 6f6e 4d65  .ConfigurationMe
+0001d770: 7461 4e61 6d65 732e 5255 4e54 494d 455f  taNames.RUNTIME_
+0001d780: 4944 206e 6f74 2069 6e20 6d65 7461 5f70  ID not in meta_p
+0001d790: 726f 7073 0a20 2020 2020 2020 2020 2020  rops.           
+0001d7a0: 2029 3a0a 2020 2020 2020 2020 2020 2020   ):.            
+0001d7b0: 2020 2020 7261 6973 6520 574d 4c43 6c69      raise WMLCli
+0001d7c0: 656e 7445 7272 6f72 280a 2020 2020 2020  entError(.      
+0001d7d0: 2020 2020 2020 2020 2020 2020 2020 2249                "I
+0001d7e0: 6e76 616c 6964 2069 6e70 7574 2e20 4974  nvalid input. It
+0001d7f0: 2069 7320 6d61 6e64 6174 6f72 7920 746f   is mandatory to
+0001d800: 2070 726f 7669 6465 2053 4f46 5457 4152   provide SOFTWAR
+0001d810: 455f 5350 4543 5f49 4420 696e 206d 6574  E_SPEC_ID in met
+0001d820: 615f 7072 6f70 732e 220a 2020 2020 2020  a_props.".      
+0001d830: 2020 2020 2020 2020 2020 290a 2020 2020            ).    
+0001d840: 2020 2020 7472 793a 0a20 2020 2020 2020      try:.       
+0001d850: 2020 2020 2069 6620 2270 7973 7061 726b       if "pyspark
+0001d860: 2e6d 6c2e 7069 7065 6c69 6e65 2e50 6970  .ml.pipeline.Pip
+0001d870: 656c 696e 654d 6f64 656c 2220 696e 2073  elineModel" in s
+0001d880: 7472 2874 7970 6528 6d6f 6465 6c29 293a  tr(type(model)):
+0001d890: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0001d8a0: 2069 6620 7069 7065 6c69 6e65 2069 7320   if pipeline is 
+0001d8b0: 4e6f 6e65 206f 7220 7472 6169 6e69 6e67  None or training
+0001d8c0: 5f64 6174 6120 6973 204e 6f6e 653a 0a20  _data is None:. 
+0001d8d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d8e0: 2020 2072 6169 7365 2057 4d4c 436c 6965     raise WMLClie
+0001d8f0: 6e74 4572 726f 7228 0a20 2020 2020 2020  ntError(.       
+0001d900: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d910: 2022 7069 7065 6c69 6e65 2061 6e64 2074   "pipeline and t
+0001d920: 7261 696e 696e 675f 6461 7461 2061 7265  raining_data are
+0001d930: 2065 7870 6563 7465 6420 666f 7220 7370   expected for sp
+0001d940: 6172 6b20 6d6f 6465 6c73 2e22 0a20 2020  ark models.".   
+0001d950: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d960: 2029 0a20 2020 2020 2020 2020 2020 2020   ).             
+0001d970: 2020 206e 616d 6520 3d20 6d65 7461 5f70     name = meta_p
+0001d980: 726f 7073 5b73 656c 662e 436f 6e66 6967  rops[self.Config
+0001d990: 7572 6174 696f 6e4d 6574 614e 616d 6573  urationMetaNames
+0001d9a0: 2e4e 414d 455d 0a20 2020 2020 2020 2020  .NAME].         
+0001d9b0: 2020 2020 2020 2076 6572 7369 6f6e 203d         version =
+0001d9c0: 2022 312e 3022 0a20 2020 2020 2020 2020   "1.0".         
+0001d9d0: 2020 2020 2020 2070 6c61 7466 6f72 6d20         platform 
+0001d9e0: 3d20 7b22 6e61 6d65 223a 2022 7079 7468  = {"name": "pyth
+0001d9f0: 6f6e 222c 2022 7665 7273 696f 6e73 223a  on", "versions":
+0001da00: 205b 2233 2e36 225d 7d0a 2020 2020 2020   ["3.6"]}.      
+0001da10: 2020 2020 2020 2020 2020 6c69 6272 6172            librar
+0001da20: 795f 7461 7220 3d20 7365 6c66 2e5f 7361  y_tar = self._sa
+0001da30: 7665 5f6c 6962 7261 7279 5f61 7263 6869  ve_library_archi
+0001da40: 7665 2870 6970 656c 696e 6529 0a20 2020  ve(pipeline).   
+0001da50: 2020 2020 2020 2020 2020 2020 206d 6f64               mod
+0001da60: 656c 5f64 6566 696e 6974 696f 6e5f 7072  el_definition_pr
+0001da70: 6f70 7320 3d20 7b0a 2020 2020 2020 2020  ops = {.        
+0001da80: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+0001da90: 2e5f 636c 6965 6e74 2e6d 6f64 656c 5f64  ._client.model_d
+0001daa0: 6566 696e 6974 696f 6e73 2e43 6f6e 6669  efinitions.Confi
+0001dab0: 6775 7261 7469 6f6e 4d65 7461 4e61 6d65  gurationMetaName
+0001dac0: 732e 4e41 4d45 3a20 6e61 6d65 0a20 2020  s.NAME: name.   
+0001dad0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001dae0: 202b 2022 5f22 0a20 2020 2020 2020 2020   + "_".         
+0001daf0: 2020 2020 2020 2020 2020 202b 2075 6964             + uid
+0001db00: 5f67 656e 6572 6174 6528 3829 2c0a 2020  _generate(8),.  
+0001db10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001db20: 2020 7365 6c66 2e5f 636c 6965 6e74 2e6d    self._client.m
+0001db30: 6f64 656c 5f64 6566 696e 6974 696f 6e73  odel_definitions
+0001db40: 2e43 6f6e 6669 6775 7261 7469 6f6e 4d65  .ConfigurationMe
+0001db50: 7461 4e61 6d65 732e 5645 5253 494f 4e3a  taNames.VERSION:
+0001db60: 2076 6572 7369 6f6e 2c0a 2020 2020 2020   version,.      
+0001db70: 2020 2020 2020 2020 2020 2020 2020 7365                se
+0001db80: 6c66 2e5f 636c 6965 6e74 2e6d 6f64 656c  lf._client.model
+0001db90: 5f64 6566 696e 6974 696f 6e73 2e43 6f6e  _definitions.Con
+0001dba0: 6669 6775 7261 7469 6f6e 4d65 7461 4e61  figurationMetaNa
+0001dbb0: 6d65 732e 504c 4154 464f 524d 3a20 706c  mes.PLATFORM: pl
+0001dbc0: 6174 666f 726d 2c0a 2020 2020 2020 2020  atform,.        
+0001dbd0: 2020 2020 2020 2020 7d0a 2020 2020 2020          }.      
+0001dbe0: 2020 2020 2020 2020 2020 6d6f 6465 6c5f            model_
+0001dbf0: 6465 6669 6e69 7469 6f6e 5f64 6574 6169  definition_detai
+0001dc00: 6c73 203d 2073 656c 662e 5f63 6c69 656e  ls = self._clien
+0001dc10: 742e 6d6f 6465 6c5f 6465 6669 6e69 7469  t.model_definiti
+0001dc20: 6f6e 732e 7374 6f72 6528 0a20 2020 2020  ons.store(.     
+0001dc30: 2020 2020 2020 2020 2020 2020 2020 206c                 l
+0001dc40: 6962 7261 7279 5f74 6172 2c20 6d6f 6465  ibrary_tar, mode
+0001dc50: 6c5f 6465 6669 6e69 7469 6f6e 5f70 726f  l_definition_pro
+0001dc60: 7073 0a20 2020 2020 2020 2020 2020 2020  ps.             
+0001dc70: 2020 2029 0a20 2020 2020 2020 2020 2020     ).           
+0001dc80: 2020 2020 206d 6f64 656c 5f64 6566 696e       model_defin
+0001dc90: 6974 696f 6e5f 6964 203d 2073 656c 662e  ition_id = self.
+0001dca0: 5f63 6c69 656e 742e 6d6f 6465 6c5f 6465  _client.model_de
+0001dcb0: 6669 6e69 7469 6f6e 732e 6765 745f 6964  finitions.get_id
+0001dcc0: 280a 2020 2020 2020 2020 2020 2020 2020  (.              
+0001dcd0: 2020 2020 2020 6d6f 6465 6c5f 6465 6669        model_defi
+0001dce0: 6e69 7469 6f6e 5f64 6574 6169 6c73 0a20  nition_details. 
+0001dcf0: 2020 2020 2020 2020 2020 2020 2020 2029                 )
+0001dd00: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0001dd10: 2023 2063 7265 6174 6520 6120 7069 7065   # create a pipe
+0001dd20: 6c69 6e65 2066 6f72 206d 6f64 656c 2064  line for model d
+0001dd30: 6566 696e 6974 696f 6e0a 2020 2020 2020  efinition.      
+0001dd40: 2020 2020 2020 2020 2020 7069 7065 6c69            pipeli
+0001dd50: 6e65 5f6d 6574 6164 6174 6120 3d20 7b0a  ne_metadata = {.
+0001dd60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001dd70: 2020 2020 7365 6c66 2e5f 636c 6965 6e74      self._client
+0001dd80: 2e70 6970 656c 696e 6573 2e43 6f6e 6669  .pipelines.Confi
+0001dd90: 6775 7261 7469 6f6e 4d65 7461 4e61 6d65  gurationMetaName
+0001dda0: 732e 4e41 4d45 3a20 6e61 6d65 0a20 2020  s.NAME: name.   
+0001ddb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001ddc0: 202b 2022 5f22 0a20 2020 2020 2020 2020   + "_".         
+0001ddd0: 2020 2020 2020 2020 2020 202b 2075 6964             + uid
+0001dde0: 5f67 656e 6572 6174 6528 3829 2c0a 2020  _generate(8),.  
+0001ddf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001de00: 2020 7365 6c66 2e5f 636c 6965 6e74 2e70    self._client.p
+0001de10: 6970 656c 696e 6573 2e43 6f6e 6669 6775  ipelines.Configu
+0001de20: 7261 7469 6f6e 4d65 7461 4e61 6d65 732e  rationMetaNames.
+0001de30: 444f 4355 4d45 4e54 3a20 7b0a 2020 2020  DOCUMENT: {.    
+0001de40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001de50: 2020 2020 2264 6f63 5f74 7970 6522 3a20      "doc_type": 
+0001de60: 2270 6970 656c 696e 6522 2c0a 2020 2020  "pipeline",.    
 0001de70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001de80: 2020 2020 6d6f 6465 6c5f 6465 6669 6e69      model_defini
-0001de90: 7469 6f6e 5f64 6574 6169 6c73 0a20 2020  tion_details.   
-0001dea0: 2020 2020 2020 2020 2020 2020 2029 0a20               ). 
-0001deb0: 2020 2020 2020 2020 2020 2020 2020 2023                 #
-0001dec0: 2063 7265 6174 6520 6120 7069 7065 6c69   create a pipeli
-0001ded0: 6e65 2066 6f72 206d 6f64 656c 2064 6566  ne for model def
-0001dee0: 696e 6974 696f 6e0a 2020 2020 2020 2020  inition.        
-0001def0: 2020 2020 2020 2020 7069 7065 6c69 6e65          pipeline
-0001df00: 5f6d 6574 6164 6174 6120 3d20 7b0a 2020  _metadata = {.  
-0001df10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001df20: 2020 7365 6c66 2e5f 636c 6965 6e74 2e70    self._client.p
-0001df30: 6970 656c 696e 6573 2e43 6f6e 6669 6775  ipelines.Configu
-0001df40: 7261 7469 6f6e 4d65 7461 4e61 6d65 732e  rationMetaNames.
-0001df50: 4e41 4d45 3a20 6e61 6d65 0a20 2020 2020  NAME: name.     
-0001df60: 2020 2020 2020 2020 2020 2020 2020 202b                 +
-0001df70: 2022 5f22 0a20 2020 2020 2020 2020 2020   "_".           
-0001df80: 2020 2020 2020 2020 202b 2075 6964 5f67           + uid_g
-0001df90: 656e 6572 6174 6528 3829 2c0a 2020 2020  enerate(8),.    
-0001dfa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001dfb0: 7365 6c66 2e5f 636c 6965 6e74 2e70 6970  self._client.pip
-0001dfc0: 656c 696e 6573 2e43 6f6e 6669 6775 7261  elines.Configura
-0001dfd0: 7469 6f6e 4d65 7461 4e61 6d65 732e 444f  tionMetaNames.DO
-0001dfe0: 4355 4d45 4e54 3a20 7b0a 2020 2020 2020  CUMENT: {.      
-0001dff0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e000: 2020 2264 6f63 5f74 7970 6522 3a20 2270    "doc_type": "p
-0001e010: 6970 656c 696e 6522 2c0a 2020 2020 2020  ipeline",.      
+0001de80: 2020 2020 2276 6572 7369 6f6e 223a 2022      "version": "
+0001de90: 322e 3022 2c0a 2020 2020 2020 2020 2020  2.0",.          
+0001dea0: 2020 2020 2020 2020 2020 2020 2020 2270                "p
+0001deb0: 7269 6d61 7279 5f70 6970 656c 696e 6522  rimary_pipeline"
+0001dec0: 3a20 2264 6c61 6173 5f6f 6e6c 7922 2c0a  : "dlaas_only",.
+0001ded0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001dee0: 2020 2020 2020 2020 2270 6970 656c 696e          "pipelin
+0001def0: 6573 223a 205b 0a20 2020 2020 2020 2020  es": [.         
+0001df00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001df10: 2020 207b 0a20 2020 2020 2020 2020 2020     {.           
+0001df20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001df30: 2020 2020 2022 6964 223a 2022 646c 6161       "id": "dlaa
+0001df40: 735f 6f6e 6c79 222c 0a20 2020 2020 2020  s_only",.       
+0001df50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001df60: 2020 2020 2020 2020 2022 7275 6e74 696d           "runtim
+0001df70: 655f 7265 6622 3a20 2273 7061 726b 222c  e_ref": "spark",
+0001df80: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0001df90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001dfa0: 2022 6e6f 6465 7322 3a20 5b0a 2020 2020   "nodes": [.    
+0001dfb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001dfc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001dfd0: 7b0a 2020 2020 2020 2020 2020 2020 2020  {.              
+0001dfe0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001dff0: 2020 2020 2020 2020 2020 2269 6422 3a20            "id": 
+0001e000: 2272 6570 6f73 6974 6f72 7922 2c0a 2020  "repository",.  
+0001e010: 2020 2020 2020 2020 2020 2020 2020 2020                  
 0001e020: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e030: 2020 2276 6572 7369 6f6e 223a 2022 322e    "version": "2.
-0001e040: 3022 2c0a 2020 2020 2020 2020 2020 2020  0",.            
-0001e050: 2020 2020 2020 2020 2020 2020 2270 7269              "pri
-0001e060: 6d61 7279 5f70 6970 656c 696e 6522 3a20  mary_pipeline": 
-0001e070: 2264 6c61 6173 5f6f 6e6c 7922 2c0a 2020  "dlaas_only",.  
-0001e080: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e090: 2020 2020 2020 2270 6970 656c 696e 6573        "pipelines
-0001e0a0: 223a 205b 0a20 2020 2020 2020 2020 2020  ": [.           
-0001e0b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e0c0: 207b 0a20 2020 2020 2020 2020 2020 2020   {.             
+0001e030: 2020 2020 2020 2274 7970 6522 3a20 226d        "type": "m
+0001e040: 6f64 656c 5f6e 6f64 6522 2c0a 2020 2020  odel_node",.    
+0001e050: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e060: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e070: 2020 2020 2269 6e70 7574 7322 3a20 5b5d      "inputs": []
+0001e080: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+0001e090: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e0a0: 2020 2020 2020 2020 2020 226f 7574 7075            "outpu
+0001e0b0: 7473 223a 205b 5d2c 0a20 2020 2020 2020  ts": [],.       
+0001e0c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 0001e0d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e0e0: 2020 2022 6964 223a 2022 646c 6161 735f     "id": "dlaas_
-0001e0f0: 6f6e 6c79 222c 0a20 2020 2020 2020 2020  only",.         
+0001e0e0: 2022 7061 7261 6d65 7465 7273 223a 207b   "parameters": {
+0001e0f0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
 0001e100: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e110: 2020 2020 2020 2022 7275 6e74 696d 655f         "runtime_
-0001e120: 7265 6622 3a20 2273 7061 726b 222c 0a20  ref": "spark",. 
-0001e130: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e140: 2020 2020 2020 2020 2020 2020 2020 2022                 "
-0001e150: 6e6f 6465 7322 3a20 5b0a 2020 2020 2020  nodes": [.      
-0001e160: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e170: 2020 2020 2020 2020 2020 2020 2020 7b0a                {.
+0001e110: 2020 2020 2020 2020 2020 2020 2022 6d6f               "mo
+0001e120: 6465 6c5f 6465 6669 6e69 7469 6f6e 223a  del_definition":
+0001e130: 207b 0a20 2020 2020 2020 2020 2020 2020   {.             
+0001e140: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e150: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e160: 2020 2022 6964 223a 206d 6f64 656c 5f64     "id": model_d
+0001e170: 6566 696e 6974 696f 6e5f 6964 0a20 2020  efinition_id.   
 0001e180: 2020 2020 2020 2020 2020 2020 2020 2020                  
 0001e190: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e1a0: 2020 2020 2020 2020 2269 6422 3a20 2272          "id": "r
-0001e1b0: 6570 6f73 6974 6f72 7922 2c0a 2020 2020  epository",.    
+0001e1a0: 2020 2020 2020 2020 207d 0a20 2020 2020           }.     
+0001e1b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 0001e1c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e1d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e1e0: 2020 2020 2274 7970 6522 3a20 226d 6f64      "type": "mod
-0001e1f0: 656c 5f6e 6f64 6522 2c0a 2020 2020 2020  el_node",.      
+0001e1d0: 2020 207d 2c0a 2020 2020 2020 2020 2020     },.          
+0001e1e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e1f0: 2020 2020 2020 2020 2020 7d0a 2020 2020            }.    
 0001e200: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e210: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e220: 2020 2269 6e70 7574 7322 3a20 5b5d 2c0a    "inputs": [],.
-0001e230: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e210: 2020 2020 2020 2020 2020 2020 5d2c 0a20              ],. 
+0001e220: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e230: 2020 2020 2020 2020 2020 207d 0a20 2020             }.   
 0001e240: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e250: 2020 2020 2020 2020 226f 7574 7075 7473          "outputs
-0001e260: 223a 205b 5d2c 0a20 2020 2020 2020 2020  ": [],.         
-0001e270: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e280: 2020 2020 2020 2020 2020 2020 2020 2022                 "
-0001e290: 7061 7261 6d65 7465 7273 223a 207b 0a20  parameters": {. 
-0001e2a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e2b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e2c0: 2020 2020 2020 2020 2020 2022 6d6f 6465             "mode
-0001e2d0: 6c5f 6465 6669 6e69 7469 6f6e 223a 207b  l_definition": {
-0001e2e0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0001e2f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e300: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e310: 2022 6964 223a 206d 6f64 656c 5f64 6566   "id": model_def
-0001e320: 696e 6974 696f 6e5f 6964 0a20 2020 2020  inition_id.     
-0001e330: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e340: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e350: 2020 2020 2020 207d 0a20 2020 2020 2020         }.       
-0001e360: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e250: 2020 2020 205d 2c0a 2020 2020 2020 2020       ],.        
+0001e260: 2020 2020 2020 2020 2020 2020 7d2c 0a20              },. 
+0001e270: 2020 2020 2020 2020 2020 2020 2020 207d                 }
+0001e280: 0a0a 2020 2020 2020 2020 2020 2020 2020  ..              
+0001e290: 2020 7069 7065 6c69 6e65 5f73 6176 6520    pipeline_save 
+0001e2a0: 3d20 7365 6c66 2e5f 636c 6965 6e74 2e70  = self._client.p
+0001e2b0: 6970 656c 696e 6573 2e73 746f 7265 2870  ipelines.store(p
+0001e2c0: 6970 656c 696e 655f 6d65 7461 6461 7461  ipeline_metadata
+0001e2d0: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
+0001e2e0: 2020 7069 7065 6c69 6e65 5f69 6420 3d20    pipeline_id = 
+0001e2f0: 7365 6c66 2e5f 636c 6965 6e74 2e70 6970  self._client.pip
+0001e300: 656c 696e 6573 2e67 6574 5f69 6428 7069  elines.get_id(pi
+0001e310: 7065 6c69 6e65 5f73 6176 6529 0a20 2020  peline_save).   
+0001e320: 2020 2020 2020 2020 2020 2020 206d 6574               met
+0001e330: 615f 7072 6f70 735b 7365 6c66 2e5f 636c  a_props[self._cl
+0001e340: 6965 6e74 2e72 6570 6f73 6974 6f72 792e  ient.repository.
+0001e350: 4d6f 6465 6c4d 6574 614e 616d 6573 2e50  ModelMetaNames.P
+0001e360: 4950 454c 494e 455f 4944 5d20 3d20 7b0a  IPELINE_ID] = {.
 0001e370: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e380: 207d 2c0a 2020 2020 2020 2020 2020 2020   },.            
-0001e390: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e3a0: 2020 2020 2020 2020 7d0a 2020 2020 2020          }.      
-0001e3b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e3c0: 2020 2020 2020 2020 2020 5d2c 0a20 2020            ],.   
-0001e3d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e3e0: 2020 2020 2020 2020 207d 0a20 2020 2020           }.     
-0001e3f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e400: 2020 205d 2c0a 2020 2020 2020 2020 2020     ],.          
-0001e410: 2020 2020 2020 2020 2020 7d2c 0a20 2020            },.   
-0001e420: 2020 2020 2020 2020 2020 2020 207d 0a0a               }..
-0001e430: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e440: 7069 7065 6c69 6e65 5f73 6176 6520 3d20  pipeline_save = 
-0001e450: 7365 6c66 2e5f 636c 6965 6e74 2e70 6970  self._client.pip
-0001e460: 656c 696e 6573 2e73 746f 7265 2870 6970  elines.store(pip
-0001e470: 656c 696e 655f 6d65 7461 6461 7461 290a  eline_metadata).
-0001e480: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e490: 7069 7065 6c69 6e65 5f69 6420 3d20 7365  pipeline_id = se
-0001e4a0: 6c66 2e5f 636c 6965 6e74 2e70 6970 656c  lf._client.pipel
-0001e4b0: 696e 6573 2e67 6574 5f69 6428 7069 7065  ines.get_id(pipe
-0001e4c0: 6c69 6e65 5f73 6176 6529 0a20 2020 2020  line_save).     
-0001e4d0: 2020 2020 2020 2020 2020 206d 6574 615f             meta_
-0001e4e0: 7072 6f70 735b 7365 6c66 2e5f 636c 6965  props[self._clie
-0001e4f0: 6e74 2e72 6570 6f73 6974 6f72 792e 4d6f  nt.repository.Mo
-0001e500: 6465 6c4d 6574 614e 616d 6573 2e50 4950  delMetaNames.PIP
-0001e510: 454c 494e 455f 4944 5d20 3d20 7b0a 2020  ELINE_ID] = {.  
-0001e520: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e530: 2020 2269 6422 3a20 7069 7065 6c69 6e65    "id": pipeline
-0001e540: 5f69 640a 2020 2020 2020 2020 2020 2020  _id.            
-0001e550: 2020 2020 7d0a 2020 2020 2020 2020 2020      }.          
-0001e560: 2020 656c 7365 3a0a 2020 2020 2020 2020    else:.        
-0001e570: 2020 2020 2020 2020 6966 2073 656c 662e          if self.
-0001e580: 436f 6e66 6967 7572 6174 696f 6e4d 6574  ConfigurationMet
-0001e590: 614e 616d 6573 2e50 4950 454c 494e 455f  aNames.PIPELINE_
-0001e5a0: 4944 2069 6e20 6d65 7461 5f70 726f 7073  ID in meta_props
-0001e5b0: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
-0001e5c0: 2020 2020 2020 7365 6c66 2e5f 7661 6c69        self._vali
-0001e5d0: 6461 7465 5f6d 6574 615f 7072 6f70 280a  date_meta_prop(.
-0001e5e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e5f0: 2020 2020 2020 2020 6d65 7461 5f70 726f          meta_pro
-0001e600: 7073 2c20 7365 6c66 2e43 6f6e 6669 6775  ps, self.Configu
-0001e610: 7261 7469 6f6e 4d65 7461 4e61 6d65 732e  rationMetaNames.
-0001e620: 5049 5045 4c49 4e45 5f49 442c 2073 7472  PIPELINE_ID, str
-0001e630: 2c20 4661 6c73 650a 2020 2020 2020 2020  , False.        
-0001e640: 2020 2020 2020 2020 2020 2020 290a 2020              ).  
-0001e650: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e660: 2020 6d65 7461 5f70 726f 7073 5b73 656c    meta_props[sel
-0001e670: 662e 5f63 6c69 656e 742e 7265 706f 7369  f._client.reposi
-0001e680: 746f 7279 2e4d 6f64 656c 4d65 7461 4e61  tory.ModelMetaNa
-0001e690: 6d65 732e 5049 5045 4c49 4e45 5f49 445d  mes.PIPELINE_ID]
-0001e6a0: 203d 207b 0a20 2020 2020 2020 2020 2020   = {.           
-0001e6b0: 2020 2020 2020 2020 2020 2020 2022 6964               "id
-0001e6c0: 223a 206d 6574 615f 7072 6f70 735b 0a20  ": meta_props[. 
-0001e6d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e6e0: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-0001e6f0: 5f63 6c69 656e 742e 7265 706f 7369 746f  _client.reposito
-0001e700: 7279 2e4d 6f64 656c 4d65 7461 4e61 6d65  ry.ModelMetaName
-0001e710: 732e 5049 5045 4c49 4e45 5f49 440a 2020  s.PIPELINE_ID.  
-0001e720: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e730: 2020 2020 2020 5d0a 2020 2020 2020 2020        ].        
-0001e740: 2020 2020 2020 2020 2020 2020 7d0a 0a20              }.. 
-0001e750: 2020 2020 2020 2020 2020 2069 6620 280a             if (.
-0001e760: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e770: 7365 6c66 2e43 6f6e 6669 6775 7261 7469  self.Configurati
-0001e780: 6f6e 4d65 7461 4e61 6d65 732e 5350 4143  onMetaNames.SPAC
-0001e790: 455f 4944 2069 6e20 6d65 7461 5f70 726f  E_ID in meta_pro
-0001e7a0: 7073 0a20 2020 2020 2020 2020 2020 2020  ps.             
-0001e7b0: 2020 2061 6e64 206d 6574 615f 7072 6f70     and meta_prop
-0001e7c0: 735b 7365 6c66 2e5f 636c 6965 6e74 2e72  s[self._client.r
-0001e7d0: 6570 6f73 6974 6f72 792e 4d6f 6465 6c4d  epository.ModelM
-0001e7e0: 6574 614e 616d 6573 2e53 5041 4345 5f49  etaNames.SPACE_I
-0001e7f0: 445d 0a20 2020 2020 2020 2020 2020 2020  D].             
-0001e800: 2020 2069 7320 6e6f 7420 4e6f 6e65 0a20     is not None. 
-0001e810: 2020 2020 2020 2020 2020 2029 3a0a 2020             ):.  
-0001e820: 2020 2020 2020 2020 2020 2020 2020 7365                se
-0001e830: 6c66 2e5f 7661 6c69 6461 7465 5f6d 6574  lf._validate_met
-0001e840: 615f 7072 6f70 280a 2020 2020 2020 2020  a_prop(.        
-0001e850: 2020 2020 2020 2020 2020 2020 6d65 7461              meta
-0001e860: 5f70 726f 7073 2c20 7365 6c66 2e43 6f6e  _props, self.Con
-0001e870: 6669 6775 7261 7469 6f6e 4d65 7461 4e61  figurationMetaNa
-0001e880: 6d65 732e 5350 4143 455f 4944 2c20 7374  mes.SPACE_ID, st
-0001e890: 722c 2046 616c 7365 0a20 2020 2020 2020  r, False.       
-0001e8a0: 2020 2020 2020 2020 2029 0a20 2020 2020           ).     
-0001e8b0: 2020 2020 2020 2020 2020 206d 6574 615f             meta_
-0001e8c0: 7072 6f70 735b 2273 7061 6365 5f69 6422  props["space_id"
-0001e8d0: 5d20 3d20 6d65 7461 5f70 726f 7073 5b0a  ] = meta_props[.
-0001e8e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e8f0: 2020 2020 7365 6c66 2e5f 636c 6965 6e74      self._client
-0001e900: 2e72 6570 6f73 6974 6f72 792e 4d6f 6465  .repository.Mode
-0001e910: 6c4d 6574 614e 616d 6573 2e53 5041 4345  lMetaNames.SPACE
-0001e920: 5f49 440a 2020 2020 2020 2020 2020 2020  _ID.            
-0001e930: 2020 2020 5d0a 2020 2020 2020 2020 2020      ].          
-0001e940: 2020 2020 2020 6d65 7461 5f70 726f 7073        meta_props
-0001e950: 2e70 6f70 2873 656c 662e 436f 6e66 6967  .pop(self.Config
-0001e960: 7572 6174 696f 6e4d 6574 614e 616d 6573  urationMetaNames
-0001e970: 2e53 5041 4345 5f49 4429 0a20 2020 2020  .SPACE_ID).     
-0001e980: 2020 2020 2020 2065 6c73 653a 0a20 2020         else:.   
-0001e990: 2020 2020 2020 2020 2020 2020 2069 6620               if 
-0001e9a0: 7365 6c66 2e5f 636c 6965 6e74 2e64 6566  self._client.def
-0001e9b0: 6175 6c74 5f70 726f 6a65 6374 5f69 6420  ault_project_id 
-0001e9c0: 6973 206e 6f74 204e 6f6e 653a 0a20 2020  is not None:.   
-0001e9d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e9e0: 206d 6574 615f 7072 6f70 735b 2270 726f   meta_props["pro
-0001e9f0: 6a65 6374 5f69 6422 5d20 3d20 7365 6c66  ject_id"] = self
-0001ea00: 2e5f 636c 6965 6e74 2e64 6566 6175 6c74  ._client.default
-0001ea10: 5f70 726f 6a65 6374 5f69 640a 2020 2020  _project_id.    
-0001ea20: 2020 2020 2020 2020 2020 2020 656c 7365              else
-0001ea30: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
-0001ea40: 2020 2020 2020 6d65 7461 5f70 726f 7073        meta_props
-0001ea50: 5b22 7370 6163 655f 6964 225d 203d 2073  ["space_id"] = s
-0001ea60: 656c 662e 5f63 6c69 656e 742e 6465 6661  elf._client.defa
-0001ea70: 756c 745f 7370 6163 655f 6964 0a0a 2020  ult_space_id..  
-0001ea80: 2020 2020 2020 2020 2020 6966 2073 656c            if sel
-0001ea90: 662e 436f 6e66 6967 7572 6174 696f 6e4d  f.ConfigurationM
-0001eaa0: 6574 614e 616d 6573 2e53 4f46 5457 4152  etaNames.SOFTWAR
-0001eab0: 455f 5350 4543 5f49 4420 696e 206d 6574  E_SPEC_ID in met
-0001eac0: 615f 7072 6f70 733a 0a20 2020 2020 2020  a_props:.       
-0001ead0: 2020 2020 2020 2020 2073 656c 662e 5f76           self._v
-0001eae0: 616c 6964 6174 655f 6d65 7461 5f70 726f  alidate_meta_pro
-0001eaf0: 7028 0a20 2020 2020 2020 2020 2020 2020  p(.             
-0001eb00: 2020 2020 2020 206d 6574 615f 7072 6f70         meta_prop
-0001eb10: 732c 2073 656c 662e 436f 6e66 6967 7572  s, self.Configur
-0001eb20: 6174 696f 6e4d 6574 614e 616d 6573 2e53  ationMetaNames.S
-0001eb30: 4f46 5457 4152 455f 5350 4543 5f49 442c  OFTWARE_SPEC_ID,
-0001eb40: 2073 7472 2c20 5472 7565 0a20 2020 2020   str, True.     
-0001eb50: 2020 2020 2020 2020 2020 2029 0a20 2020             ).   
-0001eb60: 2020 2020 2020 2020 2020 2020 206d 6574               met
-0001eb70: 615f 7072 6f70 735b 7365 6c66 2e43 6f6e  a_props[self.Con
-0001eb80: 6669 6775 7261 7469 6f6e 4d65 7461 4e61  figurationMetaNa
-0001eb90: 6d65 732e 534f 4654 5741 5245 5f53 5045  mes.SOFTWARE_SPE
-0001eba0: 435f 4944 5d20 3d20 7b0a 2020 2020 2020  C_ID] = {.      
-0001ebb0: 2020 2020 2020 2020 2020 2020 2020 2269                "i
-0001ebc0: 6422 3a20 6d65 7461 5f70 726f 7073 5b73  d": meta_props[s
-0001ebd0: 656c 662e 436f 6e66 6967 7572 6174 696f  elf.Configuratio
-0001ebe0: 6e4d 6574 614e 616d 6573 2e53 4f46 5457  nMetaNames.SOFTW
-0001ebf0: 4152 455f 5350 4543 5f49 445d 0a20 2020  ARE_SPEC_ID].   
-0001ec00: 2020 2020 2020 2020 2020 2020 207d 0a0a               }..
-0001ec10: 2020 2020 2020 2020 2020 2020 6966 2073              if s
-0001ec20: 656c 662e 436f 6e66 6967 7572 6174 696f  elf.Configuratio
-0001ec30: 6e4d 6574 614e 616d 6573 2e4d 4f44 454c  nMetaNames.MODEL
-0001ec40: 5f44 4546 494e 4954 494f 4e5f 4944 2069  _DEFINITION_ID i
-0001ec50: 6e20 6d65 7461 5f70 726f 7073 3a0a 2020  n meta_props:.  
-0001ec60: 2020 2020 2020 2020 2020 2020 2020 7365                se
-0001ec70: 6c66 2e5f 7661 6c69 6461 7465 5f6d 6574  lf._validate_met
-0001ec80: 615f 7072 6f70 280a 2020 2020 2020 2020  a_prop(.        
-0001ec90: 2020 2020 2020 2020 2020 2020 6d65 7461              meta
-0001eca0: 5f70 726f 7073 2c0a 2020 2020 2020 2020  _props,.        
-0001ecb0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-0001ecc0: 2e43 6f6e 6669 6775 7261 7469 6f6e 4d65  .ConfigurationMe
-0001ecd0: 7461 4e61 6d65 732e 4d4f 4445 4c5f 4445  taNames.MODEL_DE
-0001ece0: 4649 4e49 5449 4f4e 5f49 442c 0a20 2020  FINITION_ID,.   
-0001ecf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ed00: 2073 7472 2c0a 2020 2020 2020 2020 2020   str,.          
-0001ed10: 2020 2020 2020 2020 2020 4661 6c73 652c            False,
-0001ed20: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0001ed30: 2029 0a20 2020 2020 2020 2020 2020 2020   ).             
-0001ed40: 2020 206d 6574 615f 7072 6f70 735b 0a20     meta_props[. 
-0001ed50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ed60: 2020 2073 656c 662e 5f63 6c69 656e 742e     self._client.
-0001ed70: 7265 706f 7369 746f 7279 2e4d 6f64 656c  repository.Model
-0001ed80: 4d65 7461 4e61 6d65 732e 4d4f 4445 4c5f  MetaNames.MODEL_
-0001ed90: 4445 4649 4e49 5449 4f4e 5f49 440a 2020  DEFINITION_ID.  
-0001eda0: 2020 2020 2020 2020 2020 2020 2020 5d20                ] 
-0001edb0: 3d20 7b0a 2020 2020 2020 2020 2020 2020  = {.            
-0001edc0: 2020 2020 2020 2020 2269 6422 3a20 6d65          "id": me
-0001edd0: 7461 5f70 726f 7073 5b0a 2020 2020 2020  ta_props[.      
-0001ede0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001edf0: 2020 7365 6c66 2e5f 636c 6965 6e74 2e72    self._client.r
-0001ee00: 6570 6f73 6974 6f72 792e 4d6f 6465 6c4d  epository.ModelM
-0001ee10: 6574 614e 616d 6573 2e4d 4f44 454c 5f44  etaNames.MODEL_D
-0001ee20: 4546 494e 4954 494f 4e5f 4944 0a20 2020  EFINITION_ID.   
-0001ee30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ee40: 205d 0a20 2020 2020 2020 2020 2020 2020   ].             
-0001ee50: 2020 207d 0a0a 2020 2020 2020 2020 2020     }..          
-0001ee60: 2020 6966 2073 7472 286d 6574 615f 7072    if str(meta_pr
-0001ee70: 6f70 735b 7365 6c66 2e43 6f6e 6669 6775  ops[self.Configu
-0001ee80: 7261 7469 6f6e 4d65 7461 4e61 6d65 732e  rationMetaNames.
-0001ee90: 5459 5045 5d29 2e73 7461 7274 7377 6974  TYPE]).startswit
-0001eea0: 6828 0a20 2020 2020 2020 2020 2020 2020  h(.             
-0001eeb0: 2020 2022 7465 6e73 6f72 666c 6f77 5f22     "tensorflow_"
-0001eec0: 0a20 2020 2020 2020 2020 2020 2029 3a0a  .            ):.
-0001eed0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001eee0: 7361 7665 645f 6d6f 6465 6c20 3d20 7365  saved_model = se
-0001eef0: 6c66 2e5f 7374 6f72 655f 7466 5f6d 6f64  lf._store_tf_mod
-0001ef00: 656c 280a 2020 2020 2020 2020 2020 2020  el(.            
-0001ef10: 2020 2020 2020 2020 6d6f 6465 6c2c 0a20          model,. 
-0001ef20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ef30: 2020 206d 6574 615f 7072 6f70 732c 0a20     meta_props,. 
+0001e380: 2020 2020 2269 6422 3a20 7069 7065 6c69      "id": pipeli
+0001e390: 6e65 5f69 640a 2020 2020 2020 2020 2020  ne_id.          
+0001e3a0: 2020 2020 2020 7d0a 2020 2020 2020 2020        }.        
+0001e3b0: 2020 2020 656c 7365 3a0a 2020 2020 2020      else:.      
+0001e3c0: 2020 2020 2020 2020 2020 6966 2073 656c            if sel
+0001e3d0: 662e 436f 6e66 6967 7572 6174 696f 6e4d  f.ConfigurationM
+0001e3e0: 6574 614e 616d 6573 2e50 4950 454c 494e  etaNames.PIPELIN
+0001e3f0: 455f 4944 2069 6e20 6d65 7461 5f70 726f  E_ID in meta_pro
+0001e400: 7073 3a0a 2020 2020 2020 2020 2020 2020  ps:.            
+0001e410: 2020 2020 2020 2020 7365 6c66 2e5f 7661          self._va
+0001e420: 6c69 6461 7465 5f6d 6574 615f 7072 6f70  lidate_meta_prop
+0001e430: 280a 2020 2020 2020 2020 2020 2020 2020  (.              
+0001e440: 2020 2020 2020 2020 2020 6d65 7461 5f70            meta_p
+0001e450: 726f 7073 2c20 7365 6c66 2e43 6f6e 6669  rops, self.Confi
+0001e460: 6775 7261 7469 6f6e 4d65 7461 4e61 6d65  gurationMetaName
+0001e470: 732e 5049 5045 4c49 4e45 5f49 442c 2073  s.PIPELINE_ID, s
+0001e480: 7472 2c20 4661 6c73 650a 2020 2020 2020  tr, False.      
+0001e490: 2020 2020 2020 2020 2020 2020 2020 290a                ).
+0001e4a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e4b0: 2020 2020 6d65 7461 5f70 726f 7073 5b73      meta_props[s
+0001e4c0: 656c 662e 5f63 6c69 656e 742e 7265 706f  elf._client.repo
+0001e4d0: 7369 746f 7279 2e4d 6f64 656c 4d65 7461  sitory.ModelMeta
+0001e4e0: 4e61 6d65 732e 5049 5045 4c49 4e45 5f49  Names.PIPELINE_I
+0001e4f0: 445d 203d 207b 0a20 2020 2020 2020 2020  D] = {.         
+0001e500: 2020 2020 2020 2020 2020 2020 2020 2022                 "
+0001e510: 6964 223a 206d 6574 615f 7072 6f70 735b  id": meta_props[
+0001e520: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0001e530: 2020 2020 2020 2020 2020 2020 2073 656c               sel
+0001e540: 662e 5f63 6c69 656e 742e 7265 706f 7369  f._client.reposi
+0001e550: 746f 7279 2e4d 6f64 656c 4d65 7461 4e61  tory.ModelMetaNa
+0001e560: 6d65 732e 5049 5045 4c49 4e45 5f49 440a  mes.PIPELINE_ID.
+0001e570: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e580: 2020 2020 2020 2020 5d0a 2020 2020 2020          ].      
+0001e590: 2020 2020 2020 2020 2020 2020 2020 7d0a                }.
+0001e5a0: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if 
+0001e5b0: 280a 2020 2020 2020 2020 2020 2020 2020  (.              
+0001e5c0: 2020 7365 6c66 2e43 6f6e 6669 6775 7261    self.Configura
+0001e5d0: 7469 6f6e 4d65 7461 4e61 6d65 732e 5350  tionMetaNames.SP
+0001e5e0: 4143 455f 4944 2069 6e20 6d65 7461 5f70  ACE_ID in meta_p
+0001e5f0: 726f 7073 0a20 2020 2020 2020 2020 2020  rops.           
+0001e600: 2020 2020 2061 6e64 206d 6574 615f 7072       and meta_pr
+0001e610: 6f70 735b 7365 6c66 2e5f 636c 6965 6e74  ops[self._client
+0001e620: 2e72 6570 6f73 6974 6f72 792e 4d6f 6465  .repository.Mode
+0001e630: 6c4d 6574 614e 616d 6573 2e53 5041 4345  lMetaNames.SPACE
+0001e640: 5f49 445d 0a20 2020 2020 2020 2020 2020  _ID].           
+0001e650: 2020 2020 2069 7320 6e6f 7420 4e6f 6e65       is not None
+0001e660: 0a20 2020 2020 2020 2020 2020 2029 3a0a  .            ):.
+0001e670: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e680: 7365 6c66 2e5f 7661 6c69 6461 7465 5f6d  self._validate_m
+0001e690: 6574 615f 7072 6f70 280a 2020 2020 2020  eta_prop(.      
+0001e6a0: 2020 2020 2020 2020 2020 2020 2020 6d65                me
+0001e6b0: 7461 5f70 726f 7073 2c20 7365 6c66 2e43  ta_props, self.C
+0001e6c0: 6f6e 6669 6775 7261 7469 6f6e 4d65 7461  onfigurationMeta
+0001e6d0: 4e61 6d65 732e 5350 4143 455f 4944 2c20  Names.SPACE_ID, 
+0001e6e0: 7374 722c 2046 616c 7365 0a20 2020 2020  str, False.     
+0001e6f0: 2020 2020 2020 2020 2020 2029 0a20 2020             ).   
+0001e700: 2020 2020 2020 2020 2020 2020 206d 6574               met
+0001e710: 615f 7072 6f70 735b 2273 7061 6365 5f69  a_props["space_i
+0001e720: 6422 5d20 3d20 6d65 7461 5f70 726f 7073  d"] = meta_props
+0001e730: 5b0a 2020 2020 2020 2020 2020 2020 2020  [.              
+0001e740: 2020 2020 2020 7365 6c66 2e5f 636c 6965        self._clie
+0001e750: 6e74 2e72 6570 6f73 6974 6f72 792e 4d6f  nt.repository.Mo
+0001e760: 6465 6c4d 6574 614e 616d 6573 2e53 5041  delMetaNames.SPA
+0001e770: 4345 5f49 440a 2020 2020 2020 2020 2020  CE_ID.          
+0001e780: 2020 2020 2020 5d0a 2020 2020 2020 2020        ].        
+0001e790: 2020 2020 2020 2020 6d65 7461 5f70 726f          meta_pro
+0001e7a0: 7073 2e70 6f70 2873 656c 662e 436f 6e66  ps.pop(self.Conf
+0001e7b0: 6967 7572 6174 696f 6e4d 6574 614e 616d  igurationMetaNam
+0001e7c0: 6573 2e53 5041 4345 5f49 4429 0a20 2020  es.SPACE_ID).   
+0001e7d0: 2020 2020 2020 2020 2065 6c73 653a 0a20           else:. 
+0001e7e0: 2020 2020 2020 2020 2020 2020 2020 2069                 i
+0001e7f0: 6620 7365 6c66 2e5f 636c 6965 6e74 2e64  f self._client.d
+0001e800: 6566 6175 6c74 5f70 726f 6a65 6374 5f69  efault_project_i
+0001e810: 6420 6973 206e 6f74 204e 6f6e 653a 0a20  d is not None:. 
+0001e820: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e830: 2020 206d 6574 615f 7072 6f70 735b 2270     meta_props["p
+0001e840: 726f 6a65 6374 5f69 6422 5d20 3d20 7365  roject_id"] = se
+0001e850: 6c66 2e5f 636c 6965 6e74 2e64 6566 6175  lf._client.defau
+0001e860: 6c74 5f70 726f 6a65 6374 5f69 640a 2020  lt_project_id.  
+0001e870: 2020 2020 2020 2020 2020 2020 2020 656c                el
+0001e880: 7365 3a0a 2020 2020 2020 2020 2020 2020  se:.            
+0001e890: 2020 2020 2020 2020 6d65 7461 5f70 726f          meta_pro
+0001e8a0: 7073 5b22 7370 6163 655f 6964 225d 203d  ps["space_id"] =
+0001e8b0: 2073 656c 662e 5f63 6c69 656e 742e 6465   self._client.de
+0001e8c0: 6661 756c 745f 7370 6163 655f 6964 0a0a  fault_space_id..
+0001e8d0: 2020 2020 2020 2020 2020 2020 6966 2073              if s
+0001e8e0: 656c 662e 436f 6e66 6967 7572 6174 696f  elf.Configuratio
+0001e8f0: 6e4d 6574 614e 616d 6573 2e53 4f46 5457  nMetaNames.SOFTW
+0001e900: 4152 455f 5350 4543 5f49 4420 696e 206d  ARE_SPEC_ID in m
+0001e910: 6574 615f 7072 6f70 733a 0a20 2020 2020  eta_props:.     
+0001e920: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+0001e930: 5f76 616c 6964 6174 655f 6d65 7461 5f70  _validate_meta_p
+0001e940: 726f 7028 0a20 2020 2020 2020 2020 2020  rop(.           
+0001e950: 2020 2020 2020 2020 206d 6574 615f 7072           meta_pr
+0001e960: 6f70 732c 2073 656c 662e 436f 6e66 6967  ops, self.Config
+0001e970: 7572 6174 696f 6e4d 6574 614e 616d 6573  urationMetaNames
+0001e980: 2e53 4f46 5457 4152 455f 5350 4543 5f49  .SOFTWARE_SPEC_I
+0001e990: 442c 2073 7472 2c20 5472 7565 0a20 2020  D, str, True.   
+0001e9a0: 2020 2020 2020 2020 2020 2020 2029 0a20               ). 
+0001e9b0: 2020 2020 2020 2020 2020 2020 2020 206d                 m
+0001e9c0: 6574 615f 7072 6f70 735b 7365 6c66 2e43  eta_props[self.C
+0001e9d0: 6f6e 6669 6775 7261 7469 6f6e 4d65 7461  onfigurationMeta
+0001e9e0: 4e61 6d65 732e 534f 4654 5741 5245 5f53  Names.SOFTWARE_S
+0001e9f0: 5045 435f 4944 5d20 3d20 7b0a 2020 2020  PEC_ID] = {.    
+0001ea00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001ea10: 2269 6422 3a20 6d65 7461 5f70 726f 7073  "id": meta_props
+0001ea20: 5b73 656c 662e 436f 6e66 6967 7572 6174  [self.Configurat
+0001ea30: 696f 6e4d 6574 614e 616d 6573 2e53 4f46  ionMetaNames.SOF
+0001ea40: 5457 4152 455f 5350 4543 5f49 445d 0a20  TWARE_SPEC_ID]. 
+0001ea50: 2020 2020 2020 2020 2020 2020 2020 207d                 }
+0001ea60: 0a0a 2020 2020 2020 2020 2020 2020 6966  ..            if
+0001ea70: 2073 656c 662e 436f 6e66 6967 7572 6174   self.Configurat
+0001ea80: 696f 6e4d 6574 614e 616d 6573 2e4d 4f44  ionMetaNames.MOD
+0001ea90: 454c 5f44 4546 494e 4954 494f 4e5f 4944  EL_DEFINITION_ID
+0001eaa0: 2069 6e20 6d65 7461 5f70 726f 7073 3a0a   in meta_props:.
+0001eab0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001eac0: 7365 6c66 2e5f 7661 6c69 6461 7465 5f6d  self._validate_m
+0001ead0: 6574 615f 7072 6f70 280a 2020 2020 2020  eta_prop(.      
+0001eae0: 2020 2020 2020 2020 2020 2020 2020 6d65                me
+0001eaf0: 7461 5f70 726f 7073 2c0a 2020 2020 2020  ta_props,.      
+0001eb00: 2020 2020 2020 2020 2020 2020 2020 7365                se
+0001eb10: 6c66 2e43 6f6e 6669 6775 7261 7469 6f6e  lf.Configuration
+0001eb20: 4d65 7461 4e61 6d65 732e 4d4f 4445 4c5f  MetaNames.MODEL_
+0001eb30: 4445 4649 4e49 5449 4f4e 5f49 442c 0a20  DEFINITION_ID,. 
+0001eb40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001eb50: 2020 2073 7472 2c0a 2020 2020 2020 2020     str,.        
+0001eb60: 2020 2020 2020 2020 2020 2020 4661 6c73              Fals
+0001eb70: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
+0001eb80: 2020 2029 0a20 2020 2020 2020 2020 2020     ).           
+0001eb90: 2020 2020 206d 6574 615f 7072 6f70 735b       meta_props[
+0001eba0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0001ebb0: 2020 2020 2073 656c 662e 5f63 6c69 656e       self._clien
+0001ebc0: 742e 7265 706f 7369 746f 7279 2e4d 6f64  t.repository.Mod
+0001ebd0: 656c 4d65 7461 4e61 6d65 732e 4d4f 4445  elMetaNames.MODE
+0001ebe0: 4c5f 4445 4649 4e49 5449 4f4e 5f49 440a  L_DEFINITION_ID.
+0001ebf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001ec00: 5d20 3d20 7b0a 2020 2020 2020 2020 2020  ] = {.          
+0001ec10: 2020 2020 2020 2020 2020 2269 6422 3a20            "id": 
+0001ec20: 6d65 7461 5f70 726f 7073 5b0a 2020 2020  meta_props[.    
+0001ec30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001ec40: 2020 2020 7365 6c66 2e5f 636c 6965 6e74      self._client
+0001ec50: 2e72 6570 6f73 6974 6f72 792e 4d6f 6465  .repository.Mode
+0001ec60: 6c4d 6574 614e 616d 6573 2e4d 4f44 454c  lMetaNames.MODEL
+0001ec70: 5f44 4546 494e 4954 494f 4e5f 4944 0a20  _DEFINITION_ID. 
+0001ec80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001ec90: 2020 205d 0a20 2020 2020 2020 2020 2020     ].           
+0001eca0: 2020 2020 207d 0a0a 2020 2020 2020 2020       }..        
+0001ecb0: 2020 2020 6966 2073 7472 286d 6574 615f      if str(meta_
+0001ecc0: 7072 6f70 735b 7365 6c66 2e43 6f6e 6669  props[self.Confi
+0001ecd0: 6775 7261 7469 6f6e 4d65 7461 4e61 6d65  gurationMetaName
+0001ece0: 732e 5459 5045 5d29 2e73 7461 7274 7377  s.TYPE]).startsw
+0001ecf0: 6974 6828 0a20 2020 2020 2020 2020 2020  ith(.           
+0001ed00: 2020 2020 2022 7465 6e73 6f72 666c 6f77       "tensorflow
+0001ed10: 5f22 0a20 2020 2020 2020 2020 2020 2029  _".            )
+0001ed20: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
+0001ed30: 2020 7361 7665 645f 6d6f 6465 6c20 3d20    saved_model = 
+0001ed40: 7365 6c66 2e5f 7374 6f72 655f 7466 5f6d  self._store_tf_m
+0001ed50: 6f64 656c 280a 2020 2020 2020 2020 2020  odel(.          
+0001ed60: 2020 2020 2020 2020 2020 6d6f 6465 6c2c            model,
+0001ed70: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0001ed80: 2020 2020 206d 6574 615f 7072 6f70 732c       meta_props,
+0001ed90: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0001eda0: 2020 2020 2066 6561 7475 7265 5f6e 616d       feature_nam
+0001edb0: 6573 3d66 6561 7475 7265 5f6e 616d 6573  es=feature_names
+0001edc0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+0001edd0: 2020 2020 2020 6c61 6265 6c5f 636f 6c75        label_colu
+0001ede0: 6d6e 5f6e 616d 6573 3d6c 6162 656c 5f63  mn_names=label_c
+0001edf0: 6f6c 756d 6e5f 6e61 6d65 732c 0a20 2020  olumn_names,.   
+0001ee00: 2020 2020 2020 2020 2020 2020 2029 0a20               ). 
+0001ee10: 2020 2020 2020 2020 2020 2020 2020 2072                 r
+0001ee20: 6574 7572 6e20 7361 7665 645f 6d6f 6465  eturn saved_mode
+0001ee30: 6c0a 2020 2020 2020 2020 2020 2020 656c  l.            el
+0001ee40: 7365 3a0a 2020 2020 2020 2020 2020 2020  se:.            
+0001ee50: 2020 2020 6d65 7461 5f64 6174 6120 3d20      meta_data = 
+0001ee60: 4d65 7461 5072 6f70 7328 6d65 7461 5f70  MetaProps(meta_p
+0001ee70: 726f 7073 290a 2020 2020 2020 2020 2020  rops).          
+0001ee80: 2020 2020 2020 6d6f 6465 6c5f 6172 7469        model_arti
+0001ee90: 6661 6374 203d 204d 4c52 6570 6f73 6974  fact = MLReposit
+0001eea0: 6f72 7941 7274 6966 6163 7428 0a20 2020  oryArtifact(.   
+0001eeb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001eec0: 206d 6f64 656c 2c0a 2020 2020 2020 2020   model,.        
+0001eed0: 2020 2020 2020 2020 2020 2020 6e61 6d65              name
+0001eee0: 3d73 7472 286d 6574 615f 7072 6f70 735b  =str(meta_props[
+0001eef0: 7365 6c66 2e43 6f6e 6669 6775 7261 7469  self.Configurati
+0001ef00: 6f6e 4d65 7461 4e61 6d65 732e 4e41 4d45  onMetaNames.NAME
+0001ef10: 5d29 2c0a 2020 2020 2020 2020 2020 2020  ]),.            
+0001ef20: 2020 2020 2020 2020 6d65 7461 5f70 726f          meta_pro
+0001ef30: 7073 3d6d 6574 615f 6461 7461 2c0a 2020  ps=meta_data,.  
 0001ef40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ef50: 2020 2066 6561 7475 7265 5f6e 616d 6573     feature_names
-0001ef60: 3d66 6561 7475 7265 5f6e 616d 6573 2c0a  =feature_names,.
+0001ef50: 2020 7472 6169 6e69 6e67 5f64 6174 613d    training_data=
+0001ef60: 7472 6169 6e69 6e67 5f64 6174 612c 0a20  training_data,. 
 0001ef70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ef80: 2020 2020 6c61 6265 6c5f 636f 6c75 6d6e      label_column
-0001ef90: 5f6e 616d 6573 3d6c 6162 656c 5f63 6f6c  _names=label_col
-0001efa0: 756d 6e5f 6e61 6d65 732c 0a20 2020 2020  umn_names,.     
-0001efb0: 2020 2020 2020 2020 2020 2029 0a20 2020             ).   
-0001efc0: 2020 2020 2020 2020 2020 2020 2072 6574               ret
-0001efd0: 7572 6e20 7361 7665 645f 6d6f 6465 6c0a  urn saved_model.
-0001efe0: 2020 2020 2020 2020 2020 2020 656c 7365              else
-0001eff0: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
-0001f000: 2020 6d65 7461 5f64 6174 6120 3d20 4d65    meta_data = Me
-0001f010: 7461 5072 6f70 7328 6d65 7461 5f70 726f  taProps(meta_pro
-0001f020: 7073 290a 2020 2020 2020 2020 2020 2020  ps).            
-0001f030: 2020 2020 6d6f 6465 6c5f 6172 7469 6661      model_artifa
-0001f040: 6374 203d 204d 4c52 6570 6f73 6974 6f72  ct = MLRepositor
-0001f050: 7941 7274 6966 6163 7428 0a20 2020 2020  yArtifact(.     
-0001f060: 2020 2020 2020 2020 2020 2020 2020 206d                 m
-0001f070: 6f64 656c 2c0a 2020 2020 2020 2020 2020  odel,.          
-0001f080: 2020 2020 2020 2020 2020 6e61 6d65 3d73            name=s
-0001f090: 7472 286d 6574 615f 7072 6f70 735b 7365  tr(meta_props[se
-0001f0a0: 6c66 2e43 6f6e 6669 6775 7261 7469 6f6e  lf.Configuration
-0001f0b0: 4d65 7461 4e61 6d65 732e 4e41 4d45 5d29  MetaNames.NAME])
-0001f0c0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-0001f0d0: 2020 2020 2020 6d65 7461 5f70 726f 7073        meta_props
-0001f0e0: 3d6d 6574 615f 6461 7461 2c0a 2020 2020  =meta_data,.    
-0001f0f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001f100: 7472 6169 6e69 6e67 5f64 6174 613d 7472  training_data=tr
-0001f110: 6169 6e69 6e67 5f64 6174 612c 0a20 2020  aining_data,.   
-0001f120: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001f130: 2074 7261 696e 696e 675f 7461 7267 6574   training_target
-0001f140: 3d74 7261 696e 696e 675f 7461 7267 6574  =training_target
-0001f150: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-0001f160: 2020 2020 2020 6665 6174 7572 655f 6e61        feature_na
-0001f170: 6d65 733d 6665 6174 7572 655f 6e61 6d65  mes=feature_name
-0001f180: 732c 0a20 2020 2020 2020 2020 2020 2020  s,.             
-0001f190: 2020 2020 2020 206c 6162 656c 5f63 6f6c         label_col
-0001f1a0: 756d 6e5f 6e61 6d65 733d 6c61 6265 6c5f  umn_names=label_
-0001f1b0: 636f 6c75 6d6e 5f6e 616d 6573 2c0a 2020  column_names,.  
-0001f1c0: 2020 2020 2020 2020 2020 2020 2020 290a                ).
-0001f1d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001f1e0: 7175 6572 795f 7061 7261 6d5f 666f 725f  query_param_for_
-0001f1f0: 7265 706f 5f63 6c69 656e 7420 3d20 7365  repo_client = se
-0001f200: 6c66 2e5f 636c 6965 6e74 2e5f 7061 7261  lf._client._para
-0001f210: 6d73 2829 0a20 2020 2020 2020 2020 2020  ms().           
-0001f220: 2020 2020 2073 6176 6564 5f6d 6f64 656c       saved_model
-0001f230: 203d 2073 656c 662e 5f63 6c69 656e 742e   = self._client.
-0001f240: 7265 706f 7369 746f 7279 2e5f 6d6c 5f72  repository._ml_r
-0001f250: 6570 6f73 6974 6f72 795f 636c 6965 6e74  epository_client
-0001f260: 2e6d 6f64 656c 732e 7361 7665 280a 2020  .models.save(.  
-0001f270: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001f280: 2020 6d6f 6465 6c5f 6172 7469 6661 6374    model_artifact
-0001f290: 2c20 7175 6572 795f 7061 7261 6d3d 7175  , query_param=qu
-0001f2a0: 6572 795f 7061 7261 6d5f 666f 725f 7265  ery_param_for_re
-0001f2b0: 706f 5f63 6c69 656e 740a 2020 2020 2020  po_client.      
-0001f2c0: 2020 2020 2020 2020 2020 290a 2020 2020            ).    
-0001f2d0: 2020 2020 2020 2020 2020 2020 7265 7475              retu
-0001f2e0: 726e 2073 656c 662e 6765 745f 6465 7461  rn self.get_deta
-0001f2f0: 696c 7328 227b 7d22 2e66 6f72 6d61 7428  ils("{}".format(
-0001f300: 7361 7665 645f 6d6f 6465 6c2e 7569 6429  saved_model.uid)
-0001f310: 2920 2023 2074 7970 653a 2069 676e 6f72  )  # type: ignor
-0001f320: 655b 6174 7472 2d64 6566 696e 6564 5d0a  e[attr-defined].
-0001f330: 0a20 2020 2020 2020 2065 7863 6570 7420  .        except 
-0001f340: 4578 6365 7074 696f 6e20 6173 2065 3a0a  Exception as e:.
-0001f350: 2020 2020 2020 2020 2020 2020 7261 6973              rais
-0001f360: 6520 574d 4c43 6c69 656e 7445 7272 6f72  e WMLClientError
-0001f370: 2822 5075 626c 6973 6869 6e67 206d 6f64  ("Publishing mod
-0001f380: 656c 2066 6169 6c65 642e 222c 2065 290a  el failed.", e).
-0001f390: 0a20 2020 2064 6566 205f 7570 6c6f 6164  .    def _upload
-0001f3a0: 5f61 7574 6f61 695f 6d6f 6465 6c5f 636f  _autoai_model_co
-0001f3b0: 6e74 656e 7428 0a20 2020 2020 2020 2073  ntent(.        s
-0001f3c0: 656c 662c 2066 696c 653a 2073 7472 207c  elf, file: str |
-0001f3d0: 2042 696e 6172 7949 4f2c 2075 726c 3a20   BinaryIO, url: 
-0001f3e0: 7374 722c 2071 7061 7261 6d73 3a20 6469  str, qparams: di
-0001f3f0: 6374 5b73 7472 2c20 416e 795d 0a20 2020  ct[str, Any].   
-0001f400: 2029 202d 3e20 5265 7175 6573 7473 2e52   ) -> Requests.R
-0001f410: 6573 706f 6e73 653a 0a20 2020 2020 2020  esponse:.       
-0001f420: 2069 6d70 6f72 7420 7a69 7066 696c 650a   import zipfile.
-0001f430: 2020 2020 2020 2020 696d 706f 7274 206a          import j
-0001f440: 736f 6e0a 0a20 2020 2020 2020 206e 6f64  son..        nod
-0001f450: 655f 6964 7320 3d20 4e6f 6e65 0a20 2020  e_ids = None.   
-0001f460: 2020 2020 2077 6974 6820 7a69 7066 696c       with zipfil
-0001f470: 652e 5a69 7046 696c 6528 6669 6c65 2c20  e.ZipFile(file, 
-0001f480: 2272 2229 2061 7320 7a69 704f 626a 3a0a  "r") as zipObj:.
-0001f490: 2020 2020 2020 2020 2020 2020 2320 4765              # Ge
-0001f4a0: 7420 6120 6c69 7374 206f 6620 616c 6c20  t a list of all 
-0001f4b0: 6172 6368 6976 6564 2066 696c 6520 6e61  archived file na
-0001f4c0: 6d65 7320 6672 6f6d 2074 6865 207a 6970  mes from the zip
-0001f4d0: 0a20 2020 2020 2020 2020 2020 206c 6973  .            lis
-0001f4e0: 744f 6646 696c 654e 616d 6573 203d 207a  tOfFileNames = z
-0001f4f0: 6970 4f62 6a2e 6e61 6d65 6c69 7374 2829  ipObj.namelist()
-0001f500: 0a20 2020 2020 2020 2020 2020 2074 3120  .            t1 
-0001f510: 3d20 7a69 704f 626a 2e65 7874 7261 6374  = zipObj.extract
-0001f520: 2822 7069 7065 6c69 6e65 2d6d 6f64 656c  ("pipeline-model
-0001f530: 2e6a 736f 6e22 290a 2020 2020 2020 2020  .json").        
-0001f540: 2020 2020 7769 7468 206f 7065 6e28 7431      with open(t1
-0001f550: 2c20 2272 2229 2061 7320 6632 3a0a 2020  , "r") as f2:.  
-0001f560: 2020 2020 2020 2020 2020 2020 2020 6461                da
-0001f570: 7461 203d 206a 736f 6e2e 6c6f 6164 2866  ta = json.load(f
-0001f580: 3229 0a20 2020 2020 2020 2020 2020 2020  2).             
-0001f590: 2020 2023 206e 6f74 653a 2077 6520 6361     # note: we ca
-0001f5a0: 6e20 6861 7665 206d 756c 7469 706c 6520  n have multiple 
-0001f5b0: 6e6f 6465 7320 284f 424d 202b 204b 4229  nodes (OBM + KB)
-0001f5c0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0001f5d0: 206e 6f64 655f 6964 7320 3d20 5b0a 2020   node_ids = [.  
-0001f5e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001f5f0: 2020 6e6f 6465 2e67 6574 2822 6964 2229    node.get("id")
-0001f600: 2066 6f72 206e 6f64 6520 696e 2064 6174   for node in dat
-0001f610: 612e 6765 7428 2270 6970 656c 696e 6573  a.get("pipelines
-0001f620: 2229 5b30 5d2e 6765 7428 226e 6f64 6573  ")[0].get("nodes
-0001f630: 2229 0a20 2020 2020 2020 2020 2020 2020  ").             
-0001f640: 2020 205d 0a0a 2020 2020 2020 2020 2020     ]..          
-0001f650: 2020 6966 206e 6f64 655f 6964 7320 6973    if node_ids is
-0001f660: 204e 6f6e 653a 0a20 2020 2020 2020 2020   None:.         
-0001f670: 2020 2020 2020 2072 6169 7365 2057 4d4c         raise WML
-0001f680: 436c 6965 6e74 4572 726f 7228 0a20 2020  ClientError(.   
-0001f690: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001f6a0: 2022 496e 7661 6c69 6420 7069 706c 696e   "Invalid piplin
-0001f6b0: 652d 6d6f 6465 6c2e 6a73 6f6e 2063 6f6e  e-model.json con
-0001f6c0: 7465 6e74 2066 696c 652e 2054 6865 7265  tent file. There
-0001f6d0: 2069 7320 6e6f 206e 6f64 6520 6964 2076   is no node id v
-0001f6e0: 616c 7565 2066 6f75 6e64 220a 2020 2020  alue found".    
-0001f6f0: 2020 2020 2020 2020 2020 2020 290a 0a20              ).. 
-0001f700: 2020 2020 2020 2020 2020 2071 7061 7261             qpara
-0001f710: 6d73 2e75 7064 6174 6528 7b22 636f 6e74  ms.update({"cont
-0001f720: 656e 745f 666f 726d 6174 223a 2022 6e61  ent_format": "na
-0001f730: 7469 7665 227d 290a 2020 2020 2020 2020  tive"}).        
-0001f740: 2020 2020 7170 6172 616d 732e 7570 6461      qparams.upda
-0001f750: 7465 287b 226e 616d 6522 3a20 2270 6970  te({"name": "pip
-0001f760: 656c 696e 652d 6d6f 6465 6c2e 6a73 6f6e  eline-model.json
-0001f770: 227d 290a 0a20 2020 2020 2020 2020 2020  "})..           
-0001f780: 2069 6620 2270 6970 656c 696e 655f 6e6f   if "pipeline_no
-0001f790: 6465 5f69 6422 2069 6e20 7170 6172 616d  de_id" in qparam
-0001f7a0: 732e 6b65 7973 2829 3a0a 2020 2020 2020  s.keys():.      
-0001f7b0: 2020 2020 2020 2020 2020 7170 6172 616d            qparam
-0001f7c0: 732e 706f 7028 2270 6970 656c 696e 655f  s.pop("pipeline_
-0001f7d0: 6e6f 6465 5f69 6422 290a 0a20 2020 2020  node_id")..     
-0001f7e0: 2020 2020 2020 2072 6573 706f 6e73 6520         response 
-0001f7f0: 3d20 7265 7175 6573 7473 2e70 7574 280a  = requests.put(.
-0001f800: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001f810: 7572 6c2c 0a20 2020 2020 2020 2020 2020  url,.           
-0001f820: 2020 2020 2064 6174 613d 6f70 656e 2874       data=open(t
-0001f830: 312c 2022 7262 2229 2e72 6561 6428 292c  1, "rb").read(),
-0001f840: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0001f850: 2070 6172 616d 733d 7170 6172 616d 732c   params=qparams,
-0001f860: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0001f870: 2068 6561 6465 7273 3d73 656c 662e 5f63   headers=self._c
-0001f880: 6c69 656e 742e 5f67 6574 5f68 6561 6465  lient._get_heade
-0001f890: 7273 2863 6f6e 7465 6e74 5f74 7970 653d  rs(content_type=
-0001f8a0: 2261 7070 6c69 6361 7469 6f6e 2f6a 736f  "application/jso
-0001f8b0: 6e22 292c 0a20 2020 2020 2020 2020 2020  n"),.           
-0001f8c0: 2029 0a0a 2020 2020 2020 2020 2020 2020   )..            
-0001f8d0: 6c69 7374 4f66 4669 6c65 4e61 6d65 732e  listOfFileNames.
-0001f8e0: 7265 6d6f 7665 2822 7069 7065 6c69 6e65  remove("pipeline
-0001f8f0: 2d6d 6f64 656c 2e6a 736f 6e22 290a 0a20  -model.json").. 
-0001f900: 2020 2020 2020 2020 2020 2023 206e 6f74             # not
-0001f910: 653a 2074 6865 2066 696c 6520 6f72 6465  e: the file orde
-0001f920: 7220 6973 2069 6d70 6f72 7461 6e64 2c20  r is importand, 
-0001f930: 7368 6f75 6c64 2062 6520 4f42 4d20 6d6f  should be OBM mo
-0001f940: 6465 6c20 6669 7273 7420 7468 656e 204b  del first then K
-0001f950: 4220 6d6f 6465 6c0a 2020 2020 2020 2020  B model.        
-0001f960: 2020 2020 666f 7220 6669 6c65 4e61 6d65      for fileName
-0001f970: 2c20 6e6f 6465 5f69 6420 696e 207a 6970  , node_id in zip
-0001f980: 286c 6973 744f 6646 696c 654e 616d 6573  (listOfFileNames
-0001f990: 2c20 6e6f 6465 5f69 6473 293a 0a20 2020  , node_ids):.   
-0001f9a0: 2020 2020 2020 2020 2020 2020 2023 2043               # C
-0001f9b0: 6865 636b 2066 696c 656e 616d 6520 656e  heck filename en
-0001f9c0: 6473 7769 7468 206a 736f 6e0a 2020 2020  dswith json.    
-0001f9d0: 2020 2020 2020 2020 2020 2020 6966 2066              if f
-0001f9e0: 696c 654e 616d 652e 656e 6473 7769 7468  ileName.endswith
-0001f9f0: 2822 2e74 6172 2e67 7a22 2920 6f72 2066  (".tar.gz") or f
-0001fa00: 696c 654e 616d 652e 656e 6473 7769 7468  ileName.endswith
-0001fa10: 2822 2e7a 6970 2229 3a0a 2020 2020 2020  (".zip"):.      
-0001fa20: 2020 2020 2020 2020 2020 2020 2020 2320                # 
-0001fa30: 4578 7472 6163 7420 6120 7369 6e67 6c65  Extract a single
-0001fa40: 2066 696c 6520 6672 6f6d 207a 6970 0a20   file from zip. 
-0001fa50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001fa60: 2020 2071 7061 7261 6d73 2e75 7064 6174     qparams.updat
-0001fa70: 6528 7b22 636f 6e74 656e 745f 666f 726d  e({"content_form
-0001fa80: 6174 223a 2022 7069 7065 6c69 6e65 2d6e  at": "pipeline-n
-0001fa90: 6f64 6522 7d29 0a20 2020 2020 2020 2020  ode"}).         
-0001faa0: 2020 2020 2020 2020 2020 2071 7061 7261             qpara
-0001fab0: 6d73 2e75 7064 6174 6528 7b22 7069 7065  ms.update({"pipe
-0001fac0: 6c69 6e65 5f6e 6f64 655f 6964 223a 206e  line_node_id": n
-0001fad0: 6f64 655f 6964 7d29 0a20 2020 2020 2020  ode_id}).       
-0001fae0: 2020 2020 2020 2020 2020 2020 2071 7061               qpa
-0001faf0: 7261 6d73 2e75 7064 6174 6528 7b22 6e61  rams.update({"na
-0001fb00: 6d65 223a 2066 696c 654e 616d 657d 290a  me": fileName}).
-0001fb10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001fb20: 2020 2020 7432 203d 207a 6970 4f62 6a2e      t2 = zipObj.
-0001fb30: 6578 7472 6163 7428 6669 6c65 4e61 6d65  extract(fileName
-0001fb40: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
-0001fb50: 2020 2020 2020 7265 7370 6f6e 7365 203d        response =
-0001fb60: 2072 6571 7565 7374 732e 7075 7428 0a20   requests.put(. 
-0001fb70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001fb80: 2020 2020 2020 2075 726c 2c0a 2020 2020         url,.    
-0001fb90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001fba0: 2020 2020 6461 7461 3d6f 7065 6e28 7432      data=open(t2
-0001fbb0: 2c20 2272 6222 292e 7265 6164 2829 2c0a  , "rb").read(),.
-0001fbc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001fbd0: 2020 2020 2020 2020 7061 7261 6d73 3d71          params=q
-0001fbe0: 7061 7261 6d73 2c0a 2020 2020 2020 2020  params,.        
-0001fbf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001fc00: 6865 6164 6572 733d 7365 6c66 2e5f 636c  headers=self._cl
-0001fc10: 6965 6e74 2e5f 6765 745f 6865 6164 6572  ient._get_header
-0001fc20: 7328 0a20 2020 2020 2020 2020 2020 2020  s(.             
-0001fc30: 2020 2020 2020 2020 2020 2020 2020 2063                 c
-0001fc40: 6f6e 7465 6e74 5f74 7970 653d 2261 7070  ontent_type="app
-0001fc50: 6c69 6361 7469 6f6e 2f6f 6374 6574 2d73  lication/octet-s
-0001fc60: 7472 6561 6d22 0a20 2020 2020 2020 2020  tream".         
-0001fc70: 2020 2020 2020 2020 2020 2020 2020 2029                 )
-0001fc80: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-0001fc90: 2020 2020 2020 290a 2020 2020 2020 2020        ).        
-0001fca0: 7265 7475 726e 2072 6573 706f 6e73 650a  return response.
-0001fcb0: 0a20 2020 2064 6566 205f 646f 776e 6c6f  .    def _downlo
-0001fcc0: 6164 5f61 7574 6f5f 6169 5f6d 6f64 656c  ad_auto_ai_model
-0001fcd0: 5f63 6f6e 7465 6e74 280a 2020 2020 2020  _content(.      
-0001fce0: 2020 7365 6c66 2c20 6d6f 6465 6c5f 6964    self, model_id
-0001fcf0: 3a20 7374 722c 2063 6f6e 7465 6e74 5f75  : str, content_u
-0001fd00: 726c 3a20 7374 722c 2066 696c 656e 616d  rl: str, filenam
-0001fd10: 653a 2073 7472 0a20 2020 2029 202d 3e20  e: str.    ) -> 
-0001fd20: 4e6f 6e65 3a0a 2020 2020 2020 2020 696d  None:.        im
-0001fd30: 706f 7274 207a 6970 6669 6c65 0a0a 2020  port zipfile..  
-0001fd40: 2020 2020 2020 7769 7468 207a 6970 6669        with zipfi
-0001fd50: 6c65 2e5a 6970 4669 6c65 2866 696c 656e  le.ZipFile(filen
-0001fd60: 616d 652c 2022 7722 2920 6173 207a 6970  ame, "w") as zip
-0001fd70: 3a0a 2020 2020 2020 2020 2020 2020 2320  :.            # 
-0001fd80: 7772 6974 696e 6720 6561 6368 2066 696c  writing each fil
-0001fd90: 6520 6f6e 6520 6279 206f 6e65 0a20 2020  e one by one.   
-0001fda0: 2020 2020 2020 2020 2070 6970 656c 696e           pipelin
-0001fdb0: 655f 6d6f 6465 6c5f 6669 6c65 203d 2022  e_model_file = "
-0001fdc0: 7069 7065 6c69 6e65 2d6d 6f64 656c 2e6a  pipeline-model.j
-0001fdd0: 736f 6e22 0a20 2020 2020 2020 2020 2020  son".           
-0001fde0: 2077 6974 6820 6f70 656e 2870 6970 656c   with open(pipel
-0001fdf0: 696e 655f 6d6f 6465 6c5f 6669 6c65 2c20  ine_model_file, 
-0001fe00: 2277 6222 2920 6173 2066 3a0a 2020 2020  "wb") as f:.    
-0001fe10: 2020 2020 2020 2020 2020 2020 7061 7261              para
-0001fe20: 6d73 203d 2073 656c 662e 5f63 6c69 656e  ms = self._clien
-0001fe30: 742e 5f70 6172 616d 7328 290a 2020 2020  t._params().    
-0001fe40: 2020 2020 2020 2020 2020 2020 7061 7261              para
-0001fe50: 6d73 2e75 7064 6174 6528 7b22 636f 6e74  ms.update({"cont
-0001fe60: 656e 745f 666f 726d 6174 223a 2022 6e61  ent_format": "na
-0001fe70: 7469 7665 227d 290a 2020 2020 2020 2020  tive"}).        
-0001fe80: 2020 2020 2020 2020 7220 3d20 7265 7175          r = requ
-0001fe90: 6573 7473 2e67 6574 280a 2020 2020 2020  ests.get(.      
-0001fea0: 2020 2020 2020 2020 2020 2020 2020 636f                co
-0001feb0: 6e74 656e 745f 7572 6c2c 0a20 2020 2020  ntent_url,.     
-0001fec0: 2020 2020 2020 2020 2020 2020 2020 2070                 p
-0001fed0: 6172 616d 733d 7061 7261 6d73 2c0a 2020  arams=params,.  
+0001ef80: 2020 2074 7261 696e 696e 675f 7461 7267     training_targ
+0001ef90: 6574 3d74 7261 696e 696e 675f 7461 7267  et=training_targ
+0001efa0: 6574 2c0a 2020 2020 2020 2020 2020 2020  et,.            
+0001efb0: 2020 2020 2020 2020 6665 6174 7572 655f          feature_
+0001efc0: 6e61 6d65 733d 6665 6174 7572 655f 6e61  names=feature_na
+0001efd0: 6d65 732c 0a20 2020 2020 2020 2020 2020  mes,.           
+0001efe0: 2020 2020 2020 2020 206c 6162 656c 5f63           label_c
+0001eff0: 6f6c 756d 6e5f 6e61 6d65 733d 6c61 6265  olumn_names=labe
+0001f000: 6c5f 636f 6c75 6d6e 5f6e 616d 6573 2c0a  l_column_names,.
+0001f010: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001f020: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
+0001f030: 2020 7175 6572 795f 7061 7261 6d5f 666f    query_param_fo
+0001f040: 725f 7265 706f 5f63 6c69 656e 7420 3d20  r_repo_client = 
+0001f050: 7365 6c66 2e5f 636c 6965 6e74 2e5f 7061  self._client._pa
+0001f060: 7261 6d73 2829 0a20 2020 2020 2020 2020  rams().         
+0001f070: 2020 2020 2020 2073 6176 6564 5f6d 6f64         saved_mod
+0001f080: 656c 203d 2073 656c 662e 5f63 6c69 656e  el = self._clien
+0001f090: 742e 7265 706f 7369 746f 7279 2e5f 6d6c  t.repository._ml
+0001f0a0: 5f72 6570 6f73 6974 6f72 795f 636c 6965  _repository_clie
+0001f0b0: 6e74 2e6d 6f64 656c 732e 7361 7665 280a  nt.models.save(.
+0001f0c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001f0d0: 2020 2020 6d6f 6465 6c5f 6172 7469 6661      model_artifa
+0001f0e0: 6374 2c20 7175 6572 795f 7061 7261 6d3d  ct, query_param=
+0001f0f0: 7175 6572 795f 7061 7261 6d5f 666f 725f  query_param_for_
+0001f100: 7265 706f 5f63 6c69 656e 740a 2020 2020  repo_client.    
+0001f110: 2020 2020 2020 2020 2020 2020 290a 2020              ).  
+0001f120: 2020 2020 2020 2020 2020 2020 2020 7265                re
+0001f130: 7475 726e 2073 656c 662e 6765 745f 6465  turn self.get_de
+0001f140: 7461 696c 7328 227b 7d22 2e66 6f72 6d61  tails("{}".forma
+0001f150: 7428 7361 7665 645f 6d6f 6465 6c2e 7569  t(saved_model.ui
+0001f160: 6429 2920 2023 2074 7970 653a 2069 676e  d))  # type: ign
+0001f170: 6f72 655b 6174 7472 2d64 6566 696e 6564  ore[attr-defined
+0001f180: 5d0a 0a20 2020 2020 2020 2065 7863 6570  ]..        excep
+0001f190: 7420 4578 6365 7074 696f 6e20 6173 2065  t Exception as e
+0001f1a0: 3a0a 2020 2020 2020 2020 2020 2020 7261  :.            ra
+0001f1b0: 6973 6520 574d 4c43 6c69 656e 7445 7272  ise WMLClientErr
+0001f1c0: 6f72 2822 5075 626c 6973 6869 6e67 206d  or("Publishing m
+0001f1d0: 6f64 656c 2066 6169 6c65 642e 222c 2065  odel failed.", e
+0001f1e0: 290a 0a20 2020 2064 6566 205f 7570 6c6f  )..    def _uplo
+0001f1f0: 6164 5f61 7574 6f61 695f 6d6f 6465 6c5f  ad_autoai_model_
+0001f200: 636f 6e74 656e 7428 0a20 2020 2020 2020  content(.       
+0001f210: 2073 656c 662c 2066 696c 653a 2073 7472   self, file: str
+0001f220: 207c 2042 696e 6172 7949 4f2c 2075 726c   | BinaryIO, url
+0001f230: 3a20 7374 722c 2071 7061 7261 6d73 3a20  : str, qparams: 
+0001f240: 6469 6374 5b73 7472 2c20 416e 795d 0a20  dict[str, Any]. 
+0001f250: 2020 2029 202d 3e20 5265 7175 6573 7473     ) -> Requests
+0001f260: 2e52 6573 706f 6e73 653a 0a20 2020 2020  .Response:.     
+0001f270: 2020 2069 6d70 6f72 7420 7a69 7066 696c     import zipfil
+0001f280: 650a 2020 2020 2020 2020 696d 706f 7274  e.        import
+0001f290: 206a 736f 6e0a 0a20 2020 2020 2020 206e   json..        n
+0001f2a0: 6f64 655f 6964 7320 3d20 4e6f 6e65 0a20  ode_ids = None. 
+0001f2b0: 2020 2020 2020 2077 6974 6820 7a69 7066         with zipf
+0001f2c0: 696c 652e 5a69 7046 696c 6528 6669 6c65  ile.ZipFile(file
+0001f2d0: 2c20 2272 2229 2061 7320 7a69 704f 626a  , "r") as zipObj
+0001f2e0: 3a0a 2020 2020 2020 2020 2020 2020 2320  :.            # 
+0001f2f0: 4765 7420 6120 6c69 7374 206f 6620 616c  Get a list of al
+0001f300: 6c20 6172 6368 6976 6564 2066 696c 6520  l archived file 
+0001f310: 6e61 6d65 7320 6672 6f6d 2074 6865 207a  names from the z
+0001f320: 6970 0a20 2020 2020 2020 2020 2020 206c  ip.            l
+0001f330: 6973 744f 6646 696c 654e 616d 6573 203d  istOfFileNames =
+0001f340: 207a 6970 4f62 6a2e 6e61 6d65 6c69 7374   zipObj.namelist
+0001f350: 2829 0a20 2020 2020 2020 2020 2020 2074  ().            t
+0001f360: 3120 3d20 7a69 704f 626a 2e65 7874 7261  1 = zipObj.extra
+0001f370: 6374 2822 7069 7065 6c69 6e65 2d6d 6f64  ct("pipeline-mod
+0001f380: 656c 2e6a 736f 6e22 290a 2020 2020 2020  el.json").      
+0001f390: 2020 2020 2020 7769 7468 206f 7065 6e28        with open(
+0001f3a0: 7431 2c20 2272 2229 2061 7320 6632 3a0a  t1, "r") as f2:.
+0001f3b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001f3c0: 6461 7461 203d 206a 736f 6e2e 6c6f 6164  data = json.load
+0001f3d0: 2866 3229 0a20 2020 2020 2020 2020 2020  (f2).           
+0001f3e0: 2020 2020 2023 206e 6f74 653a 2077 6520       # note: we 
+0001f3f0: 6361 6e20 6861 7665 206d 756c 7469 706c  can have multipl
+0001f400: 6520 6e6f 6465 7320 284f 424d 202b 204b  e nodes (OBM + K
+0001f410: 4229 0a20 2020 2020 2020 2020 2020 2020  B).             
+0001f420: 2020 206e 6f64 655f 6964 7320 3d20 5b0a     node_ids = [.
+0001f430: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001f440: 2020 2020 6e6f 6465 2e67 6574 2822 6964      node.get("id
+0001f450: 2229 2066 6f72 206e 6f64 6520 696e 2064  ") for node in d
+0001f460: 6174 612e 6765 7428 2270 6970 656c 696e  ata.get("pipelin
+0001f470: 6573 2229 5b30 5d2e 6765 7428 226e 6f64  es")[0].get("nod
+0001f480: 6573 2229 0a20 2020 2020 2020 2020 2020  es").           
+0001f490: 2020 2020 205d 0a0a 2020 2020 2020 2020       ]..        
+0001f4a0: 2020 2020 6966 206e 6f64 655f 6964 7320      if node_ids 
+0001f4b0: 6973 204e 6f6e 653a 0a20 2020 2020 2020  is None:.       
+0001f4c0: 2020 2020 2020 2020 2072 6169 7365 2057           raise W
+0001f4d0: 4d4c 436c 6965 6e74 4572 726f 7228 0a20  MLClientError(. 
+0001f4e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001f4f0: 2020 2022 496e 7661 6c69 6420 7069 706c     "Invalid pipl
+0001f500: 696e 652d 6d6f 6465 6c2e 6a73 6f6e 2063  ine-model.json c
+0001f510: 6f6e 7465 6e74 2066 696c 652e 2054 6865  ontent file. The
+0001f520: 7265 2069 7320 6e6f 206e 6f64 6520 6964  re is no node id
+0001f530: 2076 616c 7565 2066 6f75 6e64 220a 2020   value found".  
+0001f540: 2020 2020 2020 2020 2020 2020 2020 290a                ).
+0001f550: 0a20 2020 2020 2020 2020 2020 2071 7061  .            qpa
+0001f560: 7261 6d73 2e75 7064 6174 6528 7b22 636f  rams.update({"co
+0001f570: 6e74 656e 745f 666f 726d 6174 223a 2022  ntent_format": "
+0001f580: 6e61 7469 7665 227d 290a 2020 2020 2020  native"}).      
+0001f590: 2020 2020 2020 7170 6172 616d 732e 7570        qparams.up
+0001f5a0: 6461 7465 287b 226e 616d 6522 3a20 2270  date({"name": "p
+0001f5b0: 6970 656c 696e 652d 6d6f 6465 6c2e 6a73  ipeline-model.js
+0001f5c0: 6f6e 227d 290a 0a20 2020 2020 2020 2020  on"})..         
+0001f5d0: 2020 2069 6620 2270 6970 656c 696e 655f     if "pipeline_
+0001f5e0: 6e6f 6465 5f69 6422 2069 6e20 7170 6172  node_id" in qpar
+0001f5f0: 616d 732e 6b65 7973 2829 3a0a 2020 2020  ams.keys():.    
+0001f600: 2020 2020 2020 2020 2020 2020 7170 6172              qpar
+0001f610: 616d 732e 706f 7028 2270 6970 656c 696e  ams.pop("pipelin
+0001f620: 655f 6e6f 6465 5f69 6422 290a 0a20 2020  e_node_id")..   
+0001f630: 2020 2020 2020 2020 2072 6573 706f 6e73           respons
+0001f640: 6520 3d20 7265 7175 6573 7473 2e70 7574  e = requests.put
+0001f650: 280a 2020 2020 2020 2020 2020 2020 2020  (.              
+0001f660: 2020 7572 6c2c 0a20 2020 2020 2020 2020    url,.         
+0001f670: 2020 2020 2020 2064 6174 613d 6f70 656e         data=open
+0001f680: 2874 312c 2022 7262 2229 2e72 6561 6428  (t1, "rb").read(
+0001f690: 292c 0a20 2020 2020 2020 2020 2020 2020  ),.             
+0001f6a0: 2020 2070 6172 616d 733d 7170 6172 616d     params=qparam
+0001f6b0: 732c 0a20 2020 2020 2020 2020 2020 2020  s,.             
+0001f6c0: 2020 2068 6561 6465 7273 3d73 656c 662e     headers=self.
+0001f6d0: 5f63 6c69 656e 742e 5f67 6574 5f68 6561  _client._get_hea
+0001f6e0: 6465 7273 2863 6f6e 7465 6e74 5f74 7970  ders(content_typ
+0001f6f0: 653d 2261 7070 6c69 6361 7469 6f6e 2f6a  e="application/j
+0001f700: 736f 6e22 292c 0a20 2020 2020 2020 2020  son"),.         
+0001f710: 2020 2029 0a0a 2020 2020 2020 2020 2020     )..          
+0001f720: 2020 6c69 7374 4f66 4669 6c65 4e61 6d65    listOfFileName
+0001f730: 732e 7265 6d6f 7665 2822 7069 7065 6c69  s.remove("pipeli
+0001f740: 6e65 2d6d 6f64 656c 2e6a 736f 6e22 290a  ne-model.json").
+0001f750: 0a20 2020 2020 2020 2020 2020 2023 206e  .            # n
+0001f760: 6f74 653a 2074 6865 2066 696c 6520 6f72  ote: the file or
+0001f770: 6465 7220 6973 2069 6d70 6f72 7461 6e64  der is importand
+0001f780: 2c20 7368 6f75 6c64 2062 6520 4f42 4d20  , should be OBM 
+0001f790: 6d6f 6465 6c20 6669 7273 7420 7468 656e  model first then
+0001f7a0: 204b 4220 6d6f 6465 6c0a 2020 2020 2020   KB model.      
+0001f7b0: 2020 2020 2020 666f 7220 6669 6c65 4e61        for fileNa
+0001f7c0: 6d65 2c20 6e6f 6465 5f69 6420 696e 207a  me, node_id in z
+0001f7d0: 6970 286c 6973 744f 6646 696c 654e 616d  ip(listOfFileNam
+0001f7e0: 6573 2c20 6e6f 6465 5f69 6473 293a 0a20  es, node_ids):. 
+0001f7f0: 2020 2020 2020 2020 2020 2020 2020 2023                 #
+0001f800: 2043 6865 636b 2066 696c 656e 616d 6520   Check filename 
+0001f810: 656e 6473 7769 7468 206a 736f 6e0a 2020  endswith json.  
+0001f820: 2020 2020 2020 2020 2020 2020 2020 6966                if
+0001f830: 2066 696c 654e 616d 652e 656e 6473 7769   fileName.endswi
+0001f840: 7468 2822 2e74 6172 2e67 7a22 2920 6f72  th(".tar.gz") or
+0001f850: 2066 696c 654e 616d 652e 656e 6473 7769   fileName.endswi
+0001f860: 7468 2822 2e7a 6970 2229 3a0a 2020 2020  th(".zip"):.    
+0001f870: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001f880: 2320 4578 7472 6163 7420 6120 7369 6e67  # Extract a sing
+0001f890: 6c65 2066 696c 6520 6672 6f6d 207a 6970  le file from zip
+0001f8a0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0001f8b0: 2020 2020 2071 7061 7261 6d73 2e75 7064       qparams.upd
+0001f8c0: 6174 6528 7b22 636f 6e74 656e 745f 666f  ate({"content_fo
+0001f8d0: 726d 6174 223a 2022 7069 7065 6c69 6e65  rmat": "pipeline
+0001f8e0: 2d6e 6f64 6522 7d29 0a20 2020 2020 2020  -node"}).       
+0001f8f0: 2020 2020 2020 2020 2020 2020 2071 7061               qpa
+0001f900: 7261 6d73 2e75 7064 6174 6528 7b22 7069  rams.update({"pi
+0001f910: 7065 6c69 6e65 5f6e 6f64 655f 6964 223a  peline_node_id":
+0001f920: 206e 6f64 655f 6964 7d29 0a20 2020 2020   node_id}).     
+0001f930: 2020 2020 2020 2020 2020 2020 2020 2071                 q
+0001f940: 7061 7261 6d73 2e75 7064 6174 6528 7b22  params.update({"
+0001f950: 6e61 6d65 223a 2066 696c 654e 616d 657d  name": fileName}
+0001f960: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
+0001f970: 2020 2020 2020 7432 203d 207a 6970 4f62        t2 = zipOb
+0001f980: 6a2e 6578 7472 6163 7428 6669 6c65 4e61  j.extract(fileNa
+0001f990: 6d65 290a 2020 2020 2020 2020 2020 2020  me).            
+0001f9a0: 2020 2020 2020 2020 7265 7370 6f6e 7365          response
+0001f9b0: 203d 2072 6571 7565 7374 732e 7075 7428   = requests.put(
+0001f9c0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0001f9d0: 2020 2020 2020 2020 2075 726c 2c0a 2020           url,.  
+0001f9e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001f9f0: 2020 2020 2020 6461 7461 3d6f 7065 6e28        data=open(
+0001fa00: 7432 2c20 2272 6222 292e 7265 6164 2829  t2, "rb").read()
+0001fa10: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+0001fa20: 2020 2020 2020 2020 2020 7061 7261 6d73            params
+0001fa30: 3d71 7061 7261 6d73 2c0a 2020 2020 2020  =qparams,.      
+0001fa40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001fa50: 2020 6865 6164 6572 733d 7365 6c66 2e5f    headers=self._
+0001fa60: 636c 6965 6e74 2e5f 6765 745f 6865 6164  client._get_head
+0001fa70: 6572 7328 0a20 2020 2020 2020 2020 2020  ers(.           
+0001fa80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001fa90: 2063 6f6e 7465 6e74 5f74 7970 653d 2261   content_type="a
+0001faa0: 7070 6c69 6361 7469 6f6e 2f6f 6374 6574  pplication/octet
+0001fab0: 2d73 7472 6561 6d22 0a20 2020 2020 2020  -stream".       
+0001fac0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001fad0: 2029 2c0a 2020 2020 2020 2020 2020 2020   ),.            
+0001fae0: 2020 2020 2020 2020 290a 2020 2020 2020          ).      
+0001faf0: 2020 7265 7475 726e 2072 6573 706f 6e73    return respons
+0001fb00: 650a 0a20 2020 2064 6566 205f 646f 776e  e..    def _down
+0001fb10: 6c6f 6164 5f61 7574 6f5f 6169 5f6d 6f64  load_auto_ai_mod
+0001fb20: 656c 5f63 6f6e 7465 6e74 280a 2020 2020  el_content(.    
+0001fb30: 2020 2020 7365 6c66 2c20 6d6f 6465 6c5f      self, model_
+0001fb40: 6964 3a20 7374 722c 2063 6f6e 7465 6e74  id: str, content
+0001fb50: 5f75 726c 3a20 7374 722c 2066 696c 656e  _url: str, filen
+0001fb60: 616d 653a 2073 7472 0a20 2020 2029 202d  ame: str.    ) -
+0001fb70: 3e20 4e6f 6e65 3a0a 2020 2020 2020 2020  > None:.        
+0001fb80: 696d 706f 7274 207a 6970 6669 6c65 0a0a  import zipfile..
+0001fb90: 2020 2020 2020 2020 7769 7468 207a 6970          with zip
+0001fba0: 6669 6c65 2e5a 6970 4669 6c65 2866 696c  file.ZipFile(fil
+0001fbb0: 656e 616d 652c 2022 7722 2920 6173 207a  ename, "w") as z
+0001fbc0: 6970 3a0a 2020 2020 2020 2020 2020 2020  ip:.            
+0001fbd0: 2320 7772 6974 696e 6720 6561 6368 2066  # writing each f
+0001fbe0: 696c 6520 6f6e 6520 6279 206f 6e65 0a20  ile one by one. 
+0001fbf0: 2020 2020 2020 2020 2020 2070 6970 656c             pipel
+0001fc00: 696e 655f 6d6f 6465 6c5f 6669 6c65 203d  ine_model_file =
+0001fc10: 2022 7069 7065 6c69 6e65 2d6d 6f64 656c   "pipeline-model
+0001fc20: 2e6a 736f 6e22 0a20 2020 2020 2020 2020  .json".         
+0001fc30: 2020 2077 6974 6820 6f70 656e 2870 6970     with open(pip
+0001fc40: 656c 696e 655f 6d6f 6465 6c5f 6669 6c65  eline_model_file
+0001fc50: 2c20 2277 6222 2920 6173 2066 3a0a 2020  , "wb") as f:.  
+0001fc60: 2020 2020 2020 2020 2020 2020 2020 7061                pa
+0001fc70: 7261 6d73 203d 2073 656c 662e 5f63 6c69  rams = self._cli
+0001fc80: 656e 742e 5f70 6172 616d 7328 290a 2020  ent._params().  
+0001fc90: 2020 2020 2020 2020 2020 2020 2020 7061                pa
+0001fca0: 7261 6d73 2e75 7064 6174 6528 7b22 636f  rams.update({"co
+0001fcb0: 6e74 656e 745f 666f 726d 6174 223a 2022  ntent_format": "
+0001fcc0: 6e61 7469 7665 227d 290a 2020 2020 2020  native"}).      
+0001fcd0: 2020 2020 2020 2020 2020 7220 3d20 7265            r = re
+0001fce0: 7175 6573 7473 2e67 6574 280a 2020 2020  quests.get(.    
+0001fcf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001fd00: 636f 6e74 656e 745f 7572 6c2c 0a20 2020  content_url,.   
+0001fd10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001fd20: 2070 6172 616d 733d 7061 7261 6d73 2c0a   params=params,.
+0001fd30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001fd40: 2020 2020 6865 6164 6572 733d 7365 6c66      headers=self
+0001fd50: 2e5f 636c 6965 6e74 2e5f 6765 745f 6865  ._client._get_he
+0001fd60: 6164 6572 7328 292c 0a20 2020 2020 2020  aders(),.       
+0001fd70: 2020 2020 2020 2020 2020 2020 2073 7472               str
+0001fd80: 6561 6d3d 5472 7565 2c0a 2020 2020 2020  eam=True,.      
+0001fd90: 2020 2020 2020 2020 2020 290a 2020 2020            ).    
+0001fda0: 2020 2020 2020 2020 2020 2020 6966 2072              if r
+0001fdb0: 2e73 7461 7475 735f 636f 6465 2021 3d20  .status_code != 
+0001fdc0: 3230 303a 0a20 2020 2020 2020 2020 2020  200:.           
+0001fdd0: 2020 2020 2020 2020 2072 6169 7365 2041           raise A
+0001fde0: 7069 5265 7175 6573 7446 6169 6c75 7265  piRequestFailure
+0001fdf0: 280a 2020 2020 2020 2020 2020 2020 2020  (.              
+0001fe00: 2020 2020 2020 2020 2020 2246 6169 6c75            "Failu
+0001fe10: 7265 2064 7572 696e 6720 7b7d 2e22 2e66  re during {}.".f
+0001fe20: 6f72 6d61 7428 2264 6f77 6e6c 6f61 6469  ormat("downloadi
+0001fe30: 6e67 206d 6f64 656c 2229 2c20 720a 2020  ng model"), r.  
+0001fe40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001fe50: 2020 290a 2020 2020 2020 2020 2020 2020    ).            
+0001fe60: 2020 2020 7365 6c66 2e5f 6c6f 6767 6572      self._logger
+0001fe70: 2e69 6e66 6f28 0a20 2020 2020 2020 2020  .info(.         
+0001fe80: 2020 2020 2020 2020 2020 2022 5375 6363             "Succ
+0001fe90: 6573 7366 756c 6c79 2064 6f77 6e6c 6f61  essfully downloa
+0001fea0: 6465 6420 6172 7469 6661 6374 2070 6970  ded artifact pip
+0001feb0: 656c 696e 655f 6d6f 6465 6c2e 6a73 6f6e  eline_model.json
+0001fec0: 2061 7274 6966 6163 745f 7572 6c3a 207b   artifact_url: {
+0001fed0: 7d22 2e66 6f72 6d61 7428 0a20 2020 2020  }".format(.     
 0001fee0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001fef0: 2020 6865 6164 6572 733d 7365 6c66 2e5f    headers=self._
-0001ff00: 636c 6965 6e74 2e5f 6765 745f 6865 6164  client._get_head
-0001ff10: 6572 7328 292c 0a20 2020 2020 2020 2020  ers(),.         
-0001ff20: 2020 2020 2020 2020 2020 2073 7472 6561             strea
-0001ff30: 6d3d 5472 7565 2c0a 2020 2020 2020 2020  m=True,.        
-0001ff40: 2020 2020 2020 2020 290a 2020 2020 2020          ).      
-0001ff50: 2020 2020 2020 2020 2020 6966 2072 2e73            if r.s
-0001ff60: 7461 7475 735f 636f 6465 2021 3d20 3230  tatus_code != 20
-0001ff70: 303a 0a20 2020 2020 2020 2020 2020 2020  0:.             
-0001ff80: 2020 2020 2020 2072 6169 7365 2041 7069         raise Api
-0001ff90: 5265 7175 6573 7446 6169 6c75 7265 280a  RequestFailure(.
-0001ffa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ffb0: 2020 2020 2020 2020 2246 6169 6c75 7265          "Failure
-0001ffc0: 2064 7572 696e 6720 7b7d 2e22 2e66 6f72   during {}.".for
-0001ffd0: 6d61 7428 2264 6f77 6e6c 6f61 6469 6e67  mat("downloading
-0001ffe0: 206d 6f64 656c 2229 2c20 720a 2020 2020   model"), r.    
-0001fff0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020000: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
-00020010: 2020 7365 6c66 2e5f 6c6f 6767 6572 2e69    self._logger.i
-00020020: 6e66 6f28 0a20 2020 2020 2020 2020 2020  nfo(.           
-00020030: 2020 2020 2020 2020 2022 5375 6363 6573           "Succes
-00020040: 7366 756c 6c79 2064 6f77 6e6c 6f61 6465  sfully downloade
-00020050: 6420 6172 7469 6661 6374 2070 6970 656c  d artifact pipel
-00020060: 696e 655f 6d6f 6465 6c2e 6a73 6f6e 2061  ine_model.json a
-00020070: 7274 6966 6163 745f 7572 6c3a 207b 7d22  rtifact_url: {}"
-00020080: 2e66 6f72 6d61 7428 0a20 2020 2020 2020  .format(.       
+0001fef0: 2020 2063 6f6e 7465 6e74 5f75 726c 0a20     content_url. 
+0001ff00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001ff10: 2020 2029 0a20 2020 2020 2020 2020 2020     ).           
+0001ff20: 2020 2020 2029 0a20 2020 2020 2020 2020       ).         
+0001ff30: 2020 2020 2020 2066 2e77 7269 7465 2872         f.write(r
+0001ff40: 2e63 6f6e 7465 6e74 290a 2020 2020 2020  .content).      
+0001ff50: 2020 2020 2020 662e 636c 6f73 6528 290a        f.close().
+0001ff60: 2020 2020 2020 2020 2020 2020 7a69 702e              zip.
+0001ff70: 7772 6974 6528 7069 7065 6c69 6e65 5f6d  write(pipeline_m
+0001ff80: 6f64 656c 5f66 696c 6529 0a20 2020 2020  odel_file).     
+0001ff90: 2020 2020 2020 206d 6669 6c65 6e61 6d65         mfilename
+0001ffa0: 203d 2022 6d6f 6465 6c5f 2220 2b20 6d6f   = "model_" + mo
+0001ffb0: 6465 6c5f 6964 202b 2022 2e70 6963 6b6c  del_id + ".pickl
+0001ffc0: 652e 7461 722e 677a 220a 2020 2020 2020  e.tar.gz".      
+0001ffd0: 2020 2020 2020 7769 7468 206f 7065 6e28        with open(
+0001ffe0: 6d66 696c 656e 616d 652c 2022 7762 2229  mfilename, "wb")
+0001fff0: 2061 7320 663a 0a20 2020 2020 2020 2020   as f:.         
+00020000: 2020 2020 2020 2070 6172 616d 7331 203d         params1 =
+00020010: 2073 656c 662e 5f63 6c69 656e 742e 5f70   self._client._p
+00020020: 6172 616d 7328 290a 2020 2020 2020 2020  arams().        
+00020030: 2020 2020 2020 2020 7061 7261 6d73 312e          params1.
+00020040: 7570 6461 7465 287b 2263 6f6e 7465 6e74  update({"content
+00020050: 5f66 6f72 6d61 7422 3a20 2270 6970 656c  _format": "pipel
+00020060: 696e 652d 6e6f 6465 227d 290a 2020 2020  ine-node"}).    
+00020070: 2020 2020 2020 2020 2020 2020 7265 7320              res 
+00020080: 3d20 7265 7175 6573 7473 2e67 6574 280a  = requests.get(.
 00020090: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000200a0: 2063 6f6e 7465 6e74 5f75 726c 0a20 2020   content_url.   
-000200b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000200c0: 2029 0a20 2020 2020 2020 2020 2020 2020   ).             
-000200d0: 2020 2029 0a20 2020 2020 2020 2020 2020     ).           
-000200e0: 2020 2020 2066 2e77 7269 7465 2872 2e63       f.write(r.c
-000200f0: 6f6e 7465 6e74 290a 2020 2020 2020 2020  ontent).        
-00020100: 2020 2020 662e 636c 6f73 6528 290a 2020      f.close().  
-00020110: 2020 2020 2020 2020 2020 7a69 702e 7772            zip.wr
-00020120: 6974 6528 7069 7065 6c69 6e65 5f6d 6f64  ite(pipeline_mod
-00020130: 656c 5f66 696c 6529 0a20 2020 2020 2020  el_file).       
-00020140: 2020 2020 206d 6669 6c65 6e61 6d65 203d       mfilename =
-00020150: 2022 6d6f 6465 6c5f 2220 2b20 6d6f 6465   "model_" + mode
-00020160: 6c5f 6964 202b 2022 2e70 6963 6b6c 652e  l_id + ".pickle.
-00020170: 7461 722e 677a 220a 2020 2020 2020 2020  tar.gz".        
-00020180: 2020 2020 7769 7468 206f 7065 6e28 6d66      with open(mf
-00020190: 696c 656e 616d 652c 2022 7762 2229 2061  ilename, "wb") a
-000201a0: 7320 663a 0a20 2020 2020 2020 2020 2020  s f:.           
-000201b0: 2020 2020 2070 6172 616d 7331 203d 2073       params1 = s
-000201c0: 656c 662e 5f63 6c69 656e 742e 5f70 6172  elf._client._par
-000201d0: 616d 7328 290a 2020 2020 2020 2020 2020  ams().          
-000201e0: 2020 2020 2020 7061 7261 6d73 312e 7570        params1.up
-000201f0: 6461 7465 287b 2263 6f6e 7465 6e74 5f66  date({"content_f
-00020200: 6f72 6d61 7422 3a20 2270 6970 656c 696e  ormat": "pipelin
-00020210: 652d 6e6f 6465 227d 290a 2020 2020 2020  e-node"}).      
-00020220: 2020 2020 2020 2020 2020 7265 7320 3d20            res = 
-00020230: 7265 7175 6573 7473 2e67 6574 280a 2020  requests.get(.  
-00020240: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020250: 2020 636f 6e74 656e 745f 7572 6c2c 0a20    content_url,. 
-00020260: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020270: 2020 2070 6172 616d 733d 7061 7261 6d73     params=params
-00020280: 312c 0a20 2020 2020 2020 2020 2020 2020  1,.             
-00020290: 2020 2020 2020 2068 6561 6465 7273 3d73         headers=s
-000202a0: 656c 662e 5f63 6c69 656e 742e 5f67 6574  elf._client._get
-000202b0: 5f68 6561 6465 7273 2829 2c0a 2020 2020  _headers(),.    
+000200a0: 2020 2020 636f 6e74 656e 745f 7572 6c2c      content_url,
+000200b0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+000200c0: 2020 2020 2070 6172 616d 733d 7061 7261       params=para
+000200d0: 6d73 312c 0a20 2020 2020 2020 2020 2020  ms1,.           
+000200e0: 2020 2020 2020 2020 2068 6561 6465 7273           headers
+000200f0: 3d73 656c 662e 5f63 6c69 656e 742e 5f67  =self._client._g
+00020100: 6574 5f68 6561 6465 7273 2829 2c0a 2020  et_headers(),.  
+00020110: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020120: 2020 7374 7265 616d 3d54 7275 652c 0a20    stream=True,. 
+00020130: 2020 2020 2020 2020 2020 2020 2020 2029                 )
+00020140: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00020150: 2069 6620 7265 732e 7374 6174 7573 5f63   if res.status_c
+00020160: 6f64 6520 213d 2032 3030 3a0a 2020 2020  ode != 200:.    
+00020170: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020180: 7261 6973 6520 4170 6952 6571 7565 7374  raise ApiRequest
+00020190: 4661 696c 7572 6528 0a20 2020 2020 2020  Failure(.       
+000201a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000201b0: 2022 4661 696c 7572 6520 6475 7269 6e67   "Failure during
+000201c0: 207b 7d2e 222e 666f 726d 6174 2822 646f   {}.".format("do
+000201d0: 776e 6c6f 6164 696e 6720 6d6f 6465 6c22  wnloading model"
+000201e0: 292c 2072 0a20 2020 2020 2020 2020 2020  ), r.           
+000201f0: 2020 2020 2020 2020 2029 0a20 2020 2020           ).     
+00020200: 2020 2020 2020 2020 2020 2066 2e77 7269             f.wri
+00020210: 7465 2872 6573 2e63 6f6e 7465 6e74 290a  te(res.content).
+00020220: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020230: 7365 6c66 2e5f 6c6f 6767 6572 2e69 6e66  self._logger.inf
+00020240: 6f28 0a20 2020 2020 2020 2020 2020 2020  o(.             
+00020250: 2020 2020 2020 2022 5375 6363 6573 7366         "Successf
+00020260: 756c 6c79 2064 6f77 6e6c 6f61 6465 6420  ully downloaded 
+00020270: 6172 7469 6661 6374 2077 6974 6820 6172  artifact with ar
+00020280: 7469 6661 6374 5f75 726c 3a20 7b7d 222e  tifact_url: {}".
+00020290: 666f 726d 6174 280a 2020 2020 2020 2020  format(.        
+000202a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000202b0: 636f 6e74 656e 745f 7572 6c0a 2020 2020  content_url.    
 000202c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000202d0: 7374 7265 616d 3d54 7275 652c 0a20 2020  stream=True,.   
-000202e0: 2020 2020 2020 2020 2020 2020 2029 0a20               ). 
-000202f0: 2020 2020 2020 2020 2020 2020 2020 2069                 i
-00020300: 6620 7265 732e 7374 6174 7573 5f63 6f64  f res.status_cod
-00020310: 6520 213d 2032 3030 3a0a 2020 2020 2020  e != 200:.      
-00020320: 2020 2020 2020 2020 2020 2020 2020 7261                ra
-00020330: 6973 6520 4170 6952 6571 7565 7374 4661  ise ApiRequestFa
-00020340: 696c 7572 6528 0a20 2020 2020 2020 2020  ilure(.         
-00020350: 2020 2020 2020 2020 2020 2020 2020 2022                 "
-00020360: 4661 696c 7572 6520 6475 7269 6e67 207b  Failure during {
-00020370: 7d2e 222e 666f 726d 6174 2822 646f 776e  }.".format("down
-00020380: 6c6f 6164 696e 6720 6d6f 6465 6c22 292c  loading model"),
-00020390: 2072 0a20 2020 2020 2020 2020 2020 2020   r.             
-000203a0: 2020 2020 2020 2029 0a20 2020 2020 2020         ).       
-000203b0: 2020 2020 2020 2020 2066 2e77 7269 7465           f.write
-000203c0: 2872 6573 2e63 6f6e 7465 6e74 290a 2020  (res.content).  
-000203d0: 2020 2020 2020 2020 2020 2020 2020 7365                se
-000203e0: 6c66 2e5f 6c6f 6767 6572 2e69 6e66 6f28  lf._logger.info(
-000203f0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00020400: 2020 2020 2022 5375 6363 6573 7366 756c       "Successful
-00020410: 6c79 2064 6f77 6e6c 6f61 6465 6420 6172  ly downloaded ar
-00020420: 7469 6661 6374 2077 6974 6820 6172 7469  tifact with arti
-00020430: 6661 6374 5f75 726c 3a20 7b7d 222e 666f  fact_url: {}".fo
-00020440: 726d 6174 280a 2020 2020 2020 2020 2020  rmat(.          
-00020450: 2020 2020 2020 2020 2020 2020 2020 636f                co
-00020460: 6e74 656e 745f 7572 6c0a 2020 2020 2020  ntent_url.      
-00020470: 2020 2020 2020 2020 2020 2020 2020 290a                ).
-00020480: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020490: 290a 2020 2020 2020 2020 2020 2020 662e  ).            f.
-000204a0: 636c 6f73 6528 290a 2020 2020 2020 2020  close().        
-000204b0: 2020 2020 7a69 702e 7772 6974 6528 6d66      zip.write(mf
-000204c0: 696c 656e 616d 6529 0a0a 2020 2020 6465  ilename)..    de
-000204d0: 6620 5f73 746f 7265 5f74 665f 6d6f 6465  f _store_tf_mode
-000204e0: 6c28 0a20 2020 2020 2020 2073 656c 662c  l(.        self,
-000204f0: 0a20 2020 2020 2020 206d 6f64 656c 3a20  .        model: 
-00020500: 416e 792c 0a20 2020 2020 2020 206d 6574  Any,.        met
-00020510: 615f 7072 6f70 733a 2064 6963 745b 7374  a_props: dict[st
-00020520: 722c 2041 6e79 5d2c 0a20 2020 2020 2020  r, Any],.       
-00020530: 2066 6561 7475 7265 5f6e 616d 6573 3a20   feature_names: 
-00020540: 4665 6174 7572 654e 616d 6573 4172 7261  FeatureNamesArra
-00020550: 7954 7970 6520 7c20 4e6f 6e65 203d 204e  yType | None = N
-00020560: 6f6e 652c 0a20 2020 2020 2020 206c 6162  one,.        lab
-00020570: 656c 5f63 6f6c 756d 6e5f 6e61 6d65 733a  el_column_names:
-00020580: 204c 6162 656c 436f 6c75 6d6e 4e61 6d65   LabelColumnName
-00020590: 7354 7970 6520 7c20 4e6f 6e65 203d 204e  sType | None = N
-000205a0: 6f6e 652c 0a20 2020 2029 202d 3e20 6469  one,.    ) -> di
-000205b0: 6374 5b73 7472 2c20 416e 795d 3a0a 2020  ct[str, Any]:.  
-000205c0: 2020 2020 2020 2320 4d6f 6465 6c20 7479        # Model ty
-000205d0: 7065 2069 730a 2020 2020 2020 2020 696d  pe is.        im
-000205e0: 706f 7274 2074 656e 736f 7266 6c6f 7720  port tensorflow 
-000205f0: 6173 2074 660a 0a20 2020 2020 2020 2075  as tf..        u
-00020600: 726c 203d 2028 0a20 2020 2020 2020 2020  rl = (.         
-00020610: 2020 2073 656c 662e 5f63 6c69 656e 742e     self._client.
-00020620: 7365 7276 6963 655f 696e 7374 616e 6365  service_instance
-00020630: 2e5f 6872 6566 5f64 6566 696e 6974 696f  ._href_definitio
-00020640: 6e73 2e67 6574 5f70 7562 6c69 7368 6564  ns.get_published
-00020650: 5f6d 6f64 656c 735f 6872 6566 2829 0a20  _models_href(). 
-00020660: 2020 2020 2020 2029 0a20 2020 2020 2020         ).       
-00020670: 2069 645f 6c65 6e67 7468 203d 2032 300a   id_length = 20.
-00020680: 2020 2020 2020 2020 6765 6e5f 6964 203d          gen_id =
-00020690: 2075 6964 5f67 656e 6572 6174 6528 6964   uid_generate(id
-000206a0: 5f6c 656e 6774 6829 0a0a 2020 2020 2020  _length)..      
-000206b0: 2020 7466 5f6d 6574 6120 3d20 4e6f 6e65    tf_meta = None
-000206c0: 0a20 2020 2020 2020 206f 7074 696f 6e73  .        options
-000206d0: 203d 204e 6f6e 650a 2020 2020 2020 2020   = None.        
-000206e0: 7369 676e 6174 7572 6520 3d20 4e6f 6e65  signature = None
-000206f0: 0a20 2020 2020 2020 2073 6176 655f 666f  .        save_fo
-00020700: 726d 6174 203d 204e 6f6e 650a 2020 2020  rmat = None.    
-00020710: 2020 2020 696e 636c 7564 655f 6f70 7469      include_opti
-00020720: 6d69 7a65 7220 3d20 4e6f 6e65 0a20 2020  mizer = None.   
-00020730: 2020 2020 2069 6620 280a 2020 2020 2020       if (.      
-00020740: 2020 2020 2020 2274 665f 6d6f 6465 6c5f        "tf_model_
-00020750: 7061 7261 6d73 2220 696e 206d 6574 615f  params" in meta_
-00020760: 7072 6f70 730a 2020 2020 2020 2020 2020  props.          
-00020770: 2020 616e 6420 6d65 7461 5f70 726f 7073    and meta_props
-00020780: 5b73 656c 662e 436f 6e66 6967 7572 6174  [self.Configurat
-00020790: 696f 6e4d 6574 614e 616d 6573 2e54 465f  ionMetaNames.TF_
-000207a0: 4d4f 4445 4c5f 5041 5241 4d53 5d20 6973  MODEL_PARAMS] is
-000207b0: 206e 6f74 204e 6f6e 650a 2020 2020 2020   not None.      
-000207c0: 2020 293a 0a20 2020 2020 2020 2020 2020    ):.           
-000207d0: 2074 665f 6d65 7461 203d 2063 6f70 792e   tf_meta = copy.
-000207e0: 6465 6570 636f 7079 280a 2020 2020 2020  deepcopy(.      
-000207f0: 2020 2020 2020 2020 2020 6d65 7461 5f70            meta_p
-00020800: 726f 7073 5b73 656c 662e 436f 6e66 6967  rops[self.Config
-00020810: 7572 6174 696f 6e4d 6574 614e 616d 6573  urationMetaNames
-00020820: 2e54 465f 4d4f 4445 4c5f 5041 5241 4d53  .TF_MODEL_PARAMS
-00020830: 5d0a 2020 2020 2020 2020 2020 2020 290a  ].            ).
-00020840: 2020 2020 2020 2020 2020 2020 7361 7665              save
-00020850: 5f66 6f72 6d61 7420 3d20 7466 5f6d 6574  _format = tf_met
-00020860: 612e 6765 7428 2273 6176 655f 666f 726d  a.get("save_form
-00020870: 6174 2229 0a20 2020 2020 2020 2020 2020  at").           
-00020880: 206f 7074 696f 6e73 203d 2074 665f 6d65   options = tf_me
-00020890: 7461 2e67 6574 2822 6f70 7469 6f6e 7322  ta.get("options"
-000208a0: 290a 2020 2020 2020 2020 2020 2020 7369  ).            si
-000208b0: 676e 6174 7572 6520 3d20 7466 5f6d 6574  gnature = tf_met
-000208c0: 612e 6765 7428 2273 6967 6e61 7475 7265  a.get("signature
-000208d0: 2229 0a20 2020 2020 2020 2020 2020 2069  ").            i
-000208e0: 6e63 6c75 6465 5f6f 7074 696d 697a 6572  nclude_optimizer
-000208f0: 203d 2074 665f 6d65 7461 2e67 6574 2822   = tf_meta.get("
-00020900: 696e 636c 7564 655f 6f70 7469 6d69 7a65  include_optimize
-00020910: 7222 290a 0a20 2020 2020 2020 2069 6620  r")..        if 
-00020920: 7361 7665 5f66 6f72 6d61 7420 3d3d 2022  save_format == "
-00020930: 7466 2220 6f72 2028 0a20 2020 2020 2020  tf" or (.       
-00020940: 2020 2020 2073 6176 655f 666f 726d 6174       save_format
-00020950: 2069 7320 4e6f 6e65 0a20 2020 2020 2020   is None.       
-00020960: 2020 2020 2061 6e64 2022 7465 6e73 6f72       and "tensor
-00020970: 666c 6f77 2e70 7974 686f 6e2e 6b65 7261  flow.python.kera
-00020980: 732e 656e 6769 6e65 2e74 7261 696e 696e  s.engine.trainin
-00020990: 672e 4d6f 6465 6c22 2069 6e20 7374 7228  g.Model" in str(
-000209a0: 7479 7065 286d 6f64 656c 2929 0a20 2020  type(model)).   
-000209b0: 2020 2020 2029 3a0a 2020 2020 2020 2020       ):.        
-000209c0: 2020 2020 7465 6d70 5f64 6972 5f6e 616d      temp_dir_nam
-000209d0: 6520 3d20 227b 7d22 2e66 6f72 6d61 7428  e = "{}".format(
-000209e0: 2270 6222 202b 2067 656e 5f69 6429 0a20  "pb" + gen_id). 
+000202d0: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
+000202e0: 2020 290a 2020 2020 2020 2020 2020 2020    ).            
+000202f0: 662e 636c 6f73 6528 290a 2020 2020 2020  f.close().      
+00020300: 2020 2020 2020 7a69 702e 7772 6974 6528        zip.write(
+00020310: 6d66 696c 656e 616d 6529 0a0a 2020 2020  mfilename)..    
+00020320: 6465 6620 5f73 746f 7265 5f74 665f 6d6f  def _store_tf_mo
+00020330: 6465 6c28 0a20 2020 2020 2020 2073 656c  del(.        sel
+00020340: 662c 0a20 2020 2020 2020 206d 6f64 656c  f,.        model
+00020350: 3a20 416e 792c 0a20 2020 2020 2020 206d  : Any,.        m
+00020360: 6574 615f 7072 6f70 733a 2064 6963 745b  eta_props: dict[
+00020370: 7374 722c 2041 6e79 5d2c 0a20 2020 2020  str, Any],.     
+00020380: 2020 2066 6561 7475 7265 5f6e 616d 6573     feature_names
+00020390: 3a20 4665 6174 7572 654e 616d 6573 4172  : FeatureNamesAr
+000203a0: 7261 7954 7970 6520 7c20 4e6f 6e65 203d  rayType | None =
+000203b0: 204e 6f6e 652c 0a20 2020 2020 2020 206c   None,.        l
+000203c0: 6162 656c 5f63 6f6c 756d 6e5f 6e61 6d65  abel_column_name
+000203d0: 733a 204c 6162 656c 436f 6c75 6d6e 4e61  s: LabelColumnNa
+000203e0: 6d65 7354 7970 6520 7c20 4e6f 6e65 203d  mesType | None =
+000203f0: 204e 6f6e 652c 0a20 2020 2029 202d 3e20   None,.    ) -> 
+00020400: 6469 6374 5b73 7472 2c20 416e 795d 3a0a  dict[str, Any]:.
+00020410: 2020 2020 2020 2020 2320 4d6f 6465 6c20          # Model 
+00020420: 7479 7065 2069 730a 2020 2020 2020 2020  type is.        
+00020430: 696d 706f 7274 2074 656e 736f 7266 6c6f  import tensorflo
+00020440: 7720 6173 2074 660a 0a20 2020 2020 2020  w as tf..       
+00020450: 2075 726c 203d 2028 0a20 2020 2020 2020   url = (.       
+00020460: 2020 2020 2073 656c 662e 5f63 6c69 656e       self._clien
+00020470: 742e 7365 7276 6963 655f 696e 7374 616e  t.service_instan
+00020480: 6365 2e5f 6872 6566 5f64 6566 696e 6974  ce._href_definit
+00020490: 696f 6e73 2e67 6574 5f70 7562 6c69 7368  ions.get_publish
+000204a0: 6564 5f6d 6f64 656c 735f 6872 6566 2829  ed_models_href()
+000204b0: 0a20 2020 2020 2020 2029 0a20 2020 2020  .        ).     
+000204c0: 2020 2069 645f 6c65 6e67 7468 203d 2032     id_length = 2
+000204d0: 300a 2020 2020 2020 2020 6765 6e5f 6964  0.        gen_id
+000204e0: 203d 2075 6964 5f67 656e 6572 6174 6528   = uid_generate(
+000204f0: 6964 5f6c 656e 6774 6829 0a0a 2020 2020  id_length)..    
+00020500: 2020 2020 7466 5f6d 6574 6120 3d20 4e6f      tf_meta = No
+00020510: 6e65 0a20 2020 2020 2020 206f 7074 696f  ne.        optio
+00020520: 6e73 203d 204e 6f6e 650a 2020 2020 2020  ns = None.      
+00020530: 2020 7369 676e 6174 7572 6520 3d20 4e6f    signature = No
+00020540: 6e65 0a20 2020 2020 2020 2073 6176 655f  ne.        save_
+00020550: 666f 726d 6174 203d 204e 6f6e 650a 2020  format = None.  
+00020560: 2020 2020 2020 696e 636c 7564 655f 6f70        include_op
+00020570: 7469 6d69 7a65 7220 3d20 4e6f 6e65 0a20  timizer = None. 
+00020580: 2020 2020 2020 2069 6620 280a 2020 2020         if (.    
+00020590: 2020 2020 2020 2020 2274 665f 6d6f 6465          "tf_mode
+000205a0: 6c5f 7061 7261 6d73 2220 696e 206d 6574  l_params" in met
+000205b0: 615f 7072 6f70 730a 2020 2020 2020 2020  a_props.        
+000205c0: 2020 2020 616e 6420 6d65 7461 5f70 726f      and meta_pro
+000205d0: 7073 5b73 656c 662e 436f 6e66 6967 7572  ps[self.Configur
+000205e0: 6174 696f 6e4d 6574 614e 616d 6573 2e54  ationMetaNames.T
+000205f0: 465f 4d4f 4445 4c5f 5041 5241 4d53 5d20  F_MODEL_PARAMS] 
+00020600: 6973 206e 6f74 204e 6f6e 650a 2020 2020  is not None.    
+00020610: 2020 2020 293a 0a20 2020 2020 2020 2020      ):.         
+00020620: 2020 2074 665f 6d65 7461 203d 2063 6f70     tf_meta = cop
+00020630: 792e 6465 6570 636f 7079 280a 2020 2020  y.deepcopy(.    
+00020640: 2020 2020 2020 2020 2020 2020 6d65 7461              meta
+00020650: 5f70 726f 7073 5b73 656c 662e 436f 6e66  _props[self.Conf
+00020660: 6967 7572 6174 696f 6e4d 6574 614e 616d  igurationMetaNam
+00020670: 6573 2e54 465f 4d4f 4445 4c5f 5041 5241  es.TF_MODEL_PARA
+00020680: 4d53 5d0a 2020 2020 2020 2020 2020 2020  MS].            
+00020690: 290a 2020 2020 2020 2020 2020 2020 7361  ).            sa
+000206a0: 7665 5f66 6f72 6d61 7420 3d20 7466 5f6d  ve_format = tf_m
+000206b0: 6574 612e 6765 7428 2273 6176 655f 666f  eta.get("save_fo
+000206c0: 726d 6174 2229 0a20 2020 2020 2020 2020  rmat").         
+000206d0: 2020 206f 7074 696f 6e73 203d 2074 665f     options = tf_
+000206e0: 6d65 7461 2e67 6574 2822 6f70 7469 6f6e  meta.get("option
+000206f0: 7322 290a 2020 2020 2020 2020 2020 2020  s").            
+00020700: 7369 676e 6174 7572 6520 3d20 7466 5f6d  signature = tf_m
+00020710: 6574 612e 6765 7428 2273 6967 6e61 7475  eta.get("signatu
+00020720: 7265 2229 0a20 2020 2020 2020 2020 2020  re").           
+00020730: 2069 6e63 6c75 6465 5f6f 7074 696d 697a   include_optimiz
+00020740: 6572 203d 2074 665f 6d65 7461 2e67 6574  er = tf_meta.get
+00020750: 2822 696e 636c 7564 655f 6f70 7469 6d69  ("include_optimi
+00020760: 7a65 7222 290a 0a20 2020 2020 2020 2069  zer")..        i
+00020770: 6620 7361 7665 5f66 6f72 6d61 7420 3d3d  f save_format ==
+00020780: 2022 7466 2220 6f72 2028 0a20 2020 2020   "tf" or (.     
+00020790: 2020 2020 2020 2073 6176 655f 666f 726d         save_form
+000207a0: 6174 2069 7320 4e6f 6e65 0a20 2020 2020  at is None.     
+000207b0: 2020 2020 2020 2061 6e64 2022 7465 6e73         and "tens
+000207c0: 6f72 666c 6f77 2e70 7974 686f 6e2e 6b65  orflow.python.ke
+000207d0: 7261 732e 656e 6769 6e65 2e74 7261 696e  ras.engine.train
+000207e0: 696e 672e 4d6f 6465 6c22 2069 6e20 7374  ing.Model" in st
+000207f0: 7228 7479 7065 286d 6f64 656c 2929 0a20  r(type(model)). 
+00020800: 2020 2020 2020 2029 3a0a 2020 2020 2020         ):.      
+00020810: 2020 2020 2020 7465 6d70 5f64 6972 5f6e        temp_dir_n
+00020820: 616d 6520 3d20 227b 7d22 2e66 6f72 6d61  ame = "{}".forma
+00020830: 7428 2270 6222 202b 2067 656e 5f69 6429  t("pb" + gen_id)
+00020840: 0a20 2020 2020 2020 2020 2020 2074 656d  .            tem
+00020850: 705f 6469 7220 3d20 7465 6d70 5f64 6972  p_dir = temp_dir
+00020860: 5f6e 616d 650a 2020 2020 2020 2020 2020  _name.          
+00020870: 2020 6966 206e 6f74 206f 732e 7061 7468    if not os.path
+00020880: 2e65 7869 7374 7328 7465 6d70 5f64 6972  .exists(temp_dir
+00020890: 293a 0a20 2020 2020 2020 2020 2020 2020  ):.             
+000208a0: 2020 206f 732e 6d61 6b65 6469 7273 2874     os.makedirs(t
+000208b0: 656d 705f 6469 7229 0a20 2020 2020 2020  emp_dir).       
+000208c0: 2020 2020 2069 6d70 6f72 7420 7465 6e73       import tens
+000208d0: 6f72 666c 6f77 2061 7320 7466 0a0a 2020  orflow as tf..  
+000208e0: 2020 2020 2020 2020 2020 7466 2e73 6176            tf.sav
+000208f0: 6564 5f6d 6f64 656c 2e73 6176 6528 6d6f  ed_model.save(mo
+00020900: 6465 6c2c 2074 656d 705f 6469 722c 2073  del, temp_dir, s
+00020910: 6967 6e61 7475 7265 733d 7369 676e 6174  ignatures=signat
+00020920: 7572 652c 206f 7074 696f 6e73 3d6f 7074  ure, options=opt
+00020930: 696f 6e73 290a 0a20 2020 2020 2020 2065  ions)..        e
+00020940: 6c69 6620 7361 7665 5f66 6f72 6d61 7420  lif save_format 
+00020950: 3d3d 2022 6835 2220 6f72 2028 0a20 2020  == "h5" or (.   
+00020960: 2020 2020 2020 2020 2073 6176 655f 666f           save_fo
+00020970: 726d 6174 2069 7320 4e6f 6e65 0a20 2020  rmat is None.   
+00020980: 2020 2020 2020 2020 2061 6e64 2022 7465           and "te
+00020990: 6e73 6f72 666c 6f77 2e70 7974 686f 6e2e  nsorflow.python.
+000209a0: 6b65 7261 732e 656e 6769 6e65 2e73 6571  keras.engine.seq
+000209b0: 7565 6e74 6961 6c2e 5365 7175 656e 7469  uential.Sequenti
+000209c0: 616c 220a 2020 2020 2020 2020 2020 2020  al".            
+000209d0: 696e 2073 7472 2874 7970 6528 6d6f 6465  in str(type(mode
+000209e0: 6c29 290a 2020 2020 2020 2020 293a 0a20  l)).        ):. 
 000209f0: 2020 2020 2020 2020 2020 2074 656d 705f             temp_
-00020a00: 6469 7220 3d20 7465 6d70 5f64 6972 5f6e  dir = temp_dir_n
-00020a10: 616d 650a 2020 2020 2020 2020 2020 2020  ame.            
-00020a20: 6966 206e 6f74 206f 732e 7061 7468 2e65  if not os.path.e
-00020a30: 7869 7374 7328 7465 6d70 5f64 6972 293a  xists(temp_dir):
-00020a40: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00020a50: 206f 732e 6d61 6b65 6469 7273 2874 656d   os.makedirs(tem
-00020a60: 705f 6469 7229 0a20 2020 2020 2020 2020  p_dir).         
-00020a70: 2020 2069 6d70 6f72 7420 7465 6e73 6f72     import tensor
-00020a80: 666c 6f77 2061 7320 7466 0a0a 2020 2020  flow as tf..    
-00020a90: 2020 2020 2020 2020 7466 2e73 6176 6564          tf.saved
-00020aa0: 5f6d 6f64 656c 2e73 6176 6528 6d6f 6465  _model.save(mode
-00020ab0: 6c2c 2074 656d 705f 6469 722c 2073 6967  l, temp_dir, sig
-00020ac0: 6e61 7475 7265 733d 7369 676e 6174 7572  natures=signatur
-00020ad0: 652c 206f 7074 696f 6e73 3d6f 7074 696f  e, options=optio
-00020ae0: 6e73 290a 0a20 2020 2020 2020 2065 6c69  ns)..        eli
-00020af0: 6620 7361 7665 5f66 6f72 6d61 7420 3d3d  f save_format ==
-00020b00: 2022 6835 2220 6f72 2028 0a20 2020 2020   "h5" or (.     
-00020b10: 2020 2020 2020 2073 6176 655f 666f 726d         save_form
-00020b20: 6174 2069 7320 4e6f 6e65 0a20 2020 2020  at is None.     
-00020b30: 2020 2020 2020 2061 6e64 2022 7465 6e73         and "tens
-00020b40: 6f72 666c 6f77 2e70 7974 686f 6e2e 6b65  orflow.python.ke
-00020b50: 7261 732e 656e 6769 6e65 2e73 6571 7565  ras.engine.seque
-00020b60: 6e74 6961 6c2e 5365 7175 656e 7469 616c  ntial.Sequential
-00020b70: 220a 2020 2020 2020 2020 2020 2020 696e  ".            in
-00020b80: 2073 7472 2874 7970 6528 6d6f 6465 6c29   str(type(model)
-00020b90: 290a 2020 2020 2020 2020 293a 0a20 2020  ).        ):.   
-00020ba0: 2020 2020 2020 2020 2074 656d 705f 6469           temp_di
-00020bb0: 725f 6e61 6d65 203d 2022 7b7d 222e 666f  r_name = "{}".fo
-00020bc0: 726d 6174 2822 6864 6673 2220 2b20 6765  rmat("hdfs" + ge
-00020bd0: 6e5f 6964 290a 2020 2020 2020 2020 2020  n_id).          
-00020be0: 2020 7465 6d70 5f64 6972 203d 2074 656d    temp_dir = tem
-00020bf0: 705f 6469 725f 6e61 6d65 0a20 2020 2020  p_dir_name.     
-00020c00: 2020 2020 2020 2069 6620 6e6f 7420 6f73         if not os
-00020c10: 2e70 6174 682e 6578 6973 7473 2874 656d  .path.exists(tem
-00020c20: 705f 6469 7229 3a0a 2020 2020 2020 2020  p_dir):.        
-00020c30: 2020 2020 2020 2020 6f73 2e6d 616b 6564          os.maked
-00020c40: 6972 7328 7465 6d70 5f64 6972 290a 2020  irs(temp_dir).  
-00020c50: 2020 2020 2020 2020 2020 6d6f 6465 6c5f            model_
-00020c60: 6669 6c65 203d 2074 656d 705f 6469 7220  file = temp_dir 
-00020c70: 2b20 222f 7365 7175 656e 7469 616c 5f6d  + "/sequential_m
-00020c80: 6f64 656c 2e68 3522 0a20 2020 2020 2020  odel.h5".       
-00020c90: 2020 2020 2074 662e 6b65 7261 732e 6d6f       tf.keras.mo
-00020ca0: 6465 6c73 2e73 6176 655f 6d6f 6465 6c28  dels.save_model(
-00020cb0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00020cc0: 206d 6f64 656c 2c0a 2020 2020 2020 2020   model,.        
-00020cd0: 2020 2020 2020 2020 6d6f 6465 6c5f 6669          model_fi
-00020ce0: 6c65 2c0a 2020 2020 2020 2020 2020 2020  le,.            
-00020cf0: 2020 2020 696e 636c 7564 655f 6f70 7469      include_opti
-00020d00: 6d69 7a65 723d 696e 636c 7564 655f 6f70  mizer=include_op
-00020d10: 7469 6d69 7a65 722c 0a20 2020 2020 2020  timizer,.       
-00020d20: 2020 2020 2020 2020 2073 6176 655f 666f           save_fo
-00020d30: 726d 6174 3d22 6835 222c 0a20 2020 2020  rmat="h5",.     
-00020d40: 2020 2020 2020 2020 2020 2073 6967 6e61             signa
-00020d50: 7475 7265 733d 4e6f 6e65 2c0a 2020 2020  tures=None,.    
-00020d60: 2020 2020 2020 2020 2020 2020 6f70 7469              opti
-00020d70: 6f6e 733d 6f70 7469 6f6e 732c 0a20 2020  ons=options,.   
-00020d80: 2020 2020 2020 2020 2029 0a20 2020 2020           ).     
-00020d90: 2020 2065 6c69 6620 280a 2020 2020 2020     elif (.      
-00020da0: 2020 2020 2020 6973 696e 7374 616e 6365        isinstance
-00020db0: 286d 6f64 656c 2c20 7374 7229 0a20 2020  (model, str).   
-00020dc0: 2020 2020 2020 2020 2061 6e64 206f 732e           and os.
-00020dd0: 7061 7468 2e73 706c 6974 6578 7428 6f73  path.splitext(os
-00020de0: 2e70 6174 682e 6261 7365 6e61 6d65 286d  .path.basename(m
-00020df0: 6f64 656c 2929 5b2d 315d 203d 3d20 222e  odel))[-1] == ".
-00020e00: 6835 220a 2020 2020 2020 2020 293a 0a20  h5".        ):. 
-00020e10: 2020 2020 2020 2020 2020 2074 656d 705f             temp_
-00020e20: 6469 725f 6e61 6d65 203d 2022 7b7d 222e  dir_name = "{}".
-00020e30: 666f 726d 6174 2822 6864 6673 2220 2b20  format("hdfs" + 
-00020e40: 6765 6e5f 6964 290a 2020 2020 2020 2020  gen_id).        
-00020e50: 2020 2020 7465 6d70 5f64 6972 203d 2074      temp_dir = t
-00020e60: 656d 705f 6469 725f 6e61 6d65 0a20 2020  emp_dir_name.   
-00020e70: 2020 2020 2020 2020 2069 6620 6e6f 7420           if not 
-00020e80: 6f73 2e70 6174 682e 6578 6973 7473 2874  os.path.exists(t
-00020e90: 656d 705f 6469 7229 3a0a 2020 2020 2020  emp_dir):.      
-00020ea0: 2020 2020 2020 2020 2020 696d 706f 7274            import
-00020eb0: 2073 6875 7469 6c0a 0a20 2020 2020 2020   shutil..       
-00020ec0: 2020 2020 2020 2020 206f 732e 6d61 6b65           os.make
-00020ed0: 6469 7273 2874 656d 705f 6469 7229 0a20  dirs(temp_dir). 
-00020ee0: 2020 2020 2020 2020 2020 2020 2020 2073                 s
-00020ef0: 6875 7469 6c2e 636f 7079 3228 6d6f 6465  hutil.copy2(mode
-00020f00: 6c2c 2074 656d 705f 6469 7229 0a20 2020  l, temp_dir).   
-00020f10: 2020 2020 2065 6c73 653a 0a0a 2020 2020       else:..    
-00020f20: 2020 2020 2020 2020 7261 6973 6520 574d          raise WM
-00020f30: 4c43 6c69 656e 7445 7272 6f72 280a 2020  LClientError(.  
-00020f40: 2020 2020 2020 2020 2020 2020 2020 2253                "S
-00020f50: 6176 696e 6720 7468 6520 7465 6e73 6f72  aving the tensor
-00020f60: 666c 6f77 206d 6f64 656c 2072 6571 7569  flow model requi
-00020f70: 7265 7320 7468 6520 6d6f 6465 6c20 6f66  res the model of
-00020f80: 2065 6974 6865 7220 7466 2066 6f72 6d61   either tf forma
-00020f90: 7420 6f72 2068 3520 666f 726d 6174 2066  t or h5 format f
-00020fa0: 6f72 2053 6571 7565 6e74 6961 6c20 6d6f  or Sequential mo
-00020fb0: 6465 6c2e 220a 2020 2020 2020 2020 2020  del.".          
-00020fc0: 2020 290a 0a20 2020 2020 2020 2070 6174    )..        pat
-00020fd0: 685f 746f 5f61 7263 6869 7665 203d 2073  h_to_archive = s
-00020fe0: 656c 662e 5f6d 6f64 656c 5f63 6f6e 7465  elf._model_conte
-00020ff0: 6e74 5f63 6f6d 7072 6573 735f 6172 7469  nt_compress_arti
-00021000: 6661 6374 2874 656d 705f 6469 725f 6e61  fact(temp_dir_na
-00021010: 6d65 2c20 7465 6d70 5f64 6972 290a 2020  me, temp_dir).  
-00021020: 2020 2020 2020 7061 796c 6f61 6420 3d20        payload = 
-00021030: 636f 7079 2e64 6565 7063 6f70 7928 6d65  copy.deepcopy(me
-00021040: 7461 5f70 726f 7073 290a 2020 2020 2020  ta_props).      
-00021050: 2020 6966 206c 6162 656c 5f63 6f6c 756d    if label_colum
-00021060: 6e5f 6e61 6d65 733a 0a20 2020 2020 2020  n_names:.       
-00021070: 2020 2020 2070 6179 6c6f 6164 5b22 6c61       payload["la
-00021080: 6265 6c5f 636f 6c75 6d6e 225d 203d 206c  bel_column"] = l
-00021090: 6162 656c 5f63 6f6c 756d 6e5f 6e61 6d65  abel_column_name
-000210a0: 735b 305d 0a0a 2020 2020 2020 2020 7265  s[0]..        re
-000210b0: 7370 6f6e 7365 203d 2072 6571 7565 7374  sponse = request
-000210c0: 732e 706f 7374 280a 2020 2020 2020 2020  s.post(.        
-000210d0: 2020 2020 7572 6c2c 0a20 2020 2020 2020      url,.       
-000210e0: 2020 2020 206a 736f 6e3d 7061 796c 6f61       json=payloa
-000210f0: 642c 0a20 2020 2020 2020 2020 2020 2070  d,.            p
-00021100: 6172 616d 733d 7365 6c66 2e5f 636c 6965  arams=self._clie
-00021110: 6e74 2e5f 7061 7261 6d73 2829 2c0a 2020  nt._params(),.  
-00021120: 2020 2020 2020 2020 2020 6865 6164 6572            header
-00021130: 733d 7365 6c66 2e5f 636c 6965 6e74 2e5f  s=self._client._
-00021140: 6765 745f 6865 6164 6572 7328 292c 0a20  get_headers(),. 
-00021150: 2020 2020 2020 2029 0a20 2020 2020 2020         ).       
-00021160: 2072 6573 756c 7420 3d20 7365 6c66 2e5f   result = self._
-00021170: 6861 6e64 6c65 5f72 6573 706f 6e73 6528  handle_response(
-00021180: 3230 312c 2022 6372 6561 7469 6e67 206d  201, "creating m
-00021190: 6f64 656c 222c 2072 6573 706f 6e73 6529  odel", response)
-000211a0: 0a20 2020 2020 2020 206d 6f64 656c 5f69  .        model_i
-000211b0: 6420 3d20 7365 6c66 2e5f 6765 745f 7265  d = self._get_re
-000211c0: 7175 6972 6564 5f65 6c65 6d65 6e74 5f66  quired_element_f
-000211d0: 726f 6d5f 6469 6374 280a 2020 2020 2020  rom_dict(.      
-000211e0: 2020 2020 2020 7265 7375 6c74 2c20 226d        result, "m
-000211f0: 6f64 656c 5f64 6574 6169 6c73 222c 205b  odel_details", [
-00021200: 226d 6574 6164 6174 6122 2c20 2269 6422  "metadata", "id"
-00021210: 5d0a 2020 2020 2020 2020 290a 0a20 2020  ].        )..   
-00021220: 2020 2020 2075 726c 203d 2028 0a20 2020       url = (.   
-00021230: 2020 2020 2020 2020 2073 656c 662e 5f63           self._c
-00021240: 6c69 656e 742e 7365 7276 6963 655f 696e  lient.service_in
-00021250: 7374 616e 6365 2e5f 6872 6566 5f64 6566  stance._href_def
-00021260: 696e 6974 696f 6e73 2e67 6574 5f70 7562  initions.get_pub
-00021270: 6c69 7368 6564 5f6d 6f64 656c 5f68 7265  lished_model_hre
-00021280: 6628 0a20 2020 2020 2020 2020 2020 2020  f(.             
-00021290: 2020 206d 6f64 656c 5f69 640a 2020 2020     model_id.    
-000212a0: 2020 2020 2020 2020 290a 2020 2020 2020          ).      
-000212b0: 2020 2020 2020 2b20 222f 636f 6e74 656e        + "/conten
-000212c0: 7422 0a20 2020 2020 2020 2029 0a20 2020  t".        ).   
-000212d0: 2020 2020 2077 6974 6820 6f70 656e 2870       with open(p
-000212e0: 6174 685f 746f 5f61 7263 6869 7665 2c20  ath_to_archive, 
-000212f0: 2272 6222 2920 6173 2066 3a0a 2020 2020  "rb") as f:.    
-00021300: 2020 2020 2020 2020 7170 6172 616d 7320          qparams 
-00021310: 3d20 7365 6c66 2e5f 636c 6965 6e74 2e5f  = self._client._
-00021320: 7061 7261 6d73 2829 0a0a 2020 2020 2020  params()..      
-00021330: 2020 2020 2020 7170 6172 616d 732e 7570        qparams.up
-00021340: 6461 7465 287b 2263 6f6e 7465 6e74 5f66  date({"content_f
-00021350: 6f72 6d61 7422 3a20 226e 6174 6976 6522  ormat": "native"
-00021360: 7d29 0a20 2020 2020 2020 2020 2020 2071  }).            q
-00021370: 7061 7261 6d73 2e75 7064 6174 6528 7b22  params.update({"
-00021380: 7665 7273 696f 6e22 3a20 7365 6c66 2e5f  version": self._
-00021390: 636c 6965 6e74 2e76 6572 7369 6f6e 5f70  client.version_p
-000213a0: 6172 616d 7d29 0a20 2020 2020 2020 2020  aram}).         
-000213b0: 2020 2023 2075 7064 6174 6520 7468 6520     # update the 
-000213c0: 636f 6e74 656e 7420 7061 7468 2066 6f72  content path for
-000213d0: 2074 6865 2041 7574 6f2d 6169 206d 6f64   the Auto-ai mod
-000213e0: 656c 2e0a 0a20 2020 2020 2020 2020 2020  el...           
-000213f0: 2072 6573 706f 6e73 6520 3d20 7265 7175   response = requ
-00021400: 6573 7473 2e70 7574 280a 2020 2020 2020  ests.put(.      
-00021410: 2020 2020 2020 2020 2020 7572 6c2c 0a20            url,. 
-00021420: 2020 2020 2020 2020 2020 2020 2020 2064                 d
-00021430: 6174 613d 662c 0a20 2020 2020 2020 2020  ata=f,.         
-00021440: 2020 2020 2020 2070 6172 616d 733d 7170         params=qp
-00021450: 6172 616d 732c 0a20 2020 2020 2020 2020  arams,.         
-00021460: 2020 2020 2020 2068 6561 6465 7273 3d73         headers=s
-00021470: 656c 662e 5f63 6c69 656e 742e 5f67 6574  elf._client._get
-00021480: 5f68 6561 6465 7273 280a 2020 2020 2020  _headers(.      
-00021490: 2020 2020 2020 2020 2020 2020 2020 636f                co
-000214a0: 6e74 656e 745f 7479 7065 3d22 6170 706c  ntent_type="appl
-000214b0: 6963 6174 696f 6e2f 6f63 7465 742d 7374  ication/octet-st
-000214c0: 7265 616d 220a 2020 2020 2020 2020 2020  ream".          
-000214d0: 2020 2020 2020 292c 0a20 2020 2020 2020        ),.       
-000214e0: 2020 2020 2029 0a20 2020 2020 2020 2020       ).         
-000214f0: 2020 2069 6620 7265 7370 6f6e 7365 2e73     if response.s
-00021500: 7461 7475 735f 636f 6465 2021 3d20 3230  tatus_code != 20
-00021510: 3020 616e 6420 7265 7370 6f6e 7365 2e73  0 and response.s
-00021520: 7461 7475 735f 636f 6465 2021 3d20 3230  tatus_code != 20
-00021530: 313a 0a20 2020 2020 2020 2020 2020 2020  1:.             
-00021540: 2020 2073 656c 662e 6465 6c65 7465 286d     self.delete(m
-00021550: 6f64 656c 5f69 6429 0a20 2020 2020 2020  odel_id).       
-00021560: 2020 2020 2073 656c 662e 5f68 616e 646c       self._handl
-00021570: 655f 7265 7370 6f6e 7365 2832 3031 2c20  e_response(201, 
-00021580: 2275 706c 6f61 6469 6e67 206d 6f64 656c  "uploading model
-00021590: 2063 6f6e 7465 6e74 222c 2072 6573 706f   content", respo
-000215a0: 6e73 652c 2046 616c 7365 290a 0a20 2020  nse, False)..   
-000215b0: 2020 2020 2020 2020 2069 6620 6f73 2e70           if os.p
-000215c0: 6174 682e 6578 6973 7473 2874 656d 705f  ath.exists(temp_
-000215d0: 6469 7229 3a0a 2020 2020 2020 2020 2020  dir):.          
-000215e0: 2020 2020 2020 696d 706f 7274 2073 6875        import shu
-000215f0: 7469 6c0a 0a20 2020 2020 2020 2020 2020  til..           
-00021600: 2020 2020 2073 6875 7469 6c2e 726d 7472       shutil.rmtr
-00021610: 6565 2874 656d 705f 6469 722c 2069 676e  ee(temp_dir, ign
-00021620: 6f72 655f 6572 726f 7273 3d54 7275 6529  ore_errors=True)
-00021630: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00021640: 206f 732e 7265 6d6f 7665 2870 6174 685f   os.remove(path_
-00021650: 746f 5f61 7263 6869 7665 290a 2020 2020  to_archive).    
-00021660: 2020 2020 2020 2020 7265 7475 726e 2073          return s
-00021670: 656c 662e 6765 745f 6465 7461 696c 7328  elf.get_details(
-00021680: 6d6f 6465 6c5f 6964 290a 0a20 2020 2064  model_id)..    d
-00021690: 6566 205f 7374 6f72 655f 6375 7374 6f6d  ef _store_custom
-000216a0: 5f66 6f75 6e64 6174 696f 6e5f 6d6f 6465  _foundation_mode
-000216b0: 6c28 0a20 2020 2020 2020 2073 656c 662c  l(.        self,
-000216c0: 206d 6f64 656c 3a20 7374 722c 206d 6574   model: str, met
-000216d0: 615f 7072 6f70 733a 2064 6963 745b 7374  a_props: dict[st
-000216e0: 722c 2041 6e79 5d0a 2020 2020 2920 2d3e  r, Any].    ) ->
-000216f0: 2064 6963 745b 7374 722c 2041 6e79 5d3a   dict[str, Any]:
-00021700: 0a20 2020 2020 2020 2023 2053 746f 7265  .        # Store
-00021710: 2063 7573 746f 6d20 666f 756e 6461 7469   custom foundati
-00021720: 6f6e 206d 6f64 656c 0a0a 2020 2020 2020  on model..      
-00021730: 2020 7061 796c 6f61 6420 3d20 7365 6c66    payload = self
-00021740: 2e5f 6372 6561 7465 5f63 7573 746f 6d5f  ._create_custom_
-00021750: 6d6f 6465 6c5f 7061 796c 6f61 6428 6d6f  model_payload(mo
-00021760: 6465 6c3d 6d6f 6465 6c2c 206d 6574 615f  del=model, meta_
-00021770: 7072 6f70 733d 6d65 7461 5f70 726f 7073  props=meta_props
-00021780: 290a 0a20 2020 2020 2020 2063 7265 6174  )..        creat
-00021790: 696f 6e5f 7265 7370 6f6e 7365 203d 2072  ion_response = r
-000217a0: 6571 7565 7374 732e 706f 7374 280a 2020  equests.post(.  
-000217b0: 2020 2020 2020 2020 2020 7572 6c3d 7365            url=se
-000217c0: 6c66 2e5f 636c 6965 6e74 2e73 6572 7669  lf._client.servi
-000217d0: 6365 5f69 6e73 7461 6e63 652e 5f68 7265  ce_instance._hre
-000217e0: 665f 6465 6669 6e69 7469 6f6e 732e 6765  f_definitions.ge
-000217f0: 745f 7075 626c 6973 6865 645f 6d6f 6465  t_published_mode
-00021800: 6c73 5f68 7265 6628 292c 0a20 2020 2020  ls_href(),.     
-00021810: 2020 2020 2020 2070 6172 616d 733d 7365         params=se
-00021820: 6c66 2e5f 636c 6965 6e74 2e5f 7061 7261  lf._client._para
-00021830: 6d73 2873 6b69 705f 666f 725f 6372 6561  ms(skip_for_crea
-00021840: 7465 3d54 7275 6529 2c0a 2020 2020 2020  te=True),.      
-00021850: 2020 2020 2020 6865 6164 6572 733d 7365        headers=se
-00021860: 6c66 2e5f 636c 6965 6e74 2e5f 6765 745f  lf._client._get_
-00021870: 6865 6164 6572 7328 292c 0a20 2020 2020  headers(),.     
-00021880: 2020 2020 2020 206a 736f 6e3d 7061 796c         json=payl
-00021890: 6f61 642c 0a20 2020 2020 2020 2029 0a0a  oad,.        )..
-000218a0: 2020 2020 2020 2020 6966 2063 7265 6174          if creat
-000218b0: 696f 6e5f 7265 7370 6f6e 7365 2e73 7461  ion_response.sta
-000218c0: 7475 735f 636f 6465 203d 3d20 3230 313a  tus_code == 201:
-000218d0: 0a20 2020 2020 2020 2020 2020 206d 6f64  .            mod
-000218e0: 656c 5f64 6574 6169 6c73 203d 2073 656c  el_details = sel
-000218f0: 662e 5f68 616e 646c 655f 7265 7370 6f6e  f._handle_respon
-00021900: 7365 280a 2020 2020 2020 2020 2020 2020  se(.            
-00021910: 2020 2020 3230 312c 2022 6372 6561 7469      201, "creati
-00021920: 6e67 206e 6577 206d 6f64 656c 222c 2063  ng new model", c
-00021930: 7265 6174 696f 6e5f 7265 7370 6f6e 7365  reation_response
-00021940: 0a20 2020 2020 2020 2020 2020 2029 0a20  .            ). 
-00021950: 2020 2020 2020 2065 6c73 653a 0a20 2020         else:.   
-00021960: 2020 2020 2020 2020 206d 6f64 656c 5f64           model_d
-00021970: 6574 6169 6c73 203d 2073 656c 662e 5f68  etails = self._h
-00021980: 616e 646c 655f 7265 7370 6f6e 7365 280a  andle_response(.
-00021990: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000219a0: 3230 322c 2022 6372 6561 7469 6e67 206e  202, "creating n
-000219b0: 6577 206d 6f64 656c 222c 2063 7265 6174  ew model", creat
-000219c0: 696f 6e5f 7265 7370 6f6e 7365 0a20 2020  ion_response.   
-000219d0: 2020 2020 2020 2020 2029 0a20 2020 2020           ).     
-000219e0: 2020 206d 6f64 656c 5f69 6420 3d20 6d6f     model_id = mo
-000219f0: 6465 6c5f 6465 7461 696c 735b 226d 6574  del_details["met
-00021a00: 6164 6174 6122 5d5b 2269 6422 5d0a 0a20  adata"]["id"].. 
-00021a10: 2020 2020 2020 2069 6620 2265 6e74 6974         if "entit
-00021a20: 7922 2069 6e20 6d6f 6465 6c5f 6465 7461  y" in model_deta
-00021a30: 696c 733a 0a20 2020 2020 2020 2020 2020  ils:.           
-00021a40: 2073 7461 7274 5f74 696d 6520 3d20 7469   start_time = ti
-00021a50: 6d65 2e74 696d 6528 290a 2020 2020 2020  me.time().      
-00021a60: 2020 2020 2020 656c 6170 7365 645f 7469        elapsed_ti
-00021a70: 6d65 203d 2030 2e30 0a20 2020 2020 2020  me = 0.0.       
-00021a80: 2020 2020 2077 6869 6c65 2028 0a20 2020       while (.   
-00021a90: 2020 2020 2020 2020 2020 2020 206d 6f64               mod
-00021aa0: 656c 5f64 6574 6169 6c73 5b22 656e 7469  el_details["enti
-00021ab0: 7479 225d 2e67 6574 2822 636f 6e74 656e  ty"].get("conten
-00021ac0: 745f 696d 706f 7274 5f73 7461 7465 2229  t_import_state")
-00021ad0: 203d 3d20 2272 756e 6e69 6e67 220a 2020   == "running".  
-00021ae0: 2020 2020 2020 2020 2020 2020 2020 616e                an
-00021af0: 6420 656c 6170 7365 645f 7469 6d65 203c  d elapsed_time <
-00021b00: 2036 300a 2020 2020 2020 2020 2020 2020   60.            
-00021b10: 293a 0a20 2020 2020 2020 2020 2020 2020  ):.             
-00021b20: 2020 2074 696d 652e 736c 6565 7028 3229     time.sleep(2)
-00021b30: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00021b40: 2065 6c61 7073 6564 5f74 696d 6520 3d20   elapsed_time = 
-00021b50: 7469 6d65 2e74 696d 6528 2920 2d20 7374  time.time() - st
-00021b60: 6172 745f 7469 6d65 0a20 2020 2020 2020  art_time.       
-00021b70: 2020 2020 2020 2020 206d 6f64 656c 5f64           model_d
-00021b80: 6574 6169 6c73 203d 2073 656c 662e 6765  etails = self.ge
-00021b90: 745f 6465 7461 696c 7328 6d6f 6465 6c5f  t_details(model_
-00021ba0: 6964 290a 2020 2020 2020 2020 7265 7475  id).        retu
-00021bb0: 726e 2073 656c 662e 6765 745f 6465 7461  rn self.get_deta
-00021bc0: 696c 7328 6d6f 6465 6c5f 6964 290a 0a20  ils(model_id).. 
-00021bd0: 2020 2064 6566 205f 6372 6561 7465 5f63     def _create_c
-00021be0: 7573 746f 6d5f 6d6f 6465 6c5f 7061 796c  ustom_model_payl
-00021bf0: 6f61 6428 0a20 2020 2020 2020 2073 656c  oad(.        sel
-00021c00: 662c 206d 6f64 656c 3a20 7374 722c 206d  f, model: str, m
-00021c10: 6574 615f 7072 6f70 733a 2064 6963 745b  eta_props: dict[
-00021c20: 7374 722c 2041 6e79 5d0a 2020 2020 2920  str, Any].    ) 
-00021c30: 2d3e 2064 6963 745b 7374 722c 2041 6e79  -> dict[str, Any
-00021c40: 5d3a 0a20 2020 2020 2020 206d 6574 6164  ]:.        metad
-00021c50: 6174 6120 3d20 636f 7079 2e64 6565 7063  ata = copy.deepc
-00021c60: 6f70 7928 6d65 7461 5f70 726f 7073 290a  opy(meta_props).
-00021c70: 0a20 2020 2020 2020 204d 6f64 656c 732e  .        Models.
-00021c80: 5f76 616c 6964 6174 655f 7479 7065 286d  _validate_type(m
-00021c90: 6f64 656c 2c20 226d 6f64 656c 222c 2073  odel, "model", s
-00021ca0: 7472 2c20 5472 7565 290a 0a20 2020 2020  tr, True)..     
-00021cb0: 2020 2069 6620 7365 6c66 2e43 6f6e 6669     if self.Confi
-00021cc0: 6775 7261 7469 6f6e 4d65 7461 4e61 6d65  gurationMetaName
-00021cd0: 732e 5459 5045 2069 6e20 6d65 7461 5f70  s.TYPE in meta_p
-00021ce0: 726f 7073 3a0a 2020 2020 2020 2020 2020  rops:.          
-00021cf0: 2020 7365 6c66 2e5f 7661 6c69 6461 7465    self._validate
-00021d00: 5f6d 6574 615f 7072 6f70 280a 2020 2020  _meta_prop(.    
-00021d10: 2020 2020 2020 2020 2020 2020 6d65 7461              meta
-00021d20: 5f70 726f 7073 2c20 7365 6c66 2e43 6f6e  _props, self.Con
-00021d30: 6669 6775 7261 7469 6f6e 4d65 7461 4e61  figurationMetaNa
-00021d40: 6d65 732e 5459 5045 2c20 7374 722c 2054  mes.TYPE, str, T
-00021d50: 7275 650a 2020 2020 2020 2020 2020 2020  rue.            
-00021d60: 290a 0a20 2020 2020 2020 2069 6620 7365  )..        if se
-00021d70: 6c66 2e43 6f6e 6669 6775 7261 7469 6f6e  lf.Configuration
-00021d80: 4d65 7461 4e61 6d65 732e 534f 4654 5741  MetaNames.SOFTWA
-00021d90: 5245 5f53 5045 435f 4944 2069 6e20 6d65  RE_SPEC_ID in me
-00021da0: 7461 5f70 726f 7073 3a0a 2020 2020 2020  ta_props:.      
-00021db0: 2020 2020 2020 7365 6c66 2e5f 7661 6c69        self._vali
-00021dc0: 6461 7465 5f6d 6574 615f 7072 6f70 280a  date_meta_prop(.
-00021dd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021de0: 6d65 7461 5f70 726f 7073 2c20 7365 6c66  meta_props, self
-00021df0: 2e43 6f6e 6669 6775 7261 7469 6f6e 4d65  .ConfigurationMe
-00021e00: 7461 4e61 6d65 732e 534f 4654 5741 5245  taNames.SOFTWARE
-00021e10: 5f53 5045 435f 4944 2c20 7374 722c 2054  _SPEC_ID, str, T
-00021e20: 7275 650a 2020 2020 2020 2020 2020 2020  rue.            
-00021e30: 290a 2020 2020 2020 2020 2020 2020 6d65  ).            me
-00021e40: 7461 6461 7461 2e75 7064 6174 6528 0a20  tadata.update(. 
-00021e50: 2020 2020 2020 2020 2020 2020 2020 207b                 {
-00021e60: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00021e70: 2020 2020 2073 656c 662e 436f 6e66 6967       self.Config
-00021e80: 7572 6174 696f 6e4d 6574 614e 616d 6573  urationMetaNames
-00021e90: 2e53 4f46 5457 4152 455f 5350 4543 5f49  .SOFTWARE_SPEC_I
-00021ea0: 443a 207b 0a20 2020 2020 2020 2020 2020  D: {.           
-00021eb0: 2020 2020 2020 2020 2020 2020 2022 6964               "id
-00021ec0: 223a 206d 6574 615f 7072 6f70 735b 7365  ": meta_props[se
-00021ed0: 6c66 2e43 6f6e 6669 6775 7261 7469 6f6e  lf.Configuration
-00021ee0: 4d65 7461 4e61 6d65 732e 534f 4654 5741  MetaNames.SOFTWA
-00021ef0: 5245 5f53 5045 435f 4944 5d0a 2020 2020  RE_SPEC_ID].    
-00021f00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021f10: 7d0a 2020 2020 2020 2020 2020 2020 2020  }.              
-00021f20: 2020 7d0a 2020 2020 2020 2020 2020 2020    }.            
-00021f30: 290a 0a20 2020 2020 2020 206d 6574 6164  )..        metad
-00021f40: 6174 612e 7570 6461 7465 280a 2020 2020  ata.update(.    
-00021f50: 2020 2020 2020 2020 7b73 656c 662e 436f          {self.Co
-00021f60: 6e66 6967 7572 6174 696f 6e4d 6574 614e  nfigurationMetaN
-00021f70: 616d 6573 2e46 4f55 4e44 4154 494f 4e5f  ames.FOUNDATION_
-00021f80: 4d4f 4445 4c3a 207b 226d 6f64 656c 5f69  MODEL: {"model_i
-00021f90: 6422 3a20 6d6f 6465 6c7d 7d0a 2020 2020  d": model}}.    
-00021fa0: 2020 2020 290a 0a20 2020 2020 2020 2069      )..        i
-00021fb0: 6620 7365 6c66 2e5f 636c 6965 6e74 2e64  f self._client.d
-00021fc0: 6566 6175 6c74 5f73 7061 6365 5f69 6420  efault_space_id 
-00021fd0: 6973 206e 6f74 204e 6f6e 653a 0a20 2020  is not None:.   
-00021fe0: 2020 2020 2020 2020 206d 6574 6164 6174           metadat
-00021ff0: 615b 2273 7061 6365 5f69 6422 5d20 3d20  a["space_id"] = 
-00022000: 7365 6c66 2e5f 636c 6965 6e74 2e64 6566  self._client.def
-00022010: 6175 6c74 5f73 7061 6365 5f69 640a 2020  ault_space_id.  
-00022020: 2020 2020 2020 656c 7365 3a0a 2020 2020        else:.    
-00022030: 2020 2020 2020 2020 6966 2073 656c 662e          if self.
-00022040: 5f63 6c69 656e 742e 6465 6661 756c 745f  _client.default_
-00022050: 7072 6f6a 6563 745f 6964 2069 7320 6e6f  project_id is no
-00022060: 7420 4e6f 6e65 3a0a 2020 2020 2020 2020  t None:.        
-00022070: 2020 2020 2020 2020 6d65 7461 6461 7461          metadata
-00022080: 2e75 7064 6174 6528 7b22 7072 6f6a 6563  .update({"projec
-00022090: 745f 6964 223a 2073 656c 662e 5f63 6c69  t_id": self._cli
-000220a0: 656e 742e 6465 6661 756c 745f 7072 6f6a  ent.default_proj
-000220b0: 6563 745f 6964 7d29 0a20 2020 2020 2020  ect_id}).       
-000220c0: 2020 2020 2065 6c73 653a 0a20 2020 2020       else:.     
-000220d0: 2020 2020 2020 2020 2020 2072 6169 7365             raise
-000220e0: 2057 4d4c 436c 6965 6e74 4572 726f 7228   WMLClientError(
-000220f0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00022100: 2020 2020 204d 6573 7361 6765 732e 6765       Messages.ge
-00022110: 745f 6d65 7373 6167 6528 0a20 2020 2020  t_message(.     
-00022120: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022130: 2020 206d 6573 7361 6765 5f69 643d 2269     message_id="i
-00022140: 745f 6973 5f6d 616e 6461 746f 7279 5f74  t_is_mandatory_t
-00022150: 6f5f 7365 745f 7468 655f 7370 6163 655f  o_set_the_space_
-00022160: 7072 6f6a 6563 745f 6964 220a 2020 2020  project_id".    
-00022170: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022180: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
-00022190: 2020 290a 0a20 2020 2020 2020 2072 6574    )..        ret
-000221a0: 7572 6e20 6d65 7461 6461 7461 0a0a 2020  urn metadata..  
-000221b0: 2020 6465 6620 5f6d 6f64 656c 5f63 6f6e    def _model_con
-000221c0: 7465 6e74 5f63 6f6d 7072 6573 735f 6172  tent_compress_ar
-000221d0: 7469 6661 6374 280a 2020 2020 2020 2020  tifact(.        
-000221e0: 7365 6c66 2c20 7479 7065 5f6e 616d 653a  self, type_name:
-000221f0: 2073 7472 2c20 636f 6d70 7265 7373 5f61   str, compress_a
-00022200: 7274 6966 6163 743a 2073 7472 0a20 2020  rtifact: str.   
-00022210: 2029 202d 3e20 7374 723a 0a20 2020 2020   ) -> str:.     
-00022220: 2020 2074 6172 5f66 696c 656e 616d 6520     tar_filename 
-00022230: 3d20 227b 7d5f 636f 6e74 656e 742e 7461  = "{}_content.ta
-00022240: 7222 2e66 6f72 6d61 7428 7479 7065 5f6e  r".format(type_n
-00022250: 616d 6529 0a20 2020 2020 2020 2067 7a5f  ame).        gz_
-00022260: 6669 6c65 6e61 6d65 203d 2022 7b7d 2e67  filename = "{}.g
-00022270: 7a22 2e66 6f72 6d61 7428 7461 725f 6669  z".format(tar_fi
-00022280: 6c65 6e61 6d65 290a 2020 2020 2020 2020  lename).        
-00022290: 436f 6d70 7265 7373 696f 6e55 7469 6c2e  CompressionUtil.
-000222a0: 6372 6561 7465 5f74 6172 2863 6f6d 7072  create_tar(compr
-000222b0: 6573 735f 6172 7469 6661 6374 2c20 222e  ess_artifact, ".
-000222c0: 222c 2074 6172 5f66 696c 656e 616d 6529  ", tar_filename)
-000222d0: 0a20 2020 2020 2020 2043 6f6d 7072 6573  .        Compres
-000222e0: 7369 6f6e 5574 696c 2e63 6f6d 7072 6573  sionUtil.compres
-000222f0: 735f 6669 6c65 5f67 7a69 7028 7461 725f  s_file_gzip(tar_
-00022300: 6669 6c65 6e61 6d65 2c20 677a 5f66 696c  filename, gz_fil
-00022310: 656e 616d 6529 0a20 2020 2020 2020 206f  ename).        o
-00022320: 732e 7265 6d6f 7665 2874 6172 5f66 696c  s.remove(tar_fil
-00022330: 656e 616d 6529 0a20 2020 2020 2020 2072  ename).        r
-00022340: 6574 7572 6e20 677a 5f66 696c 656e 616d  eturn gz_filenam
-00022350: 650a 0a20 2020 2064 6566 205f 6973 5f68  e..    def _is_h
-00022360: 3528 7365 6c66 2c20 6d6f 6465 6c5f 6669  5(self, model_fi
-00022370: 6c65 7061 7468 3a20 7374 7229 202d 3e20  lepath: str) -> 
-00022380: 626f 6f6c 3a0a 2020 2020 2020 2020 7265  bool:.        re
-00022390: 7475 726e 206f 732e 7061 7468 2e73 706c  turn os.path.spl
-000223a0: 6974 6578 7428 6f73 2e70 6174 682e 6261  itext(os.path.ba
-000223b0: 7365 6e61 6d65 286d 6f64 656c 5f66 696c  sename(model_fil
-000223c0: 6570 6174 6829 295b 2d31 5d20 3d3d 2022  epath))[-1] == "
-000223d0: 2e68 3522 0a0a 2020 2020 6465 6620 5f67  .h5"..    def _g
-000223e0: 6574 5f6c 6173 745f 7275 6e5f 6d65 7472  et_last_run_metr
-000223f0: 6963 735f 6e61 6d65 2873 656c 662c 2074  ics_name(self, t
-00022400: 7261 696e 696e 675f 6964 3a20 7374 7229  raining_id: str)
-00022410: 202d 3e20 7374 723a 0a20 2020 2020 2020   -> str:.       
-00022420: 2072 756e 5f6d 6574 7269 6373 203d 2073   run_metrics = s
-00022430: 656c 662e 5f63 6c69 656e 742e 7472 6169  elf._client.trai
-00022440: 6e69 6e67 2e67 6574 5f6d 6574 7269 6373  ning.get_metrics
-00022450: 2874 7261 696e 696e 675f 6964 3d74 7261  (training_id=tra
-00022460: 696e 696e 675f 6964 290a 2020 2020 2020  ining_id).      
-00022470: 2020 6c61 7374 5f72 756e 5f6d 6574 7269    last_run_metri
-00022480: 6320 3d20 7275 6e5f 6d65 7472 6963 735b  c = run_metrics[
-00022490: 2d31 5d0a 2020 2020 2020 2020 6d65 7472  -1].        metr
-000224a0: 6963 735f 6e61 6d65 203d 2028 0a20 2020  ics_name = (.   
-000224b0: 2020 2020 2020 2020 206c 6173 745f 7275           last_ru
-000224c0: 6e5f 6d65 7472 6963 2e67 6574 2822 636f  n_metric.get("co
-000224d0: 6e74 6578 7422 292e 6765 7428 2269 6e74  ntext").get("int
-000224e0: 6572 6d65 6469 6174 655f 6d6f 6465 6c22  ermediate_model"
-000224f0: 292e 6765 7428 226e 616d 6522 290a 2020  ).get("name").  
-00022500: 2020 2020 2020 290a 2020 2020 2020 2020        ).        
-00022510: 7265 7475 726e 206d 6574 7269 6373 5f6e  return metrics_n
-00022520: 616d 650a                                ame.
+00020a00: 6469 725f 6e61 6d65 203d 2022 7b7d 222e  dir_name = "{}".
+00020a10: 666f 726d 6174 2822 6864 6673 2220 2b20  format("hdfs" + 
+00020a20: 6765 6e5f 6964 290a 2020 2020 2020 2020  gen_id).        
+00020a30: 2020 2020 7465 6d70 5f64 6972 203d 2074      temp_dir = t
+00020a40: 656d 705f 6469 725f 6e61 6d65 0a20 2020  emp_dir_name.   
+00020a50: 2020 2020 2020 2020 2069 6620 6e6f 7420           if not 
+00020a60: 6f73 2e70 6174 682e 6578 6973 7473 2874  os.path.exists(t
+00020a70: 656d 705f 6469 7229 3a0a 2020 2020 2020  emp_dir):.      
+00020a80: 2020 2020 2020 2020 2020 6f73 2e6d 616b            os.mak
+00020a90: 6564 6972 7328 7465 6d70 5f64 6972 290a  edirs(temp_dir).
+00020aa0: 2020 2020 2020 2020 2020 2020 6d6f 6465              mode
+00020ab0: 6c5f 6669 6c65 203d 2074 656d 705f 6469  l_file = temp_di
+00020ac0: 7220 2b20 222f 7365 7175 656e 7469 616c  r + "/sequential
+00020ad0: 5f6d 6f64 656c 2e68 3522 0a20 2020 2020  _model.h5".     
+00020ae0: 2020 2020 2020 2074 662e 6b65 7261 732e         tf.keras.
+00020af0: 6d6f 6465 6c73 2e73 6176 655f 6d6f 6465  models.save_mode
+00020b00: 6c28 0a20 2020 2020 2020 2020 2020 2020  l(.             
+00020b10: 2020 206d 6f64 656c 2c0a 2020 2020 2020     model,.      
+00020b20: 2020 2020 2020 2020 2020 6d6f 6465 6c5f            model_
+00020b30: 6669 6c65 2c0a 2020 2020 2020 2020 2020  file,.          
+00020b40: 2020 2020 2020 696e 636c 7564 655f 6f70        include_op
+00020b50: 7469 6d69 7a65 723d 696e 636c 7564 655f  timizer=include_
+00020b60: 6f70 7469 6d69 7a65 722c 0a20 2020 2020  optimizer,.     
+00020b70: 2020 2020 2020 2020 2020 2073 6176 655f             save_
+00020b80: 666f 726d 6174 3d22 6835 222c 0a20 2020  format="h5",.   
+00020b90: 2020 2020 2020 2020 2020 2020 2073 6967               sig
+00020ba0: 6e61 7475 7265 733d 4e6f 6e65 2c0a 2020  natures=None,.  
+00020bb0: 2020 2020 2020 2020 2020 2020 2020 6f70                op
+00020bc0: 7469 6f6e 733d 6f70 7469 6f6e 732c 0a20  tions=options,. 
+00020bd0: 2020 2020 2020 2020 2020 2029 0a20 2020             ).   
+00020be0: 2020 2020 2065 6c69 6620 280a 2020 2020       elif (.    
+00020bf0: 2020 2020 2020 2020 6973 696e 7374 616e          isinstan
+00020c00: 6365 286d 6f64 656c 2c20 7374 7229 0a20  ce(model, str). 
+00020c10: 2020 2020 2020 2020 2020 2061 6e64 206f             and o
+00020c20: 732e 7061 7468 2e73 706c 6974 6578 7428  s.path.splitext(
+00020c30: 6f73 2e70 6174 682e 6261 7365 6e61 6d65  os.path.basename
+00020c40: 286d 6f64 656c 2929 5b2d 315d 203d 3d20  (model))[-1] == 
+00020c50: 222e 6835 220a 2020 2020 2020 2020 293a  ".h5".        ):
+00020c60: 0a20 2020 2020 2020 2020 2020 2074 656d  .            tem
+00020c70: 705f 6469 725f 6e61 6d65 203d 2022 7b7d  p_dir_name = "{}
+00020c80: 222e 666f 726d 6174 2822 6864 6673 2220  ".format("hdfs" 
+00020c90: 2b20 6765 6e5f 6964 290a 2020 2020 2020  + gen_id).      
+00020ca0: 2020 2020 2020 7465 6d70 5f64 6972 203d        temp_dir =
+00020cb0: 2074 656d 705f 6469 725f 6e61 6d65 0a20   temp_dir_name. 
+00020cc0: 2020 2020 2020 2020 2020 2069 6620 6e6f             if no
+00020cd0: 7420 6f73 2e70 6174 682e 6578 6973 7473  t os.path.exists
+00020ce0: 2874 656d 705f 6469 7229 3a0a 2020 2020  (temp_dir):.    
+00020cf0: 2020 2020 2020 2020 2020 2020 696d 706f              impo
+00020d00: 7274 2073 6875 7469 6c0a 0a20 2020 2020  rt shutil..     
+00020d10: 2020 2020 2020 2020 2020 206f 732e 6d61             os.ma
+00020d20: 6b65 6469 7273 2874 656d 705f 6469 7229  kedirs(temp_dir)
+00020d30: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00020d40: 2073 6875 7469 6c2e 636f 7079 3228 6d6f   shutil.copy2(mo
+00020d50: 6465 6c2c 2074 656d 705f 6469 7229 0a20  del, temp_dir). 
+00020d60: 2020 2020 2020 2065 6c73 653a 0a0a 2020         else:..  
+00020d70: 2020 2020 2020 2020 2020 7261 6973 6520            raise 
+00020d80: 574d 4c43 6c69 656e 7445 7272 6f72 280a  WMLClientError(.
+00020d90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020da0: 2253 6176 696e 6720 7468 6520 7465 6e73  "Saving the tens
+00020db0: 6f72 666c 6f77 206d 6f64 656c 2072 6571  orflow model req
+00020dc0: 7569 7265 7320 7468 6520 6d6f 6465 6c20  uires the model 
+00020dd0: 6f66 2065 6974 6865 7220 7466 2066 6f72  of either tf for
+00020de0: 6d61 7420 6f72 2068 3520 666f 726d 6174  mat or h5 format
+00020df0: 2066 6f72 2053 6571 7565 6e74 6961 6c20   for Sequential 
+00020e00: 6d6f 6465 6c2e 220a 2020 2020 2020 2020  model.".        
+00020e10: 2020 2020 290a 0a20 2020 2020 2020 2070      )..        p
+00020e20: 6174 685f 746f 5f61 7263 6869 7665 203d  ath_to_archive =
+00020e30: 2073 656c 662e 5f6d 6f64 656c 5f63 6f6e   self._model_con
+00020e40: 7465 6e74 5f63 6f6d 7072 6573 735f 6172  tent_compress_ar
+00020e50: 7469 6661 6374 2874 656d 705f 6469 725f  tifact(temp_dir_
+00020e60: 6e61 6d65 2c20 7465 6d70 5f64 6972 290a  name, temp_dir).
+00020e70: 2020 2020 2020 2020 7061 796c 6f61 6420          payload 
+00020e80: 3d20 636f 7079 2e64 6565 7063 6f70 7928  = copy.deepcopy(
+00020e90: 6d65 7461 5f70 726f 7073 290a 2020 2020  meta_props).    
+00020ea0: 2020 2020 6966 206c 6162 656c 5f63 6f6c      if label_col
+00020eb0: 756d 6e5f 6e61 6d65 733a 0a20 2020 2020  umn_names:.     
+00020ec0: 2020 2020 2020 2070 6179 6c6f 6164 5b22         payload["
+00020ed0: 6c61 6265 6c5f 636f 6c75 6d6e 225d 203d  label_column"] =
+00020ee0: 206c 6162 656c 5f63 6f6c 756d 6e5f 6e61   label_column_na
+00020ef0: 6d65 735b 305d 0a0a 2020 2020 2020 2020  mes[0]..        
+00020f00: 7265 7370 6f6e 7365 203d 2072 6571 7565  response = reque
+00020f10: 7374 732e 706f 7374 280a 2020 2020 2020  sts.post(.      
+00020f20: 2020 2020 2020 7572 6c2c 0a20 2020 2020        url,.     
+00020f30: 2020 2020 2020 206a 736f 6e3d 7061 796c         json=payl
+00020f40: 6f61 642c 0a20 2020 2020 2020 2020 2020  oad,.           
+00020f50: 2070 6172 616d 733d 7365 6c66 2e5f 636c   params=self._cl
+00020f60: 6965 6e74 2e5f 7061 7261 6d73 2829 2c0a  ient._params(),.
+00020f70: 2020 2020 2020 2020 2020 2020 6865 6164              head
+00020f80: 6572 733d 7365 6c66 2e5f 636c 6965 6e74  ers=self._client
+00020f90: 2e5f 6765 745f 6865 6164 6572 7328 292c  ._get_headers(),
+00020fa0: 0a20 2020 2020 2020 2029 0a20 2020 2020  .        ).     
+00020fb0: 2020 2072 6573 756c 7420 3d20 7365 6c66     result = self
+00020fc0: 2e5f 6861 6e64 6c65 5f72 6573 706f 6e73  ._handle_respons
+00020fd0: 6528 3230 312c 2022 6372 6561 7469 6e67  e(201, "creating
+00020fe0: 206d 6f64 656c 222c 2072 6573 706f 6e73   model", respons
+00020ff0: 6529 0a20 2020 2020 2020 206d 6f64 656c  e).        model
+00021000: 5f69 6420 3d20 7365 6c66 2e5f 6765 745f  _id = self._get_
+00021010: 7265 7175 6972 6564 5f65 6c65 6d65 6e74  required_element
+00021020: 5f66 726f 6d5f 6469 6374 280a 2020 2020  _from_dict(.    
+00021030: 2020 2020 2020 2020 7265 7375 6c74 2c20          result, 
+00021040: 226d 6f64 656c 5f64 6574 6169 6c73 222c  "model_details",
+00021050: 205b 226d 6574 6164 6174 6122 2c20 2269   ["metadata", "i
+00021060: 6422 5d0a 2020 2020 2020 2020 290a 0a20  d"].        ).. 
+00021070: 2020 2020 2020 2075 726c 203d 2028 0a20         url = (. 
+00021080: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+00021090: 5f63 6c69 656e 742e 7365 7276 6963 655f  _client.service_
+000210a0: 696e 7374 616e 6365 2e5f 6872 6566 5f64  instance._href_d
+000210b0: 6566 696e 6974 696f 6e73 2e67 6574 5f70  efinitions.get_p
+000210c0: 7562 6c69 7368 6564 5f6d 6f64 656c 5f68  ublished_model_h
+000210d0: 7265 6628 0a20 2020 2020 2020 2020 2020  ref(.           
+000210e0: 2020 2020 206d 6f64 656c 5f69 640a 2020       model_id.  
+000210f0: 2020 2020 2020 2020 2020 290a 2020 2020            ).    
+00021100: 2020 2020 2020 2020 2b20 222f 636f 6e74          + "/cont
+00021110: 656e 7422 0a20 2020 2020 2020 2029 0a20  ent".        ). 
+00021120: 2020 2020 2020 2077 6974 6820 6f70 656e         with open
+00021130: 2870 6174 685f 746f 5f61 7263 6869 7665  (path_to_archive
+00021140: 2c20 2272 6222 2920 6173 2066 3a0a 2020  , "rb") as f:.  
+00021150: 2020 2020 2020 2020 2020 7170 6172 616d            qparam
+00021160: 7320 3d20 7365 6c66 2e5f 636c 6965 6e74  s = self._client
+00021170: 2e5f 7061 7261 6d73 2829 0a0a 2020 2020  ._params()..    
+00021180: 2020 2020 2020 2020 7170 6172 616d 732e          qparams.
+00021190: 7570 6461 7465 287b 2263 6f6e 7465 6e74  update({"content
+000211a0: 5f66 6f72 6d61 7422 3a20 226e 6174 6976  _format": "nativ
+000211b0: 6522 7d29 0a20 2020 2020 2020 2020 2020  e"}).           
+000211c0: 2071 7061 7261 6d73 2e75 7064 6174 6528   qparams.update(
+000211d0: 7b22 7665 7273 696f 6e22 3a20 7365 6c66  {"version": self
+000211e0: 2e5f 636c 6965 6e74 2e76 6572 7369 6f6e  ._client.version
+000211f0: 5f70 6172 616d 7d29 0a20 2020 2020 2020  _param}).       
+00021200: 2020 2020 2023 2075 7064 6174 6520 7468       # update th
+00021210: 6520 636f 6e74 656e 7420 7061 7468 2066  e content path f
+00021220: 6f72 2074 6865 2041 7574 6f2d 6169 206d  or the Auto-ai m
+00021230: 6f64 656c 2e0a 0a20 2020 2020 2020 2020  odel...         
+00021240: 2020 2072 6573 706f 6e73 6520 3d20 7265     response = re
+00021250: 7175 6573 7473 2e70 7574 280a 2020 2020  quests.put(.    
+00021260: 2020 2020 2020 2020 2020 2020 7572 6c2c              url,
+00021270: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00021280: 2064 6174 613d 662c 0a20 2020 2020 2020   data=f,.       
+00021290: 2020 2020 2020 2020 2070 6172 616d 733d           params=
+000212a0: 7170 6172 616d 732c 0a20 2020 2020 2020  qparams,.       
+000212b0: 2020 2020 2020 2020 2068 6561 6465 7273           headers
+000212c0: 3d73 656c 662e 5f63 6c69 656e 742e 5f67  =self._client._g
+000212d0: 6574 5f68 6561 6465 7273 280a 2020 2020  et_headers(.    
+000212e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000212f0: 636f 6e74 656e 745f 7479 7065 3d22 6170  content_type="ap
+00021300: 706c 6963 6174 696f 6e2f 6f63 7465 742d  plication/octet-
+00021310: 7374 7265 616d 220a 2020 2020 2020 2020  stream".        
+00021320: 2020 2020 2020 2020 292c 0a20 2020 2020          ),.     
+00021330: 2020 2020 2020 2029 0a20 2020 2020 2020         ).       
+00021340: 2020 2020 2069 6620 7265 7370 6f6e 7365       if response
+00021350: 2e73 7461 7475 735f 636f 6465 2021 3d20  .status_code != 
+00021360: 3230 3020 616e 6420 7265 7370 6f6e 7365  200 and response
+00021370: 2e73 7461 7475 735f 636f 6465 2021 3d20  .status_code != 
+00021380: 3230 313a 0a20 2020 2020 2020 2020 2020  201:.           
+00021390: 2020 2020 2073 656c 662e 6465 6c65 7465       self.delete
+000213a0: 286d 6f64 656c 5f69 6429 0a20 2020 2020  (model_id).     
+000213b0: 2020 2020 2020 2073 656c 662e 5f68 616e         self._han
+000213c0: 646c 655f 7265 7370 6f6e 7365 2832 3031  dle_response(201
+000213d0: 2c20 2275 706c 6f61 6469 6e67 206d 6f64  , "uploading mod
+000213e0: 656c 2063 6f6e 7465 6e74 222c 2072 6573  el content", res
+000213f0: 706f 6e73 652c 2046 616c 7365 290a 0a20  ponse, False).. 
+00021400: 2020 2020 2020 2020 2020 2069 6620 6f73             if os
+00021410: 2e70 6174 682e 6578 6973 7473 2874 656d  .path.exists(tem
+00021420: 705f 6469 7229 3a0a 2020 2020 2020 2020  p_dir):.        
+00021430: 2020 2020 2020 2020 696d 706f 7274 2073          import s
+00021440: 6875 7469 6c0a 0a20 2020 2020 2020 2020  hutil..         
+00021450: 2020 2020 2020 2073 6875 7469 6c2e 726d         shutil.rm
+00021460: 7472 6565 2874 656d 705f 6469 722c 2069  tree(temp_dir, i
+00021470: 676e 6f72 655f 6572 726f 7273 3d54 7275  gnore_errors=Tru
+00021480: 6529 0a20 2020 2020 2020 2020 2020 2020  e).             
+00021490: 2020 206f 732e 7265 6d6f 7665 2870 6174     os.remove(pat
+000214a0: 685f 746f 5f61 7263 6869 7665 290a 2020  h_to_archive).  
+000214b0: 2020 2020 2020 2020 2020 7265 7475 726e            return
+000214c0: 2073 656c 662e 6765 745f 6465 7461 696c   self.get_detail
+000214d0: 7328 6d6f 6465 6c5f 6964 290a 0a20 2020  s(model_id)..   
+000214e0: 2064 6566 205f 7374 6f72 655f 6375 7374   def _store_cust
+000214f0: 6f6d 5f66 6f75 6e64 6174 696f 6e5f 6d6f  om_foundation_mo
+00021500: 6465 6c28 0a20 2020 2020 2020 2073 656c  del(.        sel
+00021510: 662c 206d 6f64 656c 3a20 7374 722c 206d  f, model: str, m
+00021520: 6574 615f 7072 6f70 733a 2064 6963 745b  eta_props: dict[
+00021530: 7374 722c 2041 6e79 5d0a 2020 2020 2920  str, Any].    ) 
+00021540: 2d3e 2064 6963 745b 7374 722c 2041 6e79  -> dict[str, Any
+00021550: 5d3a 0a20 2020 2020 2020 2023 2053 746f  ]:.        # Sto
+00021560: 7265 2063 7573 746f 6d20 666f 756e 6461  re custom founda
+00021570: 7469 6f6e 206d 6f64 656c 0a0a 2020 2020  tion model..    
+00021580: 2020 2020 7061 796c 6f61 6420 3d20 7365      payload = se
+00021590: 6c66 2e5f 6372 6561 7465 5f63 7573 746f  lf._create_custo
+000215a0: 6d5f 6d6f 6465 6c5f 7061 796c 6f61 6428  m_model_payload(
+000215b0: 6d6f 6465 6c3d 6d6f 6465 6c2c 206d 6574  model=model, met
+000215c0: 615f 7072 6f70 733d 6d65 7461 5f70 726f  a_props=meta_pro
+000215d0: 7073 290a 0a20 2020 2020 2020 2063 7265  ps)..        cre
+000215e0: 6174 696f 6e5f 7265 7370 6f6e 7365 203d  ation_response =
+000215f0: 2072 6571 7565 7374 732e 706f 7374 280a   requests.post(.
+00021600: 2020 2020 2020 2020 2020 2020 7572 6c3d              url=
+00021610: 7365 6c66 2e5f 636c 6965 6e74 2e73 6572  self._client.ser
+00021620: 7669 6365 5f69 6e73 7461 6e63 652e 5f68  vice_instance._h
+00021630: 7265 665f 6465 6669 6e69 7469 6f6e 732e  ref_definitions.
+00021640: 6765 745f 7075 626c 6973 6865 645f 6d6f  get_published_mo
+00021650: 6465 6c73 5f68 7265 6628 292c 0a20 2020  dels_href(),.   
+00021660: 2020 2020 2020 2020 2070 6172 616d 733d           params=
+00021670: 7365 6c66 2e5f 636c 6965 6e74 2e5f 7061  self._client._pa
+00021680: 7261 6d73 2873 6b69 705f 666f 725f 6372  rams(skip_for_cr
+00021690: 6561 7465 3d54 7275 6529 2c0a 2020 2020  eate=True),.    
+000216a0: 2020 2020 2020 2020 6865 6164 6572 733d          headers=
+000216b0: 7365 6c66 2e5f 636c 6965 6e74 2e5f 6765  self._client._ge
+000216c0: 745f 6865 6164 6572 7328 292c 0a20 2020  t_headers(),.   
+000216d0: 2020 2020 2020 2020 206a 736f 6e3d 7061           json=pa
+000216e0: 796c 6f61 642c 0a20 2020 2020 2020 2029  yload,.        )
+000216f0: 0a0a 2020 2020 2020 2020 6966 2063 7265  ..        if cre
+00021700: 6174 696f 6e5f 7265 7370 6f6e 7365 2e73  ation_response.s
+00021710: 7461 7475 735f 636f 6465 203d 3d20 3230  tatus_code == 20
+00021720: 313a 0a20 2020 2020 2020 2020 2020 206d  1:.            m
+00021730: 6f64 656c 5f64 6574 6169 6c73 203d 2073  odel_details = s
+00021740: 656c 662e 5f68 616e 646c 655f 7265 7370  elf._handle_resp
+00021750: 6f6e 7365 280a 2020 2020 2020 2020 2020  onse(.          
+00021760: 2020 2020 2020 3230 312c 2022 6372 6561        201, "crea
+00021770: 7469 6e67 206e 6577 206d 6f64 656c 222c  ting new model",
+00021780: 2063 7265 6174 696f 6e5f 7265 7370 6f6e   creation_respon
+00021790: 7365 0a20 2020 2020 2020 2020 2020 2029  se.            )
+000217a0: 0a20 2020 2020 2020 2065 6c73 653a 0a20  .        else:. 
+000217b0: 2020 2020 2020 2020 2020 206d 6f64 656c             model
+000217c0: 5f64 6574 6169 6c73 203d 2073 656c 662e  _details = self.
+000217d0: 5f68 616e 646c 655f 7265 7370 6f6e 7365  _handle_response
+000217e0: 280a 2020 2020 2020 2020 2020 2020 2020  (.              
+000217f0: 2020 3230 322c 2022 6372 6561 7469 6e67    202, "creating
+00021800: 206e 6577 206d 6f64 656c 222c 2063 7265   new model", cre
+00021810: 6174 696f 6e5f 7265 7370 6f6e 7365 0a20  ation_response. 
+00021820: 2020 2020 2020 2020 2020 2029 0a20 2020             ).   
+00021830: 2020 2020 206d 6f64 656c 5f69 6420 3d20       model_id = 
+00021840: 6d6f 6465 6c5f 6465 7461 696c 735b 226d  model_details["m
+00021850: 6574 6164 6174 6122 5d5b 2269 6422 5d0a  etadata"]["id"].
+00021860: 0a20 2020 2020 2020 2069 6620 2265 6e74  .        if "ent
+00021870: 6974 7922 2069 6e20 6d6f 6465 6c5f 6465  ity" in model_de
+00021880: 7461 696c 733a 0a20 2020 2020 2020 2020  tails:.         
+00021890: 2020 2073 7461 7274 5f74 696d 6520 3d20     start_time = 
+000218a0: 7469 6d65 2e74 696d 6528 290a 2020 2020  time.time().    
+000218b0: 2020 2020 2020 2020 656c 6170 7365 645f          elapsed_
+000218c0: 7469 6d65 203d 2030 2e30 0a20 2020 2020  time = 0.0.     
+000218d0: 2020 2020 2020 2077 6869 6c65 2028 0a20         while (. 
+000218e0: 2020 2020 2020 2020 2020 2020 2020 206d                 m
+000218f0: 6f64 656c 5f64 6574 6169 6c73 5b22 656e  odel_details["en
+00021900: 7469 7479 225d 2e67 6574 2822 636f 6e74  tity"].get("cont
+00021910: 656e 745f 696d 706f 7274 5f73 7461 7465  ent_import_state
+00021920: 2229 203d 3d20 2272 756e 6e69 6e67 220a  ") == "running".
+00021930: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021940: 616e 6420 656c 6170 7365 645f 7469 6d65  and elapsed_time
+00021950: 203c 2036 300a 2020 2020 2020 2020 2020   < 60.          
+00021960: 2020 293a 0a20 2020 2020 2020 2020 2020    ):.           
+00021970: 2020 2020 2074 696d 652e 736c 6565 7028       time.sleep(
+00021980: 3229 0a20 2020 2020 2020 2020 2020 2020  2).             
+00021990: 2020 2065 6c61 7073 6564 5f74 696d 6520     elapsed_time 
+000219a0: 3d20 7469 6d65 2e74 696d 6528 2920 2d20  = time.time() - 
+000219b0: 7374 6172 745f 7469 6d65 0a20 2020 2020  start_time.     
+000219c0: 2020 2020 2020 2020 2020 206d 6f64 656c             model
+000219d0: 5f64 6574 6169 6c73 203d 2073 656c 662e  _details = self.
+000219e0: 6765 745f 6465 7461 696c 7328 6d6f 6465  get_details(mode
+000219f0: 6c5f 6964 290a 2020 2020 2020 2020 7265  l_id).        re
+00021a00: 7475 726e 2073 656c 662e 6765 745f 6465  turn self.get_de
+00021a10: 7461 696c 7328 6d6f 6465 6c5f 6964 290a  tails(model_id).
+00021a20: 0a20 2020 2064 6566 205f 6372 6561 7465  .    def _create
+00021a30: 5f63 7573 746f 6d5f 6d6f 6465 6c5f 7061  _custom_model_pa
+00021a40: 796c 6f61 6428 0a20 2020 2020 2020 2073  yload(.        s
+00021a50: 656c 662c 206d 6f64 656c 3a20 7374 722c  elf, model: str,
+00021a60: 206d 6574 615f 7072 6f70 733a 2064 6963   meta_props: dic
+00021a70: 745b 7374 722c 2041 6e79 5d0a 2020 2020  t[str, Any].    
+00021a80: 2920 2d3e 2064 6963 745b 7374 722c 2041  ) -> dict[str, A
+00021a90: 6e79 5d3a 0a20 2020 2020 2020 206d 6574  ny]:.        met
+00021aa0: 6164 6174 6120 3d20 636f 7079 2e64 6565  adata = copy.dee
+00021ab0: 7063 6f70 7928 6d65 7461 5f70 726f 7073  pcopy(meta_props
+00021ac0: 290a 0a20 2020 2020 2020 204d 6f64 656c  )..        Model
+00021ad0: 732e 5f76 616c 6964 6174 655f 7479 7065  s._validate_type
+00021ae0: 286d 6f64 656c 2c20 226d 6f64 656c 222c  (model, "model",
+00021af0: 2073 7472 2c20 5472 7565 290a 0a20 2020   str, True)..   
+00021b00: 2020 2020 2069 6620 7365 6c66 2e43 6f6e       if self.Con
+00021b10: 6669 6775 7261 7469 6f6e 4d65 7461 4e61  figurationMetaNa
+00021b20: 6d65 732e 5459 5045 2069 6e20 6d65 7461  mes.TYPE in meta
+00021b30: 5f70 726f 7073 3a0a 2020 2020 2020 2020  _props:.        
+00021b40: 2020 2020 7365 6c66 2e5f 7661 6c69 6461      self._valida
+00021b50: 7465 5f6d 6574 615f 7072 6f70 280a 2020  te_meta_prop(.  
+00021b60: 2020 2020 2020 2020 2020 2020 2020 6d65                me
+00021b70: 7461 5f70 726f 7073 2c20 7365 6c66 2e43  ta_props, self.C
+00021b80: 6f6e 6669 6775 7261 7469 6f6e 4d65 7461  onfigurationMeta
+00021b90: 4e61 6d65 732e 5459 5045 2c20 7374 722c  Names.TYPE, str,
+00021ba0: 2054 7275 650a 2020 2020 2020 2020 2020   True.          
+00021bb0: 2020 290a 0a20 2020 2020 2020 2069 6620    )..        if 
+00021bc0: 7365 6c66 2e43 6f6e 6669 6775 7261 7469  self.Configurati
+00021bd0: 6f6e 4d65 7461 4e61 6d65 732e 534f 4654  onMetaNames.SOFT
+00021be0: 5741 5245 5f53 5045 435f 4944 2069 6e20  WARE_SPEC_ID in 
+00021bf0: 6d65 7461 5f70 726f 7073 3a0a 2020 2020  meta_props:.    
+00021c00: 2020 2020 2020 2020 7365 6c66 2e5f 7661          self._va
+00021c10: 6c69 6461 7465 5f6d 6574 615f 7072 6f70  lidate_meta_prop
+00021c20: 280a 2020 2020 2020 2020 2020 2020 2020  (.              
+00021c30: 2020 6d65 7461 5f70 726f 7073 2c20 7365    meta_props, se
+00021c40: 6c66 2e43 6f6e 6669 6775 7261 7469 6f6e  lf.Configuration
+00021c50: 4d65 7461 4e61 6d65 732e 534f 4654 5741  MetaNames.SOFTWA
+00021c60: 5245 5f53 5045 435f 4944 2c20 7374 722c  RE_SPEC_ID, str,
+00021c70: 2054 7275 650a 2020 2020 2020 2020 2020   True.          
+00021c80: 2020 290a 2020 2020 2020 2020 2020 2020    ).            
+00021c90: 6d65 7461 6461 7461 2e75 7064 6174 6528  metadata.update(
+00021ca0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00021cb0: 207b 0a20 2020 2020 2020 2020 2020 2020   {.             
+00021cc0: 2020 2020 2020 2073 656c 662e 436f 6e66         self.Conf
+00021cd0: 6967 7572 6174 696f 6e4d 6574 614e 616d  igurationMetaNam
+00021ce0: 6573 2e53 4f46 5457 4152 455f 5350 4543  es.SOFTWARE_SPEC
+00021cf0: 5f49 443a 207b 0a20 2020 2020 2020 2020  _ID: {.         
+00021d00: 2020 2020 2020 2020 2020 2020 2020 2022                 "
+00021d10: 6964 223a 206d 6574 615f 7072 6f70 735b  id": meta_props[
+00021d20: 7365 6c66 2e43 6f6e 6669 6775 7261 7469  self.Configurati
+00021d30: 6f6e 4d65 7461 4e61 6d65 732e 534f 4654  onMetaNames.SOFT
+00021d40: 5741 5245 5f53 5045 435f 4944 5d0a 2020  WARE_SPEC_ID].  
+00021d50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021d60: 2020 7d0a 2020 2020 2020 2020 2020 2020    }.            
+00021d70: 2020 2020 7d0a 2020 2020 2020 2020 2020      }.          
+00021d80: 2020 290a 0a20 2020 2020 2020 206d 6574    )..        met
+00021d90: 6164 6174 612e 7570 6461 7465 280a 2020  adata.update(.  
+00021da0: 2020 2020 2020 2020 2020 7b73 656c 662e            {self.
+00021db0: 436f 6e66 6967 7572 6174 696f 6e4d 6574  ConfigurationMet
+00021dc0: 614e 616d 6573 2e46 4f55 4e44 4154 494f  aNames.FOUNDATIO
+00021dd0: 4e5f 4d4f 4445 4c3a 207b 226d 6f64 656c  N_MODEL: {"model
+00021de0: 5f69 6422 3a20 6d6f 6465 6c7d 7d0a 2020  _id": model}}.  
+00021df0: 2020 2020 2020 290a 0a20 2020 2020 2020        )..       
+00021e00: 2069 6620 7365 6c66 2e5f 636c 6965 6e74   if self._client
+00021e10: 2e64 6566 6175 6c74 5f73 7061 6365 5f69  .default_space_i
+00021e20: 6420 6973 206e 6f74 204e 6f6e 653a 0a20  d is not None:. 
+00021e30: 2020 2020 2020 2020 2020 206d 6574 6164             metad
+00021e40: 6174 615b 2273 7061 6365 5f69 6422 5d20  ata["space_id"] 
+00021e50: 3d20 7365 6c66 2e5f 636c 6965 6e74 2e64  = self._client.d
+00021e60: 6566 6175 6c74 5f73 7061 6365 5f69 640a  efault_space_id.
+00021e70: 2020 2020 2020 2020 656c 7365 3a0a 2020          else:.  
+00021e80: 2020 2020 2020 2020 2020 6966 2073 656c            if sel
+00021e90: 662e 5f63 6c69 656e 742e 6465 6661 756c  f._client.defaul
+00021ea0: 745f 7072 6f6a 6563 745f 6964 2069 7320  t_project_id is 
+00021eb0: 6e6f 7420 4e6f 6e65 3a0a 2020 2020 2020  not None:.      
+00021ec0: 2020 2020 2020 2020 2020 6d65 7461 6461            metada
+00021ed0: 7461 2e75 7064 6174 6528 7b22 7072 6f6a  ta.update({"proj
+00021ee0: 6563 745f 6964 223a 2073 656c 662e 5f63  ect_id": self._c
+00021ef0: 6c69 656e 742e 6465 6661 756c 745f 7072  lient.default_pr
+00021f00: 6f6a 6563 745f 6964 7d29 0a20 2020 2020  oject_id}).     
+00021f10: 2020 2020 2020 2065 6c73 653a 0a20 2020         else:.   
+00021f20: 2020 2020 2020 2020 2020 2020 2072 6169               rai
+00021f30: 7365 2057 4d4c 436c 6965 6e74 4572 726f  se WMLClientErro
+00021f40: 7228 0a20 2020 2020 2020 2020 2020 2020  r(.             
+00021f50: 2020 2020 2020 204d 6573 7361 6765 732e         Messages.
+00021f60: 6765 745f 6d65 7373 6167 6528 0a20 2020  get_message(.   
+00021f70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021f80: 2020 2020 206d 6573 7361 6765 5f69 643d       message_id=
+00021f90: 2269 745f 6973 5f6d 616e 6461 746f 7279  "it_is_mandatory
+00021fa0: 5f74 6f5f 7365 745f 7468 655f 7370 6163  _to_set_the_spac
+00021fb0: 655f 7072 6f6a 6563 745f 6964 220a 2020  e_project_id".  
+00021fc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021fd0: 2020 290a 2020 2020 2020 2020 2020 2020    ).            
+00021fe0: 2020 2020 290a 0a20 2020 2020 2020 2072      )..        r
+00021ff0: 6574 7572 6e20 6d65 7461 6461 7461 0a0a  eturn metadata..
+00022000: 2020 2020 6465 6620 5f6d 6f64 656c 5f63      def _model_c
+00022010: 6f6e 7465 6e74 5f63 6f6d 7072 6573 735f  ontent_compress_
+00022020: 6172 7469 6661 6374 280a 2020 2020 2020  artifact(.      
+00022030: 2020 7365 6c66 2c20 7479 7065 5f6e 616d    self, type_nam
+00022040: 653a 2073 7472 2c20 636f 6d70 7265 7373  e: str, compress
+00022050: 5f61 7274 6966 6163 743a 2073 7472 0a20  _artifact: str. 
+00022060: 2020 2029 202d 3e20 7374 723a 0a20 2020     ) -> str:.   
+00022070: 2020 2020 2074 6172 5f66 696c 656e 616d       tar_filenam
+00022080: 6520 3d20 227b 7d5f 636f 6e74 656e 742e  e = "{}_content.
+00022090: 7461 7222 2e66 6f72 6d61 7428 7479 7065  tar".format(type
+000220a0: 5f6e 616d 6529 0a20 2020 2020 2020 2067  _name).        g
+000220b0: 7a5f 6669 6c65 6e61 6d65 203d 2022 7b7d  z_filename = "{}
+000220c0: 2e67 7a22 2e66 6f72 6d61 7428 7461 725f  .gz".format(tar_
+000220d0: 6669 6c65 6e61 6d65 290a 2020 2020 2020  filename).      
+000220e0: 2020 436f 6d70 7265 7373 696f 6e55 7469    CompressionUti
+000220f0: 6c2e 6372 6561 7465 5f74 6172 2863 6f6d  l.create_tar(com
+00022100: 7072 6573 735f 6172 7469 6661 6374 2c20  press_artifact, 
+00022110: 222e 222c 2074 6172 5f66 696c 656e 616d  ".", tar_filenam
+00022120: 6529 0a20 2020 2020 2020 2043 6f6d 7072  e).        Compr
+00022130: 6573 7369 6f6e 5574 696c 2e63 6f6d 7072  essionUtil.compr
+00022140: 6573 735f 6669 6c65 5f67 7a69 7028 7461  ess_file_gzip(ta
+00022150: 725f 6669 6c65 6e61 6d65 2c20 677a 5f66  r_filename, gz_f
+00022160: 696c 656e 616d 6529 0a20 2020 2020 2020  ilename).       
+00022170: 206f 732e 7265 6d6f 7665 2874 6172 5f66   os.remove(tar_f
+00022180: 696c 656e 616d 6529 0a20 2020 2020 2020  ilename).       
+00022190: 2072 6574 7572 6e20 677a 5f66 696c 656e   return gz_filen
+000221a0: 616d 650a 0a20 2020 2064 6566 205f 6973  ame..    def _is
+000221b0: 5f68 3528 7365 6c66 2c20 6d6f 6465 6c5f  _h5(self, model_
+000221c0: 6669 6c65 7061 7468 3a20 7374 7229 202d  filepath: str) -
+000221d0: 3e20 626f 6f6c 3a0a 2020 2020 2020 2020  > bool:.        
+000221e0: 7265 7475 726e 206f 732e 7061 7468 2e73  return os.path.s
+000221f0: 706c 6974 6578 7428 6f73 2e70 6174 682e  plitext(os.path.
+00022200: 6261 7365 6e61 6d65 286d 6f64 656c 5f66  basename(model_f
+00022210: 696c 6570 6174 6829 295b 2d31 5d20 3d3d  ilepath))[-1] ==
+00022220: 2022 2e68 3522 0a0a 2020 2020 6465 6620   ".h5"..    def 
+00022230: 5f67 6574 5f6c 6173 745f 7275 6e5f 6d65  _get_last_run_me
+00022240: 7472 6963 735f 6e61 6d65 2873 656c 662c  trics_name(self,
+00022250: 2074 7261 696e 696e 675f 6964 3a20 7374   training_id: st
+00022260: 7229 202d 3e20 7374 723a 0a20 2020 2020  r) -> str:.     
+00022270: 2020 2072 756e 5f6d 6574 7269 6373 203d     run_metrics =
+00022280: 2073 656c 662e 5f63 6c69 656e 742e 7472   self._client.tr
+00022290: 6169 6e69 6e67 2e67 6574 5f6d 6574 7269  aining.get_metri
+000222a0: 6373 2874 7261 696e 696e 675f 6964 3d74  cs(training_id=t
+000222b0: 7261 696e 696e 675f 6964 290a 2020 2020  raining_id).    
+000222c0: 2020 2020 6c61 7374 5f72 756e 5f6d 6574      last_run_met
+000222d0: 7269 6320 3d20 7275 6e5f 6d65 7472 6963  ric = run_metric
+000222e0: 735b 2d31 5d0a 2020 2020 2020 2020 6d65  s[-1].        me
+000222f0: 7472 6963 735f 6e61 6d65 203d 2028 0a20  trics_name = (. 
+00022300: 2020 2020 2020 2020 2020 206c 6173 745f             last_
+00022310: 7275 6e5f 6d65 7472 6963 2e67 6574 2822  run_metric.get("
+00022320: 636f 6e74 6578 7422 292e 6765 7428 2269  context").get("i
+00022330: 6e74 6572 6d65 6469 6174 655f 6d6f 6465  ntermediate_mode
+00022340: 6c22 292e 6765 7428 226e 616d 6522 290a  l").get("name").
+00022350: 2020 2020 2020 2020 290a 2020 2020 2020          ).      
+00022360: 2020 7265 7475 726e 206d 6574 7269 6373    return metrics
+00022370: 5f6e 616d 650a                           _name.
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/parameter_sets.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/parameter_sets.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/pipelines.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/pipelines.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/pkg_extn.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/pkg_extn.py`

 * *Files 0% similar despite different names*

```diff
@@ -415,15 +415,15 @@
         if self._client.default_space_id is not None:
             new_el = {
                 "metadata": {
                     "space_id": response_data["metadata"]["space_id"],
                     "name": response_data["metadata"]["name"],
                     "asset_id": response_data["metadata"]["asset_id"],
                     "asset_type": response_data["metadata"]["asset_type"],
-                    "created_at": response_data["metadata"]["created_at"]
+                    "created_at": response_data["metadata"]["created_at"],
                     #'updated_at': response_data['metadata']['updated_at']
                 },
                 "entity": response_data["entity"],
             }
         elif self._client.default_project_id is not None:
             new_el = {
                 "metadata": {
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/remote_training_system.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/remote_training_system.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/repository.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/repository.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/script.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/script.py`

 * *Files 1% similar despite different names*

```diff
@@ -214,15 +214,15 @@
                             elif (
                                 SpecStates.DEPRECATED.value
                                 in sw_spec["metadata"]["life_cycle"]
                             ):
                                 deprecated_rscript_sw_specs.append(
                                     sw_spec["metadata"]["name"]
                                 )
-                            else:
+                            elif "rstudio" not in sw_spec["metadata"]["name"].lower():
                                 rscript_sw_specs.append(sw_spec["metadata"]["name"])
 
             elif self._client.CPD_version == 4.6:
                 rscript_sw_specs = ["runtime-22.2-r4.2"]
                 deprecated_rscript_sw_specs = ["default_r3.6", "runtime-22.1-r3.6"]
             else:
                 rscript_sw_specs = ["default_r3.6", "runtime-22.1-r3.6"]
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/service_instance.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/service_instance.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/shiny.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/shiny.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/spaces.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/spaces.py`

 * *Files 0% similar despite different names*

```diff
@@ -803,17 +803,19 @@
         )
 
         member_resources = self._get_resources(href, "space members", params)[
             "resources"
         ]
 
         space_values = [
-            (m["id"], m["type"], m["role"], m["state"])
-            if "state" in m
-            else (m["id"], m["type"], m["role"], None)
+            (
+                (m["id"], m["type"], m["role"], m["state"])
+                if "state" in m
+                else (m["id"], m["type"], m["role"], None)
+            )
             for m in member_resources
         ]
 
         if limit is None:
             print(
                 "Note: 'limit' is not provided. Only first 50 records will be displayed if the number of records "
                 "exceed 50"
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/sw_spec.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/sw_spec.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/task_credentials.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/task_credentials.py`

 * *Files 1% similar despite different names*

```diff
@@ -202,17 +202,15 @@
         .. code-block:: python
 
             task_credentials_id = client.task_credentials.get_id(task_credentials_details)
 
         """
         return task_credentials_details["id"]
 
-    def delete(
-        self, task_credentials_id: str, **kwargs: Any
-    ) -> Literal["SUCCESS"]:
+    def delete(self, task_credentials_id: str, **kwargs: Any) -> Literal["SUCCESS"]:
         """Delete a software specification.
 
         :param task_credentials_id: Unique Id of task credentials
         :type task_credentials_id: str
 
         :return: status "SUCCESS" if deletion is successful
         :rtype: Literal["SUCCESS"]
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/training.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/training.py`

 * *Files 1% similar despite different names*

```diff
@@ -416,21 +416,21 @@
             raise WMLClientError(
                 Messages.get_message(
                     message_id="it_is_mandatory_to_set_the_space_project_id"
                 )
             )
         else:
             if self._client.default_space_id is not None:
-                training_configuration_metadata[
-                    "space_id"
-                ] = self._client.default_space_id
+                training_configuration_metadata["space_id"] = (
+                    self._client.default_space_id
+                )
             elif self._client.default_project_id is not None:
-                training_configuration_metadata[
-                    "project_id"
-                ] = self._client.default_project_id
+                training_configuration_metadata["project_id"] = (
+                    self._client.default_project_id
+                )
 
         if self.ConfigurationMetaNames.FEDERATED_LEARNING in meta_props:
             training_configuration_metadata["federated_learning"] = meta_props[
                 self.ConfigurationMetaNames.FEDERATED_LEARNING
             ]
 
         train_endpoint = (
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/utils/autoai/connection.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/utils/autoai/connection.py`

 * *Files 8% similar despite different names*

```diff
@@ -2,17 +2,24 @@
 #  (C) Copyright IBM Corp. 2023-2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
 from typing import TYPE_CHECKING, List, Union
 
 from ibm_watsonx_ai.helpers.connections import (
-    DataConnection, S3Location, FSLocation, AssetLocation, CloudAssetLocation,
-    DeploymentOutputAssetLocation, ContainerLocation, S3Connection)
-from ibm_watsonx_ai.utils.autoai.enums import (DataConnectionTypes)
+    DataConnection,
+    S3Location,
+    FSLocation,
+    AssetLocation,
+    CloudAssetLocation,
+    DeploymentOutputAssetLocation,
+    ContainerLocation,
+    S3Connection,
+)
+from ibm_watsonx_ai.utils.autoai.enums import DataConnectionTypes
 from ibm_watsonx_ai.utils.autoai.errors import ContainerTypeNotSupported
 
 
 from os import environ
 from re import findall
 
 
@@ -22,150 +29,179 @@
 
 __all__ = [
     "validate_source_data_connections",
     "create_results_data_connection",
     "validate_results_data_connection",
     "create_deployment_output_data_connection",
     "validate_deployment_output_connection",
-    "get_max_sample_size_limit"
+    "get_max_sample_size_limit",
 ]
 
 
-def validate_source_data_connections(source_data_connections: List['DataConnection'],
-                                     workspace: 'WorkSpace',
-                                     deployment=False) -> List['DataConnection']:
+def validate_source_data_connections(
+    source_data_connections: List["DataConnection"],
+    workspace: "WorkSpace",
+    deployment=False,
+) -> List["DataConnection"]:
     for data_connection in source_data_connections:
 
         if isinstance(data_connection.location, FSLocation):
             # note: save data as an data asset
             if workspace.api_client.ICP_PLATFORM_SPACES:
-                asset_id = data_connection.location._save_file_as_data_asset(workspace=workspace)
+                asset_id = data_connection.location._save_file_as_data_asset(
+                    workspace=workspace
+                )
                 data_connection.location = AssetLocation(asset_id)
                 data_connection.type = DataConnectionTypes.DS
             # --- end note
 
         elif isinstance(data_connection.location, ContainerLocation):
             if workspace.api_client.ICP_PLATFORM_SPACES:
-                raise ContainerTypeNotSupported() # block Container type on CPD
+                raise ContainerTypeNotSupported()  # block Container type on CPD
             elif isinstance(data_connection.connection, S3Connection):
                 # note: remove S3 inline credential from data asset before training
                 data_connection.connection = None
-                if hasattr(data_connection.location, 'bucket'):
-                    delattr(data_connection.location, 'bucket')
+                if hasattr(data_connection.location, "bucket"):
+                    delattr(data_connection.location, "bucket")
                 # --- end note
 
-        elif isinstance(data_connection.connection, S3Connection) and isinstance(data_connection.location,
-                                                                                 AssetLocation):
+        elif isinstance(data_connection.connection, S3Connection) and isinstance(
+            data_connection.location, AssetLocation
+        ):
             # note: remove S3 inline credential from data asset before training
             data_connection.connection = None
 
-            for s3_attr in ['bucket', 'path']:
+            for s3_attr in ["bucket", "path"]:
                 if hasattr(data_connection.location, s3_attr):
                     delattr(data_connection.location, s3_attr)
             # --- end note
 
     return source_data_connections
 
 
-def create_results_data_connection(source_data_connections: List['DataConnection'],
-                                   workspace: 'WorkSpace') -> 'DataConnection':
+def create_results_data_connection(
+    source_data_connections: List["DataConnection"], workspace: "WorkSpace"
+) -> "DataConnection":
     if isinstance(source_data_connections[0].location, S3Location):
         results_data_connection = DataConnection(
             connection=source_data_connections[0].connection,
-            location=S3Location(bucket=source_data_connections[0].location.bucket,
-                                path='.')
+            location=S3Location(
+                bucket=source_data_connections[0].location.bucket, path="."
+            ),
         )
     else:
         location = FSLocation()
         if workspace.api_client.default_space_id:
-            location.path = location.path.format(option='spaces',
-                                                 id=workspace.api_client.default_space_id)
+            location.path = location.path.format(
+                option="spaces", id=workspace.api_client.default_space_id
+            )
         else:
-            location.path = location.path.format(option='projects',
-                                                 id=workspace.api_client.default_project_id)
-        results_data_connection = DataConnection(
-            connection=None,
-            location=location
-        )
+            location.path = location.path.format(
+                option="projects", id=workspace.api_client.default_project_id
+            )
+        results_data_connection = DataConnection(connection=None, location=location)
     return results_data_connection
 
 
-def validate_results_data_connection(results_data_connection: Union['DataConnection', None],
-                                     workspace: 'WorkSpace',
-                                     source_data_connections: List['DataConnection'] = None) -> 'DataConnection':
+def validate_results_data_connection(
+    results_data_connection: Union["DataConnection", None],
+    workspace: "WorkSpace",
+    source_data_connections: List["DataConnection"] = None,
+) -> "DataConnection":
     # note: if user did not provide results storage information, use default ones
     if results_data_connection is None and source_data_connections is not None:
-        results_data_connection = create_results_data_connection(source_data_connections=source_data_connections,
-                                                                 workspace=workspace)
+        results_data_connection = create_results_data_connection(
+            source_data_connections=source_data_connections, workspace=workspace
+        )
     # -- end note
     # note: results can be stored only on FS or COS
     if not isinstance(results_data_connection.location, (S3Location, FSLocation)):
-        raise TypeError('Unsupported results location type. Results reference can be stored'
-                        ' only on S3Location or FSLocation.')
+        raise TypeError(
+            "Unsupported results location type. Results reference can be stored"
+            " only on S3Location or FSLocation."
+        )
     # -- end
     return results_data_connection
 
 
-def create_deployment_output_data_connection(source_data_connections: List['DataConnection'],
-                                             output_filename = 'deployment_output.csv') -> 'DataConnection':
+def create_deployment_output_data_connection(
+    source_data_connections: List["DataConnection"],
+    output_filename="deployment_output.csv",
+) -> "DataConnection":
     if isinstance(source_data_connections[0].location, S3Location):
         results_data_connection = DataConnection(
             connection=source_data_connections[0].connection,
-            location=S3Location(bucket=source_data_connections[0].location.bucket,
-                                path=output_filename)
+            location=S3Location(
+                bucket=source_data_connections[0].location.bucket, path=output_filename
+            ),
         )
     else:
         location = DeploymentOutputAssetLocation(name=output_filename)
-        results_data_connection = DataConnection(
-            connection=None,
-            location=location
-        )
+        results_data_connection = DataConnection(connection=None, location=location)
     return results_data_connection
 
 
-def validate_deployment_output_connection(results_data_connection: Union['DataConnection', None],
-                                          workspace: 'WorkSpace',
-                                          source_data_connections: List['DataConnection'] = None) -> 'DataConnection':
+def validate_deployment_output_connection(
+    results_data_connection: Union["DataConnection", None],
+    workspace: "WorkSpace",
+    source_data_connections: List["DataConnection"] = None,
+) -> "DataConnection":
     # note: if user did not provide results storage information, use default ones
     if results_data_connection is None and source_data_connections is not None:
-        results_data_connection = create_results_data_connection(source_data_connections=source_data_connections,
-                                                                 workspace=workspace)
+        results_data_connection = create_results_data_connection(
+            source_data_connections=source_data_connections, workspace=workspace
+        )
     # -- end note
 
-    if isinstance(results_data_connection.location, (AssetLocation, CloudAssetLocation)):
+    if isinstance(
+        results_data_connection.location, (AssetLocation, CloudAssetLocation)
+    ):
         if workspace.api_client.default_space_id:
-            results_data_connection.location.path = results_data_connection.location.path.format(option='spaces',
-                                                                                                 id=workspace.api_client.default_space_id)
+            results_data_connection.location.path = (
+                results_data_connection.location.path.format(
+                    option="spaces", id=workspace.api_client.default_space_id
+                )
+            )
         else:
-            results_data_connection.location.path = results_data_connection.location.path.format(option='projects',
-                                                                                                 id=workspace.api_client.default_project_id)
+            results_data_connection.location.path = (
+                results_data_connection.location.path.format(
+                    option="projects", id=workspace.api_client.default_project_id
+                )
+            )
 
     # note: results can be stored only on COS or CPD DataAsset or DeploymentOutputAssetLocation
-    if not isinstance(results_data_connection.location, (S3Location, AssetLocation, DeploymentOutputAssetLocation)):
-        raise TypeError('Unsupported results location type. Results reference can be stored only in '
-                        'one of [S3Location, AssetLocation, DeploymentOutputAssetLocation].')
+    if not isinstance(
+        results_data_connection.location,
+        (S3Location, AssetLocation, DeploymentOutputAssetLocation),
+    ):
+        raise TypeError(
+            "Unsupported results location type. Results reference can be stored only in "
+            "one of [S3Location, AssetLocation, DeploymentOutputAssetLocation]."
+        )
     # -- end
     return results_data_connection
 
 
 def get_max_sample_size_limit() -> int:
     """Get Sample Size Limit based on T-Shirt size if code is run on training pod:
     For memory < 16 (T-Shirts: XS,S) default is 10MB,
     For memory < 32 & >= 16 (T-Shirts: M) default is 100MB,
     For memory = 32 (T-Shirt L) default is 0.7GB,
     For memory > 32 (T-Shirt XL) or runs outside pod default is 1GB.
     Returns sample size limit in bytes.
     """
-    from ibm_watsonx_ai.data_loaders.datasets.experiment import DEFAULT_SAMPLE_SIZE_LIMIT, \
-        DEFAULT_REDUCED_SAMPLE_SIZE_LIMIT
-    size_mem = int(findall(r'[0-9]+', environ.get('MEM', '32'))[0])
+    from ibm_watsonx_ai.data_loaders.datasets.experiment import (
+        DEFAULT_SAMPLE_SIZE_LIMIT,
+        DEFAULT_REDUCED_SAMPLE_SIZE_LIMIT,
+    )
+
+    size_mem = int(findall(r"[0-9]+", environ.get("MEM", "32"))[0])
     if size_mem > 32:
-        size_limit = DEFAULT_SAMPLE_SIZE_LIMIT             # 1GB in bytes
+        size_limit = DEFAULT_SAMPLE_SIZE_LIMIT  # 1GB in bytes
     elif size_mem == 32:
         size_limit = int(0.7 * DEFAULT_SAMPLE_SIZE_LIMIT)  # 0.7GB in bytes
     elif 32 > size_mem >= 16:
-        size_limit = DEFAULT_REDUCED_SAMPLE_SIZE_LIMIT     # 100MB in bytes
+        size_limit = DEFAULT_REDUCED_SAMPLE_SIZE_LIMIT  # 100MB in bytes
     else:
         size_limit = int(0.1 * DEFAULT_REDUCED_SAMPLE_SIZE_LIMIT)  # 10MB in bytes
 
     return size_limit
-
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/utils/autoai/enums.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/utils/autoai/enums.py`

 * *Files 4% similar despite different names*

```diff
@@ -17,26 +17,27 @@
     "Transformers",
     "DataConnectionTypes",
     "RunStateTypes",
     "PipelineTypes",
     "Directions",
     "TShirtSize",
     "MetricsToDirections",
-    'PositiveLabelClass',
-    'VisualizationTypes',
-    'SamplingTypes',
-    'ImputationStrategy',
-    'ForecastingPipelineTypes',
-    'TimeseriesAnomalyPredictionPipelineTypes',
-    'TimeseriesAnomalyPredictionAlgorithms'
+    "PositiveLabelClass",
+    "VisualizationTypes",
+    "SamplingTypes",
+    "ImputationStrategy",
+    "ForecastingPipelineTypes",
+    "TimeseriesAnomalyPredictionPipelineTypes",
+    "TimeseriesAnomalyPredictionAlgorithms",
 ]
 
 
 class ClassificationAlgorithms(Enum):
     """Classification algorithms that AutoAI could use for IBM Cloud."""
+
     EX_TREES = "ExtraTreesClassifier"
     GB = "GradientBoostingClassifier"
     LGBM = "LGBMClassifier"
     LR = "LogisticRegression"
     RF = "RandomForestClassifier"
     XGB = "XGBClassifier"
     DT = "DecisionTreeClassifier"
@@ -49,14 +50,15 @@
 
 class ClassificationAlgorithmsCP4D(Enum):
     """
     Classification algorithms that AutoAI could use for IBM Cloud Pak for Data(CP4D).
     The SnapML estimators (SnapDT, SnapRF, SnapSVM, SnapLR) are supported
     on IBM Cloud Pak for Data version 4.0.2 and above.
     """
+
     EX_TREES = "ExtraTreesClassifierEstimator"
     GB = "GradientBoostingClassifierEstimator"
     LGBM = "LGBMClassifierEstimator"
     LR = "LogisticRegressionEstimator"
     RF = "RandomForestClassifierEstimator"
     XGB = "XGBClassifierEstimator"
     DT = "DecisionTreeClassifierEstimator"
@@ -65,24 +67,26 @@
     SnapSVM = "SnapSVMClassifier"
     SnapLR = "SnapLogisticRegression"
     SnapBM = "SnapBoostingMachineClassifier"
 
 
 class BatchedClassificationAlgorithms(Enum):
     """Batched tree ensemble classification algorithms that AutoAI could use for IBM Cloud."""
+
     RF = "BatchedTreeEnsembleClassifier(RandomForestClassifier)"
     EX_TREES = "BatchedTreeEnsembleClassifier(ExtraTreesClassifier)"
     LGBM = "BatchedTreeEnsembleClassifier(LGBMClassifier)"
     XGB = "BatchedTreeEnsembleClassifier(XGBClassifier)"
     SnapRF = "BatchedTreeEnsembleClassifier(SnapRandomForestClassifier)"
     SnapBM = "BatchedTreeEnsembleClassifier(SnapBoostingMachineClassifier)"
 
 
 class RegressionAlgorithms(Enum):
     """Regression algorithms that AutoAI could use for IBM Cloud."""
+
     RF = "RandomForestRegressor"
     RIDGE = "Ridge"
     EX_TREES = "ExtraTreesRegressor"
     GB = "GradientBoostingRegressor"
     LR = "LinearRegression"
     XGB = "XGBRegressor"
     LGBM = "LGBMRegressor"
@@ -94,14 +98,15 @@
 
 class RegressionAlgorithmsCP4D(Enum):
     """
     Regression algorithms that AutoAI could use for IBM Cloud Pak for Data(CP4D).
     The SnapML estimators (SnapDT, SnapRF, SnapBM) are supported
     on IBM Cloud Pak for Data version 4.0.2 and above.
     """
+
     RF = "RandomForestRegressorEstimator"
     RIDGE = "RidgeEstimator"
     EX_TREES = "ExtraTreesRegressorEstimator"
     GB = "GradientBoostingRegressorEstimator"
     LR = "LinearRegressionEstimator"
     XGB = "XGBRegressorEstimator"
     LGBM = "LGBMRegressorEstimator"
@@ -109,116 +114,132 @@
     SnapDT = "SnapDecisionTreeRegressor"
     SnapRF = "SnapRandomForestRegressor"
     SnapBM = "SnapBoostingMachineRegressor"
 
 
 class BatchedRegressionAlgorithms(Enum):
     """Batched tree ensemble regression algorithms that AutoAI could use for IBM Cloud."""
+
     RF = "BatchedTreeEnsembleRegressor(RandomForestRegressor)"
     EX_TREES = "BatchedTreeEnsembleRegressor(ExtraTreesRegressor)"
     LGBM = "BatchedTreeEnsembleRegressor(LGBMRegressor)"
     SnapRF = "BatchedTreeEnsembleRegressor(SnapRandomForestRegressor)"
     SnapBM = "BatchedTreeEnsembleRegressor(SnapBoostingMachineRegressor)"
     XGB = "BatchedTreeEnsembleRegressor(XGBRegressor)"
 
 
 class ForecastingAlgorithmsCP4D(Enum):
     """Forecasting algorithms that AutoAI could use for IBM Cloud."""
+
     LR = "LinearRegression"
     ENSEMBLER = "Ensembler"
     ARIMA = "ARIMA"
     HW = "HoltWinters"
     BATS = "BATS"
     RF = "RandomForest"
     SVM = "SVM"
 
 
 class ForecastingAlgorithms(Enum):
     """Forecasting algorithms that AutoAI could use for IBM watsonx.ai software with IBM Cloud Pak for Data."""
+
     LR = "LinearRegression"
     ENSEMBLER = "Ensembler"
     ARIMA = "ARIMA"
     HW = "HoltWinters"
     BATS = "BATS"
     RF = "RandomForest"
     SVM = "SVM"
 
 
 class ForecastingPipelineTypes(Enum):
     """Forecasting pipeline types that AutoAI could use for IBM Cloud Pak for Data(CP4D)."""
-    RandomForestRegressor = 'RandomForestRegressor'
-    ExogenousRandomForestRegressor = 'ExogenousRandomForestRegressor'
-    SVM = 'SVM'
-    ExogenousSVM = 'ExogenousSVM'
-    LocalizedFlattenEnsembler = 'LocalizedFlattenEnsembler'
-    DifferenceFlattenEnsembler = 'DifferenceFlattenEnsembler'
-    FlattenEnsembler = 'FlattenEnsembler'
-    ExogenousLocalizedFlattenEnsembler = 'ExogenousLocalizedFlattenEnsembler'
-    ExogenousDifferenceFlattenEnsembler = 'ExogenousDifferenceFlattenEnsembler'
-    ExogenousFlattenEnsembler = 'ExogenousFlattenEnsembler'
-    MT2RForecaster = 'MT2RForecaster'
-    ExogenousMT2RForecaster = 'ExogenousMT2RForecaster'
-    HoltWinterAdditive = 'HoltWinterAdditive'
-    HoltWinterMultiplicative = 'HoltWinterMultiplicative'
-    Bats = 'Bats'
-    ARIMA = 'ARIMA'
-    ARIMAX = 'ARIMAX'
-    ARIMAX_RSAR = 'ARIMAX_RSAR'
-    ARIMAX_PALR = 'ARIMAX_PALR'
-    ARIMAX_RAR = 'ARIMAX_RAR'
-    ARIMAX_DMLR = 'ARIMAX_DMLR'
+
+    RandomForestRegressor = "RandomForestRegressor"
+    ExogenousRandomForestRegressor = "ExogenousRandomForestRegressor"
+    SVM = "SVM"
+    ExogenousSVM = "ExogenousSVM"
+    LocalizedFlattenEnsembler = "LocalizedFlattenEnsembler"
+    DifferenceFlattenEnsembler = "DifferenceFlattenEnsembler"
+    FlattenEnsembler = "FlattenEnsembler"
+    ExogenousLocalizedFlattenEnsembler = "ExogenousLocalizedFlattenEnsembler"
+    ExogenousDifferenceFlattenEnsembler = "ExogenousDifferenceFlattenEnsembler"
+    ExogenousFlattenEnsembler = "ExogenousFlattenEnsembler"
+    MT2RForecaster = "MT2RForecaster"
+    ExogenousMT2RForecaster = "ExogenousMT2RForecaster"
+    HoltWinterAdditive = "HoltWinterAdditive"
+    HoltWinterMultiplicative = "HoltWinterMultiplicative"
+    Bats = "Bats"
+    ARIMA = "ARIMA"
+    ARIMAX = "ARIMAX"
+    ARIMAX_RSAR = "ARIMAX_RSAR"
+    ARIMAX_PALR = "ARIMAX_PALR"
+    ARIMAX_RAR = "ARIMAX_RAR"
+    ARIMAX_DMLR = "ARIMAX_DMLR"
 
     @staticmethod
     def get_exogenous():
         """Get list of pipelines that use supporting features (exogenous pipelines).
 
         :return: list of pipelines using supporting features
         :rtype: list[ForecastingPipelineTypes]
         """
-        return [pipeline for pipeline in ForecastingPipelineTypes if pipeline.value.startswith('Exogenous')]
+        return [
+            pipeline
+            for pipeline in ForecastingPipelineTypes
+            if pipeline.value.startswith("Exogenous")
+        ]
 
     @staticmethod
     def get_non_exogenous():
         """Get list of pipelines not using supporting features (non-exogenous pipelines).
 
         :return: list of pipelines that do not use supporting features
         :rtype: list[ForecastingPipelineTypes]
         """
-        return [pipeline for pipeline in ForecastingPipelineTypes if not pipeline.value.startswith('Exogenous')]
+        return [
+            pipeline
+            for pipeline in ForecastingPipelineTypes
+            if not pipeline.value.startswith("Exogenous")
+        ]
 
 
 class TimeseriesAnomalyPredictionAlgorithms(Enum):
     """Timeseries Anomaly Prediction algorithms that AutoAI could use for IBM Cloud."""
+
     Forecasting = "Forecasting"
     Window = "Window"
     Relationship = "Relationship"
 
 
 class TimeseriesAnomalyPredictionPipelineTypes(Enum):
     """Timeseries Anomaly Prediction pipeline types that AutoAI could use for IBM Cloud."""
-    PointwiseBoundedHoltWintersAdditive = 'PointwiseBoundedHoltWintersAdditive'
-    PointwiseBoundedBATS = 'PointwiseBoundedBATS'
-    PointwiseBoundedBATSForceUpdate = 'PointwiseBoundedBATSForceUpdate'
-    WindowNN = 'WindowNN'
-    WindowPCA = 'WindowPCA'
-    WindowLOF = 'WindowLOF'
+
+    PointwiseBoundedHoltWintersAdditive = "PointwiseBoundedHoltWintersAdditive"
+    PointwiseBoundedBATS = "PointwiseBoundedBATS"
+    PointwiseBoundedBATSForceUpdate = "PointwiseBoundedBATSForceUpdate"
+    WindowNN = "WindowNN"
+    WindowPCA = "WindowPCA"
+    WindowLOF = "WindowLOF"
 
 
 class PredictionType:
     """Supported types of learning."""
-    CLASSIFICATION = 'classification'
+
+    CLASSIFICATION = "classification"
     BINARY = "binary"
     MULTICLASS = "multiclass"
     REGRESSION = "regression"
     FORECASTING = "forecasting"
     TIMESERIES_ANOMALY_PREDICTION = "timeseries_anomaly_prediction"
 
 
 class PositiveLabelClass:
     """Metrics that need positive label definition for binary classification."""
+
     AVERAGE_PRECISION_SCORE = "average_precision"
     F1_SCORE = "f1"
     PRECISION_SCORE = "precision"
     RECALL_SCORE = "recall"
     F1_SCORE_MICRO = "f1_micro"
     F1_SCORE_MACRO = "f1_macro"
     F1_SCORE_WEIGHTED = "f1_weighted"
@@ -228,14 +249,15 @@
     RECALL_SCORE_MICRO = "recall_micro"
     RECALL_SCORE_MACRO = "recall_macro"
     RECALL_SCORE_WEIGHTED = "recall_weighted"
 
 
 class Metrics:
     """Supported types of classification and regression metrics in autoai."""
+
     ACCURACY_SCORE = "accuracy"
     ACCURACY_AND_DISPARATE_IMPACT_SCORE = "accuracy_and_disparate_impact"
     AVERAGE_PRECISION_SCORE = "average_precision"
     F1_SCORE = "f1"
     LOG_LOSS = "neg_log_loss"
     PRECISION_SCORE = "precision"
     RECALL_SCORE = "recall"
@@ -260,14 +282,15 @@
     ROOT_MEAN_SQUARED_LOG_ERROR = "neg_root_mean_squared_log_error"
     R2_SCORE = "r2"
     R2_AND_DISPARATE_IMPACT_SCORE = "r2_and_disparate_impact"
 
 
 class Transformers:
     """Supported types of congito transformers names in autoai."""
+
     SQRT = "sqrt"
     LOG = "log"
     ROUND = "round"
     SQUARE = "square"
     CBRT = "cbrt"
     SIN = "sin"
     COS = "cos"
@@ -288,56 +311,62 @@
     CUBE = "cube"
     FEATUREAGGLOMERATION = "featureagglomeration"
     ISOFORESTANOMALY = "isoforestanomaly"
 
 
 class DataConnectionTypes:
     """Supported types of DataConnection."""
+
     S3 = "s3"
-    FS = 'fs'
-    DS = 'data_asset'
-    CA = 'connection_asset'
-    CN = 'container'
+    FS = "fs"
+    DS = "data_asset"
+    CA = "connection_asset"
+    CN = "container"
 
 
 class RunStateTypes:
     """Supported types of AutoAI fit/run."""
+
     COMPLETED = "completed"
     FAILED = "failed"
 
 
 class PipelineTypes:
     """Supported types of Pipelines."""
+
     LALE = "lale"
     SKLEARN = "sklearn"
 
 
 class Directions:
     """Possible metrics directions"""
+
     ASCENDING = "ascending"
     DESCENDING = "descending"
 
 
 class TShirtSize:
     """Possible sizes of the AutoAI POD.
     Depends on the POD size, AutoAI could support different data sets sizes.
 
     - S - small (2vCPUs and 8GB of RAM)
     - M - Medium (4vCPUs and 16GB of RAM)
     - L - Large (8vCPUs and 32GB of RAM))
     - XL - Extra Large (16vCPUs and 64GB of RAM)
     """
-    S = 's'
-    M = 'm'
-    L = 'l'
-    XL = 'xl'
+
+    S = "s"
+    M = "m"
+    L = "l"
+    XL = "xl"
 
 
 class MetricsToDirections(Enum):
     """Map of metrics directions."""
+
     ROC_AUC = Directions.ASCENDING
     NORMALIZED_GINI_COEFFICIENT = Directions.ASCENDING
     PRECISION = Directions.ASCENDING
     AVERAGE_PRECISION = Directions.ASCENDING
     NEG_LOG_LOSS = Directions.DESCENDING
     RECALL = Directions.ASCENDING
     ACCURACY = Directions.ASCENDING
@@ -361,32 +390,35 @@
     NEG_MEDIAN_ABSOLUTE_ERROR = Directions.DESCENDING
     NEG_ROOT_MEAN_SQUARED_LOG_ERROR = Directions.DESCENDING
     R2 = Directions.ASCENDING
 
 
 class VisualizationTypes:
     """Types of visualization options."""
-    PDF = 'pdf'
-    INPLACE = 'inplace'
+
+    PDF = "pdf"
+    INPLACE = "inplace"
 
 
 class SamplingTypes:
     """Types of training data sampling."""
-    FIRST_VALUES = 'first_n_records'
-    RANDOM = 'random'
-    STRATIFIED = 'stratified'
-    LAST_VALUES = 'truncate'
+
+    FIRST_VALUES = "first_n_records"
+    RANDOM = "random"
+    STRATIFIED = "stratified"
+    LAST_VALUES = "truncate"
 
 
 class ImputationStrategy(Enum):
     """Missing values imputation strategies."""
-    MEAN = 'mean'
-    MEDIAN = 'median'
-    MOST_FREQUENT = 'most_frequent'
-    BEST_OF_DEFAULT_IMPUTERS = 'best_of_default_imputers'
-    VALUE = 'value'
-    FLATTEN_ITERATIVE = 'flatten_iterative'
-    LINEAR = 'linear'
-    CUBIC = 'cubic'
-    PREVIOUS = 'previous'
-    NEXT = 'next'
-    NO_IMPUTATION = 'no_imputation'
+
+    MEAN = "mean"
+    MEDIAN = "median"
+    MOST_FREQUENT = "most_frequent"
+    BEST_OF_DEFAULT_IMPUTERS = "best_of_default_imputers"
+    VALUE = "value"
+    FLATTEN_ITERATIVE = "flatten_iterative"
+    LINEAR = "linear"
+    CUBIC = "cubic"
+    PREVIOUS = "previous"
+    NEXT = "next"
+    NO_IMPUTATION = "no_imputation"
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/utils/autoai/errors.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/utils/autoai/errors.py`

 * *Files 10% similar despite different names*

```diff
@@ -12,413 +12,540 @@
     "UseWMLClient",
     "PipelineNotLoaded",
     "MissingCOSStudioConnection",
     "MissingProjectLib",
     "LocalInstanceButRemoteParameter",
     "DataFormatNotSupported",
     "HoldoutSplitNotSupported",
-    'LibraryNotCompatible',
-    'CannotInstallLibrary',
-    'InvalidCOSCredentials',
-    'CannotDownloadTrainingDetails',
-    'TShirtSizeNotSupported',
-    'MissingPositiveLabel',
-    'MissingDataPreprocessingStep',
-    'CannotDownloadWMLPipelineDetails',
-    'SetIDFailed',
-    'MissingLocalAsset',
-    'DataSourceSizeNotSupported',
-    'TrainingDataSourceIsNotFile',
-    'VisualizationFailed',
-    'InvalidPredictionType',
-    'InvalidIdType',
-    'NoneDataConnection',
-    'CannotReadSavedRemoteDataBeforeFit',
-    'NotExistingCOSResource',
-    'AdditionalParameterIsUnexpected',
-    'ForecastPredictionColumnsMissing',
-    'TimeseriesAnomalyPredictionFeatureColumnsMissing',
-    'TimeseriesAnomalyPredictionUnsupportedMetric',
-    'NonForecastPredictionColumnMissing',
-    'ForecastingCannotBeRunAsLocalScenario',
-    'TimeseriesAnomalyPredictionCannotBeRunAsLocalScenario',
-    'ForecastingUnsupportedOperation',
-    'InvalidSequenceValue',
-    'NoAvailableMetrics',
-    'WrongAssetType',
-    'TSNotSupported',
-    'TSADNotSupported',
-    'InvalidDataAsset',
-    'CannotConnectToWebsocket',
-    'TestDataNotPresent',
-    'NoAutomatedHoldoutSplit',
-    'DiscardedModel',
-    'WrongModelName',
-    'StrategyIsNotApplicable',
-    'NumericalImputationStrategyValueMisused',
-    'InvalidImputationParameterNonTS',
-    'InvalidImputationParameterTS',
-    'InconsistentImputationListElements',
-    'ImputationListNotSupported',
-    'MissingEstimatorForExistingBatchedEstimator',
-    'FutureExogenousFeaturesNotSupported',
-    'NoAvailableNotebookLocation',
+    "LibraryNotCompatible",
+    "CannotInstallLibrary",
+    "InvalidCOSCredentials",
+    "CannotDownloadTrainingDetails",
+    "TShirtSizeNotSupported",
+    "MissingPositiveLabel",
+    "MissingDataPreprocessingStep",
+    "CannotDownloadWMLPipelineDetails",
+    "SetIDFailed",
+    "MissingLocalAsset",
+    "DataSourceSizeNotSupported",
+    "TrainingDataSourceIsNotFile",
+    "VisualizationFailed",
+    "InvalidPredictionType",
+    "InvalidIdType",
+    "NoneDataConnection",
+    "CannotReadSavedRemoteDataBeforeFit",
+    "NotExistingCOSResource",
+    "AdditionalParameterIsUnexpected",
+    "ForecastPredictionColumnsMissing",
+    "TimeseriesAnomalyPredictionFeatureColumnsMissing",
+    "TimeseriesAnomalyPredictionUnsupportedMetric",
+    "NonForecastPredictionColumnMissing",
+    "ForecastingCannotBeRunAsLocalScenario",
+    "TimeseriesAnomalyPredictionCannotBeRunAsLocalScenario",
+    "ForecastingUnsupportedOperation",
+    "InvalidSequenceValue",
+    "NoAvailableMetrics",
+    "WrongAssetType",
+    "TSNotSupported",
+    "TSADNotSupported",
+    "InvalidDataAsset",
+    "CannotConnectToWebsocket",
+    "TestDataNotPresent",
+    "NoAutomatedHoldoutSplit",
+    "DiscardedModel",
+    "WrongModelName",
+    "StrategyIsNotApplicable",
+    "NumericalImputationStrategyValueMisused",
+    "InvalidImputationParameterNonTS",
+    "InvalidImputationParameterTS",
+    "InconsistentImputationListElements",
+    "ImputationListNotSupported",
+    "MissingEstimatorForExistingBatchedEstimator",
+    "FutureExogenousFeaturesNotSupported",
+    "NoAvailableNotebookLocation",
     "ContainerTypeNotSupported",
     "InvalidSizeLimit",
-    "CorruptedData"
+    "CorruptedData",
 ]
 
 
 from ibm_watsonx_ai.wml_client_error import WMLClientError
 from ibm_watsonx_ai.utils.autoai.enums import PredictionType
 
 
 class MissingPipeline(WMLClientError, ValueError):
     def __init__(self, value_name, reason=None):
-        WMLClientError.__init__(self, f"There is no such a Pipeline like: {value_name}", reason)
+        WMLClientError.__init__(
+            self, f"There is no such a Pipeline like: {value_name}", reason
+        )
 
 
 class FitNotCompleted(WMLClientError, ValueError):
     def __init__(self, value_name, reason=None):
-        WMLClientError.__init__(self, f"Fit run is not completed or the status is failed for run: {value_name}", reason)
+        WMLClientError.__init__(
+            self,
+            f"Fit run is not completed or the status is failed for run: {value_name}",
+            reason,
+        )
 
 
 class FitNeeded(WMLClientError, ValueError):
     def __init__(self, value_name=None, reason=None):
         WMLClientError.__init__(self, f"Fit run was not performed.", reason)
 
 
 class AutoAIComputeError(WMLClientError, ValueError):
     def __init__(self, value_name, reason=None):
-        WMLClientError.__init__(self, f"Fit run failed for run_id: {value_name}.", reason)
+        WMLClientError.__init__(
+            self, f"Fit run failed for run_id: {value_name}.", reason
+        )
 
 
 class MissingAutoPipelinesParameters(WMLClientError, ValueError):
     def __init__(self, value_name, reason=None):
-        WMLClientError.__init__(self, f"AutoPipelines parameters are {value_name}", reason)
+        WMLClientError.__init__(
+            self, f"AutoPipelines parameters are {value_name}", reason
+        )
 
 
 class UseWMLClient(WMLClientError, ValueError):
     def __init__(self, value_name, reason=None):
-        WMLClientError.__init__(self, f"Use WML v4 Client instead of {value_name}", reason)
+        WMLClientError.__init__(
+            self, f"Use WML v4 Client instead of {value_name}", reason
+        )
 
 
 class PipelineNotLoaded(WMLClientError, ValueError):
     def __init__(self, value_name, reason=None):
-        WMLClientError.__init__(self, f"Pipeline model: {value_name} cannot load.", reason)
+        WMLClientError.__init__(
+            self, f"Pipeline model: {value_name} cannot load.", reason
+        )
 
 
 class MissingCOSStudioConnection(WMLClientError, ValueError):
     def __init__(self, reason=None):
         WMLClientError.__init__(self, f"Missing COS Studio connection.", reason)
 
 
 class MissingProjectLib(WMLClientError, ValueError):
     def __init__(self, reason=None):
-        WMLClientError.__init__(self,
-                                f"project-lib package missing in the environment, please make sure you are on Watson Studio"
-                                f" and want to automatically initialize your COS connection."
-                                f" If you want to initialize COS connection manually, do not use from_studio() method.",
-                                reason)
+        WMLClientError.__init__(
+            self,
+            f"project-lib package missing in the environment, please make sure you are on Watson Studio"
+            f" and want to automatically initialize your COS connection."
+            f" If you want to initialize COS connection manually, do not use from_studio() method.",
+            reason,
+        )
 
 
 class LocalInstanceButRemoteParameter(WMLClientError, ValueError):
     def __init__(self, value_name=None, reason=None):
-        WMLClientError.__init__(self, f"Provided {value_name} parameter to local optimizer instance.", reason)
+        WMLClientError.__init__(
+            self,
+            f"Provided {value_name} parameter to local optimizer instance.",
+            reason,
+        )
 
 
 class DataFormatNotSupported(WMLClientError, ValueError):
     def __init__(self, value_name=None, reason=None):
-        WMLClientError.__init__(self,
-                                f"This data format is not supported by SDK. (CSV and XLSX formats are supported.)",
-                                reason)
+        WMLClientError.__init__(
+            self,
+            f"This data format is not supported by SDK. (CSV and XLSX formats are supported.)",
+            reason,
+        )
 
 
 class HoldoutSplitNotSupported(WMLClientError, ValueError):
     def __init__(self, value_name=None, reason=None):
-        WMLClientError.__init__(self, f"Holdout split is not supported for xlsx data.", reason)
+        WMLClientError.__init__(
+            self, f"Holdout split is not supported for xlsx data.", reason
+        )
 
 
 class LibraryNotCompatible(WMLClientError, ValueError):
     def __init__(self, value_name=None, reason=None):
         WMLClientError.__init__(self, f"Library not compatible or missing!", reason)
 
 
 class CannotInstallLibrary(WMLClientError, ValueError):
     def __init__(self, value_name=None, reason=None):
-        WMLClientError.__init__(self, f"Library cannot be installed! Error: {value_name}", reason)
+        WMLClientError.__init__(
+            self, f"Library cannot be installed! Error: {value_name}", reason
+        )
 
 
 class InvalidCOSCredentials(WMLClientError, ValueError):
     def __init__(self, value_name=None, reason=None):
         WMLClientError.__init__(self, f"Wrong COS credentials!", reason)
 
 
 class CannotDownloadTrainingDetails(WMLClientError, ValueError):
     def __init__(self, value_name, reason=None):
-        WMLClientError.__init__(self, f"Cannot download training details, training is not done yet. "
-                                      f"Please try again after training is finished.", reason)
+        WMLClientError.__init__(
+            self,
+            f"Cannot download training details, training is not done yet. "
+            f"Please try again after training is finished.",
+            reason,
+        )
+
 
 class TShirtSizeNotSupported(WMLClientError, ValueError):
     def __init__(self, value_name=None, reason=None):
-        WMLClientError.__init__(self, f"This t-shirt size: \"{value_name}\" is not supported on this environment.",
-                                reason)
+        WMLClientError.__init__(
+            self,
+            f'This t-shirt size: "{value_name}" is not supported on this environment.',
+            reason,
+        )
 
 
 class MissingPositiveLabel(WMLClientError, ValueError):
     def __init__(self, value_name=None, reason=None):
-        WMLClientError.__init__(self, f"Missing positive label for \"{value_name}\"", reason)
+        WMLClientError.__init__(
+            self, f'Missing positive label for "{value_name}"', reason
+        )
 
 
 class MissingDataPreprocessingStep(WMLClientError, ValueError):
     def __init__(self, value_name=None, reason=None):
         WMLClientError.__init__(self, f"Data preprocessing step not performed.", reason)
 
 
 class CannotDownloadWMLPipelineDetails(WMLClientError, ValueError):
     def __init__(self, value_name, reason=None):
         WMLClientError.__init__(self, f"Cannot download WML pipeline details ", reason)
 
+
 class SetIDFailed(WMLClientError, ValueError):
     def __init__(self, value_name=None, reason=None):
         WMLClientError.__init__(self, f"Cannot set {value_name}.", reason)
 
 
 class MissingLocalAsset(WMLClientError, ValueError):
     def __init__(self, value_name=None, reason=None):
-        WMLClientError.__init__(self, f"Local asset: {value_name} cannot be found.", reason)
+        WMLClientError.__init__(
+            self, f"Local asset: {value_name} cannot be found.", reason
+        )
 
 
 class DataSourceSizeNotSupported(WMLClientError, ValueError):
     def __init__(self, reason=None):
-        WMLClientError.__init__(self, f"The selected data source is too large for selected compute configuration "
-                                      f"and might fail to run. Consider increasing the compute configuration", reason)
+        WMLClientError.__init__(
+            self,
+            f"The selected data source is too large for selected compute configuration "
+            f"and might fail to run. Consider increasing the compute configuration",
+            reason,
+        )
 
 
 class TrainingDataSourceIsNotFile(WMLClientError, ValueError):
     def __init__(self, data_location=None, reason=None):
-        WMLClientError.__init__(self, f"Training data location: {data_location} is a directory or does not exist."
-                                      f"Please set training data location to dataset file location.", reason)
+        WMLClientError.__init__(
+            self,
+            f"Training data location: {data_location} is a directory or does not exist."
+            f"Please set training data location to dataset file location.",
+            reason,
+        )
 
 
 class VisualizationFailed(WMLClientError, ValueError):
     def __init__(self, value_name=None, reason=None):
         WMLClientError.__init__(self, f"Cannot perform visualization.", reason)
 
 
 class InvalidPredictionType(WMLClientError, ValueError):
     def __init__(self, value_name=None, reason=None):
-        WMLClientError.__init__(self, f"Cannot recognize prediction type: {value_name}", reason)
+        WMLClientError.__init__(
+            self, f"Cannot recognize prediction type: {value_name}", reason
+        )
 
 
 class InvalidIdType(WMLClientError, ValueError):
     def __init__(self, typ):
         WMLClientError.__init__(self, f"Unexpected type of id: {typ}")
 
 
 class NoneDataConnection(WMLClientError, ValueError):
     def __init__(self, resource_name):
-        WMLClientError.__init__(self, f"Invalid DataConnection in {resource_name} list. DataConnection cannot be None")
+        WMLClientError.__init__(
+            self,
+            f"Invalid DataConnection in {resource_name} list. DataConnection cannot be None",
+        )
 
 
 class CannotReadSavedRemoteDataBeforeFit(WMLClientError, ValueError):
     def __init__(self):
         WMLClientError.__init__(self, f"Cannot read saved remote data before fit.")
 
 
 class NotExistingCOSResource(WMLClientError, ValueError):
     def __init__(self, bucket, path):
-        WMLClientError.__init__(self, f"There is no COS resource: {path} in bucket: {bucket}.")
+        WMLClientError.__init__(
+            self, f"There is no COS resource: {path} in bucket: {bucket}."
+        )
 
 
 class AdditionalParameterIsUnexpected(WMLClientError):
     def __init__(self, param):
         WMLClientError.__init__(self, f"Additional parameter is not expected: {param}")
 
 
 class ForecastPredictionColumnsMissing(WMLClientError):
     def __init__(self):
-        WMLClientError.__init__(self,
-                                "For prediction_type = `forecasting` passing `prediction_columns` is required, while `prediction_column` should be None.")
+        WMLClientError.__init__(
+            self,
+            "For prediction_type = `forecasting` passing `prediction_columns` is required, while `prediction_column` should be None.",
+        )
 
 
 class TimeseriesAnomalyPredictionFeatureColumnsMissing(WMLClientError):
     def __init__(self):
-        WMLClientError.__init__(self,
-                                "For prediction_type = `timeseries_anomaly_prediction` passing `feature_columns` is required, while `prediction_columns` and `prediction_column` should be None.")
+        WMLClientError.__init__(
+            self,
+            "For prediction_type = `timeseries_anomaly_prediction` passing `feature_columns` is required, while `prediction_columns` and `prediction_column` should be None.",
+        )
 
 
 class TimeseriesAnomalyPredictionUnsupportedMetric(WMLClientError):
     def __init__(self, metric):
-        WMLClientError.__init__(self, f"Metric {metric} is unsupported for timeseries anomaly prediction type.")
+        WMLClientError.__init__(
+            self,
+            f"Metric {metric} is unsupported for timeseries anomaly prediction type.",
+        )
 
 
 class NonForecastPredictionColumnMissing(WMLClientError):
     def __init__(self, prediction_type):
-        WMLClientError.__init__(self,
-                                f"For prediction_type = `{prediction_type}` passing `prediction_column` is required, while `prediction_columns` should be None.")
+        WMLClientError.__init__(
+            self,
+            f"For prediction_type = `{prediction_type}` passing `prediction_column` is required, while `prediction_columns` should be None.",
+        )
 
 
 class ForecastingCannotBeRunAsLocalScenario(WMLClientError):
     def __init__(self):
         WMLClientError.__init__(self, f"Forecasting cannot be run in local scenario.")
 
 
 class TimeseriesAnomalyPredictionCannotBeRunAsLocalScenario(WMLClientError):
     def __init__(self):
-        WMLClientError.__init__(self, f"Timeseries anomaly prediction cannot be run in local scenario.")
+        WMLClientError.__init__(
+            self, f"Timeseries anomaly prediction cannot be run in local scenario."
+        )
 
 
 class ForecastingUnsupportedOperation(WMLClientError):
     def __init__(self):
-        WMLClientError.__init__(self, f"Operation is unsupported for timeseries forecasting prediction type.")
+        WMLClientError.__init__(
+            self,
+            f"Operation is unsupported for timeseries forecasting prediction type.",
+        )
+
 
 class InvalidSequenceValue(WMLClientError, ValueError):
     def __init__(self, el, correct_values):
-        WMLClientError.__init__(self, f"Invalid sequence element: '{el}' sequence must be composed from "
-                                      f"given values: {correct_values}")
+        WMLClientError.__init__(
+            self,
+            f"Invalid sequence element: '{el}' sequence must be composed from "
+            f"given values: {correct_values}",
+        )
 
 
 class NoAvailableMetrics(WMLClientError):
     def __init__(self):
         WMLClientError.__init__(self, f"Currently there is no available metrics.")
 
 
 class WrongAssetType(WMLClientError):
     def __init__(self, asset_type: str):
-        WMLClientError.__init__(self, f"This asset type: '{asset_type}' is not supported.")
+        WMLClientError.__init__(
+            self, f"This asset type: '{asset_type}' is not supported."
+        )
 
 
 class TSNotSupported(WMLClientError):
     def __init__(self):
-        WMLClientError.__init__(self, f"Time series forecasting is not supported for CPD 2.5, 3.0 and 3.5.")
+        WMLClientError.__init__(
+            self, f"Time series forecasting is not supported for CPD 2.5, 3.0 and 3.5."
+        )
 
 
 class TSADNotSupported(WMLClientError):
     def __init__(self):
-        WMLClientError.__init__(self,
-                                f"Time series anomaly prediction is not supported for CPD 2.5, 3.0, 3.5, 4.0, 4.5 and 4.6.")
+        WMLClientError.__init__(
+            self,
+            f"Time series anomaly prediction is not supported for CPD 2.5, 3.0, 3.5, 4.0, 4.5 and 4.6.",
+        )
 
 
 class InvalidDataAsset(WMLClientError, ValueError):
-    def __init__(self, reason='Wrong Data Asset'):
-        WMLClientError.__init__(self, "Data Asset without 'connection' is not supported for result store action.",
-                                reason)
+    def __init__(self, reason="Wrong Data Asset"):
+        WMLClientError.__init__(
+            self,
+            "Data Asset without 'connection' is not supported for result store action.",
+            reason,
+        )
 
 
 class CannotConnectToWebsocket(WMLClientError, ValueError):
     def __init__(self, n):
         WMLClientError.__init__(self, f"{n} times connecting to websocket failed.")
 
 
 class TestDataNotPresent(WMLClientError, ValueError):
     def __init__(self, reason):
-        WMLClientError.__init__(self, "User defined (test / holdout) data is not present for this AutoAI experiment.",
-                                reason)
+        WMLClientError.__init__(
+            self,
+            "User defined (test / holdout) data is not present for this AutoAI experiment.",
+            reason,
+        )
 
 
 class NoAutomatedHoldoutSplit(WMLClientError, ValueError):
     def __init__(self, reason):
-        WMLClientError.__init__(self, "AutoAI experiment was performed with user defined holdout."
-                                      "To recreate holdout split, please use test data connection."
-                                      "You can call optimizer.get_test_data_connections().",
-                                reason)
+        WMLClientError.__init__(
+            self,
+            "AutoAI experiment was performed with user defined holdout."
+            "To recreate holdout split, please use test data connection."
+            "You can call optimizer.get_test_data_connections().",
+            reason,
+        )
 
 
 class DiscardedModel(WMLClientError, ValueError):
     def __init__(self, model_name):
-        WMLClientError.__init__(self,
-                                "You are trying to store a discarded forecasting model pipeline. Please use not discarded one."
-                                "Look at the pipelines 'summary' method and choose the 'Winner' one.",
-                                reason=f"Pipeline: '{model_name}' is discarded!")
+        WMLClientError.__init__(
+            self,
+            "You are trying to store a discarded forecasting model pipeline. Please use not discarded one."
+            "Look at the pipelines 'summary' method and choose the 'Winner' one.",
+            reason=f"Pipeline: '{model_name}' is discarded!",
+        )
 
 
 class WrongModelName(WMLClientError, ValueError):
     def __init__(self, model_name):
-        WMLClientError.__init__(self,
-                                "This model name does not exist. Please provide correct one."
-                                "Look at the pipelines 'summary' method and choose the 'Winner' one.",
-                                reason=f"Pipeline: '{model_name}' does not exist!")
+        WMLClientError.__init__(
+            self,
+            "This model name does not exist. Please provide correct one."
+            "Look at the pipelines 'summary' method and choose the 'Winner' one.",
+            reason=f"Pipeline: '{model_name}' does not exist!",
+        )
 
 
 class StrategyIsNotApplicable(WMLClientError, ValueError):
     def __init__(self, strategy, prediction_type, valid_strategies):
-        WMLClientError.__init__(self,
-                                f'{strategy} is not valid for '
-                                f'{"forecasting" if prediction_type == PredictionType.FORECASTING else "non-forecasting"} '
-                                f'scenario. Valid strategies are: {", ".join(valid_strategies)}')
+        WMLClientError.__init__(
+            self,
+            f"{strategy} is not valid for "
+            f'{"forecasting" if prediction_type == PredictionType.FORECASTING else "non-forecasting"} '
+            f'scenario. Valid strategies are: {", ".join(valid_strategies)}',
+        )
 
 
 class NumericalImputationStrategyValueMisused(WMLClientError, ValueError):
     def __init__(self):
-        WMLClientError.__init__(self, 'Parameter `numerical_imputation_value` can only be used when '
-                                      '`numerical_imputation_strategy` is set to ImputationStrategy.VALUE.')
+        WMLClientError.__init__(
+            self,
+            "Parameter `numerical_imputation_value` can only be used when "
+            "`numerical_imputation_strategy` is set to ImputationStrategy.VALUE.",
+        )
 
 
 class InvalidImputationParameterNonTS(WMLClientError, ValueError):
     def __init__(self):
-        WMLClientError.__init__(self, '`numerical_imputation_value` and `imputation_threshold` can be set only for forecasting experiments.')
+        WMLClientError.__init__(
+            self,
+            "`numerical_imputation_value` and `imputation_threshold` can be set only for forecasting experiments.",
+        )
 
 
 class InvalidImputationParameterTS(WMLClientError, ValueError):
     def __init__(self):
-        WMLClientError.__init__(self, '`categorical_imputation_strategy` does not apply to a forecasting experiment.')
+        WMLClientError.__init__(
+            self,
+            "`categorical_imputation_strategy` does not apply to a forecasting experiment.",
+        )
 
 
 class InconsistentImputationListElements(WMLClientError, ValueError):
     def __init__(self, strategies):
-        l = ['ImputationStrategy.' + s.name for s in strategies]
-        WMLClientError.__init__(self, f'`categorical_imputation_strategy` list elements are not compatible: {", ".join(l)}')
+        l = ["ImputationStrategy." + s.name for s in strategies]
+        WMLClientError.__init__(
+            self,
+            f'`categorical_imputation_strategy` list elements are not compatible: {", ".join(l)}',
+        )
 
 
 class ImputationListNotSupported(WMLClientError, ValueError):
     def __init__(self):
-        WMLClientError.__init__(self, f'List passed as imputation strategy is not supported for non-forecasting scenarios.')
+        WMLClientError.__init__(
+            self,
+            f"List passed as imputation strategy is not supported for non-forecasting scenarios.",
+        )
 
 
 class InvalidSamplingType(WMLClientError, ValueError):
     def __init__(self, sampling_type: str, predcition_type: str):
-        WMLClientError.__init__(self, f"Sampling type for {sampling_type} is invalid for prediction type {predcition_type}.")
+        WMLClientError.__init__(
+            self,
+            f"Sampling type for {sampling_type} is invalid for prediction type {predcition_type}.",
+        )
 
 
 class MissingEstimatorForExistingBatchedEstimator(WMLClientError, ValueError):
     def __init__(self, batched_estimator: str, missing_estimator: str):
-        WMLClientError.__init__(self, f"There is no corresponding estimator in `include_only_estimators` list for " +
-                                f"{batched_estimator} estimator in `include_batched_ensemble_estimators` list. " +
-                                f"Add {missing_estimator} estimator to `include_only_estimators` list.")
+        WMLClientError.__init__(
+            self,
+            f"There is no corresponding estimator in `include_only_estimators` list for "
+            + f"{batched_estimator} estimator in `include_batched_ensemble_estimators` list. "
+            + f"Add {missing_estimator} estimator to `include_only_estimators` list.",
+        )
 
 
 class FutureExogenousFeaturesNotSupported(WMLClientError, ValueError):
     def __init__(self, prediction_type: str):
-        WMLClientError.__init__(self, f"Future Exogenous features are only supported for forecasting."
-                                      f"Current prediction type is: {prediction_type}")
+        WMLClientError.__init__(
+            self,
+            f"Future Exogenous features are only supported for forecasting."
+            f"Current prediction type is: {prediction_type}",
+        )
 
 
 class NoAvailableNotebookLocation(WMLClientError, ValueError):
     def __init__(self, pipeline_name):
-        WMLClientError.__init__(self, f"Unable to find notebook location in training details for {pipeline_name}. " +
-                                "Possible causes: pipeline failure or pipeline not classified as winner in " +
-                                "forecasting scenario.")
+        WMLClientError.__init__(
+            self,
+            f"Unable to find notebook location in training details for {pipeline_name}. "
+            + "Possible causes: pipeline failure or pipeline not classified as winner in "
+            + "forecasting scenario.",
+        )
 
 
 class ContainerTypeNotSupported(WMLClientError, ValueError):
     def __init__(self):
-        WMLClientError.__init__(self, f"The container data connection type is not supported for CP4D environment."
-                                      f" Supported types are: data asset and connection asset. ")
+        WMLClientError.__init__(
+            self,
+            f"The container data connection type is not supported for CP4D environment."
+            f" Supported types are: data asset and connection asset. ",
+        )
 
 
 class InvalidSizeLimit(WMLClientError, ValueError):
     def __init__(self, size_limit, max_size_limit):
-        WMLClientError.__init__(self,
-                                f"The value of sampling size limit (total_size_limit): {size_limit} is incorrect. "
-                                f"The maximum value allowed for chosen environment configuration is: {max_size_limit}. "
-                                f"Adjust the sampling size to the environment limitations or "
-                                f"consider a change the compute resources allocated for running the experiment.")
+        WMLClientError.__init__(
+            self,
+            f"The value of sampling size limit (total_size_limit): {size_limit} is incorrect. "
+            f"The maximum value allowed for chosen environment configuration is: {max_size_limit}. "
+            f"Adjust the sampling size to the environment limitations or "
+            f"consider a change the compute resources allocated for running the experiment.",
+        )
 
 
 class CorruptedData(WMLClientError, ValueError):
-    def __init__(self, reason='Corrupted Data'):
-        WMLClientError.__init__(self,
-                                "Cannot read corrupted data. "
-                                "Check the data for unexpected characters (e.g. non-printing, control character). "
-                                "Clean up or use other data source and try again.",
-                                reason)
+    def __init__(self, reason="Corrupted Data"):
+        WMLClientError.__init__(
+            self,
+            "Cannot read corrupted data. "
+            "Check the data for unexpected characters (e.g. non-printing, control character). "
+            "Clean up or use other data source and try again.",
+            reason,
+        )
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/utils/autoai/fairness.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/utils/autoai/fairness.py`

 * *Files 18% similar despite different names*

```diff
@@ -15,116 +15,178 @@
     :type run_details: dict
     :param pipeline_name: name of the pipeline
     :type pipeline_name: str
 
     :return: extracted fairness details
     :rtype: dict
     """
-    for pipeline in run_details['entity']['status'].get('metrics', []):
-        if pipeline['context']['intermediate_model']['name'].split('P')[-1] == pipeline_name.split('_')[-1]:
-            return pipeline['context']['fairness']
+    for pipeline in run_details["entity"]["status"].get("metrics", []):
+        if (
+            pipeline["context"]["intermediate_model"]["name"].split("P")[-1]
+            == pipeline_name.split("_")[-1]
+        ):
+            return pipeline["context"]["fairness"]
 
 
 def get_pipeline_fairness_score(fairness_details):
-    return fairness_details['visualization_data']['holdout']['monitored_group']['fairness']['value']
+    return fairness_details["visualization_data"]["holdout"]["monitored_group"][
+        "fairness"
+    ]["value"]
 
 
 def get_favorable_outcomes(fairness_details):
-    return fairness_details['visualization_data']['holdout']['monitored_group']['favorable_outcome']['values']
+    return fairness_details["visualization_data"]["holdout"]["monitored_group"][
+        "favorable_outcome"
+    ]["values"]
 
 
 def _get_pipeline_percentage_of(fairness_details, group, outcome):
-    return fairness_details['visualization_data']['holdout'][group][outcome]['percentage']
+    return fairness_details["visualization_data"]["holdout"][group][outcome][
+        "percentage"
+    ]
 
 
 def _get_pipeline_perfect_equality(fairness_details):
-    return fairness_details['visualization_data']['holdout']['monitored_group']['favorable_outcome']['perfect_equality']['percentage']
+    return fairness_details["visualization_data"]["holdout"]["monitored_group"][
+        "favorable_outcome"
+    ]["perfect_equality"]["percentage"]
 
 
 def get_attributes_scores(fairness_details):
-    return {attr['feature']: attr['fairness']['percentage'] for attr in fairness_details['visualization_data']['holdout']['monitored_group']['protected_attributes']}
+    return {
+        attr["feature"]: attr["fairness"]["percentage"]
+        for attr in fairness_details["visualization_data"]["holdout"][
+            "monitored_group"
+        ]["protected_attributes"]
+    }
 
 
 def _get_attributes_perfect_equality(fairness_details):
-    return {attr['feature']: attr['favorable_outcome']['perfect_equality']['percentage'] for attr in fairness_details['visualization_data']['holdout']['monitored_group']['protected_attributes']}
+    return {
+        attr["feature"]: attr["favorable_outcome"]["perfect_equality"]["percentage"]
+        for attr in fairness_details["visualization_data"]["holdout"][
+            "monitored_group"
+        ]["protected_attributes"]
+    }
 
 
 def _get_pipeline_fairness(fairness_details):
     """Return pandas dataframe with fairness descriptive statics for pipeline model.
 
     :param fairness_details: details of fairness
     :type fairness_details: dict
 
     :return: dataframe containing fairness outcome
     :rtype: pandas df
     """
 
-    favorable_outcomes_percentage_monitored = _get_pipeline_percentage_of(fairness_details, 'monitored_group', 'favorable_outcome')
-    favorable_outcomes_percentage_reference = _get_pipeline_percentage_of(fairness_details, 'reference_group', 'favorable_outcome')
+    favorable_outcomes_percentage_monitored = _get_pipeline_percentage_of(
+        fairness_details, "monitored_group", "favorable_outcome"
+    )
+    favorable_outcomes_percentage_reference = _get_pipeline_percentage_of(
+        fairness_details, "reference_group", "favorable_outcome"
+    )
 
-    data_model = [['favorable', 'monitored', favorable_outcomes_percentage_monitored],
-                  ['favorable', 'reference', favorable_outcomes_percentage_reference],
-                  ['unfavorable', 'monitored', 100 - favorable_outcomes_percentage_monitored],
-                  ['unfavorable', 'reference', 100 - favorable_outcomes_percentage_reference]]
+    data_model = [
+        ["favorable", "monitored", favorable_outcomes_percentage_monitored],
+        ["favorable", "reference", favorable_outcomes_percentage_reference],
+        ["unfavorable", "monitored", 100 - favorable_outcomes_percentage_monitored],
+        ["unfavorable", "reference", 100 - favorable_outcomes_percentage_reference],
+    ]
 
-    return pd.DataFrame(data_model, columns=['Outcome', 'Group', '%'])
+    return pd.DataFrame(data_model, columns=["Outcome", "Group", "%"])
 
 
 def get_protected_attributes(fairness_details):
-    return [attr['feature'] for attr in fairness_details['visualization_data']['holdout']['monitored_group']['protected_attributes']]
+    return [
+        attr["feature"]
+        for attr in fairness_details["visualization_data"]["holdout"][
+            "monitored_group"
+        ]["protected_attributes"]
+    ]
 
 
 def _get_attributes_fairness(fairness_details):
     attr_data = []
 
-    for group in ['monitored_group', 'reference_group']:
-        attributes = fairness_details['visualization_data']['holdout'][group]['protected_attributes']
+    for group in ["monitored_group", "reference_group"]:
+        attributes = fairness_details["visualization_data"]["holdout"][group][
+            "protected_attributes"
+        ]
         for attr in attributes:
-            name = attr['feature']
+            name = attr["feature"]
 
             if len(attributes) > 1:
-                fav_percentage = attr['favorable_outcome']['percentage']
+                fav_percentage = attr["favorable_outcome"]["percentage"]
             else:
-                fav_percentage = _get_pipeline_percentage_of(fairness_details, group, 'favorable_outcome')
+                fav_percentage = _get_pipeline_percentage_of(
+                    fairness_details, group, "favorable_outcome"
+                )
 
-            attr_data.append(['unfavorable', group, name, 100 - fav_percentage])
-            attr_data.append(['favorable', group, name, fav_percentage])
+            attr_data.append(["unfavorable", group, name, 100 - fav_percentage])
+            attr_data.append(["favorable", group, name, fav_percentage])
 
-    return pd.DataFrame(attr_data, columns=['Outcome', 'Group', 'Feature', '%'])
+    return pd.DataFrame(attr_data, columns=["Outcome", "Group", "Feature", "%"])
 
 
 def visualize(run_details: dict, pipeline_name: str):
     """Plot favorable outcome distributions using plotly package.
 
     :param run_details: details of run
     :type run_details: dict
     :param pipeline_name: name of pipeline
     :type pipeline_name: str
     """
     if not isinstance(run_details, dict):
-        raise ValueError('run_details should be dict not {0}'.format(type(run_details)))
+        raise ValueError("run_details should be dict not {0}".format(type(run_details)))
 
     if not isinstance(pipeline_name, str):
-        raise ValueError('pipeline_name should be str not {0}'.format(type(pipeline_name)))
+        raise ValueError(
+            "pipeline_name should be str not {0}".format(type(pipeline_name))
+        )
 
     colors = ["#2A66DE", "gray"]
-    fairness_details = _get_fairness_details(run_details=run_details, pipeline_name=pipeline_name)
+    fairness_details = _get_fairness_details(
+        run_details=run_details, pipeline_name=pipeline_name
+    )
     df = _get_pipeline_fairness(fairness_details)
     pipeline_equality = _get_pipeline_perfect_equality(fairness_details)
     df_attr = _get_attributes_fairness(fairness_details)
-    fig1 = px.bar(df, y="Group", x="%", color="Outcome", title="Pipeline", orientation='h', color_discrete_sequence=colors)
-    fig1.add_vline(x=pipeline_equality, line_width=1, line_color="black", annotation_text="Perfect equality", annotation_position="top left")
+    fig1 = px.bar(
+        df,
+        y="Group",
+        x="%",
+        color="Outcome",
+        title="Pipeline",
+        orientation="h",
+        color_discrete_sequence=colors,
+    )
+    fig1.add_vline(
+        x=pipeline_equality,
+        line_width=1,
+        line_color="black",
+        annotation_text="Perfect equality",
+        annotation_position="top left",
+    )
     fig1.show()
 
     fig2 = go.Figure()
     fig2.update_layout(
         title="Protected attributes",
         template="simple_white",
         xaxis=dict(title_text="Outcome [%]"),
         yaxis=dict(title_text="Protected attribute"),
-        barmode="stack"
+        barmode="stack",
     )
 
     for r, c in zip(sorted(df_attr.Outcome.unique()), colors):
         plot_df = df_attr[df_attr.Outcome == r]
-        fig2.add_trace(go.Bar(y=[plot_df.Feature, plot_df.Group], x=plot_df['%'], name=r, marker_color=c, orientation='h'))
+        fig2.add_trace(
+            go.Bar(
+                y=[plot_df.Feature, plot_df.Group],
+                x=plot_df["%"],
+                name=r,
+                marker_color=c,
+                orientation="h",
+            )
+        )
     fig2.show()
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/utils/autoai/local_training_message_handler.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/utils/autoai/local_training_message_handler.py`

 * *Files 26% similar despite different names*

```diff
@@ -2,45 +2,60 @@
 #  (C) Copyright IBM Corp. 2023-2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
 
 from .progress_bar import ProgressBar
 from .utils import is_ipython
 
-__all__ = [
-    "LocalTrainingMessageHandler"
-]
+__all__ = ["LocalTrainingMessageHandler"]
 
 
 class LocalTrainingMessageHandler:
     """Show progress bar during local AutoPipelines training/fit."""
+
     def __init__(self):
         self.progress_bar = None
         self.progress_bar_1 = None
         self.previous_stage = None
 
         # note: only notebook version
         if is_ipython() and False:
             self.total = 100
-            self.progress_bar_1 = ProgressBar(desc="Total", total=self.total, position=0, ncols='100%')
-            self.progress_bar_2 = ProgressBar(desc="Waiting", total=self.total, leave=False, ncols='100%')
+            self.progress_bar_1 = ProgressBar(
+                desc="Total", total=self.total, position=0, ncols="100%"
+            )
+            self.progress_bar_2 = ProgressBar(
+                desc="Waiting", total=self.total, leave=False, ncols="100%"
+            )
 
         # note: only console version
         else:
             self.total = 200
-            self.progress_bar = ProgressBar(desc="Total", total=self.total, position=0, ncols=100)
+            self.progress_bar = ProgressBar(
+                desc="Total", total=self.total, position=0, ncols=100
+            )
 
     def on_training_message(self, status_state):
         """This method should be used by the ai4ml estimator/optimization process during training."""
 
-        if 'ml_metrics' in status_state:
-            metric = status_state['ml_metrics']
-            for stage in ['pre_hpo_d_output', 'hpo_d_output', 'hpo_c_output', 'fold_output',
-                          'cognito_output', 'compose_model_type_output', 'global_output', 'daub_running_output',
-                          'cognito_running_output', 'hpo_c_running_output', 'hpo_d_running_output']:
+        if "ml_metrics" in status_state:
+            metric = status_state["ml_metrics"]
+            for stage in [
+                "pre_hpo_d_output",
+                "hpo_d_output",
+                "hpo_c_output",
+                "fold_output",
+                "cognito_output",
+                "compose_model_type_output",
+                "global_output",
+                "daub_running_output",
+                "cognito_running_output",
+                "hpo_c_running_output",
+                "hpo_d_running_output",
+            ]:
 
                 if (stage in metric) and (metric[stage] is not None):
 
                     if self.progress_bar is not None:
                         self.progress_bar.set_description(desc=stage)
                         if self.total - self.progress_bar.counter <= 5:
                             pass
@@ -51,15 +66,20 @@
 
                     else:
 
                         if self.previous_stage != stage:
                             self.previous_stage = stage
                             self.progress_bar_2.last_update()
                             self.progress_bar_2.close()
-                            self.progress_bar_2 = ProgressBar(desc="Waiting", total=self.total, leave=False, ncols='100%')
+                            self.progress_bar_2 = ProgressBar(
+                                desc="Waiting",
+                                total=self.total,
+                                leave=False,
+                                ncols="100%",
+                            )
                             if self.total - self.progress_bar_1.counter <= 10:
                                 pass
 
                             else:
                                 self.progress_bar_1.increment_counter(progress=5)
                                 self.progress_bar_1.update()
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/utils/autoai/progress_bar.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/utils/autoai/progress_bar.py`

 * *Files 9% similar despite different names*

```diff
@@ -8,34 +8,46 @@
 
 from ibm_watsonx_ai.utils.autoai.utils import try_import_tqdm
 
 try_import_tqdm()
 
 from tqdm import tqdm as TQDM
 
-__all__ = [
-    "ProgressBar"
-]
+__all__ = ["ProgressBar"]
 
 
 class ProgressBar(TQDM):
     """Progress Bar class for handling progress bar display. It is based on 'tqdm' class, could be extended."""
 
-    def __init__(self, ncols: Union[str, int], position: int = 0, desc: str = None, total: int = 100,
-                 leave: bool = True, bar_format: str = '{desc}: {percentage:3.0f}%|{bar}|') -> None:
+    def __init__(
+        self,
+        ncols: Union[str, int],
+        position: int = 0,
+        desc: str = None,
+        total: int = 100,
+        leave: bool = True,
+        bar_format: str = "{desc}: {percentage:3.0f}%|{bar}|",
+    ) -> None:
         """
         :param desc: description string to be added as a prefix to progress bar
         :type desc: str, optional
 
         :param total: total length of the progress bar
         :type total: int, optional
         """
         # note: to see possible progress bar formats please look at super 'bar_format' description
-        super().__init__(desc=desc, total=total, leave=leave, position=position, ncols=ncols, file=sys.stdout,
-                         bar_format=bar_format)
+        super().__init__(
+            desc=desc,
+            total=total,
+            leave=leave,
+            position=position,
+            ncols=ncols,
+            file=sys.stdout,
+            bar_format=bar_format,
+        )
         self.total = total
         self.previous_message = None
         self.counter = 0
         self.progress = 0
 
     def increment_counter(self, progress: int = 5) -> None:
         """Increment internal counter and waits for specified time.
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/utils/autoai/training.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/utils/autoai/training.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,55 +1,57 @@
 #  -----------------------------------------------------------------------------------------
 #  (C) Copyright IBM Corp. 2023-2024.
 #  https://opensource.org/licenses/BSD-3-Clause
 #  -----------------------------------------------------------------------------------------
-__all__ = [
-    'is_run_id_exists'
-]
+__all__ = ["is_run_id_exists"]
 
 from typing import Dict, Optional
 import warnings
 
 from ibm_watsonx_ai import APIClient
 from ibm_watsonx_ai.wml_client_error import ApiRequestFailure
 
 
-def is_run_id_exists(credentials: Dict,
-                     run_id: str,
-                     space_id: Optional[str] = None,
-                     **kwargs) -> bool:
+def is_run_id_exists(
+    credentials: Dict, run_id: str, space_id: Optional[str] = None, **kwargs
+) -> bool:
     """Check if specified run_id exists for API client initialized with passed credentials.
 
     :param credentials: Service Instance credentials
     :type credentials: dict
 
     :param run_id: training run id of AutoAI experiment
     :type run_id: str
 
     :param space_id: optional space id
     :type space_id: str, optional
 
     """
     # note: backward compatibility
-    if (wml_credentials:=kwargs.get('wml_credentials')) is not None:
+    if (wml_credentials := kwargs.get("wml_credentials")) is not None:
         if credentials is None:
             credentials = wml_credentials
-        warnings.warn(("`wml_credentials` is deprecated and will be removed in future. "
-                "Instead, please use `credentials`."), category=DeprecationWarning)
-        
+        warnings.warn(
+            (
+                "`wml_credentials` is deprecated and will be removed in future. "
+                "Instead, please use `credentials`."
+            ),
+            category=DeprecationWarning,
+        )
+
     # --- end note
     client = APIClient(credentials)
 
     if space_id is not None:
         client.set.default_space(space_id)
 
     try:
         client.training.get_details(run_id, _internal=True)
 
     except ApiRequestFailure as e:
-        if 'Status code: 404' in str(e):
+        if "Status code: 404" in str(e):
             return False
 
         else:
             raise e
 
     return True
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/utils/cpd_version.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/utils/cpd_version.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/utils/deployment/errors.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/utils/deployment/errors.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/utils/enums.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/utils/enums.py`

 * *Files 1% similar despite different names*

```diff
@@ -4,10 +4,11 @@
 #  -----------------------------------------------------------------------------------------
 
 from enum import Enum
 
 
 class RShinyAuthenticationValues(Enum):
     """Allowable values of R_Shiny authentication."""
+
     ANYONE_WITH_URL = "anyone_with_url"
     ANY_VALID_USER = "any_valid_user"
     MEMBERS_OF_DEPLOYMENT_SPACE = "members_of_deployment_space"
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/volumes.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/volumes.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/wml_client_error.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/wml_client_error.py`

 * *Files 2% similar despite different names*

```diff
@@ -36,14 +36,15 @@
     "SpaceIDandProjectIDCannotBeNone",
     "ParamOutOfRange",
     "InvalidMultipleArguments",
     "ValidationError",
     "InvalidValue",
     "PromptVariablesError",
     "UnsupportedOperation",
+    "MissingExtension",
 ]
 
 
 class WMLClientError(Exception):
     def __init__(
         self,
         error_msg: str,
@@ -301,21 +302,24 @@
         WMLClientError.__init__(
             self, f"One of {params_names_list} parameters should be set.", reason
         )
 
 
 class ValidationError(WMLClientError, KeyError):
     def __init__(self, key: str, additional_msg: str | None = None):
-        msg = (f"Invalid prompt template; check for"
-               f" mismatched or missing input variables." 
-               f" Missing input variable: {key}.")
+        msg = (
+            f"Invalid prompt template; check for"
+            f" mismatched or missing input variables."
+            f" Missing input variable: {key}."
+        )
         if additional_msg is not None:
             msg += "\n" + additional_msg
         WMLClientError.__init__(self, msg)
 
+
 class PromptVariablesError(WMLClientError, KeyError):
     def __init__(self, key: str):
         WMLClientError.__init__(
             self,
             (
                 f"Prompt template contains input variables."
                 f" Missing {key} in `prompt_variables`"
@@ -331,7 +335,15 @@
 
 
 class UnsupportedOperation(WMLClientError):
     def __init__(self, reason: str):
         WMLClientError.__init__(
             self, f"Operation is unsupported for this release.", reason
         )
+
+
+class MissingExtension(WMLClientError, ImportError):
+    def __init__(self, extension_name: str):
+        WMLClientError.__init__(
+            self,
+            f"Could not import {extension_name}: Please install `{extension_name}` extension.",
+        )
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/wml_resource.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/wml_resource.py`

 * *Files 1% similar despite different names*

```diff
@@ -57,26 +57,24 @@
     @overload
     def _handle_response(
         self,
         expected_status_code: int,
         operationName: str,
         response: requests.Response,
         json_response: Literal[True] = True,
-    ) -> dict:
-        ...
+    ) -> dict: ...
 
     @overload
     def _handle_response(
         self,
         expected_status_code: int,
         operationName: str,
         response: requests.Response,
         json_response: Literal[False],
-    ) -> str:
-        ...
+    ) -> str: ...
 
     def _handle_response(
         self,
         expected_status_code: int,
         operationName: str,
         response: requests.Response,
         json_response: bool = True,
@@ -295,32 +293,30 @@
         resource_name: str,
         summary: bool | None = None,
         pre_defined: bool | None = None,
         query_params: dict | None = None,
         _async: Literal[False] = ...,
         _all: bool = False,
         _filter_func: Callable | None = None,
-    ) -> dict:
-        ...
+    ) -> dict: ...
 
     @overload
     def _get_artifact_details(
         self,
         base_url: str,
         id: str,
         limit: int | None,
         resource_name: str,
         summary: bool | None = None,
         pre_defined: bool | None = None,
         query_params: dict | None = None,
         _async: Literal[True] = ...,
         _all: bool = False,
         _filter_func: Callable | None = None,
-    ) -> Generator:
-        ...
+    ) -> Generator: ...
 
     def _get_artifact_details(
         self,
         base_url: str,
         id: str,
         limit: int | None,
         resource_name: str,
@@ -372,16 +368,15 @@
         pre_defined: bool | None = None,
         revision: str | None = None,
         skip_space_project_chk: bool = False,
         query_params: dict | None = None,
         _async: Literal[False] = ...,
         _all: bool = False,
         _filter_func: Callable | None = None,
-    ) -> dict:
-        ...
+    ) -> dict: ...
 
     @overload
     def _get_with_or_without_limit(
         self,
         url: str,
         limit: int | None,
         op_name: str,
@@ -389,16 +384,15 @@
         pre_defined: bool | None = None,
         revision: str | None = None,
         skip_space_project_chk: bool = False,
         query_params: dict | None = None,
         _async: Literal[True] = ...,
         _all: bool = False,
         _filter_func: Callable | None = None,
-    ) -> Generator:
-        ...
+    ) -> Generator: ...
 
     @overload
     def _get_with_or_without_limit(
         self,
         url: str,
         limit: int | None,
         op_name: str,
@@ -406,16 +400,15 @@
         pre_defined: bool | None = None,
         revision: str | None = None,
         skip_space_project_chk: bool = False,
         query_params: dict | None = None,
         _async: bool = False,
         _all: bool = False,
         _filter_func: Callable | None = None,
-    ) -> ArtifactDetailsType:
-        ...
+    ) -> ArtifactDetailsType: ...
 
     def _get_with_or_without_limit(
         self,
         url: str,
         limit: int | None,
         op_name: str,
         summary: bool | None = None,
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai/workspace/workspace.py` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai/workspace/workspace.py`

 * *Files 1% similar despite different names*

```diff
@@ -101,16 +101,17 @@
                 else:
                     outcome = self.api_client.set.default_space(self.space_id)
 
                 if outcome == "FAILURE":
                     raise SetIDFailed(
                         f'{"project_id" if self.project_id is not None else "space_id"}',
                         reason=f'This {"project_id" if self.project_id is not None else "space_id"}: '
-                               f'{self.project_id if self.project_id is not None else self.space_id} '
-                               f'cannot be found in current environment.')
+                        f"{self.project_id if self.project_id is not None else self.space_id} "
+                        f"cannot be found in current environment.",
+                    )
 
     def __str__(self):
         return f"credentials: {self.credentials} project_id: {self.project_id} space_id = {self.space_id}"
 
     def __repr__(self):
         return self.__str__()
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai.egg-info/SOURCES.txt` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai.egg-info/SOURCES.txt`

 * *Files 23% similar despite different names*

```diff
@@ -122,224 +122,94 @@
 ibm_watsonx_ai/helpers/connections/base_location.py
 ibm_watsonx_ai/helpers/connections/connections.py
 ibm_watsonx_ai/helpers/connections/flight_service.py
 ibm_watsonx_ai/helpers/connections/local.py
 ibm_watsonx_ai/libs/__init__.py
 ibm_watsonx_ai/libs/ibmfl/__init__.py
 ibm_watsonx_ai/libs/ibmfl/party_env_validator.py
-ibm_watsonx_ai/libs/ibmfl/cloud/__init__.py
-ibm_watsonx_ai/libs/ibmfl/cloud/ibmfl/__init__.py
-ibm_watsonx_ai/libs/ibmfl/cloud/ibmfl/_version.py
-ibm_watsonx_ai/libs/ibmfl/cpd35/__init__.py
-ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/__init__.py
-ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/_version.py
-ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/connection/__init__.py
-ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/connection/route_declarations.py
-ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/connection/websockets_connection.py
-ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/data/__init__.py
-ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/data/data_handler.py
-ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/data/data_util.py
-ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/model/__init__.py
-ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/model/pytorch_fl_model.py
-ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/model/tensorflow_fl_model.py
-ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/model/xgb_fl_model.py
-ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/model/v2/__init__.py
-ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/model/v2/xgb_fl_model.py
-ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/party/__init__.py
-ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/party/party.py
-ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/party/party_protocol_handler.py
-ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/party/metrics/__init__.py
-ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/party/metrics/metrics_recorder.py
-ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/party/training/__init__.py
-ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/party/training/fedavg_local_training_handler.py
-ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/party/training/local_training_handler.py
-ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/party/training/pfnm_local_training_handler.py
-ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/party/training/xgboost_local_training_handler.py
-ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/party/training/v2/__init__.py
-ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/party/training/v2/xgboost_local_training_handler.py
-ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/util/__init__.py
-ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/util/config.py
-ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/util/pfnm/__init__.py
-ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/util/pfnm/core.py
-ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/util/pfnm/matching.py
-ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/util/pfnm/utils.py
-ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/util/xgboost/__init__.py
-ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/util/xgboost/export.py
-ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/util/xgboost/hyperparams.py
-ibm_watsonx_ai/libs/ibmfl/cpd35/ibmfl/util/xgboost/utils.py
-ibm_watsonx_ai/libs/ibmfl/cpd403/__init__.py
-ibm_watsonx_ai/libs/ibmfl/cpd403/ibmfl/__init__.py
-ibm_watsonx_ai/libs/ibmfl/cpd403/ibmfl/_version.py
-ibm_watsonx_ai/libs/ibmfl/cpd403/ibmfl/connection/__init__.py
-ibm_watsonx_ai/libs/ibmfl/cpd403/ibmfl/connection/websockets_connection.py
-ibm_watsonx_ai/libs/ibmfl/cpd403/ibmfl/party/__init__.py
-ibm_watsonx_ai/libs/ibmfl/cpd403/ibmfl/party/party.py
-ibm_watsonx_ai/libs/ibmfl/cpd403/ibmfl/party/party_protocol_handler.py
-ibm_watsonx_ai/libs/ibmfl/cpd403/ibmfl/party/metrics/__init__.py
-ibm_watsonx_ai/libs/ibmfl/cpd403/ibmfl/party/metrics/metrics_recorder.py
-ibm_watsonx_ai/libs/ibmfl/cpd403/ibmfl/util/__init__.py
-ibm_watsonx_ai/libs/ibmfl/cpd403/ibmfl/util/config.py
 ibm_watsonx_ai/libs/ibmfl/party/__init__.py
 ibm_watsonx_ai/libs/ibmfl/party/party.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/exceptions.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/connection/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/connection/route_declarations.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/crypto_enumeration.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/crypto_exceptions.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/crypto_library.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/helayer/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/helayer/fhe.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/infra/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/infra/crypto_asym_imp_rsa.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/infra/crypto_asym_int.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/infra/crypto_cert_imp_rsa.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/infra/crypto_cert_int.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/infra/crypto_he_imp_hely.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/infra/crypto_he_int.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/infra/crypto_sym_imp_fernet.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/infra/crypto_sym_int.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/keys_mng/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/keys_mng/crypto_key_mng_dst.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/keys_mng/crypto_key_mng_int.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/crypto/keys_mng/crypto_keys_proto_party.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/message/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/message/message_type.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/model/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/model/pytorch_fl_model.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/model/tensorflow_fl_model.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/model/xgb_fl_model.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/party/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/party/party.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/party/party_protocol_handler.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/party/training/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/party/training/crypto_local_training_handler.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/party/training/local_training_handler.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/party/training/pfnm_local_training_handler.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/party/training/xgboost_local_training_handler.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/util/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/util/xgboost/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/util/xgboost/export.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/util/xgboost/hyperparams.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf212-pt20-sk11/ibmfl/util/xgboost/utils.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/exceptions.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/connection/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/connection/route_declarations.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/crypto/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/crypto/crypto_enumeration.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/crypto/crypto_exceptions.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/crypto/crypto_library.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/crypto/helayer/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/crypto/helayer/fhe.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/crypto/infra/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/crypto/infra/crypto_asym_imp_rsa.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/crypto/infra/crypto_asym_int.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/crypto/infra/crypto_cert_imp_rsa.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/crypto/infra/crypto_cert_int.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/crypto/infra/crypto_he_imp_hely.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/crypto/infra/crypto_he_int.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/crypto/infra/crypto_sym_imp_fernet.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/crypto/infra/crypto_sym_int.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/crypto/keys_mng/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/crypto/keys_mng/crypto_key_mng_dst.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/crypto/keys_mng/crypto_key_mng_int.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/crypto/keys_mng/crypto_keys_proto_party.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/message/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/message/message_type.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/model/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/model/pytorch_fl_model.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/model/tensorflow_fl_model.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/model/xgb_fl_model.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/party/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/party/party.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/party/party_protocol_handler.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/party/training/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/party/training/crypto_local_training_handler.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/party/training/local_training_handler.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/party/training/pfnm_local_training_handler.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/party/training/xgboost_local_training_handler.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/util/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/util/xgboost/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/util/xgboost/export.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/util/xgboost/hyperparams.py
-ibm_watsonx_ai/libs/ibmfl/py310-tf29-pt112-sk11/ibmfl/util/xgboost/utils.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/_version.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/envs.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/exceptions.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/connection/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/connection/connection.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/connection/route_declarations.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/connection/router_handler.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/connection/websockets_connection.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/data/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/data/data_handler.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/data/data_util.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/data/env_data_handler.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/data/env_spec.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/data/pandas_data_handler.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/message/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/message/json_serializer.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/message/message.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/message/message_type.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/message/pickle_serializer.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/message/serializer.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/message/serializer_factory.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/message/serializer_types.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/model/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/model/fl_model.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/model/model_update.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/model/naive_bayes_fl_model.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/model/pytorch_fl_model.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/model/sklearn_SGD_linear_fl_model.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/model/sklearn_fl_model.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/model/sklearn_kmeans_fl_model.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/model/tensorflow_fl_model.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/model/xgb_fl_model.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/party/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/party/party.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/party/party_protocol_handler.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/party/status_type.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/party/metrics/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/party/metrics/metrics_recorder.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/party/training/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/party/training/fedavg_local_training_handler.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/party/training/local_training_handler.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/party/training/pfnm_local_training_handler.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/party/training/xgboost_local_training_handler.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/util/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/util/config.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/util/fl_metrics.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/util/log_config.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/util/pfnm/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/util/pfnm/core.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/util/pfnm/matching.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/util/pfnm/utils.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/util/xgboost/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/util/xgboost/export.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/util/xgboost/hyperparams.py
-ibm_watsonx_ai/libs/ibmfl/py38-tf24-pt17-sk23/ibmfl/util/xgboost/utils.py
-ibm_watsonx_ai/libs/ibmfl/py39-tf27-pt110-sk10/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py39-tf27-pt110-sk10/ibmfl/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py39-tf27-pt110-sk10/ibmfl/_version.py
-ibm_watsonx_ai/libs/ibmfl/py39-tf27-pt110-sk10/ibmfl/model/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py39-tf27-pt110-sk10/ibmfl/model/pytorch_fl_model.py
-ibm_watsonx_ai/libs/ibmfl/py39-tf27-pt110-sk10/ibmfl/model/xgb_fl_model.py
-ibm_watsonx_ai/libs/ibmfl/py39-tf27-pt110-sk10/ibmfl/party/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py39-tf27-pt110-sk10/ibmfl/party/training/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py39-tf27-pt110-sk10/ibmfl/party/training/xgboost_local_training_handler.py
-ibm_watsonx_ai/libs/ibmfl/py39-tf27-pt110-sk10/ibmfl/util/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py39-tf27-pt110-sk10/ibmfl/util/xgboost/__init__.py
-ibm_watsonx_ai/libs/ibmfl/py39-tf27-pt110-sk10/ibmfl/util/xgboost/export.py
-ibm_watsonx_ai/libs/ibmfl/py39-tf27-pt110-sk10/ibmfl/util/xgboost/hyperparams.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/__init__.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/__init__.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/_version.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/envs.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/exceptions.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/connection/__init__.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/connection/connection.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/connection/route_declarations.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/connection/router_handler.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/connection/websockets_connection.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/__init__.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/crypto_enumeration.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/crypto_exceptions.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/crypto_library.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/helayer/__init__.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/helayer/fhe.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/infra/__init__.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/infra/crypto_asym_imp_rsa.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/infra/crypto_asym_int.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/infra/crypto_cert_imp_rsa.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/infra/crypto_cert_int.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/infra/crypto_he_imp_hely.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/infra/crypto_he_int.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/infra/crypto_sym_imp_fernet.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/infra/crypto_sym_int.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/keys_mng/__init__.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/keys_mng/crypto_key_mng_dst.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/keys_mng/crypto_key_mng_int.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/crypto/keys_mng/crypto_keys_proto_party.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/data/__init__.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/data/data_handler.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/data/data_util.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/data/env_data_handler.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/data/env_spec.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/data/pandas_data_handler.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/message/__init__.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/message/message.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/message/message_type.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/model/__init__.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/model/fl_model.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/model/model_update.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/model/naive_bayes_fl_model.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/model/pytorch_fl_model.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/model/sklearn_SGD_linear_fl_model.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/model/sklearn_fl_model.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/model/sklearn_kmeans_fl_model.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/model/tensorflow_fl_model.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/model/xgb_fl_model.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/party/__init__.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/party/party.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/party/party_protocol_handler.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/party/status_type.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/party/metrics/__init__.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/party/metrics/metrics_recorder.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/party/training/__init__.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/party/training/crypto_local_training_handler.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/party/training/fedavg_local_training_handler.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/party/training/local_training_handler.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/party/training/pfnm_local_training_handler.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/party/training/xgboost_local_training_handler.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/__init__.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/config.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/fl_metrics.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/log_config.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/persistence/__init__.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/persistence/sklearn.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/pfnm/__init__.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/pfnm/core.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/pfnm/matching.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/pfnm/utils.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/serialization/__init__.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/serialization/pack.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/serialization/serializer.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/serialization/serializers.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/xgboost/__init__.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/xgboost/export.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/xgboost/hyperparams.py
+ibm_watsonx_ai/libs/ibmfl/runtime-23-1/ibmfl/util/xgboost/utils.py
 ibm_watsonx_ai/libs/repo/TestRepoSaveLoad.py
 ibm_watsonx_ai/libs/repo/__init__.py
 ibm_watsonx_ai/libs/repo/base_constants.py
 ibm_watsonx_ai/libs/repo/ml_api_client.py
 ibm_watsonx_ai/libs/repo/ml_authorization.py
 ibm_watsonx_ai/libs/repo/mlrepository/__init__.py
 ibm_watsonx_ai/libs/repo/mlrepository/artifact.py
```

### Comparing `ibm_watsonx_ai-1.0.4/ibm_watsonx_ai.egg-info/requires.txt` & `ibm_watsonx_ai-1.0.5/ibm_watsonx_ai.egg-info/requires.txt`

 * *Files 27% similar despite different names*

```diff
@@ -7,68 +7,38 @@
 packaging
 ibm-cos-sdk<2.14.0,>=2.12.0
 importlib-metadata
 
 [fl-crypto]
 pyhelayers==1.5.0.3
 
-[fl-rt22.2-py3.10]
-tensorflow==2.9.3
-scikit-learn==1.1.1
-torch==1.12.1
-numpy==1.23.1
-pandas==1.4.3
-pytest==6.2.5
-pyYAML==6.0.1
-parse==1.19.0
-websockets==10.1
-jsonpickle==1.4.2
-requests==2.27.1
-scipy==1.8.1
-environs==9.5.0
-pathlib2==2.3.6
-diffprivlib==0.5.1
-numcompress==0.1.2
-psutil
-setproctitle
-tabulate==0.8.9
-lz4
-gym
-cloudpickle==1.3.0
-image==1.5.33
-ddsketch==1.1.2
-skorch==0.12.0
-protobuf==3.19.5
-GPUtil
-joblib==1.1.1
-cryptography==39.0.1
-
 [fl-rt23.1-py3.10]
 tensorflow==2.12.0
 scikit-learn==1.1.1
 torch==2.0.0
 numpy==1.23.5
 pandas==1.5.3
 pytest==6.2.5
 pyYAML==6.0.1
 parse==1.19.0
 websockets==10.1
-jsonpickle==1.4.2
-requests==2.27.1
+requests==2.31.0
 scipy==1.10.1
 environs==9.5.0
 pathlib2==2.3.6
 diffprivlib==0.5.1
 numcompress==0.1.2
 psutil
 setproctitle
 tabulate==0.8.9
 lz4
 gym
-cloudpickle==1.3.0
 image==1.5.33
-ddsketch==1.1.2
+ddsketch==2.0.4
 skorch==0.12.0
 protobuf==4.22.1
 GPUtil
 joblib==1.1.1
+skops==0.9.0
+msgpack==1.0.7
+msgpack-numpy==0.4.8
 cryptography==39.0.1
```

### Comparing `ibm_watsonx_ai-1.0.4/ibmfl/data/data_handler.py` & `ibm_watsonx_ai-1.0.5/ibmfl/data/data_handler.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/ibmfl/party/party.py` & `ibm_watsonx_ai-1.0.5/ibmfl/party/party.py`

 * *Files identical despite different names*

### Comparing `ibm_watsonx_ai-1.0.4/setup.py` & `ibm_watsonx_ai-1.0.5/setup.py`

 * *Files 25% similar despite different names*

```diff
@@ -17,98 +17,58 @@
 
 # read the contents of README file
 this_directory = os.path.abspath(os.path.dirname(__file__))
 with open(os.path.join(this_directory, "README.md"), encoding="utf-8") as f:
     long_description = f.read()
 
 extras_require = {
-    "fl-rt22.2-py3.10": [
-        "tensorflow==2.9.3",
-        "scikit-learn==1.1.1",
-        "torch==1.12.1",
-        "numpy==1.23.1",
-        "pandas==1.4.3",
-        "pytest==6.2.5",
-        "pyYAML==6.0.1",
-        "parse==1.19.0",
-        "websockets==10.1",
-        "jsonpickle==1.4.2",
-        "requests==2.27.1",
-        "scipy==1.8.1",
-        "environs==9.5.0",
-        "pathlib2==2.3.6",
-        "diffprivlib==0.5.1",
-        "numcompress==0.1.2",
-        "psutil",
-        "setproctitle",
-        "tabulate==0.8.9",
-        "lz4",
-        "gym",
-        "cloudpickle==1.3.0",
-        "image==1.5.33",
-        "ddsketch==1.1.2",
-        "skorch==0.12.0",
-        "protobuf==3.19.5",
-        "GPUtil",
-        "joblib==1.1.1",
-        "cryptography==39.0.1",
-    ],
     "fl-rt23.1-py3.10": [
         "tensorflow==2.12.0",
         "scikit-learn==1.1.1",
         "torch==2.0.0",
         "numpy==1.23.5",
         "pandas==1.5.3",
         "pytest==6.2.5",
         "pyYAML==6.0.1",
         "parse==1.19.0",
         "websockets==10.1",
-        "jsonpickle==1.4.2",
-        "requests==2.27.1",
+        "requests==2.31.0",
         "scipy==1.10.1",
         "environs==9.5.0",
         "pathlib2==2.3.6",
         "diffprivlib==0.5.1",
         "numcompress==0.1.2",
         "psutil",
         "setproctitle",
         "tabulate==0.8.9",
         "lz4",
         "gym",
-        "cloudpickle==1.3.0",
         "image==1.5.33",
-        "ddsketch==1.1.2",
+        "ddsketch==2.0.4",
         "skorch==0.12.0",
         "protobuf==4.22.1",
         "GPUtil",
         "joblib==1.1.1",
+        "skops==0.9.0",
+        "msgpack==1.0.7",
+        "msgpack-numpy==0.4.8",
         "cryptography==39.0.1",
     ],
     "fl-crypto": ["pyhelayers==1.5.0.3"],
 }
 
 
 def retrieve_all_files_from_path_and_sub_paths(path):
     listOfFiles = list()
     for (dirpath, dirnames, filenames) in os.walk(path):
         listOfFiles += [os.path.join(dirpath, file) for file in filenames]
 
     return listOfFiles
 
 
-with open("MANIFEST.in", "a") as f:
-    f.writelines(
-        [
-            f"exclude {line}\n"
-            for line in retrieve_all_files_from_path_and_sub_paths(
-                "ibm_watsonx_ai/tests"
-            )
-        ]
-    )
-
 setup(
     name="ibm_watsonx_ai",
     version=VERSION,
     python_requires=">=3.10",
     author="IBM",
     author_email="lukasz.cmielowski@pl.ibm.com, dorota.laczak@ibm.com",
     description="IBM watsonx.ai API Client",
@@ -138,15 +98,15 @@
         "IBM",
         "Bluemix",
         "client",
         "API",
         "IBM Cloud",
     ],
     url="https://ibm.github.io/watsonx-ai-python-sdk",
-    packages=find_packages(),
+    packages=find_packages(exclude=["tests.*", "tests"]),
     package_data={
         "": ["messages/messages_en.json"],
         "api_version_param": ["utils/API_VERSION_PARAM"],
     },
     install_requires=[
         "requests",
         "urllib3",
@@ -154,10 +114,10 @@
         "certifi",
         "lomond",
         "tabulate",
         "packaging",
         "ibm-cos-sdk<2.14.0,>=2.12.0",
         "importlib-metadata",
     ],
-    extras_require=extras_require,
     include_package_data=True,
+    extras_require=extras_require,
 )
```

